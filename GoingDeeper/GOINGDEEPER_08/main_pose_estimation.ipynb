{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad3c2ce8",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12484b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있습니다\n",
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/mpii'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "MODEL_PATH = './'\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eff5522b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# train.json과 validation.json 파일은 이미지에 담겨 있는 사람들의 pose keypoint 정보들을 가지고 있다.\n",
    "# 따라서 Pose Estimation을 위한 label로 삼을 수 있다.\n",
    "# 아래는 annotation 정보 1개 출력\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2)\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea8260eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n0 - 오른쪽 발목\\n1 - 오른쪽 무릎\\n2 - 오른쪽 엉덩이\\n3 - 왼쪽 엉덩이\\n4 - 왼쪽 무릎\\n5 - 왼쪽 발목\\n6 - 골반\\n7 - 가슴(흉부)\\n8 - 목\\n9 - 머리 위\\n10 - 오른쪽 손목\\n11 - 오른쪽 팔꿈치\\n12 - 오른쪽 어깨\\n13 - 왼쪽 어깨\\n14 - 왼쪽 팔꿈치\\n15 - 왼쪽 손목\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joints_vis 는 사용할 수 있는 keypoint 인지 나타낸다.\n",
    "# MPII 의 경우 1 (visible) / 0(non) 으로만 나누어져 있다. \n",
    "# coco 의 경우 2 / 1 / 0 으로 표현해서 occlusion 상황까지 label 화 되어 있다.\n",
    "\n",
    "# joints 순서는 아래와 같은 순서로 배치\n",
    "'''\n",
    "0 - 오른쪽 발목\n",
    "1 - 오른쪽 무릎\n",
    "2 - 오른쪽 엉덩이\n",
    "3 - 왼쪽 엉덩이\n",
    "4 - 왼쪽 무릎\n",
    "5 - 왼쪽 발목\n",
    "6 - 골반\n",
    "7 - 가슴(흉부)\n",
    "8 - 목\n",
    "9 - 머리 위\n",
    "10 - 오른쪽 손목\n",
    "11 - 오른쪽 팔꿈치\n",
    "12 - 오른쪽 어깨\n",
    "13 - 왼쪽 어깨\n",
    "14 - 왼쪽 팔꿈치\n",
    "15 - 왼쪽 손목\n",
    "'''\n",
    "\n",
    "# scale과 center는 사람 몸의 크기와 중심점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "295720a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json annotation 을 파싱하는 함수\n",
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03ac66a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n"
     ]
    }
   ],
   "source": [
    "# parse_one_annotation()함수 테스트\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2a79d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFRecord 형태로 표현\n",
    "# 내부적으로 protocol buffer 라는 것을 이용\n",
    "# protocol buffer 는 크로스 플랫폼에서 사용할 수 있는 직렬화 데이터 라이브러리라고 생각하면 된다.\n",
    "def generate_tfexample(anno):\n",
    "\n",
    "    # byte 인코딩을 위한 함수\n",
    "    def _bytes_feature(value):\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ada6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 TFRecord가 아닌 다수의 TFRecord를 만들기\n",
    "\n",
    "# 얼마나 많은 TFRecord를 만들지 결정할 함수\n",
    "# 전체 데이터 l을 n그룹으로 나눈다. 결과적으로 n개의 TFRecord 파일을 만든다.\n",
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3c14103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_chunks = chunkify([0] * 1000, 64)\n",
    "print(test_chunks)\n",
    "print(len(test_chunks))\n",
    "print(len(test_chunks[0]))\n",
    "\n",
    "# 15묶음씩 64개로 나뉘는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51c91237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAY 활용\n",
    "# Ray는 병렬 처리를 위한 라이브러리\n",
    "\n",
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno in chunk:\n",
    "            tf_example = generate_tfexample(anno)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42f8e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFRecord 파일로 만들어주는 함수\n",
    "# ray를 사용하기 때문에 함수를 호출하는 문법이 약간 다르다.\n",
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n",
    "                TFRECORD_PATH,\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c38c9587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 10:17:03,129\tWARNING services.py:1729 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=3.66gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': '/aiffel/aiffel/mpii/images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2032)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2031)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=2030)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "Successfully wrote 25204 annotations to TF Records.\n"
     ]
    }
   ],
   "source": [
    "# 앞서 작성한 함수를 사용해 데이터를 TFRecord로 만든다.\n",
    "# train 데이터는 64개로, val 데이터는 8개의 파일로 만든다.\n",
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "\n",
    "ray.init()\n",
    "\n",
    "print('Start to parse annotations.')\n",
    "if not os.path.exists(TFRECORD_PATH):\n",
    "    os.makedirs(TFRECORD_PATH)\n",
    "\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    train_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH)\n",
    "        for anno in train_annos\n",
    "    ]\n",
    "    print('First train annotation: ', train_annotations[0])\n",
    "\n",
    "with open(VALID_JSON) as val_json:\n",
    "    val_annos = json.load(val_json)\n",
    "    val_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH) \n",
    "        for anno in val_annos\n",
    "    ]\n",
    "    print('First val annotation: ', val_annotations[0])\n",
    "    \n",
    "print('Start to build TF Records.')\n",
    "build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "    len(train_annotations) + len(val_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baf5ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수나 클래스에 @ray.remote 데코레이터를 붙이고 some_function.remote()형식으로 함수를 만든다.\n",
    "# 클래스의 경우에는 메서드를 호출할 때 remote()를 이용한다.\n",
    "# 함수나 메서드는 이 시점에 실행되는 것이 아니라 생성만 된다.\n",
    "# ray.get()을 통해 실행이 되는 구조이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a919184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFRecord로 저장된 데이터를 모델 학습에 필요한 데이터로 바꿔줄 함수\n",
    "def parse_tfexample(example):\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, image_feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de6ed248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image 와 label 을 이용해서 적절한 학습 형태로 변환한다.\n",
    "# 이미지를 그대로 사용하지 않고 적당히 정사각형으로 crop한다.\n",
    "# 균일하게 학습하기 위해 body width 를 적절히 정하는 것도 중요하다.\n",
    "# 임의로 조정한 crop box 가 이미지 바깥으로 나가지 않는지 예외 처리를 잘 해주어야 한다.\n",
    "def crop_roi(image, features, margin=0.2):\n",
    "    img_shape = tf.shape(image)\n",
    "    img_height = img_shape[0]\n",
    "    img_width = img_shape[1]\n",
    "    img_depth = img_shape[2]\n",
    "\n",
    "    keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "    keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "    center_x = features['image/object/center/x']\n",
    "    center_y = features['image/object/center/y']\n",
    "    body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "    # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n",
    "    masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "    masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "    # min, max 값을 찾습니다.\n",
    "    keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "    keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "    keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "    keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "    # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n",
    "    xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "    # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n",
    "    effective_xmin = xmin if xmin > 0 else 0\n",
    "    effective_ymin = ymin if ymin > 0 else 0\n",
    "    effective_xmax = xmax if xmax < img_width else img_width\n",
    "    effective_ymax = ymax if ymax < img_height else img_height\n",
    "    effective_height = effective_ymax - effective_ymin\n",
    "    effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "    image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "    new_shape = tf.shape(image)\n",
    "    new_height = new_shape[0]\n",
    "    new_width = new_shape[1]\n",
    "\n",
    "    effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "    effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "    return image, effective_keypoint_x, effective_keypoint_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd1e286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x, y) 좌표로 되어있는 keypoint 를 heatmap 으로 변경시키는 함수 (2차원 가우시안 분포)\n",
    "# \n",
    "def generate_2d_guassian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "    heatmap = tf.zeros((height, width))\n",
    "\n",
    "    xmin = x0 - 3 * sigma\n",
    "    ymin = y0 - 3 * sigma\n",
    "    xmax = x0 + 3 * sigma\n",
    "    ymax = y0 + 3 * sigma\n",
    "    \n",
    "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "        return heatmap\n",
    "\n",
    "    size = 6 * sigma + 1\n",
    "    x, y = tf.meshgrid(tf.range(0, 6 * sigma + 1, 1), tf.range(0, 6 * sigma + 1, 1), indexing='xy')\n",
    "\n",
    "    center_x = size // 2\n",
    "    center_y = size // 2\n",
    "\n",
    "    gaussian_patch = tf.cast(tf.math.exp(\n",
    "        -(tf.math.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale,\n",
    "                             dtype=tf.float32)\n",
    "\n",
    "    patch_xmin = tf.math.maximum(0, -xmin)\n",
    "    patch_ymin = tf.math.maximum(0, -ymin)\n",
    "    patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "    patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "    heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "    heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "    heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "    heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "    indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for j in tf.range(patch_ymin, patch_ymax):\n",
    "        for i in tf.range(patch_xmin, patch_xmax):\n",
    "            indices = indices.write(count, [heatmap_ymin + j, heatmap_xmin + i])\n",
    "            updates = updates.write(count, gaussian_patch[j][i])\n",
    "            count += 1\n",
    "\n",
    "    heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "    v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "    x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "    y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "    num_heatmap = heatmap_shape[2]\n",
    "    heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "    for i in range(num_heatmap):\n",
    "        gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "        heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "    heatmaps = heatmap_array.stack()\n",
    "    heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0])  # change to (64, 64, 16)\n",
    "\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0370fc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수들을 개별 함수로도 만들 수 있지만 객체 형태로 조합하는 함수\n",
    "# 객체 형태로 만들면 선언부는 복잡해 보여도 장점이 훨씬 많다.\n",
    "# 함수에서 객체의 메서드로 수정할 때는 self를 추가해야 한다.\n",
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "        \n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bfd24",
   "metadata": {},
   "source": [
    "# Hourglass 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a3864f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  residual block\n",
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d3fb019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HourglassModule\n",
    "# 재귀 함수를 이용\n",
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0cbd6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate supervision 적용을 위한 linear layer\n",
    "# 여러 모듈을 쌓을수록 모델이 깊어지는 만큼 학습이 어려워, 저자들은 Intermediate supervision을 적용했다.\n",
    "# 모델 중간에 계산되는 히트맵 결과를 출력하는 convolution layer틑 사용한다.\n",
    "# 이 히트맵과 ground truth의 차이를 intermediate loss (auxilary loss) 로 계산한다.\n",
    "# 이로써 stacked hourglass module은 보다 정교한 결과를 도출해낸다.\n",
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9714de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StackedHourglassNetwork 생성\n",
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), \n",
    "        num_stack=4, \n",
    "        num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "969f84dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"stacked_hourglass\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 64) 9472        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 128, 128, 64) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 64) 256         re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 128, 128, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 64) 4160        re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 128, 128, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 128, 128, 64) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 64) 36928       re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 128, 128, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 128, 128, 64) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 128 8320        re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 128 8320        re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 128, 128, 128 0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 128)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 64, 64, 128)  512         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 64)   8256        re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 64, 64, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 64, 64, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 64)   36928       re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 64, 64, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 128)  8320        re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 64, 128)  0           max_pooling2d[0][0]              \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 128)  512         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 64, 64, 128)  16512       re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 256)  33024       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 64, 256)  0           conv2d_9[0][0]                   \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 32, 32, 256)  1024        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_22 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 32, 32, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_23 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 32, 32, 128)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_24 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 256)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 256)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 16, 16, 256)  1024        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_31 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 16, 16, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_32 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16, 16, 128)  512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_33 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 256)  0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 256)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 8, 8, 256)    1024        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_40 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 8, 8, 128)    512         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_41 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 8, 8, 128)    512         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_42 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 256)    33024       re_lu_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 8, 8, 256)    0           max_pooling2d_3[0][0]            \n",
      "                                                                 conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 4, 4, 256)    0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 4, 4, 256)    1024        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_49 (ReLU)                 (None, 4, 4, 256)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 4, 4, 128)    32896       re_lu_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 4, 4, 128)    512         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_50 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 4, 4, 128)    147584      re_lu_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 4, 4, 128)    512         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_51 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 4, 4, 256)    33024       re_lu_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 4, 4, 256)    0           max_pooling2d_4[0][0]            \n",
      "                                                                 conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 4, 4, 256)    1024        add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_52 (ReLU)                 (None, 4, 4, 256)    0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 8, 8, 256)    1024        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 4, 4, 128)    32896       re_lu_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_43 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 4, 4, 128)    512         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_53 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 8, 8, 128)    512         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 4, 4, 128)    147584      re_lu_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_44 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 4, 4, 128)    512         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_54 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 8, 8, 128)    512         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 4, 4, 256)    33024       re_lu_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_45 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 4, 4, 256)    0           add_14[0][0]                     \n",
      "                                                                 conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 8, 8, 256)    33024       re_lu_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 4, 4, 256)    1024        add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 8, 8, 256)    0           add_11[0][0]                     \n",
      "                                                                 conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_55 (ReLU)                 (None, 4, 4, 256)    0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 8, 8, 256)    1024        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 4, 4, 128)    32896       re_lu_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_46 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 4, 4, 128)    512         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 16, 16, 256)  1024        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_56 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 8, 8, 128)    512         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_34 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 4, 4, 128)    147584      re_lu_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_47 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 4, 4, 128)    512         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 16, 16, 128)  512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_57 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 8, 8, 128)    512         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_35 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 4, 4, 256)    33024       re_lu_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_48 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 4, 4, 256)    0           add_15[0][0]                     \n",
      "                                                                 conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 8, 8, 256)    33024       re_lu_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 16, 16, 128)  512         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 8, 8, 256)    0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 8, 8, 256)    0           add_12[0][0]                     \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_36 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 8, 8, 256)    0           up_sampling2d[0][0]              \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 8, 8, 256)    1024        tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 16, 256)  0           add_8[0][0]                      \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_58 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 16, 16, 256)  1024        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_37 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 8, 8, 128)    512         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 32, 32, 256)  1024        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_59 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 16, 16, 128)  512         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_25 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_38 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 8, 8, 128)    512         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 32, 32, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_60 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 16, 16, 128)  512         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_26 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 8, 8, 256)    33024       re_lu_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_39 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 8, 8, 256)    0           tf.__operators__.add[0][0]       \n",
      "                                                                 conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 32, 32, 128)  512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 256)  0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 16, 256)  0           add_9[0][0]                      \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_27 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 16, 16, 256)  0           up_sampling2d_1[0][0]            \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 16, 16, 256)  1024        tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 256)  0           add_5[0][0]                      \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_61 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 32, 256)  1024        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_28 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 16, 16, 128)  512         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 64, 64, 256)  1024        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_62 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 32, 32, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_16 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_29 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 128)  32896       re_lu_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 16, 16, 128)  512         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 64, 64, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_63 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 32, 32, 128)  512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_17 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_30 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 16, 16, 256)  0           tf.__operators__.add_1[0][0]     \n",
      "                                                                 conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 64, 64, 128)  512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 256)  0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 32, 32, 256)  0           add_6[0][0]                      \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_18 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 32, 32, 256)  0           up_sampling2d_2[0][0]            \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 32, 32, 256)  1024        tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 64, 64, 256)  0           add_2[0][0]                      \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_64 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 64, 64, 256)  1024        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_19 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 32, 32, 128)  512         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 64, 128)  32896       re_lu_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_65 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 64, 64, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_20 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 32, 32, 128)  512         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_66 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 64, 64, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_21 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 32, 32, 256)  0           tf.__operators__.add_2[0][0]     \n",
      "                                                                 conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 64, 64, 256)  0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64, 64, 256)  0           add_3[0][0]                      \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (None, 64, 64, 256)  0           up_sampling2d_3[0][0]            \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 64, 64, 256)  1024        tf.__operators__.add_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_67 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 64, 64, 128)  32896       re_lu_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 64, 64, 128)  512         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_68 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 64, 64, 128)  512         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_69 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 64, 64, 256)  0           tf.__operators__.add_3[0][0]     \n",
      "                                                                 conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 64, 64, 256)  65792       add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 64, 64, 256)  1024        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_70 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 64, 64, 16)   4112        re_lu_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 64, 64, 256)  65792       re_lu_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 64, 64, 256)  4352        conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 64, 64, 256)  0           conv2d_69[0][0]                  \n",
      "                                                                 conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 256)  0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 32, 32, 256)  1024        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_77 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 32, 32, 128)  512         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_78 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_78[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 32, 32, 128)  512         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_79 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 32, 32, 256)  0           max_pooling2d_5[0][0]            \n",
      "                                                                 conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 256)  0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 16, 16, 256)  1024        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_86 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 16, 16, 128)  512         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_87 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 16, 16, 128)  512         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_88 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_88[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 16, 16, 256)  0           max_pooling2d_6[0][0]            \n",
      "                                                                 conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 256)    0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 8, 8, 256)    1024        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_95 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_95[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 8, 8, 128)    512         conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_96 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_96[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 8, 8, 128)    512         conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_97 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 8, 8, 256)    33024       re_lu_97[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 8, 8, 256)    0           max_pooling2d_7[0][0]            \n",
      "                                                                 conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 4, 4, 256)    0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 4, 4, 256)    1024        max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_104 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_104[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 4, 4, 128)    512         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_105 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_105[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 4, 4, 128)    512         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_106 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_106[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 4, 4, 256)    0           max_pooling2d_8[0][0]            \n",
      "                                                                 conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 4, 4, 256)    1024        add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_107 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 8, 8, 256)    1024        add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_107[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_98 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 4, 4, 128)    512         conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_108 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 8, 8, 128)    512         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_108[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_99 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 4, 4, 128)    512         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_109 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 8, 8, 128)    512         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_100 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 4, 4, 256)    0           add_33[0][0]                     \n",
      "                                                                 conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_100[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 4, 4, 256)    1024        add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 8, 8, 256)    0           add_30[0][0]                     \n",
      "                                                                 conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_110 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 8, 8, 256)    1024        add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_110[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_101 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 4, 4, 128)    512         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_101[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 16, 16, 256)  1024        add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_111 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 8, 8, 128)    512         conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_89 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_111[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_102 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_89[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 4, 4, 128)    512         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_102[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 16, 16, 128)  512         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_112 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 8, 8, 128)    512         conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_90 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_112[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_103 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_90[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 4, 4, 256)    0           add_34[0][0]                     \n",
      "                                                                 conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_103[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 16, 16, 128)  512         conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 8, 8, 256)    0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 8, 8, 256)    0           add_31[0][0]                     \n",
      "                                                                 conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_91 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (None, 8, 8, 256)    0           up_sampling2d_4[0][0]            \n",
      "                                                                 add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_91[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 8, 8, 256)    1024        tf.__operators__.add_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 16, 16, 256)  0           add_27[0][0]                     \n",
      "                                                                 conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_113 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 16, 16, 256)  1024        add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_113[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_92 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 8, 8, 128)    512         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_92[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 32, 32, 256)  1024        add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_114 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 16, 16, 128)  512         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_80 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_114[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_93 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 8, 8, 128)    512         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_93[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 32, 32, 128)  512         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_115 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 16, 16, 128)  512         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_81 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_115[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_94 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_81[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 8, 8, 256)    0           tf.__operators__.add_4[0][0]     \n",
      "                                                                 conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_94[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 32, 32, 128)  512         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 16, 16, 256)  0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 16, 16, 256)  0           add_28[0][0]                     \n",
      "                                                                 conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_82 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (None, 16, 16, 256)  0           up_sampling2d_5[0][0]            \n",
      "                                                                 add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_82[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 16, 16, 256)  1024        tf.__operators__.add_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 32, 32, 256)  0           add_24[0][0]                     \n",
      "                                                                 conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_116 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 32, 32, 256)  1024        add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_116[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_83 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 16, 16, 128)  512         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 64, 64, 256)  1024        add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_117 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 32, 32, 128)  512         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_71 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_117[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_84 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 64, 64, 128)  32896       re_lu_71[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 16, 16, 128)  512         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 64, 64, 128)  512         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_118 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 32, 32, 128)  512         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_72 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_118[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_85 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 16, 16, 256)  0           tf.__operators__.add_5[0][0]     \n",
      "                                                                 conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 64, 64, 128)  512         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 32, 32, 256)  0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 32, 32, 256)  0           add_25[0][0]                     \n",
      "                                                                 conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_73 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_6 (TFOpLam (None, 32, 32, 256)  0           up_sampling2d_6[0][0]            \n",
      "                                                                 add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 32, 32, 256)  1024        tf.__operators__.add_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 64, 64, 256)  0           add_21[0][0]                     \n",
      "                                                                 conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_119 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 64, 64, 256)  1024        add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_119[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_74 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 32, 32, 128)  512         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 64, 64, 128)  32896       re_lu_74[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_120 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 64, 64, 128)  512         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_120[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_75 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 32, 32, 128)  512         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_121 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 64, 64, 128)  512         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_121[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_76 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 32, 32, 256)  0           tf.__operators__.add_6[0][0]     \n",
      "                                                                 conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 64, 64, 256)  0           add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 64, 64, 256)  0           add_22[0][0]                     \n",
      "                                                                 conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_7 (TFOpLam (None, 64, 64, 256)  0           up_sampling2d_7[0][0]            \n",
      "                                                                 add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 64, 64, 256)  1024        tf.__operators__.add_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_122 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_122[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 64, 64, 128)  512         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_123 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_123[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 64, 64, 128)  512         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_124 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_124[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 64, 64, 256)  0           tf.__operators__.add_7[0][0]     \n",
      "                                                                 conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 64, 64, 256)  65792       add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 64, 64, 256)  1024        conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_125 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 64, 64, 16)   4112        re_lu_125[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 64, 64, 256)  65792       re_lu_125[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 64, 64, 256)  4352        conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 64, 64, 256)  0           conv2d_127[0][0]                 \n",
      "                                                                 conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 32, 32, 256)  0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 32, 32, 256)  1024        max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_132 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_132[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 32, 32, 128)  512         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_133 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_133[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 32, 32, 128)  512         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_134 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_134[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 32, 32, 256)  0           max_pooling2d_9[0][0]            \n",
      "                                                                 conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 16, 16, 256)  0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 16, 16, 256)  1024        max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_141 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_141[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 16, 16, 128)  512         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_142 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_142[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 16, 16, 128)  512         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_143 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_143[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 16, 16, 256)  0           max_pooling2d_10[0][0]           \n",
      "                                                                 conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 8, 8, 256)    0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 8, 8, 256)    1024        max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_150 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_150[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 8, 8, 128)    512         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_151 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_151[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 8, 8, 128)    512         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_152 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (None, 8, 8, 256)    0           max_pooling2d_11[0][0]           \n",
      "                                                                 conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 4, 4, 256)    0           add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 4, 4, 256)    1024        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_159 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_159[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 4, 4, 128)    512         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_160 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_160[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 4, 4, 128)    512         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_161 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_161[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_52 (Add)                    (None, 4, 4, 256)    0           max_pooling2d_12[0][0]           \n",
      "                                                                 conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 4, 4, 256)    1024        add_52[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_162 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 8, 8, 256)    1024        add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_162[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_153 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 4, 4, 128)    512         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_153[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_163 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 8, 8, 128)    512         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_163[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_154 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 4, 4, 128)    512         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_154[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_164 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 8, 8, 128)    512         conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_164[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_155 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_53 (Add)                    (None, 4, 4, 256)    0           add_52[0][0]                     \n",
      "                                                                 conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_155[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 4, 4, 256)    1024        add_53[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_50 (Add)                    (None, 8, 8, 256)    0           add_49[0][0]                     \n",
      "                                                                 conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_165 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 8, 8, 256)    1024        add_50[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_165[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_156 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 4, 4, 128)    512         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_156[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 16, 16, 256)  1024        add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_166 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 8, 8, 128)    512         conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_144 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_166[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_157 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_144[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 4, 4, 128)    512         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_157[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 16, 16, 128)  512         conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_167 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 8, 8, 128)    512         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_145 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_167[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_158 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_145[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_54 (Add)                    (None, 4, 4, 256)    0           add_53[0][0]                     \n",
      "                                                                 conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_158[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 16, 16, 128)  512         conv2d_148[0][0]                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2D)  (None, 8, 8, 256)    0           add_54[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_51 (Add)                    (None, 8, 8, 256)    0           add_50[0][0]                     \n",
      "                                                                 conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_146 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_8 (TFOpLam (None, 8, 8, 256)    0           up_sampling2d_8[0][0]            \n",
      "                                                                 add_51[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_146[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 8, 8, 256)    1024        tf.__operators__.add_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 16, 16, 256)  0           add_46[0][0]                     \n",
      "                                                                 conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_168 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 16, 16, 256)  1024        add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_168[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_147 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 8, 8, 128)    512         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_147[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 32, 32, 256)  1024        add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_169 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 16, 16, 128)  512         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_135 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_169[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_148 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_135[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 8, 8, 128)    512         conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_148[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 32, 32, 128)  512         conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_170 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 16, 16, 128)  512         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_136 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_170[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_149 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_136[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_55 (Add)                    (None, 8, 8, 256)    0           tf.__operators__.add_8[0][0]     \n",
      "                                                                 conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_149[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 32, 32, 128)  512         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2D)  (None, 16, 16, 256)  0           add_55[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 16, 16, 256)  0           add_47[0][0]                     \n",
      "                                                                 conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_137 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_9 (TFOpLam (None, 16, 16, 256)  0           up_sampling2d_9[0][0]            \n",
      "                                                                 add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_137[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 16, 16, 256)  1024        tf.__operators__.add_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 32, 32, 256)  0           add_43[0][0]                     \n",
      "                                                                 conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_171 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 32, 32, 256)  1024        add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_171[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_138 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 16, 16, 128)  512         conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_138[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 64, 64, 256)  1024        add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_172 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 32, 32, 128)  512         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_126 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_172[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_139 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_126[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 16, 16, 128)  512         conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_139[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 64, 64, 128)  512         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_173 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 32, 32, 128)  512         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_127 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_173[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_140 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_127[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_56 (Add)                    (None, 16, 16, 256)  0           tf.__operators__.add_9[0][0]     \n",
      "                                                                 conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_140[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 64, 64, 128)  512         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_10 (UpSampling2D) (None, 32, 32, 256)  0           add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 32, 32, 256)  0           add_44[0][0]                     \n",
      "                                                                 conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_128 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_10 (TFOpLa (None, 32, 32, 256)  0           up_sampling2d_10[0][0]           \n",
      "                                                                 add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_128[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 32, 32, 256)  1024        tf.__operators__.add_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 64, 64, 256)  0           add_40[0][0]                     \n",
      "                                                                 conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_174 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 64, 64, 256)  1024        add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_174[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_129 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 32, 32, 128)  512         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_129[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_175 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 64, 64, 128)  512         conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_175[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_130 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 32, 32, 128)  512         conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_130[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_176 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 64, 64, 128)  512         conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_176[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_131 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_57 (Add)                    (None, 32, 32, 256)  0           tf.__operators__.add_10[0][0]    \n",
      "                                                                 conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_131[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_11 (UpSampling2D) (None, 64, 64, 256)  0           add_57[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 64, 64, 256)  0           add_41[0][0]                     \n",
      "                                                                 conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_11 (TFOpLa (None, 64, 64, 256)  0           up_sampling2d_11[0][0]           \n",
      "                                                                 add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 64, 64, 256)  1024        tf.__operators__.add_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_177 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_177[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 64, 64, 128)  512         conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_178 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_178[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 64, 64, 128)  512         conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_179 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_179[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_58 (Add)                    (None, 64, 64, 256)  0           tf.__operators__.add_11[0][0]    \n",
      "                                                                 conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 64, 64, 256)  65792       add_58[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 64, 64, 256)  1024        conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_180 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 64, 64, 16)   4112        re_lu_180[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 64, 64, 256)  65792       re_lu_180[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 64, 64, 256)  4352        conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_59 (Add)                    (None, 64, 64, 256)  0           conv2d_185[0][0]                 \n",
      "                                                                 conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 32, 32, 256)  0           add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 32, 32, 256)  1024        max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_187 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_187[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 32, 32, 128)  512         conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_188 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_188[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, 32, 32, 128)  512         conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_189 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_189[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_62 (Add)                    (None, 32, 32, 256)  0           max_pooling2d_13[0][0]           \n",
      "                                                                 conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 16, 16, 256)  0           add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchN (None, 16, 16, 256)  1024        max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_196 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_196[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_196[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, 16, 16, 128)  512         conv2d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_197 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_197[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, 16, 16, 128)  512         conv2d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_198 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_204 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_198[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_65 (Add)                    (None, 16, 16, 256)  0           max_pooling2d_14[0][0]           \n",
      "                                                                 conv2d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 8, 8, 256)    0           add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_205 (BatchN (None, 8, 8, 256)    1024        max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_205 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_205[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_211 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_205[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_206 (BatchN (None, 8, 8, 128)    512         conv2d_211[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_206 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_206[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_212 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_206[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_207 (BatchN (None, 8, 8, 128)    512         conv2d_212[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_207 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_207[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_213 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_207[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 8, 8, 256)    0           max_pooling2d_15[0][0]           \n",
      "                                                                 conv2d_213[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D) (None, 4, 4, 256)    0           add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_214 (BatchN (None, 4, 4, 256)    1024        max_pooling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_214 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_214[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_220 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_214[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_215 (BatchN (None, 4, 4, 128)    512         conv2d_220[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_215 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_215[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_221 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_215[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_216 (BatchN (None, 4, 4, 128)    512         conv2d_221[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_216 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_216[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_222 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_216[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 4, 4, 256)    0           max_pooling2d_16[0][0]           \n",
      "                                                                 conv2d_222[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_217 (BatchN (None, 4, 4, 256)    1024        add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_217 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_217[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_208 (BatchN (None, 8, 8, 256)    1024        add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_223 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_217[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_208 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_208[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_218 (BatchN (None, 4, 4, 128)    512         conv2d_223[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_214 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_208[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_218 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_218[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_209 (BatchN (None, 8, 8, 128)    512         conv2d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_224 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_218[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_209 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_209[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_219 (BatchN (None, 4, 4, 128)    512         conv2d_224[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_215 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_209[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_219 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_219[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchN (None, 8, 8, 128)    512         conv2d_215[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_225 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_219[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_210 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_210[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_72 (Add)                    (None, 4, 4, 256)    0           add_71[0][0]                     \n",
      "                                                                 conv2d_225[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_216 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_210[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_220 (BatchN (None, 4, 4, 256)    1024        add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_69 (Add)                    (None, 8, 8, 256)    0           add_68[0][0]                     \n",
      "                                                                 conv2d_216[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_220 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_220[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_211 (BatchN (None, 8, 8, 256)    1024        add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_226 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_220[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_211 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_211[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_221 (BatchN (None, 4, 4, 128)    512         conv2d_226[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_217 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_211[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_199 (BatchN (None, 16, 16, 256)  1024        add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_221 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_221[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_212 (BatchN (None, 8, 8, 128)    512         conv2d_217[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_199 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_199[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_227 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_221[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_212 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_212[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_199[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_222 (BatchN (None, 4, 4, 128)    512         conv2d_227[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_218 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_212[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchN (None, 16, 16, 128)  512         conv2d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_222 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_222[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_213 (BatchN (None, 8, 8, 128)    512         conv2d_218[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_200 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_200[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_228 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_222[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_213 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_213[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_206 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_200[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_73 (Add)                    (None, 4, 4, 256)    0           add_72[0][0]                     \n",
      "                                                                 conv2d_228[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_219 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_213[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_201 (BatchN (None, 16, 16, 128)  512         conv2d_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_12 (UpSampling2D) (None, 8, 8, 256)    0           add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_70 (Add)                    (None, 8, 8, 256)    0           add_69[0][0]                     \n",
      "                                                                 conv2d_219[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_201 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_201[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_12 (TFOpLa (None, 8, 8, 256)    0           up_sampling2d_12[0][0]           \n",
      "                                                                 add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_207 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_201[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_223 (BatchN (None, 8, 8, 256)    1024        tf.__operators__.add_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_66 (Add)                    (None, 16, 16, 256)  0           add_65[0][0]                     \n",
      "                                                                 conv2d_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_223 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_223[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_202 (BatchN (None, 16, 16, 256)  1024        add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_229 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_223[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_202 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_202[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_224 (BatchN (None, 8, 8, 128)    512         conv2d_229[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_208 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_202[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, 32, 32, 256)  1024        add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_224 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_224[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_203 (BatchN (None, 16, 16, 128)  512         conv2d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_190 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_230 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_224[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_203 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_203[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_190[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_225 (BatchN (None, 8, 8, 128)    512         conv2d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_209 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_203[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, 32, 32, 128)  512         conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_225 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_225[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_204 (BatchN (None, 16, 16, 128)  512         conv2d_209[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_191 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_231 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_225[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_204 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_204[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_191[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_74 (Add)                    (None, 8, 8, 256)    0           tf.__operators__.add_12[0][0]    \n",
      "                                                                 conv2d_231[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_210 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_204[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchN (None, 32, 32, 128)  512         conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_13 (UpSampling2D) (None, 16, 16, 256)  0           add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_67 (Add)                    (None, 16, 16, 256)  0           add_66[0][0]                     \n",
      "                                                                 conv2d_210[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_192 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_192[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_13 (TFOpLa (None, 16, 16, 256)  0           up_sampling2d_13[0][0]           \n",
      "                                                                 add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_192[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_226 (BatchN (None, 16, 16, 256)  1024        tf.__operators__.add_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_63 (Add)                    (None, 32, 32, 256)  0           add_62[0][0]                     \n",
      "                                                                 conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_226 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_226[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_193 (BatchN (None, 32, 32, 256)  1024        add_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_232 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_226[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_193 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_193[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_227 (BatchN (None, 16, 16, 128)  512         conv2d_232[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_193[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 64, 64, 256)  1024        add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_227 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_227[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_194 (BatchN (None, 32, 32, 128)  512         conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_181 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_233 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_227[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_194 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_194[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_181[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_228 (BatchN (None, 16, 16, 128)  512         conv2d_233[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_194[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 64, 64, 128)  512         conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_228 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_228[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchN (None, 32, 32, 128)  512         conv2d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_182 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_234 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_228[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_195 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_195[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_182[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_75 (Add)                    (None, 16, 16, 256)  0           tf.__operators__.add_13[0][0]    \n",
      "                                                                 conv2d_234[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_195[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 64, 64, 128)  512         conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_14 (UpSampling2D) (None, 32, 32, 256)  0           add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_64 (Add)                    (None, 32, 32, 256)  0           add_63[0][0]                     \n",
      "                                                                 conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_183 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_14 (TFOpLa (None, 32, 32, 256)  0           up_sampling2d_14[0][0]           \n",
      "                                                                 add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_183[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_229 (BatchN (None, 32, 32, 256)  1024        tf.__operators__.add_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_60 (Add)                    (None, 64, 64, 256)  0           add_59[0][0]                     \n",
      "                                                                 conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_229 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_229[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 64, 64, 256)  1024        add_60[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_235 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_229[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_184 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_230 (BatchN (None, 32, 32, 128)  512         conv2d_235[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_184[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_230 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_230[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 64, 64, 128)  512         conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_236 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_230[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_185 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_231 (BatchN (None, 32, 32, 128)  512         conv2d_236[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_185[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_231 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_231[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 64, 64, 128)  512         conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_237 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_231[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_186 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_76 (Add)                    (None, 32, 32, 256)  0           tf.__operators__.add_14[0][0]    \n",
      "                                                                 conv2d_237[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_186[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_15 (UpSampling2D) (None, 64, 64, 256)  0           add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_61 (Add)                    (None, 64, 64, 256)  0           add_60[0][0]                     \n",
      "                                                                 conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_15 (TFOpLa (None, 64, 64, 256)  0           up_sampling2d_15[0][0]           \n",
      "                                                                 add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_232 (BatchN (None, 64, 64, 256)  1024        tf.__operators__.add_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_232 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_232[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_238 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_232[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_233 (BatchN (None, 64, 64, 128)  512         conv2d_238[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_233 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_233[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_239 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_233[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_234 (BatchN (None, 64, 64, 128)  512         conv2d_239[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_234 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_234[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_240 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_234[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_77 (Add)                    (None, 64, 64, 256)  0           tf.__operators__.add_15[0][0]    \n",
      "                                                                 conv2d_240[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_241 (Conv2D)             (None, 64, 64, 256)  65792       add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_235 (BatchN (None, 64, 64, 256)  1024        conv2d_241[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_235 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_235[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_242 (Conv2D)             (None, 64, 64, 16)   4112        re_lu_235[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 16,368,320\n",
      "Trainable params: 16,290,752\n",
      "Non-trainable params: 77,568\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca0d1a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 할 수 있는 GPU가 여러 개이고 데이터를 병렬로 학습시키는 방법\n",
    "# tf.distribute.MirroredStrategy : 한 컴퓨터에 GPU가 여러 개인 경우 사용할 수 있는 방법\n",
    "# GPU가 모델을 학습한 후 각각의 Loss를 계산하면 CPU가 전체 Loss를 종합, 그런 후 모델의 가중치 업데이트.\n",
    "# 각 GPU에서 계산한 Loss를 토대로 전체 Loss를 종합해주는 역할은 strategy.reduce 함수가 담당\n",
    "# 각 함수 하나의 객체로 생성\n",
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MODEL_PATH + '/model-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24741a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 만드는 함수\n",
    "# TFRecord 파일이 여러개이므로 tf.data.Dataset.list_files를 통해 불러온다.\n",
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fd9ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  데이터셋과 모델, 훈련용 객체를 조립\n",
    "#  with strategy.scope():부분이 반드시 필요하다.\n",
    "# 데이터셋도 experimental_distribute_dataset를 통해 연결해 줘야 한다.\n",
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f0fda7",
   "metadata": {},
   "source": [
    "# 훈련 그래프 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34132a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b23ce71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 파일 경로\n",
    "file_path = \"StackedHourglassNetwork_training_data.xlsx\"\n",
    "\n",
    "# 각 시트를 데이터 프레임으로 불러오기\n",
    "df_train = pd.read_excel(file_path, sheet_name=\"train\")\n",
    "df_validation = pd.read_excel(file_path, sheet_name=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a6da54a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Batch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Total_Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.130562</td>\n",
       "      <td>2.130562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.209572</td>\n",
       "      <td>2.170067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.333928</td>\n",
       "      <td>2.224688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2.298592</td>\n",
       "      <td>2.243164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2.345860</td>\n",
       "      <td>2.263703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch  Batch      Loss  Total_Loss\n",
       "0      1      1  2.130562    2.130562\n",
       "1      1      2  2.209572    2.170067\n",
       "2      1      3  2.333928    2.224688\n",
       "3      1      4  2.298592    2.243164\n",
       "4      1      5  2.345860    2.263703"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c89c8d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.54673707, 1.36607063, 1.27828228, 1.22301698, 1.18356013]\n"
     ]
    }
   ],
   "source": [
    "train_last_total_loss_per_epoch = df_train.groupby(\"Epoch\")[\"Total_Loss\"].last().tolist()\n",
    "print(train_last_total_loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "579e6ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.43289166, 1.306093953783784, 1.273289129227027, 1.212238144054054, 1.1768244690918919]\n"
     ]
    }
   ],
   "source": [
    "val_last_total_loss_per_epoch = df_validation.groupby(\"Epoch\")[\"Loss\"].mean().tolist()\n",
    "print(val_last_total_loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15497852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 에포크 리스트 (x축)\n",
    "epochs = list(range(1, len(train_last_total_loss_per_epoch) + 1))\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_last_total_loss_per_epoch, marker='o', linestyle='-', label='Train Loss', color='blue')\n",
    "plt.plot(epochs, val_last_total_loss_per_epoch, marker='s', linestyle='--', label='Validation Loss', color='red')\n",
    "\n",
    "# 그래프 제목 및 라벨\n",
    "plt.title(\"Train vs Validation Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xticks(epochs)  # X축 눈금 에포크에 맞추기\n",
    "plt.legend()  # 범례 추가\n",
    "plt.grid(True)  # 격자 표시\n",
    "\n",
    "# 그래프 출력\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3a4713c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_PATH: ./\n",
      "Does the path exist? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"MODEL_PATH:\", MODEL_PATH)\n",
    "print(\"Does the path exist?\", os.path.exists(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d088c461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 2.13056231 epoch total loss 2.13056231\n",
      "Trained batch 2 batch loss 2.20957208 epoch total loss 2.17006731\n",
      "Trained batch 3 batch loss 2.33392835 epoch total loss 2.22468758\n",
      "Trained batch 4 batch loss 2.29859161 epoch total loss 2.24316359\n",
      "Trained batch 5 batch loss 2.34586 epoch total loss 2.26370287\n",
      "Trained batch 6 batch loss 2.38549423 epoch total loss 2.28400135\n",
      "Trained batch 7 batch loss 2.33372664 epoch total loss 2.29110503\n",
      "Trained batch 8 batch loss 2.3252728 epoch total loss 2.29537606\n",
      "Trained batch 9 batch loss 2.25175238 epoch total loss 2.29052901\n",
      "Trained batch 10 batch loss 2.20771933 epoch total loss 2.28224802\n",
      "Trained batch 11 batch loss 2.25419402 epoch total loss 2.27969766\n",
      "Trained batch 12 batch loss 2.1771059 epoch total loss 2.27114844\n",
      "Trained batch 13 batch loss 2.07397938 epoch total loss 2.25598145\n",
      "Trained batch 14 batch loss 1.99238551 epoch total loss 2.23715329\n",
      "Trained batch 15 batch loss 1.84783316 epoch total loss 2.21119833\n",
      "Trained batch 16 batch loss 1.79622412 epoch total loss 2.18526244\n",
      "Trained batch 17 batch loss 1.99768567 epoch total loss 2.17422843\n",
      "Trained batch 18 batch loss 1.90241694 epoch total loss 2.15912771\n",
      "Trained batch 19 batch loss 1.8851347 epoch total loss 2.1447072\n",
      "Trained batch 20 batch loss 1.94826281 epoch total loss 2.13488483\n",
      "Trained batch 21 batch loss 1.91669178 epoch total loss 2.12449455\n",
      "Trained batch 22 batch loss 1.85335612 epoch total loss 2.11217022\n",
      "Trained batch 23 batch loss 1.9138763 epoch total loss 2.10354853\n",
      "Trained batch 24 batch loss 1.9567852 epoch total loss 2.09743333\n",
      "Trained batch 25 batch loss 1.98912716 epoch total loss 2.09310126\n",
      "Trained batch 26 batch loss 1.85920811 epoch total loss 2.08410525\n",
      "Trained batch 27 batch loss 1.92479503 epoch total loss 2.07820487\n",
      "Trained batch 28 batch loss 1.87795854 epoch total loss 2.07105327\n",
      "Trained batch 29 batch loss 1.87010443 epoch total loss 2.06412411\n",
      "Trained batch 30 batch loss 1.93687117 epoch total loss 2.05988216\n",
      "Trained batch 31 batch loss 1.86245775 epoch total loss 2.05351377\n",
      "Trained batch 32 batch loss 1.91456354 epoch total loss 2.04917145\n",
      "Trained batch 33 batch loss 1.92012501 epoch total loss 2.04526114\n",
      "Trained batch 34 batch loss 1.89568329 epoch total loss 2.04086161\n",
      "Trained batch 35 batch loss 1.86152041 epoch total loss 2.03573751\n",
      "Trained batch 36 batch loss 1.75301981 epoch total loss 2.02788448\n",
      "Trained batch 37 batch loss 1.77405536 epoch total loss 2.02102423\n",
      "Trained batch 38 batch loss 1.88977242 epoch total loss 2.01757\n",
      "Trained batch 39 batch loss 1.91608334 epoch total loss 2.01496792\n",
      "Trained batch 40 batch loss 1.89161503 epoch total loss 2.01188421\n",
      "Trained batch 41 batch loss 1.8383683 epoch total loss 2.00765204\n",
      "Trained batch 42 batch loss 1.79851365 epoch total loss 2.00267267\n",
      "Trained batch 43 batch loss 1.75946558 epoch total loss 1.99701679\n",
      "Trained batch 44 batch loss 1.76309502 epoch total loss 1.99170029\n",
      "Trained batch 45 batch loss 1.79132462 epoch total loss 1.98724735\n",
      "Trained batch 46 batch loss 1.76392591 epoch total loss 1.98239255\n",
      "Trained batch 47 batch loss 1.80206013 epoch total loss 1.97855568\n",
      "Trained batch 48 batch loss 1.77896369 epoch total loss 1.97439754\n",
      "Trained batch 49 batch loss 1.79137397 epoch total loss 1.97066236\n",
      "Trained batch 50 batch loss 1.71595 epoch total loss 1.96556807\n",
      "Trained batch 51 batch loss 1.7375797 epoch total loss 1.96109772\n",
      "Trained batch 52 batch loss 1.51964772 epoch total loss 1.95260823\n",
      "Trained batch 53 batch loss 1.67925799 epoch total loss 1.94745076\n",
      "Trained batch 54 batch loss 1.75980055 epoch total loss 1.94397581\n",
      "Trained batch 55 batch loss 1.6931684 epoch total loss 1.93941569\n",
      "Trained batch 56 batch loss 1.75042355 epoch total loss 1.93604088\n",
      "Trained batch 57 batch loss 1.82723558 epoch total loss 1.93413198\n",
      "Trained batch 58 batch loss 1.74570918 epoch total loss 1.93088329\n",
      "Trained batch 59 batch loss 1.79464793 epoch total loss 1.9285742\n",
      "Trained batch 60 batch loss 1.69856191 epoch total loss 1.92474067\n",
      "Trained batch 61 batch loss 1.74020863 epoch total loss 1.92171562\n",
      "Trained batch 62 batch loss 1.71201622 epoch total loss 1.91833341\n",
      "Trained batch 63 batch loss 1.66547155 epoch total loss 1.91431975\n",
      "Trained batch 64 batch loss 1.80015016 epoch total loss 1.91253579\n",
      "Trained batch 65 batch loss 1.79730272 epoch total loss 1.91076291\n",
      "Trained batch 66 batch loss 1.70430124 epoch total loss 1.90763474\n",
      "Trained batch 67 batch loss 1.72538722 epoch total loss 1.90491462\n",
      "Trained batch 68 batch loss 1.61302328 epoch total loss 1.90062225\n",
      "Trained batch 69 batch loss 1.60934544 epoch total loss 1.89640081\n",
      "Trained batch 70 batch loss 1.59387636 epoch total loss 1.892079\n",
      "Trained batch 71 batch loss 1.57313919 epoch total loss 1.88758683\n",
      "Trained batch 72 batch loss 1.51950312 epoch total loss 1.88247442\n",
      "Trained batch 73 batch loss 1.5662092 epoch total loss 1.878142\n",
      "Trained batch 74 batch loss 1.5829289 epoch total loss 1.87415278\n",
      "Trained batch 75 batch loss 1.6869272 epoch total loss 1.8716563\n",
      "Trained batch 76 batch loss 1.76998258 epoch total loss 1.87031853\n",
      "Trained batch 77 batch loss 1.69479477 epoch total loss 1.86803901\n",
      "Trained batch 78 batch loss 1.65837336 epoch total loss 1.86535096\n",
      "Trained batch 79 batch loss 1.61362195 epoch total loss 1.8621645\n",
      "Trained batch 80 batch loss 1.64765334 epoch total loss 1.85948312\n",
      "Trained batch 81 batch loss 1.70378458 epoch total loss 1.85756087\n",
      "Trained batch 82 batch loss 1.7147063 epoch total loss 1.85581875\n",
      "Trained batch 83 batch loss 1.69581091 epoch total loss 1.85389102\n",
      "Trained batch 84 batch loss 1.70137584 epoch total loss 1.85207534\n",
      "Trained batch 85 batch loss 1.68619657 epoch total loss 1.85012388\n",
      "Trained batch 86 batch loss 1.76594758 epoch total loss 1.84914505\n",
      "Trained batch 87 batch loss 1.76427436 epoch total loss 1.84816945\n",
      "Trained batch 88 batch loss 1.66257024 epoch total loss 1.84606028\n",
      "Trained batch 89 batch loss 1.65684175 epoch total loss 1.8439343\n",
      "Trained batch 90 batch loss 1.6124742 epoch total loss 1.84136248\n",
      "Trained batch 91 batch loss 1.55632544 epoch total loss 1.83823013\n",
      "Trained batch 92 batch loss 1.62628162 epoch total loss 1.83592641\n",
      "Trained batch 93 batch loss 1.67985094 epoch total loss 1.83424819\n",
      "Trained batch 94 batch loss 1.56818032 epoch total loss 1.83141768\n",
      "Trained batch 95 batch loss 1.48619485 epoch total loss 1.8277837\n",
      "Trained batch 96 batch loss 1.43895566 epoch total loss 1.82373333\n",
      "Trained batch 97 batch loss 1.53934014 epoch total loss 1.82080138\n",
      "Trained batch 98 batch loss 1.51800346 epoch total loss 1.81771171\n",
      "Trained batch 99 batch loss 1.56933165 epoch total loss 1.81520283\n",
      "Trained batch 100 batch loss 1.65369499 epoch total loss 1.81358778\n",
      "Trained batch 101 batch loss 1.58875334 epoch total loss 1.81136179\n",
      "Trained batch 102 batch loss 1.56154633 epoch total loss 1.80891263\n",
      "Trained batch 103 batch loss 1.5930438 epoch total loss 1.80681694\n",
      "Trained batch 104 batch loss 1.63060117 epoch total loss 1.80512249\n",
      "Trained batch 105 batch loss 1.69443107 epoch total loss 1.80406833\n",
      "Trained batch 106 batch loss 1.61038435 epoch total loss 1.80224109\n",
      "Trained batch 107 batch loss 1.73860383 epoch total loss 1.80164635\n",
      "Trained batch 108 batch loss 1.7197063 epoch total loss 1.80088758\n",
      "Trained batch 109 batch loss 1.75390279 epoch total loss 1.80045664\n",
      "Trained batch 110 batch loss 1.70489371 epoch total loss 1.79958785\n",
      "Trained batch 111 batch loss 1.63739467 epoch total loss 1.79812658\n",
      "Trained batch 112 batch loss 1.72699618 epoch total loss 1.79749143\n",
      "Trained batch 113 batch loss 1.60343218 epoch total loss 1.79577422\n",
      "Trained batch 114 batch loss 1.6067245 epoch total loss 1.79411578\n",
      "Trained batch 115 batch loss 1.64020419 epoch total loss 1.79277742\n",
      "Trained batch 116 batch loss 1.57530808 epoch total loss 1.79090261\n",
      "Trained batch 117 batch loss 1.61395407 epoch total loss 1.78939021\n",
      "Trained batch 118 batch loss 1.62721884 epoch total loss 1.78801584\n",
      "Trained batch 119 batch loss 1.49856913 epoch total loss 1.7855835\n",
      "Trained batch 120 batch loss 1.68452024 epoch total loss 1.78474128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 121 batch loss 1.70186734 epoch total loss 1.78405643\n",
      "Trained batch 122 batch loss 1.69838095 epoch total loss 1.78335416\n",
      "Trained batch 123 batch loss 1.66961598 epoch total loss 1.78242958\n",
      "Trained batch 124 batch loss 1.58640432 epoch total loss 1.78084874\n",
      "Trained batch 125 batch loss 1.64035463 epoch total loss 1.77972472\n",
      "Trained batch 126 batch loss 1.67242694 epoch total loss 1.77887309\n",
      "Trained batch 127 batch loss 1.64440727 epoch total loss 1.77781439\n",
      "Trained batch 128 batch loss 1.65357077 epoch total loss 1.77684367\n",
      "Trained batch 129 batch loss 1.61237335 epoch total loss 1.77556872\n",
      "Trained batch 130 batch loss 1.55901921 epoch total loss 1.77390301\n",
      "Trained batch 131 batch loss 1.51969361 epoch total loss 1.77196252\n",
      "Trained batch 132 batch loss 1.55003905 epoch total loss 1.7702812\n",
      "Trained batch 133 batch loss 1.65858793 epoch total loss 1.76944137\n",
      "Trained batch 134 batch loss 1.67695546 epoch total loss 1.76875126\n",
      "Trained batch 135 batch loss 1.64921415 epoch total loss 1.76786578\n",
      "Trained batch 136 batch loss 1.73027658 epoch total loss 1.76758933\n",
      "Trained batch 137 batch loss 1.73416197 epoch total loss 1.76734531\n",
      "Trained batch 138 batch loss 1.64510894 epoch total loss 1.76645958\n",
      "Trained batch 139 batch loss 1.69278407 epoch total loss 1.76592946\n",
      "Trained batch 140 batch loss 1.70741761 epoch total loss 1.76551151\n",
      "Trained batch 141 batch loss 1.7357347 epoch total loss 1.76530039\n",
      "Trained batch 142 batch loss 1.69012439 epoch total loss 1.76477098\n",
      "Trained batch 143 batch loss 1.70417249 epoch total loss 1.7643472\n",
      "Trained batch 144 batch loss 1.72223353 epoch total loss 1.76405478\n",
      "Trained batch 145 batch loss 1.71591306 epoch total loss 1.76372266\n",
      "Trained batch 146 batch loss 1.71622467 epoch total loss 1.76339746\n",
      "Trained batch 147 batch loss 1.70357728 epoch total loss 1.76299047\n",
      "Trained batch 148 batch loss 1.67669582 epoch total loss 1.76240742\n",
      "Trained batch 149 batch loss 1.67562032 epoch total loss 1.76182508\n",
      "Trained batch 150 batch loss 1.74327755 epoch total loss 1.76170146\n",
      "Trained batch 151 batch loss 1.69984829 epoch total loss 1.76129186\n",
      "Trained batch 152 batch loss 1.66812 epoch total loss 1.76067889\n",
      "Trained batch 153 batch loss 1.70293736 epoch total loss 1.76030159\n",
      "Trained batch 154 batch loss 1.55543303 epoch total loss 1.75897121\n",
      "Trained batch 155 batch loss 1.62335193 epoch total loss 1.75809622\n",
      "Trained batch 156 batch loss 1.68240404 epoch total loss 1.75761104\n",
      "Trained batch 157 batch loss 1.5380013 epoch total loss 1.75621212\n",
      "Trained batch 158 batch loss 1.57759237 epoch total loss 1.75508177\n",
      "Trained batch 159 batch loss 1.68019164 epoch total loss 1.7546109\n",
      "Trained batch 160 batch loss 1.58484721 epoch total loss 1.75354981\n",
      "Trained batch 161 batch loss 1.66578889 epoch total loss 1.75300479\n",
      "Trained batch 162 batch loss 1.66105068 epoch total loss 1.75243711\n",
      "Trained batch 163 batch loss 1.71578515 epoch total loss 1.75221229\n",
      "Trained batch 164 batch loss 1.61570692 epoch total loss 1.75137985\n",
      "Trained batch 165 batch loss 1.59618425 epoch total loss 1.75043929\n",
      "Trained batch 166 batch loss 1.5896101 epoch total loss 1.74947035\n",
      "Trained batch 167 batch loss 1.70017219 epoch total loss 1.74917507\n",
      "Trained batch 168 batch loss 1.60316706 epoch total loss 1.74830604\n",
      "Trained batch 169 batch loss 1.72783971 epoch total loss 1.74818504\n",
      "Trained batch 170 batch loss 1.68793154 epoch total loss 1.74783051\n",
      "Trained batch 171 batch loss 1.57557964 epoch total loss 1.74682331\n",
      "Trained batch 172 batch loss 1.51983476 epoch total loss 1.74550366\n",
      "Trained batch 173 batch loss 1.65534508 epoch total loss 1.74498236\n",
      "Trained batch 174 batch loss 1.60353589 epoch total loss 1.74416959\n",
      "Trained batch 175 batch loss 1.5540539 epoch total loss 1.74308312\n",
      "Trained batch 176 batch loss 1.57830882 epoch total loss 1.74214697\n",
      "Trained batch 177 batch loss 1.65615785 epoch total loss 1.74166107\n",
      "Trained batch 178 batch loss 1.67873442 epoch total loss 1.74130762\n",
      "Trained batch 179 batch loss 1.5529381 epoch total loss 1.74025536\n",
      "Trained batch 180 batch loss 1.61434829 epoch total loss 1.73955584\n",
      "Trained batch 181 batch loss 1.56383812 epoch total loss 1.73858511\n",
      "Trained batch 182 batch loss 1.63391376 epoch total loss 1.73800993\n",
      "Trained batch 183 batch loss 1.5742383 epoch total loss 1.73711503\n",
      "Trained batch 184 batch loss 1.5819639 epoch total loss 1.73627186\n",
      "Trained batch 185 batch loss 1.49363947 epoch total loss 1.73496044\n",
      "Trained batch 186 batch loss 1.50778687 epoch total loss 1.73373902\n",
      "Trained batch 187 batch loss 1.49568892 epoch total loss 1.7324661\n",
      "Trained batch 188 batch loss 1.52702618 epoch total loss 1.73137343\n",
      "Trained batch 189 batch loss 1.54456139 epoch total loss 1.73038495\n",
      "Trained batch 190 batch loss 1.57647574 epoch total loss 1.72957492\n",
      "Trained batch 191 batch loss 1.57877588 epoch total loss 1.72878528\n",
      "Trained batch 192 batch loss 1.67790484 epoch total loss 1.72852039\n",
      "Trained batch 193 batch loss 1.62306154 epoch total loss 1.72797394\n",
      "Trained batch 194 batch loss 1.6401726 epoch total loss 1.7275213\n",
      "Trained batch 195 batch loss 1.66576445 epoch total loss 1.72720468\n",
      "Trained batch 196 batch loss 1.6206038 epoch total loss 1.72666073\n",
      "Trained batch 197 batch loss 1.60053349 epoch total loss 1.72602046\n",
      "Trained batch 198 batch loss 1.62878323 epoch total loss 1.72552931\n",
      "Trained batch 199 batch loss 1.58909428 epoch total loss 1.72484374\n",
      "Trained batch 200 batch loss 1.69835949 epoch total loss 1.7247113\n",
      "Trained batch 201 batch loss 1.68720245 epoch total loss 1.72452462\n",
      "Trained batch 202 batch loss 1.6589427 epoch total loss 1.7242\n",
      "Trained batch 203 batch loss 1.63508511 epoch total loss 1.72376084\n",
      "Trained batch 204 batch loss 1.64040518 epoch total loss 1.72335231\n",
      "Trained batch 205 batch loss 1.65341139 epoch total loss 1.72301114\n",
      "Trained batch 206 batch loss 1.64910388 epoch total loss 1.72265244\n",
      "Trained batch 207 batch loss 1.66871643 epoch total loss 1.72239172\n",
      "Trained batch 208 batch loss 1.57148302 epoch total loss 1.72166622\n",
      "Trained batch 209 batch loss 1.65242994 epoch total loss 1.72133493\n",
      "Trained batch 210 batch loss 1.71634841 epoch total loss 1.72131109\n",
      "Trained batch 211 batch loss 1.69733453 epoch total loss 1.72119749\n",
      "Trained batch 212 batch loss 1.79862714 epoch total loss 1.72156262\n",
      "Trained batch 213 batch loss 1.77687979 epoch total loss 1.72182238\n",
      "Trained batch 214 batch loss 1.76953506 epoch total loss 1.7220453\n",
      "Trained batch 215 batch loss 1.73406255 epoch total loss 1.72210121\n",
      "Trained batch 216 batch loss 1.71929765 epoch total loss 1.72208834\n",
      "Trained batch 217 batch loss 1.61611819 epoch total loss 1.72159994\n",
      "Trained batch 218 batch loss 1.62226176 epoch total loss 1.7211442\n",
      "Trained batch 219 batch loss 1.60949874 epoch total loss 1.72063446\n",
      "Trained batch 220 batch loss 1.70295477 epoch total loss 1.72055399\n",
      "Trained batch 221 batch loss 1.66960466 epoch total loss 1.72032356\n",
      "Trained batch 222 batch loss 1.67987084 epoch total loss 1.72014129\n",
      "Trained batch 223 batch loss 1.67050445 epoch total loss 1.71991873\n",
      "Trained batch 224 batch loss 1.50417471 epoch total loss 1.71895564\n",
      "Trained batch 225 batch loss 1.62205577 epoch total loss 1.71852493\n",
      "Trained batch 226 batch loss 1.65926397 epoch total loss 1.71826279\n",
      "Trained batch 227 batch loss 1.6952908 epoch total loss 1.71816158\n",
      "Trained batch 228 batch loss 1.64495754 epoch total loss 1.71784055\n",
      "Trained batch 229 batch loss 1.51852369 epoch total loss 1.71697009\n",
      "Trained batch 230 batch loss 1.57670331 epoch total loss 1.71636021\n",
      "Trained batch 231 batch loss 1.57237387 epoch total loss 1.71573699\n",
      "Trained batch 232 batch loss 1.6182164 epoch total loss 1.71531665\n",
      "Trained batch 233 batch loss 1.63223529 epoch total loss 1.7149601\n",
      "Trained batch 234 batch loss 1.54370821 epoch total loss 1.71422815\n",
      "Trained batch 235 batch loss 1.61071086 epoch total loss 1.71378767\n",
      "Trained batch 236 batch loss 1.58937156 epoch total loss 1.71326053\n",
      "Trained batch 237 batch loss 1.57450604 epoch total loss 1.71267509\n",
      "Trained batch 238 batch loss 1.54180014 epoch total loss 1.7119571\n",
      "Trained batch 239 batch loss 1.60017085 epoch total loss 1.71148932\n",
      "Trained batch 240 batch loss 1.72179592 epoch total loss 1.71153235\n",
      "Trained batch 241 batch loss 1.64351022 epoch total loss 1.71125019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 242 batch loss 1.62316954 epoch total loss 1.71088612\n",
      "Trained batch 243 batch loss 1.67416179 epoch total loss 1.71073508\n",
      "Trained batch 244 batch loss 1.66288304 epoch total loss 1.71053886\n",
      "Trained batch 245 batch loss 1.66672158 epoch total loss 1.71036\n",
      "Trained batch 246 batch loss 1.64965153 epoch total loss 1.71011329\n",
      "Trained batch 247 batch loss 1.67030871 epoch total loss 1.70995212\n",
      "Trained batch 248 batch loss 1.65091431 epoch total loss 1.70971406\n",
      "Trained batch 249 batch loss 1.65791559 epoch total loss 1.70950615\n",
      "Trained batch 250 batch loss 1.64781642 epoch total loss 1.70925939\n",
      "Trained batch 251 batch loss 1.62875557 epoch total loss 1.70893872\n",
      "Trained batch 252 batch loss 1.63841724 epoch total loss 1.70865881\n",
      "Trained batch 253 batch loss 1.61475122 epoch total loss 1.70828772\n",
      "Trained batch 254 batch loss 1.61126637 epoch total loss 1.70790565\n",
      "Trained batch 255 batch loss 1.64362788 epoch total loss 1.70765352\n",
      "Trained batch 256 batch loss 1.63173699 epoch total loss 1.70735705\n",
      "Trained batch 257 batch loss 1.66290426 epoch total loss 1.70718408\n",
      "Trained batch 258 batch loss 1.63792622 epoch total loss 1.70691574\n",
      "Trained batch 259 batch loss 1.63999605 epoch total loss 1.70665729\n",
      "Trained batch 260 batch loss 1.6378504 epoch total loss 1.70639265\n",
      "Trained batch 261 batch loss 1.65187979 epoch total loss 1.70618379\n",
      "Trained batch 262 batch loss 1.69690216 epoch total loss 1.70614839\n",
      "Trained batch 263 batch loss 1.77391911 epoch total loss 1.706406\n",
      "Trained batch 264 batch loss 1.72308314 epoch total loss 1.70646918\n",
      "Trained batch 265 batch loss 1.77714896 epoch total loss 1.70673597\n",
      "Trained batch 266 batch loss 1.68124855 epoch total loss 1.70664012\n",
      "Trained batch 267 batch loss 1.64064157 epoch total loss 1.706393\n",
      "Trained batch 268 batch loss 1.54974413 epoch total loss 1.70580852\n",
      "Trained batch 269 batch loss 1.60329676 epoch total loss 1.70542741\n",
      "Trained batch 270 batch loss 1.68611288 epoch total loss 1.705356\n",
      "Trained batch 271 batch loss 1.7253828 epoch total loss 1.70542979\n",
      "Trained batch 272 batch loss 1.63603544 epoch total loss 1.70517468\n",
      "Trained batch 273 batch loss 1.58615971 epoch total loss 1.70473874\n",
      "Trained batch 274 batch loss 1.64675772 epoch total loss 1.70452714\n",
      "Trained batch 275 batch loss 1.61777663 epoch total loss 1.70421159\n",
      "Trained batch 276 batch loss 1.73105216 epoch total loss 1.70430887\n",
      "Trained batch 277 batch loss 1.75979042 epoch total loss 1.70450914\n",
      "Trained batch 278 batch loss 1.72741175 epoch total loss 1.70459163\n",
      "Trained batch 279 batch loss 1.64497948 epoch total loss 1.70437801\n",
      "Trained batch 280 batch loss 1.64363229 epoch total loss 1.70416105\n",
      "Trained batch 281 batch loss 1.72305036 epoch total loss 1.70422828\n",
      "Trained batch 282 batch loss 1.67864847 epoch total loss 1.70413756\n",
      "Trained batch 283 batch loss 1.61802137 epoch total loss 1.70383322\n",
      "Trained batch 284 batch loss 1.64646542 epoch total loss 1.70363128\n",
      "Trained batch 285 batch loss 1.65277267 epoch total loss 1.70345283\n",
      "Trained batch 286 batch loss 1.58493924 epoch total loss 1.70303833\n",
      "Trained batch 287 batch loss 1.42861581 epoch total loss 1.70208216\n",
      "Trained batch 288 batch loss 1.59121048 epoch total loss 1.70169723\n",
      "Trained batch 289 batch loss 1.65678763 epoch total loss 1.7015419\n",
      "Trained batch 290 batch loss 1.60150933 epoch total loss 1.70119691\n",
      "Trained batch 291 batch loss 1.65129733 epoch total loss 1.70102549\n",
      "Trained batch 292 batch loss 1.54426432 epoch total loss 1.70048857\n",
      "Trained batch 293 batch loss 1.55672503 epoch total loss 1.6999979\n",
      "Trained batch 294 batch loss 1.61285615 epoch total loss 1.69970155\n",
      "Trained batch 295 batch loss 1.53309417 epoch total loss 1.69913673\n",
      "Trained batch 296 batch loss 1.57669497 epoch total loss 1.69872308\n",
      "Trained batch 297 batch loss 1.56441438 epoch total loss 1.6982708\n",
      "Trained batch 298 batch loss 1.63498 epoch total loss 1.69805849\n",
      "Trained batch 299 batch loss 1.70336711 epoch total loss 1.69807625\n",
      "Trained batch 300 batch loss 1.72701991 epoch total loss 1.69817269\n",
      "Trained batch 301 batch loss 1.72589767 epoch total loss 1.69826484\n",
      "Trained batch 302 batch loss 1.67144656 epoch total loss 1.69817591\n",
      "Trained batch 303 batch loss 1.5433166 epoch total loss 1.69766486\n",
      "Trained batch 304 batch loss 1.52196836 epoch total loss 1.69708693\n",
      "Trained batch 305 batch loss 1.62340772 epoch total loss 1.69684541\n",
      "Trained batch 306 batch loss 1.6893028 epoch total loss 1.69682086\n",
      "Trained batch 307 batch loss 1.61639452 epoch total loss 1.69655883\n",
      "Trained batch 308 batch loss 1.52519798 epoch total loss 1.69600248\n",
      "Trained batch 309 batch loss 1.67082977 epoch total loss 1.69592106\n",
      "Trained batch 310 batch loss 1.65447092 epoch total loss 1.69578743\n",
      "Trained batch 311 batch loss 1.50463092 epoch total loss 1.69517279\n",
      "Trained batch 312 batch loss 1.59557307 epoch total loss 1.69485354\n",
      "Trained batch 313 batch loss 1.69416595 epoch total loss 1.69485128\n",
      "Trained batch 314 batch loss 1.51889753 epoch total loss 1.694291\n",
      "Trained batch 315 batch loss 1.53015685 epoch total loss 1.69376993\n",
      "Trained batch 316 batch loss 1.56948793 epoch total loss 1.69337654\n",
      "Trained batch 317 batch loss 1.61019456 epoch total loss 1.69311404\n",
      "Trained batch 318 batch loss 1.57161534 epoch total loss 1.69273198\n",
      "Trained batch 319 batch loss 1.62746429 epoch total loss 1.69252729\n",
      "Trained batch 320 batch loss 1.58628964 epoch total loss 1.6921953\n",
      "Trained batch 321 batch loss 1.62340617 epoch total loss 1.69198108\n",
      "Trained batch 322 batch loss 1.59981763 epoch total loss 1.69169474\n",
      "Trained batch 323 batch loss 1.67685318 epoch total loss 1.69164884\n",
      "Trained batch 324 batch loss 1.71894693 epoch total loss 1.69173312\n",
      "Trained batch 325 batch loss 1.57023668 epoch total loss 1.69135928\n",
      "Trained batch 326 batch loss 1.58482373 epoch total loss 1.69103253\n",
      "Trained batch 327 batch loss 1.52190542 epoch total loss 1.6905154\n",
      "Trained batch 328 batch loss 1.64249706 epoch total loss 1.69036901\n",
      "Trained batch 329 batch loss 1.65325439 epoch total loss 1.69025624\n",
      "Trained batch 330 batch loss 1.59880471 epoch total loss 1.6899792\n",
      "Trained batch 331 batch loss 1.6589309 epoch total loss 1.68988538\n",
      "Trained batch 332 batch loss 1.65240824 epoch total loss 1.68977249\n",
      "Trained batch 333 batch loss 1.64158678 epoch total loss 1.68962777\n",
      "Trained batch 334 batch loss 1.59618413 epoch total loss 1.6893481\n",
      "Trained batch 335 batch loss 1.63928354 epoch total loss 1.68919861\n",
      "Trained batch 336 batch loss 1.67153835 epoch total loss 1.68914592\n",
      "Trained batch 337 batch loss 1.69962358 epoch total loss 1.68917716\n",
      "Trained batch 338 batch loss 1.71055913 epoch total loss 1.68924046\n",
      "Trained batch 339 batch loss 1.68752134 epoch total loss 1.68923533\n",
      "Trained batch 340 batch loss 1.6766839 epoch total loss 1.68919837\n",
      "Trained batch 341 batch loss 1.67405057 epoch total loss 1.68915403\n",
      "Trained batch 342 batch loss 1.69423556 epoch total loss 1.68916881\n",
      "Trained batch 343 batch loss 1.63513923 epoch total loss 1.68901134\n",
      "Trained batch 344 batch loss 1.63587976 epoch total loss 1.68885684\n",
      "Trained batch 345 batch loss 1.65102553 epoch total loss 1.68874705\n",
      "Trained batch 346 batch loss 1.53785968 epoch total loss 1.68831086\n",
      "Trained batch 347 batch loss 1.64242268 epoch total loss 1.68817854\n",
      "Trained batch 348 batch loss 1.64672947 epoch total loss 1.68805945\n",
      "Trained batch 349 batch loss 1.55404818 epoch total loss 1.6876756\n",
      "Trained batch 350 batch loss 1.57924771 epoch total loss 1.68736577\n",
      "Trained batch 351 batch loss 1.64298356 epoch total loss 1.68723929\n",
      "Trained batch 352 batch loss 1.61637044 epoch total loss 1.68703806\n",
      "Trained batch 353 batch loss 1.57582355 epoch total loss 1.68672299\n",
      "Trained batch 354 batch loss 1.52386045 epoch total loss 1.68626297\n",
      "Trained batch 355 batch loss 1.52221274 epoch total loss 1.68580079\n",
      "Trained batch 356 batch loss 1.58839202 epoch total loss 1.68552721\n",
      "Trained batch 357 batch loss 1.67593074 epoch total loss 1.68550014\n",
      "Trained batch 358 batch loss 1.65952146 epoch total loss 1.68542767\n",
      "Trained batch 359 batch loss 1.6436919 epoch total loss 1.68531144\n",
      "Trained batch 360 batch loss 1.67713177 epoch total loss 1.68528867\n",
      "Trained batch 361 batch loss 1.64584887 epoch total loss 1.68517947\n",
      "Trained batch 362 batch loss 1.64116085 epoch total loss 1.68505788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 363 batch loss 1.59346 epoch total loss 1.68480551\n",
      "Trained batch 364 batch loss 1.69408584 epoch total loss 1.68483102\n",
      "Trained batch 365 batch loss 1.63553286 epoch total loss 1.68469608\n",
      "Trained batch 366 batch loss 1.71811199 epoch total loss 1.68478739\n",
      "Trained batch 367 batch loss 1.6525867 epoch total loss 1.68469965\n",
      "Trained batch 368 batch loss 1.5935632 epoch total loss 1.68445206\n",
      "Trained batch 369 batch loss 1.75007772 epoch total loss 1.68462992\n",
      "Trained batch 370 batch loss 1.76048958 epoch total loss 1.68483496\n",
      "Trained batch 371 batch loss 1.63992751 epoch total loss 1.68471396\n",
      "Trained batch 372 batch loss 1.53912449 epoch total loss 1.6843226\n",
      "Trained batch 373 batch loss 1.54132128 epoch total loss 1.68393922\n",
      "Trained batch 374 batch loss 1.51143599 epoch total loss 1.68347788\n",
      "Trained batch 375 batch loss 1.56529808 epoch total loss 1.68316281\n",
      "Trained batch 376 batch loss 1.45785165 epoch total loss 1.68256342\n",
      "Trained batch 377 batch loss 1.59916639 epoch total loss 1.68234229\n",
      "Trained batch 378 batch loss 1.54333735 epoch total loss 1.68197453\n",
      "Trained batch 379 batch loss 1.68268466 epoch total loss 1.68197644\n",
      "Trained batch 380 batch loss 1.61986279 epoch total loss 1.681813\n",
      "Trained batch 381 batch loss 1.65253496 epoch total loss 1.68173611\n",
      "Trained batch 382 batch loss 1.61462522 epoch total loss 1.6815604\n",
      "Trained batch 383 batch loss 1.58133292 epoch total loss 1.68129885\n",
      "Trained batch 384 batch loss 1.53898501 epoch total loss 1.68092823\n",
      "Trained batch 385 batch loss 1.62130737 epoch total loss 1.6807735\n",
      "Trained batch 386 batch loss 1.60671663 epoch total loss 1.68058157\n",
      "Trained batch 387 batch loss 1.54179311 epoch total loss 1.68022299\n",
      "Trained batch 388 batch loss 1.49956679 epoch total loss 1.67975736\n",
      "Trained batch 389 batch loss 1.57626808 epoch total loss 1.6794914\n",
      "Trained batch 390 batch loss 1.51563311 epoch total loss 1.67907119\n",
      "Trained batch 391 batch loss 1.60987639 epoch total loss 1.67889416\n",
      "Trained batch 392 batch loss 1.60176826 epoch total loss 1.67869735\n",
      "Trained batch 393 batch loss 1.60560369 epoch total loss 1.67851138\n",
      "Trained batch 394 batch loss 1.60570288 epoch total loss 1.67832661\n",
      "Trained batch 395 batch loss 1.62170744 epoch total loss 1.6781832\n",
      "Trained batch 396 batch loss 1.67414689 epoch total loss 1.67817307\n",
      "Trained batch 397 batch loss 1.57077336 epoch total loss 1.67790258\n",
      "Trained batch 398 batch loss 1.66012657 epoch total loss 1.677858\n",
      "Trained batch 399 batch loss 1.61107111 epoch total loss 1.67769063\n",
      "Trained batch 400 batch loss 1.54539788 epoch total loss 1.67735994\n",
      "Trained batch 401 batch loss 1.58700538 epoch total loss 1.67713451\n",
      "Trained batch 402 batch loss 1.60748816 epoch total loss 1.6769613\n",
      "Trained batch 403 batch loss 1.64358532 epoch total loss 1.67687857\n",
      "Trained batch 404 batch loss 1.6037364 epoch total loss 1.67669749\n",
      "Trained batch 405 batch loss 1.62217462 epoch total loss 1.67656291\n",
      "Trained batch 406 batch loss 1.58928299 epoch total loss 1.67634797\n",
      "Trained batch 407 batch loss 1.60923147 epoch total loss 1.6761831\n",
      "Trained batch 408 batch loss 1.50892937 epoch total loss 1.67577314\n",
      "Trained batch 409 batch loss 1.60731685 epoch total loss 1.67560577\n",
      "Trained batch 410 batch loss 1.47227752 epoch total loss 1.67510986\n",
      "Trained batch 411 batch loss 1.46682775 epoch total loss 1.67460322\n",
      "Trained batch 412 batch loss 1.41306806 epoch total loss 1.67396843\n",
      "Trained batch 413 batch loss 1.59395015 epoch total loss 1.6737746\n",
      "Trained batch 414 batch loss 1.61918449 epoch total loss 1.67364275\n",
      "Trained batch 415 batch loss 1.59755826 epoch total loss 1.67345941\n",
      "Trained batch 416 batch loss 1.47495675 epoch total loss 1.67298234\n",
      "Trained batch 417 batch loss 1.41691971 epoch total loss 1.67236829\n",
      "Trained batch 418 batch loss 1.40584671 epoch total loss 1.67173064\n",
      "Trained batch 419 batch loss 1.56387568 epoch total loss 1.67147326\n",
      "Trained batch 420 batch loss 1.64447474 epoch total loss 1.67140901\n",
      "Trained batch 421 batch loss 1.70232773 epoch total loss 1.67148244\n",
      "Trained batch 422 batch loss 1.6812731 epoch total loss 1.67150557\n",
      "Trained batch 423 batch loss 1.66167641 epoch total loss 1.67148232\n",
      "Trained batch 424 batch loss 1.6746335 epoch total loss 1.67148983\n",
      "Trained batch 425 batch loss 1.62337303 epoch total loss 1.67137647\n",
      "Trained batch 426 batch loss 1.62302148 epoch total loss 1.6712631\n",
      "Trained batch 427 batch loss 1.65335643 epoch total loss 1.67122114\n",
      "Trained batch 428 batch loss 1.56268239 epoch total loss 1.67096758\n",
      "Trained batch 429 batch loss 1.62077034 epoch total loss 1.67085063\n",
      "Trained batch 430 batch loss 1.59368587 epoch total loss 1.67067122\n",
      "Trained batch 431 batch loss 1.66024542 epoch total loss 1.67064691\n",
      "Trained batch 432 batch loss 1.59853244 epoch total loss 1.67047989\n",
      "Trained batch 433 batch loss 1.58869207 epoch total loss 1.67029107\n",
      "Trained batch 434 batch loss 1.49524105 epoch total loss 1.66988766\n",
      "Trained batch 435 batch loss 1.55528581 epoch total loss 1.66962433\n",
      "Trained batch 436 batch loss 1.45303512 epoch total loss 1.66912758\n",
      "Trained batch 437 batch loss 1.60364914 epoch total loss 1.66897774\n",
      "Trained batch 438 batch loss 1.55595493 epoch total loss 1.66871965\n",
      "Trained batch 439 batch loss 1.45244527 epoch total loss 1.66822708\n",
      "Trained batch 440 batch loss 1.47031283 epoch total loss 1.6677773\n",
      "Trained batch 441 batch loss 1.53396952 epoch total loss 1.66747391\n",
      "Trained batch 442 batch loss 1.51804173 epoch total loss 1.66713595\n",
      "Trained batch 443 batch loss 1.50909531 epoch total loss 1.66677916\n",
      "Trained batch 444 batch loss 1.63563228 epoch total loss 1.66670907\n",
      "Trained batch 445 batch loss 1.55172515 epoch total loss 1.6664505\n",
      "Trained batch 446 batch loss 1.39840198 epoch total loss 1.66584945\n",
      "Trained batch 447 batch loss 1.54726171 epoch total loss 1.66558409\n",
      "Trained batch 448 batch loss 1.56847239 epoch total loss 1.66536736\n",
      "Trained batch 449 batch loss 1.62047029 epoch total loss 1.66526747\n",
      "Trained batch 450 batch loss 1.61045754 epoch total loss 1.66514564\n",
      "Trained batch 451 batch loss 1.59768414 epoch total loss 1.66499603\n",
      "Trained batch 452 batch loss 1.47452986 epoch total loss 1.66457462\n",
      "Trained batch 453 batch loss 1.52110541 epoch total loss 1.664258\n",
      "Trained batch 454 batch loss 1.55114317 epoch total loss 1.66400886\n",
      "Trained batch 455 batch loss 1.38451302 epoch total loss 1.66339457\n",
      "Trained batch 456 batch loss 1.54122758 epoch total loss 1.66312659\n",
      "Trained batch 457 batch loss 1.70413804 epoch total loss 1.66321647\n",
      "Trained batch 458 batch loss 1.67445779 epoch total loss 1.66324091\n",
      "Trained batch 459 batch loss 1.56992459 epoch total loss 1.66303766\n",
      "Trained batch 460 batch loss 1.48403084 epoch total loss 1.66264844\n",
      "Trained batch 461 batch loss 1.59877157 epoch total loss 1.66250992\n",
      "Trained batch 462 batch loss 1.54326558 epoch total loss 1.66225183\n",
      "Trained batch 463 batch loss 1.56519556 epoch total loss 1.66204214\n",
      "Trained batch 464 batch loss 1.56656754 epoch total loss 1.66183639\n",
      "Trained batch 465 batch loss 1.62423408 epoch total loss 1.66175556\n",
      "Trained batch 466 batch loss 1.49971795 epoch total loss 1.66140771\n",
      "Trained batch 467 batch loss 1.50281215 epoch total loss 1.66106808\n",
      "Trained batch 468 batch loss 1.66469371 epoch total loss 1.66107583\n",
      "Trained batch 469 batch loss 1.59334683 epoch total loss 1.66093135\n",
      "Trained batch 470 batch loss 1.62238538 epoch total loss 1.66084933\n",
      "Trained batch 471 batch loss 1.56907058 epoch total loss 1.66065454\n",
      "Trained batch 472 batch loss 1.59112215 epoch total loss 1.6605072\n",
      "Trained batch 473 batch loss 1.58279681 epoch total loss 1.66034293\n",
      "Trained batch 474 batch loss 1.53540969 epoch total loss 1.66007936\n",
      "Trained batch 475 batch loss 1.50995517 epoch total loss 1.65976334\n",
      "Trained batch 476 batch loss 1.56922472 epoch total loss 1.65957308\n",
      "Trained batch 477 batch loss 1.55659342 epoch total loss 1.65935719\n",
      "Trained batch 478 batch loss 1.48584819 epoch total loss 1.6589942\n",
      "Trained batch 479 batch loss 1.58420372 epoch total loss 1.65883803\n",
      "Trained batch 480 batch loss 1.52186775 epoch total loss 1.65855265\n",
      "Trained batch 481 batch loss 1.48506129 epoch total loss 1.65819192\n",
      "Trained batch 482 batch loss 1.56048918 epoch total loss 1.65798926\n",
      "Trained batch 483 batch loss 1.57070899 epoch total loss 1.65780842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 484 batch loss 1.61353993 epoch total loss 1.65771699\n",
      "Trained batch 485 batch loss 1.61035323 epoch total loss 1.65761936\n",
      "Trained batch 486 batch loss 1.59626412 epoch total loss 1.65749311\n",
      "Trained batch 487 batch loss 1.5602417 epoch total loss 1.65729332\n",
      "Trained batch 488 batch loss 1.59765697 epoch total loss 1.65717113\n",
      "Trained batch 489 batch loss 1.48787236 epoch total loss 1.65682495\n",
      "Trained batch 490 batch loss 1.44391465 epoch total loss 1.65639043\n",
      "Trained batch 491 batch loss 1.42728162 epoch total loss 1.65592384\n",
      "Trained batch 492 batch loss 1.54441166 epoch total loss 1.65569723\n",
      "Trained batch 493 batch loss 1.43916571 epoch total loss 1.65525794\n",
      "Trained batch 494 batch loss 1.61576867 epoch total loss 1.65517807\n",
      "Trained batch 495 batch loss 1.59716165 epoch total loss 1.65506089\n",
      "Trained batch 496 batch loss 1.56386614 epoch total loss 1.65487695\n",
      "Trained batch 497 batch loss 1.3491894 epoch total loss 1.65426183\n",
      "Trained batch 498 batch loss 1.56576729 epoch total loss 1.65408421\n",
      "Trained batch 499 batch loss 1.73884153 epoch total loss 1.65425408\n",
      "Trained batch 500 batch loss 1.71136951 epoch total loss 1.65436828\n",
      "Trained batch 501 batch loss 1.61108112 epoch total loss 1.65428185\n",
      "Trained batch 502 batch loss 1.53773463 epoch total loss 1.65404975\n",
      "Trained batch 503 batch loss 1.62395811 epoch total loss 1.65398991\n",
      "Trained batch 504 batch loss 1.69899356 epoch total loss 1.65407908\n",
      "Trained batch 505 batch loss 1.59895742 epoch total loss 1.65397\n",
      "Trained batch 506 batch loss 1.52234781 epoch total loss 1.65370977\n",
      "Trained batch 507 batch loss 1.47178924 epoch total loss 1.65335107\n",
      "Trained batch 508 batch loss 1.57007456 epoch total loss 1.65318704\n",
      "Trained batch 509 batch loss 1.5938431 epoch total loss 1.65307057\n",
      "Trained batch 510 batch loss 1.60045385 epoch total loss 1.65296733\n",
      "Trained batch 511 batch loss 1.58489525 epoch total loss 1.65283418\n",
      "Trained batch 512 batch loss 1.67566 epoch total loss 1.65287876\n",
      "Trained batch 513 batch loss 1.57666779 epoch total loss 1.65273023\n",
      "Trained batch 514 batch loss 1.62797618 epoch total loss 1.65268207\n",
      "Trained batch 515 batch loss 1.60379887 epoch total loss 1.65258718\n",
      "Trained batch 516 batch loss 1.57372832 epoch total loss 1.65243435\n",
      "Trained batch 517 batch loss 1.4934088 epoch total loss 1.65212679\n",
      "Trained batch 518 batch loss 1.53143668 epoch total loss 1.65189373\n",
      "Trained batch 519 batch loss 1.48857737 epoch total loss 1.65157914\n",
      "Trained batch 520 batch loss 1.50175619 epoch total loss 1.65129101\n",
      "Trained batch 521 batch loss 1.48331392 epoch total loss 1.65096867\n",
      "Trained batch 522 batch loss 1.50012112 epoch total loss 1.65067971\n",
      "Trained batch 523 batch loss 1.56230509 epoch total loss 1.65051067\n",
      "Trained batch 524 batch loss 1.5306133 epoch total loss 1.65028191\n",
      "Trained batch 525 batch loss 1.6566453 epoch total loss 1.65029407\n",
      "Trained batch 526 batch loss 1.63470697 epoch total loss 1.65026438\n",
      "Trained batch 527 batch loss 1.64310932 epoch total loss 1.65025079\n",
      "Trained batch 528 batch loss 1.53180027 epoch total loss 1.65002644\n",
      "Trained batch 529 batch loss 1.55405343 epoch total loss 1.64984512\n",
      "Trained batch 530 batch loss 1.58357358 epoch total loss 1.64972007\n",
      "Trained batch 531 batch loss 1.61992025 epoch total loss 1.64966393\n",
      "Trained batch 532 batch loss 1.59171414 epoch total loss 1.64955509\n",
      "Trained batch 533 batch loss 1.62980616 epoch total loss 1.64951801\n",
      "Trained batch 534 batch loss 1.73789465 epoch total loss 1.64968359\n",
      "Trained batch 535 batch loss 1.69210815 epoch total loss 1.64976299\n",
      "Trained batch 536 batch loss 1.66368389 epoch total loss 1.64978898\n",
      "Trained batch 537 batch loss 1.67793584 epoch total loss 1.64984131\n",
      "Trained batch 538 batch loss 1.71124673 epoch total loss 1.64995539\n",
      "Trained batch 539 batch loss 1.69829977 epoch total loss 1.65004516\n",
      "Trained batch 540 batch loss 1.67249179 epoch total loss 1.65008664\n",
      "Trained batch 541 batch loss 1.53549206 epoch total loss 1.64987493\n",
      "Trained batch 542 batch loss 1.53644037 epoch total loss 1.64966559\n",
      "Trained batch 543 batch loss 1.59057152 epoch total loss 1.64955676\n",
      "Trained batch 544 batch loss 1.59196401 epoch total loss 1.64945102\n",
      "Trained batch 545 batch loss 1.45976925 epoch total loss 1.64910293\n",
      "Trained batch 546 batch loss 1.57724822 epoch total loss 1.64897144\n",
      "Trained batch 547 batch loss 1.57627404 epoch total loss 1.64883852\n",
      "Trained batch 548 batch loss 1.54551041 epoch total loss 1.64865\n",
      "Trained batch 549 batch loss 1.57896113 epoch total loss 1.64852309\n",
      "Trained batch 550 batch loss 1.50995326 epoch total loss 1.6482712\n",
      "Trained batch 551 batch loss 1.50482655 epoch total loss 1.64801085\n",
      "Trained batch 552 batch loss 1.46233273 epoch total loss 1.64767444\n",
      "Trained batch 553 batch loss 1.53839517 epoch total loss 1.64747679\n",
      "Trained batch 554 batch loss 1.49517035 epoch total loss 1.6472019\n",
      "Trained batch 555 batch loss 1.63831782 epoch total loss 1.64718592\n",
      "Trained batch 556 batch loss 1.7007854 epoch total loss 1.64728236\n",
      "Trained batch 557 batch loss 1.71383715 epoch total loss 1.64740193\n",
      "Trained batch 558 batch loss 1.65318036 epoch total loss 1.6474123\n",
      "Trained batch 559 batch loss 1.57014847 epoch total loss 1.64727402\n",
      "Trained batch 560 batch loss 1.49991441 epoch total loss 1.64701092\n",
      "Trained batch 561 batch loss 1.36464894 epoch total loss 1.6465075\n",
      "Trained batch 562 batch loss 1.32235312 epoch total loss 1.64593077\n",
      "Trained batch 563 batch loss 1.32035089 epoch total loss 1.64535248\n",
      "Trained batch 564 batch loss 1.37209988 epoch total loss 1.6448679\n",
      "Trained batch 565 batch loss 1.59492898 epoch total loss 1.64477944\n",
      "Trained batch 566 batch loss 1.44433856 epoch total loss 1.64442539\n",
      "Trained batch 567 batch loss 1.52361453 epoch total loss 1.64421225\n",
      "Trained batch 568 batch loss 1.52919173 epoch total loss 1.64400971\n",
      "Trained batch 569 batch loss 1.59816921 epoch total loss 1.64392912\n",
      "Trained batch 570 batch loss 1.63109398 epoch total loss 1.64390671\n",
      "Trained batch 571 batch loss 1.5523535 epoch total loss 1.64374638\n",
      "Trained batch 572 batch loss 1.56724477 epoch total loss 1.64361262\n",
      "Trained batch 573 batch loss 1.44734383 epoch total loss 1.64327\n",
      "Trained batch 574 batch loss 1.55171 epoch total loss 1.64311051\n",
      "Trained batch 575 batch loss 1.52762294 epoch total loss 1.64290977\n",
      "Trained batch 576 batch loss 1.49160433 epoch total loss 1.64264703\n",
      "Trained batch 577 batch loss 1.55598855 epoch total loss 1.64249682\n",
      "Trained batch 578 batch loss 1.56496322 epoch total loss 1.64236259\n",
      "Trained batch 579 batch loss 1.55319297 epoch total loss 1.6422087\n",
      "Trained batch 580 batch loss 1.64620686 epoch total loss 1.64221549\n",
      "Trained batch 581 batch loss 1.37451816 epoch total loss 1.64175475\n",
      "Trained batch 582 batch loss 1.49501181 epoch total loss 1.64150262\n",
      "Trained batch 583 batch loss 1.49440575 epoch total loss 1.64125025\n",
      "Trained batch 584 batch loss 1.62427747 epoch total loss 1.64122117\n",
      "Trained batch 585 batch loss 1.61799657 epoch total loss 1.64118147\n",
      "Trained batch 586 batch loss 1.54490292 epoch total loss 1.6410172\n",
      "Trained batch 587 batch loss 1.5285207 epoch total loss 1.64082551\n",
      "Trained batch 588 batch loss 1.58741736 epoch total loss 1.64073467\n",
      "Trained batch 589 batch loss 1.52963066 epoch total loss 1.64054596\n",
      "Trained batch 590 batch loss 1.5429709 epoch total loss 1.6403805\n",
      "Trained batch 591 batch loss 1.52353215 epoch total loss 1.64018285\n",
      "Trained batch 592 batch loss 1.52493525 epoch total loss 1.6399883\n",
      "Trained batch 593 batch loss 1.39735746 epoch total loss 1.63957906\n",
      "Trained batch 594 batch loss 1.39858723 epoch total loss 1.63917327\n",
      "Trained batch 595 batch loss 1.27105904 epoch total loss 1.63855457\n",
      "Trained batch 596 batch loss 1.29612565 epoch total loss 1.6379801\n",
      "Trained batch 597 batch loss 1.48903286 epoch total loss 1.6377306\n",
      "Trained batch 598 batch loss 1.61761427 epoch total loss 1.63769698\n",
      "Trained batch 599 batch loss 1.59262073 epoch total loss 1.63762164\n",
      "Trained batch 600 batch loss 1.42250681 epoch total loss 1.63726306\n",
      "Trained batch 601 batch loss 1.59092975 epoch total loss 1.63718605\n",
      "Trained batch 602 batch loss 1.56728017 epoch total loss 1.63706982\n",
      "Trained batch 603 batch loss 1.39191067 epoch total loss 1.63666332\n",
      "Trained batch 604 batch loss 1.4467957 epoch total loss 1.63634884\n",
      "Trained batch 605 batch loss 1.52037239 epoch total loss 1.63615727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 606 batch loss 1.43247235 epoch total loss 1.6358211\n",
      "Trained batch 607 batch loss 1.4990623 epoch total loss 1.63559592\n",
      "Trained batch 608 batch loss 1.57010221 epoch total loss 1.63548815\n",
      "Trained batch 609 batch loss 1.55351043 epoch total loss 1.63535368\n",
      "Trained batch 610 batch loss 1.59808791 epoch total loss 1.63529253\n",
      "Trained batch 611 batch loss 1.43786204 epoch total loss 1.63496935\n",
      "Trained batch 612 batch loss 1.51705146 epoch total loss 1.63477671\n",
      "Trained batch 613 batch loss 1.53376222 epoch total loss 1.63461185\n",
      "Trained batch 614 batch loss 1.42878962 epoch total loss 1.63427663\n",
      "Trained batch 615 batch loss 1.55268717 epoch total loss 1.63414395\n",
      "Trained batch 616 batch loss 1.53786445 epoch total loss 1.63398767\n",
      "Trained batch 617 batch loss 1.5152781 epoch total loss 1.63379514\n",
      "Trained batch 618 batch loss 1.55688429 epoch total loss 1.63367069\n",
      "Trained batch 619 batch loss 1.52791786 epoch total loss 1.63349986\n",
      "Trained batch 620 batch loss 1.59349465 epoch total loss 1.63343537\n",
      "Trained batch 621 batch loss 1.58304799 epoch total loss 1.63335431\n",
      "Trained batch 622 batch loss 1.57522142 epoch total loss 1.63326073\n",
      "Trained batch 623 batch loss 1.56620264 epoch total loss 1.6331532\n",
      "Trained batch 624 batch loss 1.57611322 epoch total loss 1.63306177\n",
      "Trained batch 625 batch loss 1.55037606 epoch total loss 1.63292944\n",
      "Trained batch 626 batch loss 1.54040134 epoch total loss 1.63278162\n",
      "Trained batch 627 batch loss 1.49488378 epoch total loss 1.63256168\n",
      "Trained batch 628 batch loss 1.45987391 epoch total loss 1.63228667\n",
      "Trained batch 629 batch loss 1.50985372 epoch total loss 1.63209212\n",
      "Trained batch 630 batch loss 1.64253426 epoch total loss 1.63210881\n",
      "Trained batch 631 batch loss 1.58887935 epoch total loss 1.63204026\n",
      "Trained batch 632 batch loss 1.50682151 epoch total loss 1.63184214\n",
      "Trained batch 633 batch loss 1.55263114 epoch total loss 1.63171697\n",
      "Trained batch 634 batch loss 1.57651627 epoch total loss 1.63163\n",
      "Trained batch 635 batch loss 1.61564183 epoch total loss 1.63160467\n",
      "Trained batch 636 batch loss 1.54526126 epoch total loss 1.63146901\n",
      "Trained batch 637 batch loss 1.5108521 epoch total loss 1.63127959\n",
      "Trained batch 638 batch loss 1.3906405 epoch total loss 1.63090241\n",
      "Trained batch 639 batch loss 1.32210505 epoch total loss 1.63041925\n",
      "Trained batch 640 batch loss 1.41944098 epoch total loss 1.63008952\n",
      "Trained batch 641 batch loss 1.41592038 epoch total loss 1.62975538\n",
      "Trained batch 642 batch loss 1.31958246 epoch total loss 1.62927222\n",
      "Trained batch 643 batch loss 1.23784113 epoch total loss 1.62866342\n",
      "Trained batch 644 batch loss 1.24967015 epoch total loss 1.62807488\n",
      "Trained batch 645 batch loss 1.23744 epoch total loss 1.62746918\n",
      "Trained batch 646 batch loss 1.50166225 epoch total loss 1.62727451\n",
      "Trained batch 647 batch loss 1.59150088 epoch total loss 1.62721932\n",
      "Trained batch 648 batch loss 1.58802044 epoch total loss 1.62715888\n",
      "Trained batch 649 batch loss 1.51745927 epoch total loss 1.62698984\n",
      "Trained batch 650 batch loss 1.4901309 epoch total loss 1.6267792\n",
      "Trained batch 651 batch loss 1.56957066 epoch total loss 1.62669134\n",
      "Trained batch 652 batch loss 1.52472365 epoch total loss 1.62653506\n",
      "Trained batch 653 batch loss 1.40356708 epoch total loss 1.62619364\n",
      "Trained batch 654 batch loss 1.32476115 epoch total loss 1.62573266\n",
      "Trained batch 655 batch loss 1.45776868 epoch total loss 1.62547612\n",
      "Trained batch 656 batch loss 1.48642206 epoch total loss 1.62526429\n",
      "Trained batch 657 batch loss 1.46407175 epoch total loss 1.62501895\n",
      "Trained batch 658 batch loss 1.47048378 epoch total loss 1.62478411\n",
      "Trained batch 659 batch loss 1.37955987 epoch total loss 1.62441182\n",
      "Trained batch 660 batch loss 1.56267524 epoch total loss 1.62431824\n",
      "Trained batch 661 batch loss 1.48108459 epoch total loss 1.62410152\n",
      "Trained batch 662 batch loss 1.49687171 epoch total loss 1.62390935\n",
      "Trained batch 663 batch loss 1.51185429 epoch total loss 1.62374032\n",
      "Trained batch 664 batch loss 1.57349086 epoch total loss 1.62366462\n",
      "Trained batch 665 batch loss 1.56445789 epoch total loss 1.62357557\n",
      "Trained batch 666 batch loss 1.21736979 epoch total loss 1.62296569\n",
      "Trained batch 667 batch loss 1.15686834 epoch total loss 1.62226689\n",
      "Trained batch 668 batch loss 1.29372716 epoch total loss 1.62177503\n",
      "Trained batch 669 batch loss 1.56964874 epoch total loss 1.62169719\n",
      "Trained batch 670 batch loss 1.73961949 epoch total loss 1.62187314\n",
      "Trained batch 671 batch loss 1.71117914 epoch total loss 1.6220063\n",
      "Trained batch 672 batch loss 1.62928462 epoch total loss 1.62201715\n",
      "Trained batch 673 batch loss 1.50857544 epoch total loss 1.62184846\n",
      "Trained batch 674 batch loss 1.50105894 epoch total loss 1.62166929\n",
      "Trained batch 675 batch loss 1.47885382 epoch total loss 1.62145782\n",
      "Trained batch 676 batch loss 1.51176751 epoch total loss 1.62129545\n",
      "Trained batch 677 batch loss 1.56366587 epoch total loss 1.62121046\n",
      "Trained batch 678 batch loss 1.58748436 epoch total loss 1.62116075\n",
      "Trained batch 679 batch loss 1.55497265 epoch total loss 1.62106323\n",
      "Trained batch 680 batch loss 1.54000211 epoch total loss 1.62094402\n",
      "Trained batch 681 batch loss 1.52463186 epoch total loss 1.62080264\n",
      "Trained batch 682 batch loss 1.53080308 epoch total loss 1.62067068\n",
      "Trained batch 683 batch loss 1.59133506 epoch total loss 1.62062764\n",
      "Trained batch 684 batch loss 1.59620321 epoch total loss 1.62059188\n",
      "Trained batch 685 batch loss 1.50926852 epoch total loss 1.6204294\n",
      "Trained batch 686 batch loss 1.49733377 epoch total loss 1.62025\n",
      "Trained batch 687 batch loss 1.42826319 epoch total loss 1.61997044\n",
      "Trained batch 688 batch loss 1.48173332 epoch total loss 1.61976945\n",
      "Trained batch 689 batch loss 1.54175127 epoch total loss 1.61965621\n",
      "Trained batch 690 batch loss 1.55407 epoch total loss 1.6195612\n",
      "Trained batch 691 batch loss 1.53839 epoch total loss 1.61944354\n",
      "Trained batch 692 batch loss 1.52899861 epoch total loss 1.619313\n",
      "Trained batch 693 batch loss 1.45432007 epoch total loss 1.61907494\n",
      "Trained batch 694 batch loss 1.514799 epoch total loss 1.61892462\n",
      "Trained batch 695 batch loss 1.48198736 epoch total loss 1.61872756\n",
      "Trained batch 696 batch loss 1.53338373 epoch total loss 1.61860478\n",
      "Trained batch 697 batch loss 1.53545403 epoch total loss 1.61848545\n",
      "Trained batch 698 batch loss 1.32222033 epoch total loss 1.61806107\n",
      "Trained batch 699 batch loss 1.37655139 epoch total loss 1.6177156\n",
      "Trained batch 700 batch loss 1.47887444 epoch total loss 1.61751723\n",
      "Trained batch 701 batch loss 1.48624492 epoch total loss 1.61733\n",
      "Trained batch 702 batch loss 1.47105598 epoch total loss 1.61712158\n",
      "Trained batch 703 batch loss 1.58822846 epoch total loss 1.61708057\n",
      "Trained batch 704 batch loss 1.53793728 epoch total loss 1.61696815\n",
      "Trained batch 705 batch loss 1.55409706 epoch total loss 1.61687899\n",
      "Trained batch 706 batch loss 1.53519917 epoch total loss 1.61676323\n",
      "Trained batch 707 batch loss 1.5211097 epoch total loss 1.61662793\n",
      "Trained batch 708 batch loss 1.45832968 epoch total loss 1.61640441\n",
      "Trained batch 709 batch loss 1.35382009 epoch total loss 1.61603391\n",
      "Trained batch 710 batch loss 1.51619852 epoch total loss 1.61589336\n",
      "Trained batch 711 batch loss 1.48096025 epoch total loss 1.61570358\n",
      "Trained batch 712 batch loss 1.52938819 epoch total loss 1.61558247\n",
      "Trained batch 713 batch loss 1.52715635 epoch total loss 1.61545837\n",
      "Trained batch 714 batch loss 1.51506186 epoch total loss 1.61531758\n",
      "Trained batch 715 batch loss 1.45312166 epoch total loss 1.61509085\n",
      "Trained batch 716 batch loss 1.43712282 epoch total loss 1.6148423\n",
      "Trained batch 717 batch loss 1.46766877 epoch total loss 1.61463702\n",
      "Trained batch 718 batch loss 1.48326683 epoch total loss 1.61445403\n",
      "Trained batch 719 batch loss 1.57092834 epoch total loss 1.61439347\n",
      "Trained batch 720 batch loss 1.37943935 epoch total loss 1.61406708\n",
      "Trained batch 721 batch loss 1.49096704 epoch total loss 1.61389637\n",
      "Trained batch 722 batch loss 1.5124526 epoch total loss 1.61375582\n",
      "Trained batch 723 batch loss 1.44365835 epoch total loss 1.6135205\n",
      "Trained batch 724 batch loss 1.60603869 epoch total loss 1.61351025\n",
      "Trained batch 725 batch loss 1.42693329 epoch total loss 1.61325276\n",
      "Trained batch 726 batch loss 1.2715379 epoch total loss 1.612782\n",
      "Trained batch 727 batch loss 1.34727705 epoch total loss 1.61241686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 728 batch loss 1.47635341 epoch total loss 1.61223\n",
      "Trained batch 729 batch loss 1.39298868 epoch total loss 1.61192906\n",
      "Trained batch 730 batch loss 1.4426198 epoch total loss 1.6116972\n",
      "Trained batch 731 batch loss 1.53461552 epoch total loss 1.61159182\n",
      "Trained batch 732 batch loss 1.41040242 epoch total loss 1.61131692\n",
      "Trained batch 733 batch loss 1.55626857 epoch total loss 1.61124182\n",
      "Trained batch 734 batch loss 1.54638994 epoch total loss 1.61115348\n",
      "Trained batch 735 batch loss 1.59036839 epoch total loss 1.61112523\n",
      "Trained batch 736 batch loss 1.73021197 epoch total loss 1.611287\n",
      "Trained batch 737 batch loss 1.72079873 epoch total loss 1.61143565\n",
      "Trained batch 738 batch loss 1.67884326 epoch total loss 1.61152697\n",
      "Trained batch 739 batch loss 1.52935886 epoch total loss 1.61141586\n",
      "Trained batch 740 batch loss 1.36555529 epoch total loss 1.61108363\n",
      "Trained batch 741 batch loss 1.27434874 epoch total loss 1.6106292\n",
      "Trained batch 742 batch loss 1.45629883 epoch total loss 1.61042118\n",
      "Trained batch 743 batch loss 1.48615313 epoch total loss 1.61025393\n",
      "Trained batch 744 batch loss 1.59736025 epoch total loss 1.61023676\n",
      "Trained batch 745 batch loss 1.51985407 epoch total loss 1.61011541\n",
      "Trained batch 746 batch loss 1.50000632 epoch total loss 1.60996783\n",
      "Trained batch 747 batch loss 1.58333659 epoch total loss 1.60993218\n",
      "Trained batch 748 batch loss 1.45535171 epoch total loss 1.60972559\n",
      "Trained batch 749 batch loss 1.42259657 epoch total loss 1.60947573\n",
      "Trained batch 750 batch loss 1.41118705 epoch total loss 1.60921121\n",
      "Trained batch 751 batch loss 1.53581548 epoch total loss 1.60911345\n",
      "Trained batch 752 batch loss 1.5477705 epoch total loss 1.6090318\n",
      "Trained batch 753 batch loss 1.4851892 epoch total loss 1.60886741\n",
      "Trained batch 754 batch loss 1.53602505 epoch total loss 1.60877085\n",
      "Trained batch 755 batch loss 1.51195264 epoch total loss 1.60864258\n",
      "Trained batch 756 batch loss 1.53027844 epoch total loss 1.60853887\n",
      "Trained batch 757 batch loss 1.5197823 epoch total loss 1.60842168\n",
      "Trained batch 758 batch loss 1.545192 epoch total loss 1.60833824\n",
      "Trained batch 759 batch loss 1.55425429 epoch total loss 1.60826683\n",
      "Trained batch 760 batch loss 1.51379144 epoch total loss 1.60814261\n",
      "Trained batch 761 batch loss 1.50020838 epoch total loss 1.60800076\n",
      "Trained batch 762 batch loss 1.47451627 epoch total loss 1.60782552\n",
      "Trained batch 763 batch loss 1.30790532 epoch total loss 1.60743248\n",
      "Trained batch 764 batch loss 1.39628816 epoch total loss 1.60715604\n",
      "Trained batch 765 batch loss 1.41570282 epoch total loss 1.6069057\n",
      "Trained batch 766 batch loss 1.52448189 epoch total loss 1.60679817\n",
      "Trained batch 767 batch loss 1.42983508 epoch total loss 1.60656738\n",
      "Trained batch 768 batch loss 1.55299056 epoch total loss 1.60649765\n",
      "Trained batch 769 batch loss 1.46898842 epoch total loss 1.60631883\n",
      "Trained batch 770 batch loss 1.54797292 epoch total loss 1.60624301\n",
      "Trained batch 771 batch loss 1.5641495 epoch total loss 1.60618854\n",
      "Trained batch 772 batch loss 1.51278973 epoch total loss 1.60606754\n",
      "Trained batch 773 batch loss 1.51951182 epoch total loss 1.6059556\n",
      "Trained batch 774 batch loss 1.4971782 epoch total loss 1.60581505\n",
      "Trained batch 775 batch loss 1.47885799 epoch total loss 1.60565126\n",
      "Trained batch 776 batch loss 1.44486654 epoch total loss 1.60544407\n",
      "Trained batch 777 batch loss 1.40071464 epoch total loss 1.60518062\n",
      "Trained batch 778 batch loss 1.49195576 epoch total loss 1.60503507\n",
      "Trained batch 779 batch loss 1.44072556 epoch total loss 1.60482407\n",
      "Trained batch 780 batch loss 1.48457146 epoch total loss 1.60466993\n",
      "Trained batch 781 batch loss 1.53203583 epoch total loss 1.60457695\n",
      "Trained batch 782 batch loss 1.49548948 epoch total loss 1.60443735\n",
      "Trained batch 783 batch loss 1.52309322 epoch total loss 1.60433352\n",
      "Trained batch 784 batch loss 1.50749362 epoch total loss 1.6042099\n",
      "Trained batch 785 batch loss 1.50255132 epoch total loss 1.60408044\n",
      "Trained batch 786 batch loss 1.50805 epoch total loss 1.60395825\n",
      "Trained batch 787 batch loss 1.51707327 epoch total loss 1.60384786\n",
      "Trained batch 788 batch loss 1.56708598 epoch total loss 1.60380125\n",
      "Trained batch 789 batch loss 1.52497554 epoch total loss 1.60370147\n",
      "Trained batch 790 batch loss 1.52353382 epoch total loss 1.6036\n",
      "Trained batch 791 batch loss 1.57918298 epoch total loss 1.60356915\n",
      "Trained batch 792 batch loss 1.60148835 epoch total loss 1.60356653\n",
      "Trained batch 793 batch loss 1.47777474 epoch total loss 1.60340786\n",
      "Trained batch 794 batch loss 1.54436147 epoch total loss 1.60333347\n",
      "Trained batch 795 batch loss 1.43867588 epoch total loss 1.60312641\n",
      "Trained batch 796 batch loss 1.59444714 epoch total loss 1.60311556\n",
      "Trained batch 797 batch loss 1.62449932 epoch total loss 1.60314238\n",
      "Trained batch 798 batch loss 1.52379227 epoch total loss 1.60304296\n",
      "Trained batch 799 batch loss 1.64176834 epoch total loss 1.60309136\n",
      "Trained batch 800 batch loss 1.42465627 epoch total loss 1.60286832\n",
      "Trained batch 801 batch loss 1.50773454 epoch total loss 1.60274947\n",
      "Trained batch 802 batch loss 1.56957734 epoch total loss 1.60270822\n",
      "Trained batch 803 batch loss 1.51619923 epoch total loss 1.60260046\n",
      "Trained batch 804 batch loss 1.46419 epoch total loss 1.60242844\n",
      "Trained batch 805 batch loss 1.4782021 epoch total loss 1.60227394\n",
      "Trained batch 806 batch loss 1.54160976 epoch total loss 1.60219872\n",
      "Trained batch 807 batch loss 1.47104764 epoch total loss 1.60203624\n",
      "Trained batch 808 batch loss 1.56271768 epoch total loss 1.6019876\n",
      "Trained batch 809 batch loss 1.5539372 epoch total loss 1.60192823\n",
      "Trained batch 810 batch loss 1.49937367 epoch total loss 1.60180163\n",
      "Trained batch 811 batch loss 1.59405649 epoch total loss 1.60179222\n",
      "Trained batch 812 batch loss 1.56568968 epoch total loss 1.60174775\n",
      "Trained batch 813 batch loss 1.54401898 epoch total loss 1.60167682\n",
      "Trained batch 814 batch loss 1.41514015 epoch total loss 1.60144758\n",
      "Trained batch 815 batch loss 1.37834084 epoch total loss 1.60117388\n",
      "Trained batch 816 batch loss 1.38225842 epoch total loss 1.60090542\n",
      "Trained batch 817 batch loss 1.37642193 epoch total loss 1.60063076\n",
      "Trained batch 818 batch loss 1.43481 epoch total loss 1.6004281\n",
      "Trained batch 819 batch loss 1.48692274 epoch total loss 1.60028946\n",
      "Trained batch 820 batch loss 1.44326651 epoch total loss 1.60009789\n",
      "Trained batch 821 batch loss 1.45213366 epoch total loss 1.59991777\n",
      "Trained batch 822 batch loss 1.48204923 epoch total loss 1.59977436\n",
      "Trained batch 823 batch loss 1.5093236 epoch total loss 1.59966445\n",
      "Trained batch 824 batch loss 1.32664919 epoch total loss 1.59933305\n",
      "Trained batch 825 batch loss 1.45652664 epoch total loss 1.59916\n",
      "Trained batch 826 batch loss 1.50980258 epoch total loss 1.59905183\n",
      "Trained batch 827 batch loss 1.44435036 epoch total loss 1.59886467\n",
      "Trained batch 828 batch loss 1.4391315 epoch total loss 1.59867167\n",
      "Trained batch 829 batch loss 1.5955745 epoch total loss 1.59866798\n",
      "Trained batch 830 batch loss 1.66944754 epoch total loss 1.59875321\n",
      "Trained batch 831 batch loss 1.5939213 epoch total loss 1.59874737\n",
      "Trained batch 832 batch loss 1.60448086 epoch total loss 1.59875429\n",
      "Trained batch 833 batch loss 1.58911467 epoch total loss 1.59874272\n",
      "Trained batch 834 batch loss 1.60934281 epoch total loss 1.59875548\n",
      "Trained batch 835 batch loss 1.59759116 epoch total loss 1.59875405\n",
      "Trained batch 836 batch loss 1.57287312 epoch total loss 1.59872305\n",
      "Trained batch 837 batch loss 1.52058506 epoch total loss 1.59862971\n",
      "Trained batch 838 batch loss 1.49350476 epoch total loss 1.5985043\n",
      "Trained batch 839 batch loss 1.51970017 epoch total loss 1.59841037\n",
      "Trained batch 840 batch loss 1.50890756 epoch total loss 1.59830379\n",
      "Trained batch 841 batch loss 1.60780776 epoch total loss 1.59831512\n",
      "Trained batch 842 batch loss 1.57671785 epoch total loss 1.59828937\n",
      "Trained batch 843 batch loss 1.45703423 epoch total loss 1.59812176\n",
      "Trained batch 844 batch loss 1.52772903 epoch total loss 1.59803832\n",
      "Trained batch 845 batch loss 1.44979846 epoch total loss 1.59786296\n",
      "Trained batch 846 batch loss 1.4744792 epoch total loss 1.59771717\n",
      "Trained batch 847 batch loss 1.54106081 epoch total loss 1.59765017\n",
      "Trained batch 848 batch loss 1.59141159 epoch total loss 1.5976429\n",
      "Trained batch 849 batch loss 1.56937671 epoch total loss 1.59760952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 850 batch loss 1.60095239 epoch total loss 1.59761345\n",
      "Trained batch 851 batch loss 1.57730222 epoch total loss 1.59758949\n",
      "Trained batch 852 batch loss 1.61809993 epoch total loss 1.59761357\n",
      "Trained batch 853 batch loss 1.52801335 epoch total loss 1.59753191\n",
      "Trained batch 854 batch loss 1.57482529 epoch total loss 1.59750533\n",
      "Trained batch 855 batch loss 1.46550107 epoch total loss 1.59735084\n",
      "Trained batch 856 batch loss 1.64344621 epoch total loss 1.59740472\n",
      "Trained batch 857 batch loss 1.5960784 epoch total loss 1.59740317\n",
      "Trained batch 858 batch loss 1.54651344 epoch total loss 1.5973438\n",
      "Trained batch 859 batch loss 1.60285473 epoch total loss 1.59735024\n",
      "Trained batch 860 batch loss 1.55751419 epoch total loss 1.59730399\n",
      "Trained batch 861 batch loss 1.43379784 epoch total loss 1.59711409\n",
      "Trained batch 862 batch loss 1.57313538 epoch total loss 1.59708631\n",
      "Trained batch 863 batch loss 1.58966804 epoch total loss 1.59707773\n",
      "Trained batch 864 batch loss 1.61983669 epoch total loss 1.59710407\n",
      "Trained batch 865 batch loss 1.52172446 epoch total loss 1.59701693\n",
      "Trained batch 866 batch loss 1.41313124 epoch total loss 1.59680462\n",
      "Trained batch 867 batch loss 1.33086872 epoch total loss 1.59649777\n",
      "Trained batch 868 batch loss 1.40121925 epoch total loss 1.59627283\n",
      "Trained batch 869 batch loss 1.45988739 epoch total loss 1.59611583\n",
      "Trained batch 870 batch loss 1.50658858 epoch total loss 1.59601295\n",
      "Trained batch 871 batch loss 1.48899138 epoch total loss 1.59589\n",
      "Trained batch 872 batch loss 1.63844812 epoch total loss 1.59593892\n",
      "Trained batch 873 batch loss 1.64927328 epoch total loss 1.596\n",
      "Trained batch 874 batch loss 1.6574899 epoch total loss 1.59607029\n",
      "Trained batch 875 batch loss 1.6195569 epoch total loss 1.59609711\n",
      "Trained batch 876 batch loss 1.34542191 epoch total loss 1.59581101\n",
      "Trained batch 877 batch loss 1.4221791 epoch total loss 1.59561288\n",
      "Trained batch 878 batch loss 1.46261799 epoch total loss 1.59546149\n",
      "Trained batch 879 batch loss 1.50371075 epoch total loss 1.59535706\n",
      "Trained batch 880 batch loss 1.42733145 epoch total loss 1.59516621\n",
      "Trained batch 881 batch loss 1.48125768 epoch total loss 1.59503675\n",
      "Trained batch 882 batch loss 1.50420785 epoch total loss 1.59493375\n",
      "Trained batch 883 batch loss 1.46882582 epoch total loss 1.59479094\n",
      "Trained batch 884 batch loss 1.44560194 epoch total loss 1.59462214\n",
      "Trained batch 885 batch loss 1.44276392 epoch total loss 1.59445059\n",
      "Trained batch 886 batch loss 1.54883242 epoch total loss 1.59439909\n",
      "Trained batch 887 batch loss 1.51960623 epoch total loss 1.59431481\n",
      "Trained batch 888 batch loss 1.44503284 epoch total loss 1.59414673\n",
      "Trained batch 889 batch loss 1.2869041 epoch total loss 1.59380102\n",
      "Trained batch 890 batch loss 1.32597959 epoch total loss 1.59350014\n",
      "Trained batch 891 batch loss 1.28552377 epoch total loss 1.59315443\n",
      "Trained batch 892 batch loss 1.44482768 epoch total loss 1.59298813\n",
      "Trained batch 893 batch loss 1.34983277 epoch total loss 1.59271586\n",
      "Trained batch 894 batch loss 1.48877168 epoch total loss 1.59259963\n",
      "Trained batch 895 batch loss 1.47330785 epoch total loss 1.59246624\n",
      "Trained batch 896 batch loss 1.45099676 epoch total loss 1.5923084\n",
      "Trained batch 897 batch loss 1.46932924 epoch total loss 1.59217143\n",
      "Trained batch 898 batch loss 1.49072182 epoch total loss 1.59205842\n",
      "Trained batch 899 batch loss 1.59465945 epoch total loss 1.59206128\n",
      "Trained batch 900 batch loss 1.44643259 epoch total loss 1.59189939\n",
      "Trained batch 901 batch loss 1.36349332 epoch total loss 1.59164596\n",
      "Trained batch 902 batch loss 1.44340265 epoch total loss 1.59148157\n",
      "Trained batch 903 batch loss 1.45331717 epoch total loss 1.59132862\n",
      "Trained batch 904 batch loss 1.44458187 epoch total loss 1.59116626\n",
      "Trained batch 905 batch loss 1.5431838 epoch total loss 1.59111333\n",
      "Trained batch 906 batch loss 1.67658103 epoch total loss 1.59120762\n",
      "Trained batch 907 batch loss 1.64747465 epoch total loss 1.59126973\n",
      "Trained batch 908 batch loss 1.53895164 epoch total loss 1.59121203\n",
      "Trained batch 909 batch loss 1.31901896 epoch total loss 1.59091258\n",
      "Trained batch 910 batch loss 1.38966441 epoch total loss 1.59069145\n",
      "Trained batch 911 batch loss 1.34170175 epoch total loss 1.5904181\n",
      "Trained batch 912 batch loss 1.45047927 epoch total loss 1.59026456\n",
      "Trained batch 913 batch loss 1.48113084 epoch total loss 1.59014499\n",
      "Trained batch 914 batch loss 1.48473954 epoch total loss 1.59002972\n",
      "Trained batch 915 batch loss 1.42406 epoch total loss 1.58984828\n",
      "Trained batch 916 batch loss 1.33027065 epoch total loss 1.58956492\n",
      "Trained batch 917 batch loss 1.50612903 epoch total loss 1.58947396\n",
      "Trained batch 918 batch loss 1.51694238 epoch total loss 1.58939493\n",
      "Trained batch 919 batch loss 1.56301343 epoch total loss 1.5893662\n",
      "Trained batch 920 batch loss 1.51646948 epoch total loss 1.58928704\n",
      "Trained batch 921 batch loss 1.48132944 epoch total loss 1.58916974\n",
      "Trained batch 922 batch loss 1.45163512 epoch total loss 1.58902061\n",
      "Trained batch 923 batch loss 1.46044636 epoch total loss 1.58888137\n",
      "Trained batch 924 batch loss 1.41495609 epoch total loss 1.58869302\n",
      "Trained batch 925 batch loss 1.45403385 epoch total loss 1.58854747\n",
      "Trained batch 926 batch loss 1.37661171 epoch total loss 1.58831859\n",
      "Trained batch 927 batch loss 1.47419274 epoch total loss 1.58819544\n",
      "Trained batch 928 batch loss 1.44862783 epoch total loss 1.588045\n",
      "Trained batch 929 batch loss 1.44769537 epoch total loss 1.58789408\n",
      "Trained batch 930 batch loss 1.51656842 epoch total loss 1.58781743\n",
      "Trained batch 931 batch loss 1.53196812 epoch total loss 1.58775747\n",
      "Trained batch 932 batch loss 1.4806366 epoch total loss 1.58764243\n",
      "Trained batch 933 batch loss 1.48999262 epoch total loss 1.58753777\n",
      "Trained batch 934 batch loss 1.50050211 epoch total loss 1.58744454\n",
      "Trained batch 935 batch loss 1.55084491 epoch total loss 1.58740544\n",
      "Trained batch 936 batch loss 1.48355627 epoch total loss 1.58729446\n",
      "Trained batch 937 batch loss 1.47098613 epoch total loss 1.58717036\n",
      "Trained batch 938 batch loss 1.51877308 epoch total loss 1.58709741\n",
      "Trained batch 939 batch loss 1.53167248 epoch total loss 1.5870384\n",
      "Trained batch 940 batch loss 1.52694869 epoch total loss 1.5869745\n",
      "Trained batch 941 batch loss 1.42566097 epoch total loss 1.58680296\n",
      "Trained batch 942 batch loss 1.49866903 epoch total loss 1.5867095\n",
      "Trained batch 943 batch loss 1.54936719 epoch total loss 1.5866698\n",
      "Trained batch 944 batch loss 1.52156329 epoch total loss 1.5866009\n",
      "Trained batch 945 batch loss 1.59819555 epoch total loss 1.58661306\n",
      "Trained batch 946 batch loss 1.48520637 epoch total loss 1.58650589\n",
      "Trained batch 947 batch loss 1.63717318 epoch total loss 1.58655941\n",
      "Trained batch 948 batch loss 1.47675347 epoch total loss 1.58644366\n",
      "Trained batch 949 batch loss 1.46453798 epoch total loss 1.58631516\n",
      "Trained batch 950 batch loss 1.48477435 epoch total loss 1.58620822\n",
      "Trained batch 951 batch loss 1.4824971 epoch total loss 1.58609927\n",
      "Trained batch 952 batch loss 1.37988269 epoch total loss 1.58588266\n",
      "Trained batch 953 batch loss 1.47984147 epoch total loss 1.58577132\n",
      "Trained batch 954 batch loss 1.51688 epoch total loss 1.58569908\n",
      "Trained batch 955 batch loss 1.48505116 epoch total loss 1.58559382\n",
      "Trained batch 956 batch loss 1.40356874 epoch total loss 1.58540332\n",
      "Trained batch 957 batch loss 1.46287143 epoch total loss 1.58527541\n",
      "Trained batch 958 batch loss 1.3886075 epoch total loss 1.58507\n",
      "Trained batch 959 batch loss 1.3423605 epoch total loss 1.58481693\n",
      "Trained batch 960 batch loss 1.46873474 epoch total loss 1.58469605\n",
      "Trained batch 961 batch loss 1.41163874 epoch total loss 1.58451593\n",
      "Trained batch 962 batch loss 1.39224386 epoch total loss 1.58431602\n",
      "Trained batch 963 batch loss 1.40961421 epoch total loss 1.5841347\n",
      "Trained batch 964 batch loss 1.30566752 epoch total loss 1.58384585\n",
      "Trained batch 965 batch loss 1.2488519 epoch total loss 1.58349872\n",
      "Trained batch 966 batch loss 1.37944734 epoch total loss 1.58328748\n",
      "Trained batch 967 batch loss 1.33315611 epoch total loss 1.58302879\n",
      "Trained batch 968 batch loss 1.40357029 epoch total loss 1.58284342\n",
      "Trained batch 969 batch loss 1.3909241 epoch total loss 1.5826453\n",
      "Trained batch 970 batch loss 1.45731473 epoch total loss 1.58251595\n",
      "Trained batch 971 batch loss 1.48238337 epoch total loss 1.58241296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 972 batch loss 1.52123642 epoch total loss 1.58235\n",
      "Trained batch 973 batch loss 1.50432277 epoch total loss 1.58226979\n",
      "Trained batch 974 batch loss 1.56962514 epoch total loss 1.58225667\n",
      "Trained batch 975 batch loss 1.52716672 epoch total loss 1.58220029\n",
      "Trained batch 976 batch loss 1.54146147 epoch total loss 1.58215857\n",
      "Trained batch 977 batch loss 1.51663065 epoch total loss 1.58209145\n",
      "Trained batch 978 batch loss 1.55468845 epoch total loss 1.58206344\n",
      "Trained batch 979 batch loss 1.37475181 epoch total loss 1.58185172\n",
      "Trained batch 980 batch loss 1.44026744 epoch total loss 1.58170724\n",
      "Trained batch 981 batch loss 1.44016218 epoch total loss 1.581563\n",
      "Trained batch 982 batch loss 1.43658638 epoch total loss 1.58141541\n",
      "Trained batch 983 batch loss 1.43763137 epoch total loss 1.58126915\n",
      "Trained batch 984 batch loss 1.45204639 epoch total loss 1.58113778\n",
      "Trained batch 985 batch loss 1.48441458 epoch total loss 1.58103955\n",
      "Trained batch 986 batch loss 1.45140088 epoch total loss 1.58090806\n",
      "Trained batch 987 batch loss 1.51006925 epoch total loss 1.5808363\n",
      "Trained batch 988 batch loss 1.51745343 epoch total loss 1.58077216\n",
      "Trained batch 989 batch loss 1.57591271 epoch total loss 1.58076727\n",
      "Trained batch 990 batch loss 1.60809636 epoch total loss 1.58079493\n",
      "Trained batch 991 batch loss 1.6371721 epoch total loss 1.58085179\n",
      "Trained batch 992 batch loss 1.55242181 epoch total loss 1.58082306\n",
      "Trained batch 993 batch loss 1.37146604 epoch total loss 1.5806123\n",
      "Trained batch 994 batch loss 1.42217231 epoch total loss 1.5804528\n",
      "Trained batch 995 batch loss 1.58850455 epoch total loss 1.58046091\n",
      "Trained batch 996 batch loss 1.63432753 epoch total loss 1.58051491\n",
      "Trained batch 997 batch loss 1.54510033 epoch total loss 1.58047938\n",
      "Trained batch 998 batch loss 1.65198851 epoch total loss 1.58055103\n",
      "Trained batch 999 batch loss 1.63345885 epoch total loss 1.58060396\n",
      "Trained batch 1000 batch loss 1.52237439 epoch total loss 1.58054566\n",
      "Trained batch 1001 batch loss 1.54168725 epoch total loss 1.58050692\n",
      "Trained batch 1002 batch loss 1.60147357 epoch total loss 1.58052778\n",
      "Trained batch 1003 batch loss 1.54096699 epoch total loss 1.58048844\n",
      "Trained batch 1004 batch loss 1.53806841 epoch total loss 1.58044612\n",
      "Trained batch 1005 batch loss 1.44718647 epoch total loss 1.58031356\n",
      "Trained batch 1006 batch loss 1.45349586 epoch total loss 1.58018744\n",
      "Trained batch 1007 batch loss 1.47485697 epoch total loss 1.58008289\n",
      "Trained batch 1008 batch loss 1.54014111 epoch total loss 1.5800432\n",
      "Trained batch 1009 batch loss 1.50940943 epoch total loss 1.57997322\n",
      "Trained batch 1010 batch loss 1.48439288 epoch total loss 1.57987857\n",
      "Trained batch 1011 batch loss 1.50360918 epoch total loss 1.57980323\n",
      "Trained batch 1012 batch loss 1.45046186 epoch total loss 1.57967532\n",
      "Trained batch 1013 batch loss 1.39860356 epoch total loss 1.57949662\n",
      "Trained batch 1014 batch loss 1.36323857 epoch total loss 1.57928336\n",
      "Trained batch 1015 batch loss 1.38634539 epoch total loss 1.57909322\n",
      "Trained batch 1016 batch loss 1.40755677 epoch total loss 1.57892442\n",
      "Trained batch 1017 batch loss 1.42186809 epoch total loss 1.57877\n",
      "Trained batch 1018 batch loss 1.39083767 epoch total loss 1.57858551\n",
      "Trained batch 1019 batch loss 1.38888669 epoch total loss 1.5783993\n",
      "Trained batch 1020 batch loss 1.53822434 epoch total loss 1.57836\n",
      "Trained batch 1021 batch loss 1.47504306 epoch total loss 1.57825875\n",
      "Trained batch 1022 batch loss 1.55743778 epoch total loss 1.57823849\n",
      "Trained batch 1023 batch loss 1.45395172 epoch total loss 1.57811701\n",
      "Trained batch 1024 batch loss 1.75318611 epoch total loss 1.57828796\n",
      "Trained batch 1025 batch loss 1.70072627 epoch total loss 1.57840741\n",
      "Trained batch 1026 batch loss 1.46657014 epoch total loss 1.57829833\n",
      "Trained batch 1027 batch loss 1.41985095 epoch total loss 1.57814407\n",
      "Trained batch 1028 batch loss 1.42453456 epoch total loss 1.57799459\n",
      "Trained batch 1029 batch loss 1.23897183 epoch total loss 1.57766521\n",
      "Trained batch 1030 batch loss 1.29007363 epoch total loss 1.5773859\n",
      "Trained batch 1031 batch loss 1.44772053 epoch total loss 1.57726026\n",
      "Trained batch 1032 batch loss 1.20214128 epoch total loss 1.57689667\n",
      "Trained batch 1033 batch loss 1.25558352 epoch total loss 1.57658565\n",
      "Trained batch 1034 batch loss 1.25040531 epoch total loss 1.57627022\n",
      "Trained batch 1035 batch loss 1.30593121 epoch total loss 1.57600904\n",
      "Trained batch 1036 batch loss 1.388762 epoch total loss 1.57582831\n",
      "Trained batch 1037 batch loss 1.41918206 epoch total loss 1.57567728\n",
      "Trained batch 1038 batch loss 1.53911436 epoch total loss 1.57564199\n",
      "Trained batch 1039 batch loss 1.5879581 epoch total loss 1.57565391\n",
      "Trained batch 1040 batch loss 1.60243702 epoch total loss 1.57567966\n",
      "Trained batch 1041 batch loss 1.58699417 epoch total loss 1.57569051\n",
      "Trained batch 1042 batch loss 1.49809802 epoch total loss 1.575616\n",
      "Trained batch 1043 batch loss 1.52373171 epoch total loss 1.57556617\n",
      "Trained batch 1044 batch loss 1.48921776 epoch total loss 1.57548356\n",
      "Trained batch 1045 batch loss 1.37567258 epoch total loss 1.57529235\n",
      "Trained batch 1046 batch loss 1.41360128 epoch total loss 1.57513773\n",
      "Trained batch 1047 batch loss 1.36540544 epoch total loss 1.57493746\n",
      "Trained batch 1048 batch loss 1.42490065 epoch total loss 1.57479429\n",
      "Trained batch 1049 batch loss 1.37533987 epoch total loss 1.57460415\n",
      "Trained batch 1050 batch loss 1.43316555 epoch total loss 1.57446945\n",
      "Trained batch 1051 batch loss 1.41926229 epoch total loss 1.57432175\n",
      "Trained batch 1052 batch loss 1.4251883 epoch total loss 1.57418\n",
      "Trained batch 1053 batch loss 1.46993089 epoch total loss 1.57408106\n",
      "Trained batch 1054 batch loss 1.45215464 epoch total loss 1.57396531\n",
      "Trained batch 1055 batch loss 1.57437634 epoch total loss 1.57396567\n",
      "Trained batch 1056 batch loss 1.4900434 epoch total loss 1.57388616\n",
      "Trained batch 1057 batch loss 1.38080955 epoch total loss 1.57370353\n",
      "Trained batch 1058 batch loss 1.41845489 epoch total loss 1.57355678\n",
      "Trained batch 1059 batch loss 1.45680594 epoch total loss 1.57344651\n",
      "Trained batch 1060 batch loss 1.49381292 epoch total loss 1.57337141\n",
      "Trained batch 1061 batch loss 1.52386796 epoch total loss 1.5733248\n",
      "Trained batch 1062 batch loss 1.66095924 epoch total loss 1.57340741\n",
      "Trained batch 1063 batch loss 1.60698807 epoch total loss 1.57343888\n",
      "Trained batch 1064 batch loss 1.47037792 epoch total loss 1.57334197\n",
      "Trained batch 1065 batch loss 1.43834591 epoch total loss 1.57321525\n",
      "Trained batch 1066 batch loss 1.46660328 epoch total loss 1.57311523\n",
      "Trained batch 1067 batch loss 1.42516971 epoch total loss 1.57297659\n",
      "Trained batch 1068 batch loss 1.50065768 epoch total loss 1.57290876\n",
      "Trained batch 1069 batch loss 1.4407841 epoch total loss 1.57278514\n",
      "Trained batch 1070 batch loss 1.42684543 epoch total loss 1.57264888\n",
      "Trained batch 1071 batch loss 1.5105474 epoch total loss 1.57259083\n",
      "Trained batch 1072 batch loss 1.60958028 epoch total loss 1.5726254\n",
      "Trained batch 1073 batch loss 1.60511589 epoch total loss 1.57265556\n",
      "Trained batch 1074 batch loss 1.46864462 epoch total loss 1.57255876\n",
      "Trained batch 1075 batch loss 1.51663589 epoch total loss 1.57250667\n",
      "Trained batch 1076 batch loss 1.49053288 epoch total loss 1.57243049\n",
      "Trained batch 1077 batch loss 1.46126342 epoch total loss 1.57232726\n",
      "Trained batch 1078 batch loss 1.39953709 epoch total loss 1.57216704\n",
      "Trained batch 1079 batch loss 1.52634311 epoch total loss 1.5721246\n",
      "Trained batch 1080 batch loss 1.38184178 epoch total loss 1.57194841\n",
      "Trained batch 1081 batch loss 1.47849822 epoch total loss 1.57186198\n",
      "Trained batch 1082 batch loss 1.43754196 epoch total loss 1.57173777\n",
      "Trained batch 1083 batch loss 1.48861873 epoch total loss 1.571661\n",
      "Trained batch 1084 batch loss 1.46497047 epoch total loss 1.57156265\n",
      "Trained batch 1085 batch loss 1.3592844 epoch total loss 1.57136691\n",
      "Trained batch 1086 batch loss 1.47366393 epoch total loss 1.5712769\n",
      "Trained batch 1087 batch loss 1.44940543 epoch total loss 1.57116485\n",
      "Trained batch 1088 batch loss 1.40012074 epoch total loss 1.57100773\n",
      "Trained batch 1089 batch loss 1.39434719 epoch total loss 1.57084537\n",
      "Trained batch 1090 batch loss 1.47950792 epoch total loss 1.57076156\n",
      "Trained batch 1091 batch loss 1.49572635 epoch total loss 1.57069278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1092 batch loss 1.53104377 epoch total loss 1.57065642\n",
      "Trained batch 1093 batch loss 1.39531267 epoch total loss 1.57049596\n",
      "Trained batch 1094 batch loss 1.40072823 epoch total loss 1.57034087\n",
      "Trained batch 1095 batch loss 1.42536974 epoch total loss 1.57020843\n",
      "Trained batch 1096 batch loss 1.46309912 epoch total loss 1.5701108\n",
      "Trained batch 1097 batch loss 1.4453603 epoch total loss 1.56999707\n",
      "Trained batch 1098 batch loss 1.42554593 epoch total loss 1.56986547\n",
      "Trained batch 1099 batch loss 1.51542068 epoch total loss 1.56981587\n",
      "Trained batch 1100 batch loss 1.37742686 epoch total loss 1.56964099\n",
      "Trained batch 1101 batch loss 1.47671473 epoch total loss 1.56955659\n",
      "Trained batch 1102 batch loss 1.49864769 epoch total loss 1.56949222\n",
      "Trained batch 1103 batch loss 1.46601129 epoch total loss 1.56939852\n",
      "Trained batch 1104 batch loss 1.55830908 epoch total loss 1.56938851\n",
      "Trained batch 1105 batch loss 1.54100239 epoch total loss 1.56936276\n",
      "Trained batch 1106 batch loss 1.27347016 epoch total loss 1.56909525\n",
      "Trained batch 1107 batch loss 1.3700732 epoch total loss 1.56891549\n",
      "Trained batch 1108 batch loss 1.37634635 epoch total loss 1.56874168\n",
      "Trained batch 1109 batch loss 1.65155256 epoch total loss 1.56881642\n",
      "Trained batch 1110 batch loss 1.49140251 epoch total loss 1.56874669\n",
      "Trained batch 1111 batch loss 1.3490932 epoch total loss 1.56854904\n",
      "Trained batch 1112 batch loss 1.42610788 epoch total loss 1.56842101\n",
      "Trained batch 1113 batch loss 1.42922199 epoch total loss 1.56829584\n",
      "Trained batch 1114 batch loss 1.54402673 epoch total loss 1.56827414\n",
      "Trained batch 1115 batch loss 1.54736924 epoch total loss 1.56825542\n",
      "Trained batch 1116 batch loss 1.53620195 epoch total loss 1.5682267\n",
      "Trained batch 1117 batch loss 1.46942616 epoch total loss 1.56813824\n",
      "Trained batch 1118 batch loss 1.46769571 epoch total loss 1.56804836\n",
      "Trained batch 1119 batch loss 1.46389496 epoch total loss 1.56795526\n",
      "Trained batch 1120 batch loss 1.38932991 epoch total loss 1.56779575\n",
      "Trained batch 1121 batch loss 1.40791774 epoch total loss 1.56765318\n",
      "Trained batch 1122 batch loss 1.56532812 epoch total loss 1.56765115\n",
      "Trained batch 1123 batch loss 1.60107172 epoch total loss 1.56768084\n",
      "Trained batch 1124 batch loss 1.43265498 epoch total loss 1.56756067\n",
      "Trained batch 1125 batch loss 1.38541389 epoch total loss 1.56739879\n",
      "Trained batch 1126 batch loss 1.44013023 epoch total loss 1.56728578\n",
      "Trained batch 1127 batch loss 1.45806289 epoch total loss 1.56718886\n",
      "Trained batch 1128 batch loss 1.37710667 epoch total loss 1.5670203\n",
      "Trained batch 1129 batch loss 1.36729121 epoch total loss 1.56684339\n",
      "Trained batch 1130 batch loss 1.56484139 epoch total loss 1.5668416\n",
      "Trained batch 1131 batch loss 1.64785302 epoch total loss 1.56691325\n",
      "Trained batch 1132 batch loss 1.55778146 epoch total loss 1.56690514\n",
      "Trained batch 1133 batch loss 1.56852865 epoch total loss 1.56690645\n",
      "Trained batch 1134 batch loss 1.60540676 epoch total loss 1.56694043\n",
      "Trained batch 1135 batch loss 1.46814656 epoch total loss 1.56685328\n",
      "Trained batch 1136 batch loss 1.33259583 epoch total loss 1.56664717\n",
      "Trained batch 1137 batch loss 1.46914685 epoch total loss 1.56656134\n",
      "Trained batch 1138 batch loss 1.54701793 epoch total loss 1.56654418\n",
      "Trained batch 1139 batch loss 1.47325253 epoch total loss 1.56646228\n",
      "Trained batch 1140 batch loss 1.55752301 epoch total loss 1.56645441\n",
      "Trained batch 1141 batch loss 1.50388312 epoch total loss 1.56639957\n",
      "Trained batch 1142 batch loss 1.53097928 epoch total loss 1.56636858\n",
      "Trained batch 1143 batch loss 1.48900938 epoch total loss 1.56630099\n",
      "Trained batch 1144 batch loss 1.50339246 epoch total loss 1.56624603\n",
      "Trained batch 1145 batch loss 1.618469 epoch total loss 1.56629157\n",
      "Trained batch 1146 batch loss 1.55961549 epoch total loss 1.56628573\n",
      "Trained batch 1147 batch loss 1.57104516 epoch total loss 1.56628978\n",
      "Trained batch 1148 batch loss 1.59192622 epoch total loss 1.56631219\n",
      "Trained batch 1149 batch loss 1.54681695 epoch total loss 1.56629527\n",
      "Trained batch 1150 batch loss 1.46645379 epoch total loss 1.56620836\n",
      "Trained batch 1151 batch loss 1.53365529 epoch total loss 1.56618011\n",
      "Trained batch 1152 batch loss 1.6084367 epoch total loss 1.56621683\n",
      "Trained batch 1153 batch loss 1.47455192 epoch total loss 1.56613731\n",
      "Trained batch 1154 batch loss 1.46336532 epoch total loss 1.56604826\n",
      "Trained batch 1155 batch loss 1.48589325 epoch total loss 1.56597888\n",
      "Trained batch 1156 batch loss 1.42146683 epoch total loss 1.56585383\n",
      "Trained batch 1157 batch loss 1.44046175 epoch total loss 1.56574547\n",
      "Trained batch 1158 batch loss 1.49590826 epoch total loss 1.56568515\n",
      "Trained batch 1159 batch loss 1.48517418 epoch total loss 1.56561565\n",
      "Trained batch 1160 batch loss 1.42652798 epoch total loss 1.56549573\n",
      "Trained batch 1161 batch loss 1.4530611 epoch total loss 1.56539893\n",
      "Trained batch 1162 batch loss 1.43084979 epoch total loss 1.56528318\n",
      "Trained batch 1163 batch loss 1.43549192 epoch total loss 1.5651716\n",
      "Trained batch 1164 batch loss 1.36735237 epoch total loss 1.56500161\n",
      "Trained batch 1165 batch loss 1.45982957 epoch total loss 1.56491137\n",
      "Trained batch 1166 batch loss 1.37021 epoch total loss 1.56474435\n",
      "Trained batch 1167 batch loss 1.37555623 epoch total loss 1.56458235\n",
      "Trained batch 1168 batch loss 1.36347342 epoch total loss 1.56441021\n",
      "Trained batch 1169 batch loss 1.41641819 epoch total loss 1.56428349\n",
      "Trained batch 1170 batch loss 1.50714183 epoch total loss 1.56423473\n",
      "Trained batch 1171 batch loss 1.45322597 epoch total loss 1.56414\n",
      "Trained batch 1172 batch loss 1.45613503 epoch total loss 1.56404781\n",
      "Trained batch 1173 batch loss 1.49545979 epoch total loss 1.5639894\n",
      "Trained batch 1174 batch loss 1.44893491 epoch total loss 1.56389141\n",
      "Trained batch 1175 batch loss 1.41335428 epoch total loss 1.56376326\n",
      "Trained batch 1176 batch loss 1.39008081 epoch total loss 1.56361568\n",
      "Trained batch 1177 batch loss 1.42163563 epoch total loss 1.56349504\n",
      "Trained batch 1178 batch loss 1.36638796 epoch total loss 1.56332767\n",
      "Trained batch 1179 batch loss 1.35830712 epoch total loss 1.56315374\n",
      "Trained batch 1180 batch loss 1.50547624 epoch total loss 1.56310487\n",
      "Trained batch 1181 batch loss 1.47717881 epoch total loss 1.56303215\n",
      "Trained batch 1182 batch loss 1.37965345 epoch total loss 1.56287694\n",
      "Trained batch 1183 batch loss 1.39021742 epoch total loss 1.56273103\n",
      "Trained batch 1184 batch loss 1.42088103 epoch total loss 1.56261122\n",
      "Trained batch 1185 batch loss 1.51948333 epoch total loss 1.56257486\n",
      "Trained batch 1186 batch loss 1.44381917 epoch total loss 1.56247473\n",
      "Trained batch 1187 batch loss 1.60149658 epoch total loss 1.56250763\n",
      "Trained batch 1188 batch loss 1.5386529 epoch total loss 1.5624876\n",
      "Trained batch 1189 batch loss 1.45770693 epoch total loss 1.56239951\n",
      "Trained batch 1190 batch loss 1.40734053 epoch total loss 1.56226921\n",
      "Trained batch 1191 batch loss 1.51637685 epoch total loss 1.56223059\n",
      "Trained batch 1192 batch loss 1.4785527 epoch total loss 1.56216037\n",
      "Trained batch 1193 batch loss 1.42624986 epoch total loss 1.56204653\n",
      "Trained batch 1194 batch loss 1.42222071 epoch total loss 1.56192946\n",
      "Trained batch 1195 batch loss 1.51032364 epoch total loss 1.56188631\n",
      "Trained batch 1196 batch loss 1.40931165 epoch total loss 1.56175876\n",
      "Trained batch 1197 batch loss 1.54927075 epoch total loss 1.56174827\n",
      "Trained batch 1198 batch loss 1.53800893 epoch total loss 1.56172848\n",
      "Trained batch 1199 batch loss 1.47027743 epoch total loss 1.56165218\n",
      "Trained batch 1200 batch loss 1.44791305 epoch total loss 1.56155741\n",
      "Trained batch 1201 batch loss 1.46509981 epoch total loss 1.56147707\n",
      "Trained batch 1202 batch loss 1.47240365 epoch total loss 1.56140304\n",
      "Trained batch 1203 batch loss 1.45847833 epoch total loss 1.56131744\n",
      "Trained batch 1204 batch loss 1.37201285 epoch total loss 1.56116033\n",
      "Trained batch 1205 batch loss 1.49049735 epoch total loss 1.56110156\n",
      "Trained batch 1206 batch loss 1.41583705 epoch total loss 1.56098115\n",
      "Trained batch 1207 batch loss 1.38628769 epoch total loss 1.56083643\n",
      "Trained batch 1208 batch loss 1.30841672 epoch total loss 1.56062746\n",
      "Trained batch 1209 batch loss 1.3101778 epoch total loss 1.56042039\n",
      "Trained batch 1210 batch loss 1.28157437 epoch total loss 1.56019\n",
      "Trained batch 1211 batch loss 1.51891947 epoch total loss 1.56015587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1212 batch loss 1.4447757 epoch total loss 1.56006074\n",
      "Trained batch 1213 batch loss 1.53712928 epoch total loss 1.56004179\n",
      "Trained batch 1214 batch loss 1.635463 epoch total loss 1.56010389\n",
      "Trained batch 1215 batch loss 1.25254083 epoch total loss 1.55985081\n",
      "Trained batch 1216 batch loss 1.29328096 epoch total loss 1.55963171\n",
      "Trained batch 1217 batch loss 1.23228824 epoch total loss 1.55936265\n",
      "Trained batch 1218 batch loss 1.47042036 epoch total loss 1.55928969\n",
      "Trained batch 1219 batch loss 1.63970113 epoch total loss 1.55935562\n",
      "Trained batch 1220 batch loss 1.71738613 epoch total loss 1.5594852\n",
      "Trained batch 1221 batch loss 1.49398422 epoch total loss 1.55943155\n",
      "Trained batch 1222 batch loss 1.29564619 epoch total loss 1.55921566\n",
      "Trained batch 1223 batch loss 1.54948843 epoch total loss 1.55920768\n",
      "Trained batch 1224 batch loss 1.46614814 epoch total loss 1.55913174\n",
      "Trained batch 1225 batch loss 1.5470407 epoch total loss 1.55912185\n",
      "Trained batch 1226 batch loss 1.48931849 epoch total loss 1.55906487\n",
      "Trained batch 1227 batch loss 1.4921838 epoch total loss 1.55901027\n",
      "Trained batch 1228 batch loss 1.56527591 epoch total loss 1.55901539\n",
      "Trained batch 1229 batch loss 1.45933008 epoch total loss 1.55893433\n",
      "Trained batch 1230 batch loss 1.50513625 epoch total loss 1.55889058\n",
      "Trained batch 1231 batch loss 1.40574694 epoch total loss 1.55876625\n",
      "Trained batch 1232 batch loss 1.50664258 epoch total loss 1.55872381\n",
      "Trained batch 1233 batch loss 1.40531135 epoch total loss 1.55859935\n",
      "Trained batch 1234 batch loss 1.48621213 epoch total loss 1.5585407\n",
      "Trained batch 1235 batch loss 1.42811978 epoch total loss 1.55843508\n",
      "Trained batch 1236 batch loss 1.32137764 epoch total loss 1.55824339\n",
      "Trained batch 1237 batch loss 1.37360823 epoch total loss 1.55809414\n",
      "Trained batch 1238 batch loss 1.45344043 epoch total loss 1.55800962\n",
      "Trained batch 1239 batch loss 1.5421381 epoch total loss 1.55799675\n",
      "Trained batch 1240 batch loss 1.72282386 epoch total loss 1.55812967\n",
      "Trained batch 1241 batch loss 1.64905643 epoch total loss 1.55820298\n",
      "Trained batch 1242 batch loss 1.59758615 epoch total loss 1.55823457\n",
      "Trained batch 1243 batch loss 1.55893826 epoch total loss 1.55823517\n",
      "Trained batch 1244 batch loss 1.56841397 epoch total loss 1.55824339\n",
      "Trained batch 1245 batch loss 1.3918469 epoch total loss 1.55810964\n",
      "Trained batch 1246 batch loss 1.37717545 epoch total loss 1.55796444\n",
      "Trained batch 1247 batch loss 1.46552241 epoch total loss 1.55789042\n",
      "Trained batch 1248 batch loss 1.52573371 epoch total loss 1.55786467\n",
      "Trained batch 1249 batch loss 1.44176853 epoch total loss 1.55777168\n",
      "Trained batch 1250 batch loss 1.43571198 epoch total loss 1.55767405\n",
      "Trained batch 1251 batch loss 1.43979168 epoch total loss 1.55757976\n",
      "Trained batch 1252 batch loss 1.3467629 epoch total loss 1.55741143\n",
      "Trained batch 1253 batch loss 1.43243527 epoch total loss 1.55731177\n",
      "Trained batch 1254 batch loss 1.37762344 epoch total loss 1.55716848\n",
      "Trained batch 1255 batch loss 1.42448425 epoch total loss 1.55706263\n",
      "Trained batch 1256 batch loss 1.41884196 epoch total loss 1.5569526\n",
      "Trained batch 1257 batch loss 1.48329258 epoch total loss 1.55689394\n",
      "Trained batch 1258 batch loss 1.31664968 epoch total loss 1.55670297\n",
      "Trained batch 1259 batch loss 1.324422 epoch total loss 1.55651855\n",
      "Trained batch 1260 batch loss 1.35230374 epoch total loss 1.55635643\n",
      "Trained batch 1261 batch loss 1.53802407 epoch total loss 1.55634189\n",
      "Trained batch 1262 batch loss 1.48593688 epoch total loss 1.5562861\n",
      "Trained batch 1263 batch loss 1.45441008 epoch total loss 1.55620551\n",
      "Trained batch 1264 batch loss 1.53782344 epoch total loss 1.55619097\n",
      "Trained batch 1265 batch loss 1.38107967 epoch total loss 1.55605257\n",
      "Trained batch 1266 batch loss 1.37844682 epoch total loss 1.55591226\n",
      "Trained batch 1267 batch loss 1.48166192 epoch total loss 1.55585372\n",
      "Trained batch 1268 batch loss 1.5174526 epoch total loss 1.55582345\n",
      "Trained batch 1269 batch loss 1.43800199 epoch total loss 1.55573058\n",
      "Trained batch 1270 batch loss 1.38728011 epoch total loss 1.5555979\n",
      "Trained batch 1271 batch loss 1.43580747 epoch total loss 1.55550373\n",
      "Trained batch 1272 batch loss 1.36788344 epoch total loss 1.55535614\n",
      "Trained batch 1273 batch loss 1.5382961 epoch total loss 1.55534279\n",
      "Trained batch 1274 batch loss 1.51017785 epoch total loss 1.55530739\n",
      "Trained batch 1275 batch loss 1.46361685 epoch total loss 1.55523539\n",
      "Trained batch 1276 batch loss 1.37409055 epoch total loss 1.55509353\n",
      "Trained batch 1277 batch loss 1.35503006 epoch total loss 1.55493677\n",
      "Trained batch 1278 batch loss 1.36831141 epoch total loss 1.55479074\n",
      "Trained batch 1279 batch loss 1.3484 epoch total loss 1.55462933\n",
      "Trained batch 1280 batch loss 1.49500382 epoch total loss 1.55458283\n",
      "Trained batch 1281 batch loss 1.43485045 epoch total loss 1.55448925\n",
      "Trained batch 1282 batch loss 1.27881503 epoch total loss 1.5542742\n",
      "Trained batch 1283 batch loss 1.31892979 epoch total loss 1.55409086\n",
      "Trained batch 1284 batch loss 1.25523341 epoch total loss 1.55385816\n",
      "Trained batch 1285 batch loss 1.29602599 epoch total loss 1.55365741\n",
      "Trained batch 1286 batch loss 1.49285626 epoch total loss 1.55361009\n",
      "Trained batch 1287 batch loss 1.5026201 epoch total loss 1.55357051\n",
      "Trained batch 1288 batch loss 1.63618886 epoch total loss 1.55363464\n",
      "Trained batch 1289 batch loss 1.49088705 epoch total loss 1.55358589\n",
      "Trained batch 1290 batch loss 1.49117446 epoch total loss 1.55353761\n",
      "Trained batch 1291 batch loss 1.50809884 epoch total loss 1.55350232\n",
      "Trained batch 1292 batch loss 1.47092175 epoch total loss 1.55343843\n",
      "Trained batch 1293 batch loss 1.5110383 epoch total loss 1.55340564\n",
      "Trained batch 1294 batch loss 1.57222533 epoch total loss 1.55342019\n",
      "Trained batch 1295 batch loss 1.54207015 epoch total loss 1.55341148\n",
      "Trained batch 1296 batch loss 1.58571315 epoch total loss 1.5534364\n",
      "Trained batch 1297 batch loss 1.56037521 epoch total loss 1.55344176\n",
      "Trained batch 1298 batch loss 1.48881364 epoch total loss 1.55339193\n",
      "Trained batch 1299 batch loss 1.44874072 epoch total loss 1.55331135\n",
      "Trained batch 1300 batch loss 1.3561542 epoch total loss 1.55315971\n",
      "Trained batch 1301 batch loss 1.31775248 epoch total loss 1.55297875\n",
      "Trained batch 1302 batch loss 1.47226012 epoch total loss 1.55291677\n",
      "Trained batch 1303 batch loss 1.41046989 epoch total loss 1.55280757\n",
      "Trained batch 1304 batch loss 1.4468441 epoch total loss 1.55272627\n",
      "Trained batch 1305 batch loss 1.51587534 epoch total loss 1.55269814\n",
      "Trained batch 1306 batch loss 1.47944427 epoch total loss 1.55264199\n",
      "Trained batch 1307 batch loss 1.45181489 epoch total loss 1.55256486\n",
      "Trained batch 1308 batch loss 1.36934209 epoch total loss 1.55242479\n",
      "Trained batch 1309 batch loss 1.45558357 epoch total loss 1.55235076\n",
      "Trained batch 1310 batch loss 1.45229924 epoch total loss 1.55227447\n",
      "Trained batch 1311 batch loss 1.49025726 epoch total loss 1.55222714\n",
      "Trained batch 1312 batch loss 1.44214952 epoch total loss 1.55214322\n",
      "Trained batch 1313 batch loss 1.41774511 epoch total loss 1.55204082\n",
      "Trained batch 1314 batch loss 1.36250949 epoch total loss 1.55189657\n",
      "Trained batch 1315 batch loss 1.47037435 epoch total loss 1.55183458\n",
      "Trained batch 1316 batch loss 1.50085378 epoch total loss 1.55179584\n",
      "Trained batch 1317 batch loss 1.46363568 epoch total loss 1.55172884\n",
      "Trained batch 1318 batch loss 1.48718095 epoch total loss 1.55168\n",
      "Trained batch 1319 batch loss 1.59654772 epoch total loss 1.55171394\n",
      "Trained batch 1320 batch loss 1.51700056 epoch total loss 1.55168772\n",
      "Trained batch 1321 batch loss 1.48592377 epoch total loss 1.55163789\n",
      "Trained batch 1322 batch loss 1.44573009 epoch total loss 1.55155778\n",
      "Trained batch 1323 batch loss 1.48075294 epoch total loss 1.55150425\n",
      "Trained batch 1324 batch loss 1.29823172 epoch total loss 1.55131304\n",
      "Trained batch 1325 batch loss 1.40347838 epoch total loss 1.55120158\n",
      "Trained batch 1326 batch loss 1.60990131 epoch total loss 1.55124581\n",
      "Trained batch 1327 batch loss 1.42794144 epoch total loss 1.55115294\n",
      "Trained batch 1328 batch loss 1.53779185 epoch total loss 1.55114281\n",
      "Trained batch 1329 batch loss 1.6159122 epoch total loss 1.55119169\n",
      "Trained batch 1330 batch loss 1.42443919 epoch total loss 1.55109644\n",
      "Trained batch 1331 batch loss 1.4400903 epoch total loss 1.55101311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1332 batch loss 1.33331037 epoch total loss 1.55084968\n",
      "Trained batch 1333 batch loss 1.48448765 epoch total loss 1.55079973\n",
      "Trained batch 1334 batch loss 1.41766024 epoch total loss 1.5507\n",
      "Trained batch 1335 batch loss 1.47085965 epoch total loss 1.55064023\n",
      "Trained batch 1336 batch loss 1.52677131 epoch total loss 1.55062246\n",
      "Trained batch 1337 batch loss 1.60865426 epoch total loss 1.55066586\n",
      "Trained batch 1338 batch loss 1.59285355 epoch total loss 1.55069733\n",
      "Trained batch 1339 batch loss 1.53020465 epoch total loss 1.55068207\n",
      "Trained batch 1340 batch loss 1.59363949 epoch total loss 1.55071425\n",
      "Trained batch 1341 batch loss 1.54178619 epoch total loss 1.55070746\n",
      "Trained batch 1342 batch loss 1.49614358 epoch total loss 1.55066681\n",
      "Trained batch 1343 batch loss 1.4937284 epoch total loss 1.55062437\n",
      "Trained batch 1344 batch loss 1.54876077 epoch total loss 1.55062306\n",
      "Trained batch 1345 batch loss 1.47132182 epoch total loss 1.55056417\n",
      "Trained batch 1346 batch loss 1.46918166 epoch total loss 1.55050373\n",
      "Trained batch 1347 batch loss 1.45359123 epoch total loss 1.55043185\n",
      "Trained batch 1348 batch loss 1.32737112 epoch total loss 1.55026639\n",
      "Trained batch 1349 batch loss 1.30368304 epoch total loss 1.55008352\n",
      "Trained batch 1350 batch loss 1.27633369 epoch total loss 1.54988086\n",
      "Trained batch 1351 batch loss 1.62426162 epoch total loss 1.54993594\n",
      "Trained batch 1352 batch loss 1.66341901 epoch total loss 1.55001974\n",
      "Trained batch 1353 batch loss 1.55351901 epoch total loss 1.55002224\n",
      "Trained batch 1354 batch loss 1.42768848 epoch total loss 1.549932\n",
      "Trained batch 1355 batch loss 1.47874188 epoch total loss 1.54987943\n",
      "Trained batch 1356 batch loss 1.39137983 epoch total loss 1.54976261\n",
      "Trained batch 1357 batch loss 1.40212452 epoch total loss 1.54965377\n",
      "Trained batch 1358 batch loss 1.44172 epoch total loss 1.54957426\n",
      "Trained batch 1359 batch loss 1.43369377 epoch total loss 1.5494889\n",
      "Trained batch 1360 batch loss 1.46559811 epoch total loss 1.54942715\n",
      "Trained batch 1361 batch loss 1.46301198 epoch total loss 1.54936361\n",
      "Trained batch 1362 batch loss 1.43655074 epoch total loss 1.54928076\n",
      "Trained batch 1363 batch loss 1.44843805 epoch total loss 1.54920673\n",
      "Trained batch 1364 batch loss 1.37576032 epoch total loss 1.54907966\n",
      "Trained batch 1365 batch loss 1.41051888 epoch total loss 1.54897797\n",
      "Trained batch 1366 batch loss 1.4372381 epoch total loss 1.54889619\n",
      "Trained batch 1367 batch loss 1.48468339 epoch total loss 1.54884923\n",
      "Trained batch 1368 batch loss 1.44645178 epoch total loss 1.54877436\n",
      "Trained batch 1369 batch loss 1.29211235 epoch total loss 1.54858685\n",
      "Trained batch 1370 batch loss 1.36875951 epoch total loss 1.54845548\n",
      "Trained batch 1371 batch loss 1.39993322 epoch total loss 1.54834712\n",
      "Trained batch 1372 batch loss 1.37633121 epoch total loss 1.54822171\n",
      "Trained batch 1373 batch loss 1.3774699 epoch total loss 1.54809725\n",
      "Trained batch 1374 batch loss 1.39627147 epoch total loss 1.54798675\n",
      "Trained batch 1375 batch loss 1.24453318 epoch total loss 1.54776621\n",
      "Trained batch 1376 batch loss 1.4733628 epoch total loss 1.54771209\n",
      "Trained batch 1377 batch loss 1.4223398 epoch total loss 1.54762113\n",
      "Trained batch 1378 batch loss 1.46107244 epoch total loss 1.54755831\n",
      "Trained batch 1379 batch loss 1.48784542 epoch total loss 1.54751503\n",
      "Trained batch 1380 batch loss 1.36112344 epoch total loss 1.54738\n",
      "Trained batch 1381 batch loss 1.4122653 epoch total loss 1.5472821\n",
      "Trained batch 1382 batch loss 1.38989019 epoch total loss 1.54716825\n",
      "Trained batch 1383 batch loss 1.49058151 epoch total loss 1.54712725\n",
      "Trained batch 1384 batch loss 1.53303218 epoch total loss 1.54711699\n",
      "Trained batch 1385 batch loss 1.51185679 epoch total loss 1.5470916\n",
      "Trained batch 1386 batch loss 1.47642851 epoch total loss 1.54704058\n",
      "Trained batch 1387 batch loss 1.26982546 epoch total loss 1.54684067\n",
      "Trained batch 1388 batch loss 1.40313172 epoch total loss 1.54673707\n",
      "Epoch 1 train loss 1.5467370748519897\n",
      "Validated batch 1 batch loss 1.40736938\n",
      "Validated batch 2 batch loss 1.41758573\n",
      "Validated batch 3 batch loss 1.32956195\n",
      "Validated batch 4 batch loss 1.51788747\n",
      "Validated batch 5 batch loss 1.42135954\n",
      "Validated batch 6 batch loss 1.45235324\n",
      "Validated batch 7 batch loss 1.51392245\n",
      "Validated batch 8 batch loss 1.50056052\n",
      "Validated batch 9 batch loss 1.48012364\n",
      "Validated batch 10 batch loss 1.4915998\n",
      "Validated batch 11 batch loss 1.52525926\n",
      "Validated batch 12 batch loss 1.43736875\n",
      "Validated batch 13 batch loss 1.3998878\n",
      "Validated batch 14 batch loss 1.52079153\n",
      "Validated batch 15 batch loss 1.46591926\n",
      "Validated batch 16 batch loss 1.42645597\n",
      "Validated batch 17 batch loss 1.53796959\n",
      "Validated batch 18 batch loss 1.28940392\n",
      "Validated batch 19 batch loss 1.44732511\n",
      "Validated batch 20 batch loss 1.31385326\n",
      "Validated batch 21 batch loss 1.46344709\n",
      "Validated batch 22 batch loss 1.48719311\n",
      "Validated batch 23 batch loss 1.37803841\n",
      "Validated batch 24 batch loss 1.40934682\n",
      "Validated batch 25 batch loss 1.40960228\n",
      "Validated batch 26 batch loss 1.36584163\n",
      "Validated batch 27 batch loss 1.38386321\n",
      "Validated batch 28 batch loss 1.46040833\n",
      "Validated batch 29 batch loss 1.43368506\n",
      "Validated batch 30 batch loss 1.47924221\n",
      "Validated batch 31 batch loss 1.39206541\n",
      "Validated batch 32 batch loss 1.42963684\n",
      "Validated batch 33 batch loss 1.48451018\n",
      "Validated batch 34 batch loss 1.49600589\n",
      "Validated batch 35 batch loss 1.4522202\n",
      "Validated batch 36 batch loss 1.39453745\n",
      "Validated batch 37 batch loss 1.37075055\n",
      "Validated batch 38 batch loss 1.45716262\n",
      "Validated batch 39 batch loss 1.46239328\n",
      "Validated batch 40 batch loss 1.49041367\n",
      "Validated batch 41 batch loss 1.48298025\n",
      "Validated batch 42 batch loss 1.39286268\n",
      "Validated batch 43 batch loss 1.49937391\n",
      "Validated batch 44 batch loss 1.36051583\n",
      "Validated batch 45 batch loss 1.39953983\n",
      "Validated batch 46 batch loss 1.46761787\n",
      "Validated batch 47 batch loss 1.53820407\n",
      "Validated batch 48 batch loss 1.42676961\n",
      "Validated batch 49 batch loss 1.41339874\n",
      "Validated batch 50 batch loss 1.39641869\n",
      "Validated batch 51 batch loss 1.39806223\n",
      "Validated batch 52 batch loss 1.45490241\n",
      "Validated batch 53 batch loss 1.46393037\n",
      "Validated batch 54 batch loss 1.34819627\n",
      "Validated batch 55 batch loss 1.44172049\n",
      "Validated batch 56 batch loss 1.40727496\n",
      "Validated batch 57 batch loss 1.43695152\n",
      "Validated batch 58 batch loss 1.42312634\n",
      "Validated batch 59 batch loss 1.43482924\n",
      "Validated batch 60 batch loss 1.3747766\n",
      "Validated batch 61 batch loss 1.38035512\n",
      "Validated batch 62 batch loss 1.44828904\n",
      "Validated batch 63 batch loss 1.46054161\n",
      "Validated batch 64 batch loss 1.45418811\n",
      "Validated batch 65 batch loss 1.56300759\n",
      "Validated batch 66 batch loss 1.63459766\n",
      "Validated batch 67 batch loss 1.49634981\n",
      "Validated batch 68 batch loss 1.46338904\n",
      "Validated batch 69 batch loss 1.32146072\n",
      "Validated batch 70 batch loss 1.36146832\n",
      "Validated batch 71 batch loss 1.39920831\n",
      "Validated batch 72 batch loss 1.42192161\n",
      "Validated batch 73 batch loss 1.33475304\n",
      "Validated batch 74 batch loss 1.40560842\n",
      "Validated batch 75 batch loss 1.48274767\n",
      "Validated batch 76 batch loss 1.48787165\n",
      "Validated batch 77 batch loss 1.4867115\n",
      "Validated batch 78 batch loss 1.46458149\n",
      "Validated batch 79 batch loss 1.47281909\n",
      "Validated batch 80 batch loss 1.41839504\n",
      "Validated batch 81 batch loss 1.50178957\n",
      "Validated batch 82 batch loss 1.45446324\n",
      "Validated batch 83 batch loss 1.51072443\n",
      "Validated batch 84 batch loss 1.51576877\n",
      "Validated batch 85 batch loss 1.45404148\n",
      "Validated batch 86 batch loss 1.46612167\n",
      "Validated batch 87 batch loss 1.32168233\n",
      "Validated batch 88 batch loss 1.40737414\n",
      "Validated batch 89 batch loss 1.46456\n",
      "Validated batch 90 batch loss 1.47371018\n",
      "Validated batch 91 batch loss 1.43621302\n",
      "Validated batch 92 batch loss 1.40278602\n",
      "Validated batch 93 batch loss 1.50747621\n",
      "Validated batch 94 batch loss 1.41538322\n",
      "Validated batch 95 batch loss 1.40202332\n",
      "Validated batch 96 batch loss 1.38101673\n",
      "Validated batch 97 batch loss 1.39806259\n",
      "Validated batch 98 batch loss 1.53007317\n",
      "Validated batch 99 batch loss 1.34333479\n",
      "Validated batch 100 batch loss 1.46106982\n",
      "Validated batch 101 batch loss 1.45729196\n",
      "Validated batch 102 batch loss 1.44560921\n",
      "Validated batch 103 batch loss 1.41527736\n",
      "Validated batch 104 batch loss 1.34538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 105 batch loss 1.48264241\n",
      "Validated batch 106 batch loss 1.45882702\n",
      "Validated batch 107 batch loss 1.46544552\n",
      "Validated batch 108 batch loss 1.41982841\n",
      "Validated batch 109 batch loss 1.43062079\n",
      "Validated batch 110 batch loss 1.34544253\n",
      "Validated batch 111 batch loss 1.36589074\n",
      "Validated batch 112 batch loss 1.41971183\n",
      "Validated batch 113 batch loss 1.34148884\n",
      "Validated batch 114 batch loss 1.42609751\n",
      "Validated batch 115 batch loss 1.42824018\n",
      "Validated batch 116 batch loss 1.54454029\n",
      "Validated batch 117 batch loss 1.51043165\n",
      "Validated batch 118 batch loss 1.35597932\n",
      "Validated batch 119 batch loss 1.30314934\n",
      "Validated batch 120 batch loss 1.42250514\n",
      "Validated batch 121 batch loss 1.39813185\n",
      "Validated batch 122 batch loss 1.39807022\n",
      "Validated batch 123 batch loss 1.38474178\n",
      "Validated batch 124 batch loss 1.34351623\n",
      "Validated batch 125 batch loss 1.50426066\n",
      "Validated batch 126 batch loss 1.3597883\n",
      "Validated batch 127 batch loss 1.37080729\n",
      "Validated batch 128 batch loss 1.37083542\n",
      "Validated batch 129 batch loss 1.53112018\n",
      "Validated batch 130 batch loss 1.44787049\n",
      "Validated batch 131 batch loss 1.52228582\n",
      "Validated batch 132 batch loss 1.38522172\n",
      "Validated batch 133 batch loss 1.5174799\n",
      "Validated batch 134 batch loss 1.39871693\n",
      "Validated batch 135 batch loss 1.49330986\n",
      "Validated batch 136 batch loss 1.4924233\n",
      "Validated batch 137 batch loss 1.23945272\n",
      "Validated batch 138 batch loss 1.44634438\n",
      "Validated batch 139 batch loss 1.38968062\n",
      "Validated batch 140 batch loss 1.4556191\n",
      "Validated batch 141 batch loss 1.5042479\n",
      "Validated batch 142 batch loss 1.43605685\n",
      "Validated batch 143 batch loss 1.44355953\n",
      "Validated batch 144 batch loss 1.55990756\n",
      "Validated batch 145 batch loss 1.31726861\n",
      "Validated batch 146 batch loss 1.45956731\n",
      "Validated batch 147 batch loss 1.4550662\n",
      "Validated batch 148 batch loss 1.43909836\n",
      "Validated batch 149 batch loss 1.47505832\n",
      "Validated batch 150 batch loss 1.40222931\n",
      "Validated batch 151 batch loss 1.21454215\n",
      "Validated batch 152 batch loss 1.45173812\n",
      "Validated batch 153 batch loss 1.43858814\n",
      "Validated batch 154 batch loss 1.44310212\n",
      "Validated batch 155 batch loss 1.44533086\n",
      "Validated batch 156 batch loss 1.39019632\n",
      "Validated batch 157 batch loss 1.47319877\n",
      "Validated batch 158 batch loss 1.49917114\n",
      "Validated batch 159 batch loss 1.43584692\n",
      "Validated batch 160 batch loss 1.42975795\n",
      "Validated batch 161 batch loss 1.35761237\n",
      "Validated batch 162 batch loss 1.39856529\n",
      "Validated batch 163 batch loss 1.44922948\n",
      "Validated batch 164 batch loss 1.42798495\n",
      "Validated batch 165 batch loss 1.26342106\n",
      "Validated batch 166 batch loss 1.36803603\n",
      "Validated batch 167 batch loss 1.48867154\n",
      "Validated batch 168 batch loss 1.32977629\n",
      "Validated batch 169 batch loss 1.36518955\n",
      "Validated batch 170 batch loss 1.402022\n",
      "Validated batch 171 batch loss 1.45199323\n",
      "Validated batch 172 batch loss 1.39557683\n",
      "Validated batch 173 batch loss 1.4365834\n",
      "Validated batch 174 batch loss 1.36935174\n",
      "Validated batch 175 batch loss 1.47176\n",
      "Validated batch 176 batch loss 1.46066165\n",
      "Validated batch 177 batch loss 1.52691936\n",
      "Validated batch 178 batch loss 1.41135657\n",
      "Validated batch 179 batch loss 1.5306493\n",
      "Validated batch 180 batch loss 1.36955237\n",
      "Validated batch 181 batch loss 1.32048821\n",
      "Validated batch 182 batch loss 1.49682689\n",
      "Validated batch 183 batch loss 1.39245129\n",
      "Validated batch 184 batch loss 1.51792645\n",
      "Validated batch 185 batch loss 1.57042813\n",
      "Epoch 1 val loss 1.4328917264938354\n",
      "Model .//model-epoch-1-loss-1.4329.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.40189338 epoch total loss 1.40189338\n",
      "Trained batch 2 batch loss 1.44458914 epoch total loss 1.42324126\n",
      "Trained batch 3 batch loss 1.3851527 epoch total loss 1.41054499\n",
      "Trained batch 4 batch loss 1.26571715 epoch total loss 1.37433803\n",
      "Trained batch 5 batch loss 1.31883454 epoch total loss 1.36323738\n",
      "Trained batch 6 batch loss 1.36973166 epoch total loss 1.3643198\n",
      "Trained batch 7 batch loss 1.34106588 epoch total loss 1.36099792\n",
      "Trained batch 8 batch loss 1.45731 epoch total loss 1.37303686\n",
      "Trained batch 9 batch loss 1.46084201 epoch total loss 1.38279295\n",
      "Trained batch 10 batch loss 1.4044224 epoch total loss 1.384956\n",
      "Trained batch 11 batch loss 1.41417885 epoch total loss 1.38761258\n",
      "Trained batch 12 batch loss 1.47996926 epoch total loss 1.39530897\n",
      "Trained batch 13 batch loss 1.45060313 epoch total loss 1.39956236\n",
      "Trained batch 14 batch loss 1.39823604 epoch total loss 1.39946759\n",
      "Trained batch 15 batch loss 1.45823503 epoch total loss 1.4033854\n",
      "Trained batch 16 batch loss 1.42427993 epoch total loss 1.40469134\n",
      "Trained batch 17 batch loss 1.29025412 epoch total loss 1.39795971\n",
      "Trained batch 18 batch loss 1.34754992 epoch total loss 1.39515924\n",
      "Trained batch 19 batch loss 1.40370893 epoch total loss 1.39560926\n",
      "Trained batch 20 batch loss 1.36189568 epoch total loss 1.39392352\n",
      "Trained batch 21 batch loss 1.36674392 epoch total loss 1.39262927\n",
      "Trained batch 22 batch loss 1.40196407 epoch total loss 1.39305353\n",
      "Trained batch 23 batch loss 1.34911108 epoch total loss 1.39114308\n",
      "Trained batch 24 batch loss 1.39025533 epoch total loss 1.39110613\n",
      "Trained batch 25 batch loss 1.57362449 epoch total loss 1.39840686\n",
      "Trained batch 26 batch loss 1.43900895 epoch total loss 1.3999685\n",
      "Trained batch 27 batch loss 1.30056679 epoch total loss 1.39628696\n",
      "Trained batch 28 batch loss 1.34459496 epoch total loss 1.39444077\n",
      "Trained batch 29 batch loss 1.22543716 epoch total loss 1.3886131\n",
      "Trained batch 30 batch loss 1.34018683 epoch total loss 1.38699889\n",
      "Trained batch 31 batch loss 1.37853026 epoch total loss 1.38672566\n",
      "Trained batch 32 batch loss 1.45312715 epoch total loss 1.38880074\n",
      "Trained batch 33 batch loss 1.4095763 epoch total loss 1.38943028\n",
      "Trained batch 34 batch loss 1.42037153 epoch total loss 1.39034033\n",
      "Trained batch 35 batch loss 1.35149956 epoch total loss 1.38923061\n",
      "Trained batch 36 batch loss 1.46852589 epoch total loss 1.39143324\n",
      "Trained batch 37 batch loss 1.49235368 epoch total loss 1.39416087\n",
      "Trained batch 38 batch loss 1.40510428 epoch total loss 1.39444888\n",
      "Trained batch 39 batch loss 1.43759966 epoch total loss 1.39555526\n",
      "Trained batch 40 batch loss 1.34713411 epoch total loss 1.39434469\n",
      "Trained batch 41 batch loss 1.32057357 epoch total loss 1.39254534\n",
      "Trained batch 42 batch loss 1.27624297 epoch total loss 1.38977623\n",
      "Trained batch 43 batch loss 1.35588527 epoch total loss 1.38898802\n",
      "Trained batch 44 batch loss 1.382128 epoch total loss 1.38883221\n",
      "Trained batch 45 batch loss 1.35696054 epoch total loss 1.38812387\n",
      "Trained batch 46 batch loss 1.51193929 epoch total loss 1.39081562\n",
      "Trained batch 47 batch loss 1.33835399 epoch total loss 1.38969946\n",
      "Trained batch 48 batch loss 1.43103898 epoch total loss 1.39056063\n",
      "Trained batch 49 batch loss 1.46283698 epoch total loss 1.3920356\n",
      "Trained batch 50 batch loss 1.31215322 epoch total loss 1.39043808\n",
      "Trained batch 51 batch loss 1.44665456 epoch total loss 1.39154041\n",
      "Trained batch 52 batch loss 1.42602539 epoch total loss 1.39220357\n",
      "Trained batch 53 batch loss 1.41923213 epoch total loss 1.39271355\n",
      "Trained batch 54 batch loss 1.50053155 epoch total loss 1.3947103\n",
      "Trained batch 55 batch loss 1.54172969 epoch total loss 1.39738345\n",
      "Trained batch 56 batch loss 1.5051862 epoch total loss 1.39930844\n",
      "Trained batch 57 batch loss 1.4359895 epoch total loss 1.39995205\n",
      "Trained batch 58 batch loss 1.48112822 epoch total loss 1.40135157\n",
      "Trained batch 59 batch loss 1.50039363 epoch total loss 1.40303028\n",
      "Trained batch 60 batch loss 1.50389183 epoch total loss 1.40471125\n",
      "Trained batch 61 batch loss 1.40099478 epoch total loss 1.40465033\n",
      "Trained batch 62 batch loss 1.39315641 epoch total loss 1.40446496\n",
      "Trained batch 63 batch loss 1.34960568 epoch total loss 1.40359426\n",
      "Trained batch 64 batch loss 1.27099991 epoch total loss 1.4015224\n",
      "Trained batch 65 batch loss 1.44295681 epoch total loss 1.40215981\n",
      "Trained batch 66 batch loss 1.48203385 epoch total loss 1.40337\n",
      "Trained batch 67 batch loss 1.44047832 epoch total loss 1.40392387\n",
      "Trained batch 68 batch loss 1.32542229 epoch total loss 1.40276945\n",
      "Trained batch 69 batch loss 1.3127532 epoch total loss 1.40146482\n",
      "Trained batch 70 batch loss 1.39952123 epoch total loss 1.40143704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 71 batch loss 1.37291956 epoch total loss 1.40103531\n",
      "Trained batch 72 batch loss 1.39848554 epoch total loss 1.4009999\n",
      "Trained batch 73 batch loss 1.35650396 epoch total loss 1.40039039\n",
      "Trained batch 74 batch loss 1.45224428 epoch total loss 1.40109122\n",
      "Trained batch 75 batch loss 1.30198395 epoch total loss 1.39976978\n",
      "Trained batch 76 batch loss 1.46676767 epoch total loss 1.40065134\n",
      "Trained batch 77 batch loss 1.40533376 epoch total loss 1.40071213\n",
      "Trained batch 78 batch loss 1.51506591 epoch total loss 1.40217829\n",
      "Trained batch 79 batch loss 1.59849131 epoch total loss 1.40466321\n",
      "Trained batch 80 batch loss 1.6277287 epoch total loss 1.40745151\n",
      "Trained batch 81 batch loss 1.44442511 epoch total loss 1.40790808\n",
      "Trained batch 82 batch loss 1.356269 epoch total loss 1.4072783\n",
      "Trained batch 83 batch loss 1.35204911 epoch total loss 1.40661287\n",
      "Trained batch 84 batch loss 1.55499768 epoch total loss 1.40837944\n",
      "Trained batch 85 batch loss 1.65365744 epoch total loss 1.41126502\n",
      "Trained batch 86 batch loss 1.50111747 epoch total loss 1.41230977\n",
      "Trained batch 87 batch loss 1.45403445 epoch total loss 1.41278934\n",
      "Trained batch 88 batch loss 1.41213238 epoch total loss 1.41278183\n",
      "Trained batch 89 batch loss 1.46628916 epoch total loss 1.41338301\n",
      "Trained batch 90 batch loss 1.45444012 epoch total loss 1.41383922\n",
      "Trained batch 91 batch loss 1.40009153 epoch total loss 1.41368806\n",
      "Trained batch 92 batch loss 1.40381706 epoch total loss 1.41358089\n",
      "Trained batch 93 batch loss 1.43659425 epoch total loss 1.41382837\n",
      "Trained batch 94 batch loss 1.41837847 epoch total loss 1.41387677\n",
      "Trained batch 95 batch loss 1.37156248 epoch total loss 1.41343141\n",
      "Trained batch 96 batch loss 1.45488036 epoch total loss 1.41386318\n",
      "Trained batch 97 batch loss 1.27149916 epoch total loss 1.41239548\n",
      "Trained batch 98 batch loss 1.46526146 epoch total loss 1.4129349\n",
      "Trained batch 99 batch loss 1.57154465 epoch total loss 1.41453707\n",
      "Trained batch 100 batch loss 1.41352427 epoch total loss 1.41452694\n",
      "Trained batch 101 batch loss 1.41614509 epoch total loss 1.41454291\n",
      "Trained batch 102 batch loss 1.34370685 epoch total loss 1.4138484\n",
      "Trained batch 103 batch loss 1.32002079 epoch total loss 1.41293752\n",
      "Trained batch 104 batch loss 1.35479474 epoch total loss 1.41237843\n",
      "Trained batch 105 batch loss 1.36696792 epoch total loss 1.41194606\n",
      "Trained batch 106 batch loss 1.38438821 epoch total loss 1.41168606\n",
      "Trained batch 107 batch loss 1.38391685 epoch total loss 1.41142642\n",
      "Trained batch 108 batch loss 1.53569913 epoch total loss 1.41257715\n",
      "Trained batch 109 batch loss 1.42087007 epoch total loss 1.41265321\n",
      "Trained batch 110 batch loss 1.31610739 epoch total loss 1.41177547\n",
      "Trained batch 111 batch loss 1.44366562 epoch total loss 1.41206276\n",
      "Trained batch 112 batch loss 1.29937649 epoch total loss 1.41105664\n",
      "Trained batch 113 batch loss 1.33271754 epoch total loss 1.41036344\n",
      "Trained batch 114 batch loss 1.36188483 epoch total loss 1.4099381\n",
      "Trained batch 115 batch loss 1.36239314 epoch total loss 1.40952468\n",
      "Trained batch 116 batch loss 1.30233526 epoch total loss 1.40860069\n",
      "Trained batch 117 batch loss 1.33661437 epoch total loss 1.40798533\n",
      "Trained batch 118 batch loss 1.31652343 epoch total loss 1.40721023\n",
      "Trained batch 119 batch loss 1.30254579 epoch total loss 1.40633082\n",
      "Trained batch 120 batch loss 1.23515344 epoch total loss 1.40490425\n",
      "Trained batch 121 batch loss 1.22795606 epoch total loss 1.40344191\n",
      "Trained batch 122 batch loss 1.29232848 epoch total loss 1.40253115\n",
      "Trained batch 123 batch loss 1.35213542 epoch total loss 1.40212142\n",
      "Trained batch 124 batch loss 1.34724271 epoch total loss 1.40167892\n",
      "Trained batch 125 batch loss 1.36665905 epoch total loss 1.40139866\n",
      "Trained batch 126 batch loss 1.45977795 epoch total loss 1.40186203\n",
      "Trained batch 127 batch loss 1.45217192 epoch total loss 1.40225816\n",
      "Trained batch 128 batch loss 1.48773921 epoch total loss 1.40292597\n",
      "Trained batch 129 batch loss 1.44863904 epoch total loss 1.40328038\n",
      "Trained batch 130 batch loss 1.43137693 epoch total loss 1.4034965\n",
      "Trained batch 131 batch loss 1.47511244 epoch total loss 1.4040432\n",
      "Trained batch 132 batch loss 1.43357205 epoch total loss 1.40426695\n",
      "Trained batch 133 batch loss 1.39844131 epoch total loss 1.40422308\n",
      "Trained batch 134 batch loss 1.35717273 epoch total loss 1.40387201\n",
      "Trained batch 135 batch loss 1.31277049 epoch total loss 1.40319717\n",
      "Trained batch 136 batch loss 1.34494 epoch total loss 1.40276885\n",
      "Trained batch 137 batch loss 1.41104877 epoch total loss 1.40282929\n",
      "Trained batch 138 batch loss 1.40996861 epoch total loss 1.40288103\n",
      "Trained batch 139 batch loss 1.67598271 epoch total loss 1.40484571\n",
      "Trained batch 140 batch loss 1.61657119 epoch total loss 1.40635812\n",
      "Trained batch 141 batch loss 1.5645535 epoch total loss 1.40748012\n",
      "Trained batch 142 batch loss 1.4547019 epoch total loss 1.4078126\n",
      "Trained batch 143 batch loss 1.42851579 epoch total loss 1.40795743\n",
      "Trained batch 144 batch loss 1.31278896 epoch total loss 1.40729654\n",
      "Trained batch 145 batch loss 1.32246828 epoch total loss 1.40671146\n",
      "Trained batch 146 batch loss 1.45148635 epoch total loss 1.40701818\n",
      "Trained batch 147 batch loss 1.48800373 epoch total loss 1.40756905\n",
      "Trained batch 148 batch loss 1.37751794 epoch total loss 1.40736604\n",
      "Trained batch 149 batch loss 1.52757907 epoch total loss 1.40817285\n",
      "Trained batch 150 batch loss 1.41725898 epoch total loss 1.40823328\n",
      "Trained batch 151 batch loss 1.39416933 epoch total loss 1.40814018\n",
      "Trained batch 152 batch loss 1.50029707 epoch total loss 1.40874636\n",
      "Trained batch 153 batch loss 1.51755571 epoch total loss 1.40945768\n",
      "Trained batch 154 batch loss 1.45210314 epoch total loss 1.40973461\n",
      "Trained batch 155 batch loss 1.41904855 epoch total loss 1.40979469\n",
      "Trained batch 156 batch loss 1.45510149 epoch total loss 1.41008508\n",
      "Trained batch 157 batch loss 1.48627746 epoch total loss 1.4105705\n",
      "Trained batch 158 batch loss 1.4447192 epoch total loss 1.41078663\n",
      "Trained batch 159 batch loss 1.45555282 epoch total loss 1.41106808\n",
      "Trained batch 160 batch loss 1.39904749 epoch total loss 1.41099298\n",
      "Trained batch 161 batch loss 1.2141521 epoch total loss 1.40977037\n",
      "Trained batch 162 batch loss 1.27333152 epoch total loss 1.40892816\n",
      "Trained batch 163 batch loss 1.21807134 epoch total loss 1.4077574\n",
      "Trained batch 164 batch loss 1.3237766 epoch total loss 1.40724528\n",
      "Trained batch 165 batch loss 1.19036555 epoch total loss 1.40593088\n",
      "Trained batch 166 batch loss 1.15879798 epoch total loss 1.40444207\n",
      "Trained batch 167 batch loss 1.12016582 epoch total loss 1.40273988\n",
      "Trained batch 168 batch loss 1.1130296 epoch total loss 1.4010154\n",
      "Trained batch 169 batch loss 1.41141081 epoch total loss 1.40107691\n",
      "Trained batch 170 batch loss 1.47072065 epoch total loss 1.40148652\n",
      "Trained batch 171 batch loss 1.45392561 epoch total loss 1.40179324\n",
      "Trained batch 172 batch loss 1.40459692 epoch total loss 1.40180957\n",
      "Trained batch 173 batch loss 1.37644196 epoch total loss 1.40166283\n",
      "Trained batch 174 batch loss 1.43180871 epoch total loss 1.40183604\n",
      "Trained batch 175 batch loss 1.43436551 epoch total loss 1.402022\n",
      "Trained batch 176 batch loss 1.65082359 epoch total loss 1.40343559\n",
      "Trained batch 177 batch loss 1.65214705 epoch total loss 1.40484071\n",
      "Trained batch 178 batch loss 1.61249185 epoch total loss 1.40600729\n",
      "Trained batch 179 batch loss 1.53650951 epoch total loss 1.40673637\n",
      "Trained batch 180 batch loss 1.4160881 epoch total loss 1.40678835\n",
      "Trained batch 181 batch loss 1.42026114 epoch total loss 1.40686285\n",
      "Trained batch 182 batch loss 1.4390775 epoch total loss 1.40703976\n",
      "Trained batch 183 batch loss 1.53802609 epoch total loss 1.40775549\n",
      "Trained batch 184 batch loss 1.42687082 epoch total loss 1.40785944\n",
      "Trained batch 185 batch loss 1.44272757 epoch total loss 1.40804791\n",
      "Trained batch 186 batch loss 1.35694575 epoch total loss 1.40777314\n",
      "Trained batch 187 batch loss 1.32728136 epoch total loss 1.40734255\n",
      "Trained batch 188 batch loss 1.37465608 epoch total loss 1.40716875\n",
      "Trained batch 189 batch loss 1.54611242 epoch total loss 1.40790391\n",
      "Trained batch 190 batch loss 1.58519697 epoch total loss 1.40883708\n",
      "Trained batch 191 batch loss 1.61965883 epoch total loss 1.40994084\n",
      "Trained batch 192 batch loss 1.56730044 epoch total loss 1.4107604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 193 batch loss 1.52858508 epoch total loss 1.41137099\n",
      "Trained batch 194 batch loss 1.54720819 epoch total loss 1.41207111\n",
      "Trained batch 195 batch loss 1.59892285 epoch total loss 1.41302943\n",
      "Trained batch 196 batch loss 1.47494936 epoch total loss 1.41334534\n",
      "Trained batch 197 batch loss 1.48299766 epoch total loss 1.41369891\n",
      "Trained batch 198 batch loss 1.41293812 epoch total loss 1.4136951\n",
      "Trained batch 199 batch loss 1.43857718 epoch total loss 1.41382\n",
      "Trained batch 200 batch loss 1.51545691 epoch total loss 1.4143281\n",
      "Trained batch 201 batch loss 1.45625 epoch total loss 1.41453671\n",
      "Trained batch 202 batch loss 1.4696269 epoch total loss 1.41480947\n",
      "Trained batch 203 batch loss 1.35051501 epoch total loss 1.41449273\n",
      "Trained batch 204 batch loss 1.29788899 epoch total loss 1.41392112\n",
      "Trained batch 205 batch loss 1.40229106 epoch total loss 1.41386437\n",
      "Trained batch 206 batch loss 1.35696101 epoch total loss 1.41358817\n",
      "Trained batch 207 batch loss 1.41984916 epoch total loss 1.41361845\n",
      "Trained batch 208 batch loss 1.47458315 epoch total loss 1.41391158\n",
      "Trained batch 209 batch loss 1.57914221 epoch total loss 1.41470206\n",
      "Trained batch 210 batch loss 1.61359775 epoch total loss 1.41564918\n",
      "Trained batch 211 batch loss 1.49011719 epoch total loss 1.41600204\n",
      "Trained batch 212 batch loss 1.5537467 epoch total loss 1.41665173\n",
      "Trained batch 213 batch loss 1.33443618 epoch total loss 1.41626573\n",
      "Trained batch 214 batch loss 1.28196812 epoch total loss 1.41563833\n",
      "Trained batch 215 batch loss 1.18057895 epoch total loss 1.41454494\n",
      "Trained batch 216 batch loss 1.18097413 epoch total loss 1.41346359\n",
      "Trained batch 217 batch loss 1.26313519 epoch total loss 1.41277075\n",
      "Trained batch 218 batch loss 1.34612215 epoch total loss 1.4124651\n",
      "Trained batch 219 batch loss 1.44671988 epoch total loss 1.4126215\n",
      "Trained batch 220 batch loss 1.44401526 epoch total loss 1.41276407\n",
      "Trained batch 221 batch loss 1.49702859 epoch total loss 1.41314542\n",
      "Trained batch 222 batch loss 1.41250336 epoch total loss 1.41314256\n",
      "Trained batch 223 batch loss 1.3884728 epoch total loss 1.41303194\n",
      "Trained batch 224 batch loss 1.43478453 epoch total loss 1.41312897\n",
      "Trained batch 225 batch loss 1.39178145 epoch total loss 1.41303408\n",
      "Trained batch 226 batch loss 1.37287807 epoch total loss 1.41285634\n",
      "Trained batch 227 batch loss 1.38451517 epoch total loss 1.41273153\n",
      "Trained batch 228 batch loss 1.42940736 epoch total loss 1.41280472\n",
      "Trained batch 229 batch loss 1.44688892 epoch total loss 1.41295362\n",
      "Trained batch 230 batch loss 1.3082962 epoch total loss 1.41249859\n",
      "Trained batch 231 batch loss 1.29222286 epoch total loss 1.41197789\n",
      "Trained batch 232 batch loss 1.31345975 epoch total loss 1.41155326\n",
      "Trained batch 233 batch loss 1.15204239 epoch total loss 1.41043937\n",
      "Trained batch 234 batch loss 1.2637248 epoch total loss 1.40981245\n",
      "Trained batch 235 batch loss 1.38830984 epoch total loss 1.4097209\n",
      "Trained batch 236 batch loss 1.43861628 epoch total loss 1.40984344\n",
      "Trained batch 237 batch loss 1.61029112 epoch total loss 1.41068923\n",
      "Trained batch 238 batch loss 1.4726932 epoch total loss 1.41094971\n",
      "Trained batch 239 batch loss 1.45711517 epoch total loss 1.41114295\n",
      "Trained batch 240 batch loss 1.49442708 epoch total loss 1.41148984\n",
      "Trained batch 241 batch loss 1.49943209 epoch total loss 1.41185474\n",
      "Trained batch 242 batch loss 1.41068316 epoch total loss 1.41184986\n",
      "Trained batch 243 batch loss 1.51153183 epoch total loss 1.41226\n",
      "Trained batch 244 batch loss 1.62208366 epoch total loss 1.41311991\n",
      "Trained batch 245 batch loss 1.52341056 epoch total loss 1.41357017\n",
      "Trained batch 246 batch loss 1.35413384 epoch total loss 1.41332841\n",
      "Trained batch 247 batch loss 1.32794631 epoch total loss 1.41298282\n",
      "Trained batch 248 batch loss 1.23803794 epoch total loss 1.41227734\n",
      "Trained batch 249 batch loss 1.24723864 epoch total loss 1.41161466\n",
      "Trained batch 250 batch loss 1.3659029 epoch total loss 1.41143179\n",
      "Trained batch 251 batch loss 1.15686631 epoch total loss 1.41041756\n",
      "Trained batch 252 batch loss 1.17796445 epoch total loss 1.40949512\n",
      "Trained batch 253 batch loss 1.1456064 epoch total loss 1.40845203\n",
      "Trained batch 254 batch loss 1.26204145 epoch total loss 1.40787578\n",
      "Trained batch 255 batch loss 1.29325366 epoch total loss 1.40742624\n",
      "Trained batch 256 batch loss 1.36836338 epoch total loss 1.40727365\n",
      "Trained batch 257 batch loss 1.44938099 epoch total loss 1.40743744\n",
      "Trained batch 258 batch loss 1.48996615 epoch total loss 1.40775728\n",
      "Trained batch 259 batch loss 1.46842432 epoch total loss 1.40799153\n",
      "Trained batch 260 batch loss 1.43100989 epoch total loss 1.40808\n",
      "Trained batch 261 batch loss 1.42979681 epoch total loss 1.40816331\n",
      "Trained batch 262 batch loss 1.35026443 epoch total loss 1.40794218\n",
      "Trained batch 263 batch loss 1.46881914 epoch total loss 1.40817368\n",
      "Trained batch 264 batch loss 1.46911442 epoch total loss 1.40840447\n",
      "Trained batch 265 batch loss 1.41671968 epoch total loss 1.40843582\n",
      "Trained batch 266 batch loss 1.39175153 epoch total loss 1.40837312\n",
      "Trained batch 267 batch loss 1.3591547 epoch total loss 1.40818882\n",
      "Trained batch 268 batch loss 1.37882042 epoch total loss 1.40807927\n",
      "Trained batch 269 batch loss 1.45035779 epoch total loss 1.40823638\n",
      "Trained batch 270 batch loss 1.47653174 epoch total loss 1.40848935\n",
      "Trained batch 271 batch loss 1.45464039 epoch total loss 1.4086597\n",
      "Trained batch 272 batch loss 1.31828308 epoch total loss 1.40832746\n",
      "Trained batch 273 batch loss 1.26562428 epoch total loss 1.40780473\n",
      "Trained batch 274 batch loss 1.42901 epoch total loss 1.40788209\n",
      "Trained batch 275 batch loss 1.48138475 epoch total loss 1.40814936\n",
      "Trained batch 276 batch loss 1.46775639 epoch total loss 1.40836537\n",
      "Trained batch 277 batch loss 1.46044123 epoch total loss 1.40855336\n",
      "Trained batch 278 batch loss 1.50291967 epoch total loss 1.40889287\n",
      "Trained batch 279 batch loss 1.4696008 epoch total loss 1.40911043\n",
      "Trained batch 280 batch loss 1.52510047 epoch total loss 1.40952468\n",
      "Trained batch 281 batch loss 1.41165018 epoch total loss 1.40953219\n",
      "Trained batch 282 batch loss 1.32270014 epoch total loss 1.40922427\n",
      "Trained batch 283 batch loss 1.41628945 epoch total loss 1.40924919\n",
      "Trained batch 284 batch loss 1.33643031 epoch total loss 1.40899277\n",
      "Trained batch 285 batch loss 1.21618807 epoch total loss 1.40831625\n",
      "Trained batch 286 batch loss 1.18111157 epoch total loss 1.40752196\n",
      "Trained batch 287 batch loss 1.19267893 epoch total loss 1.40677333\n",
      "Trained batch 288 batch loss 1.29894137 epoch total loss 1.40639901\n",
      "Trained batch 289 batch loss 1.55101418 epoch total loss 1.40689945\n",
      "Trained batch 290 batch loss 1.48091197 epoch total loss 1.40715468\n",
      "Trained batch 291 batch loss 1.49988031 epoch total loss 1.40747333\n",
      "Trained batch 292 batch loss 1.41808736 epoch total loss 1.40750968\n",
      "Trained batch 293 batch loss 1.38725352 epoch total loss 1.40744054\n",
      "Trained batch 294 batch loss 1.52217865 epoch total loss 1.40783083\n",
      "Trained batch 295 batch loss 1.36353791 epoch total loss 1.40768075\n",
      "Trained batch 296 batch loss 1.44254494 epoch total loss 1.40779841\n",
      "Trained batch 297 batch loss 1.49309528 epoch total loss 1.4080857\n",
      "Trained batch 298 batch loss 1.48587513 epoch total loss 1.40834665\n",
      "Trained batch 299 batch loss 1.60111761 epoch total loss 1.40899134\n",
      "Trained batch 300 batch loss 1.40340233 epoch total loss 1.40897274\n",
      "Trained batch 301 batch loss 1.47952938 epoch total loss 1.40920711\n",
      "Trained batch 302 batch loss 1.30726445 epoch total loss 1.4088695\n",
      "Trained batch 303 batch loss 1.29376149 epoch total loss 1.4084897\n",
      "Trained batch 304 batch loss 1.36372972 epoch total loss 1.40834248\n",
      "Trained batch 305 batch loss 1.49088264 epoch total loss 1.40861309\n",
      "Trained batch 306 batch loss 1.41475666 epoch total loss 1.40863311\n",
      "Trained batch 307 batch loss 1.45762587 epoch total loss 1.40879273\n",
      "Trained batch 308 batch loss 1.47715878 epoch total loss 1.4090147\n",
      "Trained batch 309 batch loss 1.44183326 epoch total loss 1.40912092\n",
      "Trained batch 310 batch loss 1.40931368 epoch total loss 1.40912151\n",
      "Trained batch 311 batch loss 1.47381222 epoch total loss 1.40932953\n",
      "Trained batch 312 batch loss 1.41084301 epoch total loss 1.40933442\n",
      "Trained batch 313 batch loss 1.39288092 epoch total loss 1.40928185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 314 batch loss 1.41726184 epoch total loss 1.40930724\n",
      "Trained batch 315 batch loss 1.46311307 epoch total loss 1.40947807\n",
      "Trained batch 316 batch loss 1.3980422 epoch total loss 1.40944183\n",
      "Trained batch 317 batch loss 1.34465635 epoch total loss 1.4092375\n",
      "Trained batch 318 batch loss 1.35784066 epoch total loss 1.40907598\n",
      "Trained batch 319 batch loss 1.35040629 epoch total loss 1.40889204\n",
      "Trained batch 320 batch loss 1.40868711 epoch total loss 1.40889144\n",
      "Trained batch 321 batch loss 1.40990567 epoch total loss 1.40889454\n",
      "Trained batch 322 batch loss 1.41707933 epoch total loss 1.40892\n",
      "Trained batch 323 batch loss 1.38200617 epoch total loss 1.40883672\n",
      "Trained batch 324 batch loss 1.44884586 epoch total loss 1.40896022\n",
      "Trained batch 325 batch loss 1.55442238 epoch total loss 1.40940773\n",
      "Trained batch 326 batch loss 1.53186643 epoch total loss 1.40978336\n",
      "Trained batch 327 batch loss 1.4136703 epoch total loss 1.40979528\n",
      "Trained batch 328 batch loss 1.33997774 epoch total loss 1.40958238\n",
      "Trained batch 329 batch loss 1.25438106 epoch total loss 1.40911067\n",
      "Trained batch 330 batch loss 1.4558444 epoch total loss 1.40925229\n",
      "Trained batch 331 batch loss 1.40536118 epoch total loss 1.40924048\n",
      "Trained batch 332 batch loss 1.30980539 epoch total loss 1.40894103\n",
      "Trained batch 333 batch loss 1.28613639 epoch total loss 1.40857232\n",
      "Trained batch 334 batch loss 1.20115662 epoch total loss 1.40795135\n",
      "Trained batch 335 batch loss 1.23144317 epoch total loss 1.40742445\n",
      "Trained batch 336 batch loss 1.31448114 epoch total loss 1.40714777\n",
      "Trained batch 337 batch loss 1.30066407 epoch total loss 1.40683186\n",
      "Trained batch 338 batch loss 1.29809034 epoch total loss 1.40651011\n",
      "Trained batch 339 batch loss 1.28323233 epoch total loss 1.40614653\n",
      "Trained batch 340 batch loss 1.32388854 epoch total loss 1.40590453\n",
      "Trained batch 341 batch loss 1.44071138 epoch total loss 1.40600657\n",
      "Trained batch 342 batch loss 1.28381753 epoch total loss 1.4056493\n",
      "Trained batch 343 batch loss 1.26763058 epoch total loss 1.40524697\n",
      "Trained batch 344 batch loss 1.33708763 epoch total loss 1.40504885\n",
      "Trained batch 345 batch loss 1.48159242 epoch total loss 1.4052707\n",
      "Trained batch 346 batch loss 1.41478801 epoch total loss 1.40529823\n",
      "Trained batch 347 batch loss 1.4228543 epoch total loss 1.40534878\n",
      "Trained batch 348 batch loss 1.30987334 epoch total loss 1.40507448\n",
      "Trained batch 349 batch loss 1.46476066 epoch total loss 1.40524542\n",
      "Trained batch 350 batch loss 1.54399323 epoch total loss 1.40564191\n",
      "Trained batch 351 batch loss 1.37110865 epoch total loss 1.40554345\n",
      "Trained batch 352 batch loss 1.29523134 epoch total loss 1.40523\n",
      "Trained batch 353 batch loss 1.27319527 epoch total loss 1.40485609\n",
      "Trained batch 354 batch loss 1.37088418 epoch total loss 1.40476012\n",
      "Trained batch 355 batch loss 1.45620155 epoch total loss 1.40490496\n",
      "Trained batch 356 batch loss 1.53606081 epoch total loss 1.40527344\n",
      "Trained batch 357 batch loss 1.41938329 epoch total loss 1.4053129\n",
      "Trained batch 358 batch loss 1.36121988 epoch total loss 1.40518975\n",
      "Trained batch 359 batch loss 1.38745213 epoch total loss 1.40514028\n",
      "Trained batch 360 batch loss 1.30214369 epoch total loss 1.4048543\n",
      "Trained batch 361 batch loss 1.37496769 epoch total loss 1.40477145\n",
      "Trained batch 362 batch loss 1.41008294 epoch total loss 1.40478611\n",
      "Trained batch 363 batch loss 1.57101965 epoch total loss 1.40524411\n",
      "Trained batch 364 batch loss 1.5101701 epoch total loss 1.40553236\n",
      "Trained batch 365 batch loss 1.35511827 epoch total loss 1.4053942\n",
      "Trained batch 366 batch loss 1.28436208 epoch total loss 1.40506351\n",
      "Trained batch 367 batch loss 1.38073969 epoch total loss 1.40499723\n",
      "Trained batch 368 batch loss 1.43383551 epoch total loss 1.40507555\n",
      "Trained batch 369 batch loss 1.38996577 epoch total loss 1.40503454\n",
      "Trained batch 370 batch loss 1.40082252 epoch total loss 1.40502322\n",
      "Trained batch 371 batch loss 1.51018047 epoch total loss 1.4053067\n",
      "Trained batch 372 batch loss 1.48671317 epoch total loss 1.40552545\n",
      "Trained batch 373 batch loss 1.42272818 epoch total loss 1.40557158\n",
      "Trained batch 374 batch loss 1.55809104 epoch total loss 1.40597939\n",
      "Trained batch 375 batch loss 1.44073319 epoch total loss 1.40607214\n",
      "Trained batch 376 batch loss 1.37682915 epoch total loss 1.4059943\n",
      "Trained batch 377 batch loss 1.3958708 epoch total loss 1.40596747\n",
      "Trained batch 378 batch loss 1.43425512 epoch total loss 1.40604234\n",
      "Trained batch 379 batch loss 1.56931806 epoch total loss 1.40647316\n",
      "Trained batch 380 batch loss 1.49792266 epoch total loss 1.40671384\n",
      "Trained batch 381 batch loss 1.428069 epoch total loss 1.40676987\n",
      "Trained batch 382 batch loss 1.51630855 epoch total loss 1.40705657\n",
      "Trained batch 383 batch loss 1.4486171 epoch total loss 1.40716505\n",
      "Trained batch 384 batch loss 1.3624289 epoch total loss 1.40704858\n",
      "Trained batch 385 batch loss 1.44135213 epoch total loss 1.40713763\n",
      "Trained batch 386 batch loss 1.52721989 epoch total loss 1.40744877\n",
      "Trained batch 387 batch loss 1.53047061 epoch total loss 1.40776658\n",
      "Trained batch 388 batch loss 1.43914676 epoch total loss 1.4078474\n",
      "Trained batch 389 batch loss 1.28352356 epoch total loss 1.4075278\n",
      "Trained batch 390 batch loss 1.23409367 epoch total loss 1.40708303\n",
      "Trained batch 391 batch loss 1.24171078 epoch total loss 1.40666008\n",
      "Trained batch 392 batch loss 1.23548269 epoch total loss 1.40622342\n",
      "Trained batch 393 batch loss 1.34212816 epoch total loss 1.40606022\n",
      "Trained batch 394 batch loss 1.50191903 epoch total loss 1.40630341\n",
      "Trained batch 395 batch loss 1.37558818 epoch total loss 1.40622568\n",
      "Trained batch 396 batch loss 1.51309812 epoch total loss 1.40649569\n",
      "Trained batch 397 batch loss 1.45213842 epoch total loss 1.40661073\n",
      "Trained batch 398 batch loss 1.45661807 epoch total loss 1.40673625\n",
      "Trained batch 399 batch loss 1.48067379 epoch total loss 1.40692151\n",
      "Trained batch 400 batch loss 1.52579641 epoch total loss 1.40721881\n",
      "Trained batch 401 batch loss 1.50247645 epoch total loss 1.4074564\n",
      "Trained batch 402 batch loss 1.35559726 epoch total loss 1.40732741\n",
      "Trained batch 403 batch loss 1.36963582 epoch total loss 1.40723383\n",
      "Trained batch 404 batch loss 1.46728873 epoch total loss 1.40738249\n",
      "Trained batch 405 batch loss 1.39963925 epoch total loss 1.40736341\n",
      "Trained batch 406 batch loss 1.39256883 epoch total loss 1.40732694\n",
      "Trained batch 407 batch loss 1.35785031 epoch total loss 1.40720546\n",
      "Trained batch 408 batch loss 1.31706047 epoch total loss 1.40698457\n",
      "Trained batch 409 batch loss 1.34811306 epoch total loss 1.40684056\n",
      "Trained batch 410 batch loss 1.36294699 epoch total loss 1.40673351\n",
      "Trained batch 411 batch loss 1.37143302 epoch total loss 1.40664768\n",
      "Trained batch 412 batch loss 1.47801161 epoch total loss 1.40682089\n",
      "Trained batch 413 batch loss 1.65011239 epoch total loss 1.40740991\n",
      "Trained batch 414 batch loss 1.55967212 epoch total loss 1.40777779\n",
      "Trained batch 415 batch loss 1.35421 epoch total loss 1.40764868\n",
      "Trained batch 416 batch loss 1.27237093 epoch total loss 1.4073236\n",
      "Trained batch 417 batch loss 1.35601914 epoch total loss 1.40720046\n",
      "Trained batch 418 batch loss 1.42408776 epoch total loss 1.40724087\n",
      "Trained batch 419 batch loss 1.33199835 epoch total loss 1.40706122\n",
      "Trained batch 420 batch loss 1.36391187 epoch total loss 1.40695846\n",
      "Trained batch 421 batch loss 1.44703877 epoch total loss 1.40705359\n",
      "Trained batch 422 batch loss 1.46513593 epoch total loss 1.40719128\n",
      "Trained batch 423 batch loss 1.53036833 epoch total loss 1.4074825\n",
      "Trained batch 424 batch loss 1.50840378 epoch total loss 1.40772057\n",
      "Trained batch 425 batch loss 1.37317979 epoch total loss 1.40763927\n",
      "Trained batch 426 batch loss 1.35414529 epoch total loss 1.40751374\n",
      "Trained batch 427 batch loss 1.37964046 epoch total loss 1.40744841\n",
      "Trained batch 428 batch loss 1.42967629 epoch total loss 1.40750039\n",
      "Trained batch 429 batch loss 1.39482 epoch total loss 1.40747082\n",
      "Trained batch 430 batch loss 1.36639953 epoch total loss 1.40737534\n",
      "Trained batch 431 batch loss 1.34016311 epoch total loss 1.40721929\n",
      "Trained batch 432 batch loss 1.29733253 epoch total loss 1.4069649\n",
      "Trained batch 433 batch loss 1.33625817 epoch total loss 1.40680158\n",
      "Trained batch 434 batch loss 1.27763391 epoch total loss 1.40650403\n",
      "Trained batch 435 batch loss 1.49517846 epoch total loss 1.40670788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 436 batch loss 1.36592126 epoch total loss 1.4066143\n",
      "Trained batch 437 batch loss 1.35302758 epoch total loss 1.40649164\n",
      "Trained batch 438 batch loss 1.36270404 epoch total loss 1.40639174\n",
      "Trained batch 439 batch loss 1.34336925 epoch total loss 1.40624821\n",
      "Trained batch 440 batch loss 1.39935732 epoch total loss 1.40623248\n",
      "Trained batch 441 batch loss 1.39312744 epoch total loss 1.40620279\n",
      "Trained batch 442 batch loss 1.38921857 epoch total loss 1.40616441\n",
      "Trained batch 443 batch loss 1.43052077 epoch total loss 1.40621936\n",
      "Trained batch 444 batch loss 1.43281841 epoch total loss 1.40627933\n",
      "Trained batch 445 batch loss 1.49562466 epoch total loss 1.40648\n",
      "Trained batch 446 batch loss 1.43408644 epoch total loss 1.40654194\n",
      "Trained batch 447 batch loss 1.54779696 epoch total loss 1.40685785\n",
      "Trained batch 448 batch loss 1.43276453 epoch total loss 1.40691566\n",
      "Trained batch 449 batch loss 1.41307509 epoch total loss 1.40692937\n",
      "Trained batch 450 batch loss 1.30601799 epoch total loss 1.40670514\n",
      "Trained batch 451 batch loss 1.36724782 epoch total loss 1.40661764\n",
      "Trained batch 452 batch loss 1.34611452 epoch total loss 1.40648389\n",
      "Trained batch 453 batch loss 1.19447529 epoch total loss 1.40601587\n",
      "Trained batch 454 batch loss 1.33101153 epoch total loss 1.40585053\n",
      "Trained batch 455 batch loss 1.32240415 epoch total loss 1.40566719\n",
      "Trained batch 456 batch loss 1.46668327 epoch total loss 1.40580094\n",
      "Trained batch 457 batch loss 1.36588442 epoch total loss 1.40571368\n",
      "Trained batch 458 batch loss 1.45349479 epoch total loss 1.40581799\n",
      "Trained batch 459 batch loss 1.30682313 epoch total loss 1.40560234\n",
      "Trained batch 460 batch loss 1.19376564 epoch total loss 1.40514183\n",
      "Trained batch 461 batch loss 1.36873794 epoch total loss 1.40506279\n",
      "Trained batch 462 batch loss 1.41110301 epoch total loss 1.40507591\n",
      "Trained batch 463 batch loss 1.35674775 epoch total loss 1.4049716\n",
      "Trained batch 464 batch loss 1.40014791 epoch total loss 1.40496111\n",
      "Trained batch 465 batch loss 1.48337007 epoch total loss 1.40512979\n",
      "Trained batch 466 batch loss 1.41712666 epoch total loss 1.40515554\n",
      "Trained batch 467 batch loss 1.33823097 epoch total loss 1.40501225\n",
      "Trained batch 468 batch loss 1.33425927 epoch total loss 1.40486121\n",
      "Trained batch 469 batch loss 1.31845975 epoch total loss 1.40467703\n",
      "Trained batch 470 batch loss 1.34033215 epoch total loss 1.40454006\n",
      "Trained batch 471 batch loss 1.27771342 epoch total loss 1.40427089\n",
      "Trained batch 472 batch loss 1.38618588 epoch total loss 1.4042325\n",
      "Trained batch 473 batch loss 1.42371547 epoch total loss 1.40427363\n",
      "Trained batch 474 batch loss 1.43014026 epoch total loss 1.40432811\n",
      "Trained batch 475 batch loss 1.36075783 epoch total loss 1.40423644\n",
      "Trained batch 476 batch loss 1.3347621 epoch total loss 1.40409052\n",
      "Trained batch 477 batch loss 1.31570077 epoch total loss 1.40390515\n",
      "Trained batch 478 batch loss 1.42729735 epoch total loss 1.40395415\n",
      "Trained batch 479 batch loss 1.40809309 epoch total loss 1.40396273\n",
      "Trained batch 480 batch loss 1.38976812 epoch total loss 1.40393317\n",
      "Trained batch 481 batch loss 1.43022799 epoch total loss 1.40398788\n",
      "Trained batch 482 batch loss 1.55164409 epoch total loss 1.40429425\n",
      "Trained batch 483 batch loss 1.36640084 epoch total loss 1.40421569\n",
      "Trained batch 484 batch loss 1.39874196 epoch total loss 1.40420449\n",
      "Trained batch 485 batch loss 1.49110842 epoch total loss 1.40438354\n",
      "Trained batch 486 batch loss 1.53799152 epoch total loss 1.40465844\n",
      "Trained batch 487 batch loss 1.49113441 epoch total loss 1.40483606\n",
      "Trained batch 488 batch loss 1.34065807 epoch total loss 1.40470445\n",
      "Trained batch 489 batch loss 1.41405976 epoch total loss 1.40472364\n",
      "Trained batch 490 batch loss 1.28719449 epoch total loss 1.40448368\n",
      "Trained batch 491 batch loss 1.38334131 epoch total loss 1.40444064\n",
      "Trained batch 492 batch loss 1.35121894 epoch total loss 1.40433252\n",
      "Trained batch 493 batch loss 1.42009377 epoch total loss 1.40436447\n",
      "Trained batch 494 batch loss 1.41696191 epoch total loss 1.40439\n",
      "Trained batch 495 batch loss 1.30116558 epoch total loss 1.40418148\n",
      "Trained batch 496 batch loss 1.30387259 epoch total loss 1.4039793\n",
      "Trained batch 497 batch loss 1.33884108 epoch total loss 1.40384829\n",
      "Trained batch 498 batch loss 1.27456152 epoch total loss 1.40358865\n",
      "Trained batch 499 batch loss 1.30383408 epoch total loss 1.40338874\n",
      "Trained batch 500 batch loss 1.42876172 epoch total loss 1.4034394\n",
      "Trained batch 501 batch loss 1.43562126 epoch total loss 1.40350366\n",
      "Trained batch 502 batch loss 1.56181955 epoch total loss 1.40381908\n",
      "Trained batch 503 batch loss 1.61228228 epoch total loss 1.40423357\n",
      "Trained batch 504 batch loss 1.55588317 epoch total loss 1.40453446\n",
      "Trained batch 505 batch loss 1.5593425 epoch total loss 1.40484095\n",
      "Trained batch 506 batch loss 1.49521387 epoch total loss 1.40501964\n",
      "Trained batch 507 batch loss 1.37674284 epoch total loss 1.40496397\n",
      "Trained batch 508 batch loss 1.26472485 epoch total loss 1.40468788\n",
      "Trained batch 509 batch loss 1.30245614 epoch total loss 1.40448689\n",
      "Trained batch 510 batch loss 1.47010899 epoch total loss 1.40461552\n",
      "Trained batch 511 batch loss 1.49574876 epoch total loss 1.40479386\n",
      "Trained batch 512 batch loss 1.40437257 epoch total loss 1.40479302\n",
      "Trained batch 513 batch loss 1.36752665 epoch total loss 1.40472043\n",
      "Trained batch 514 batch loss 1.22634864 epoch total loss 1.40437329\n",
      "Trained batch 515 batch loss 1.43032 epoch total loss 1.40442371\n",
      "Trained batch 516 batch loss 1.25104499 epoch total loss 1.40412641\n",
      "Trained batch 517 batch loss 1.28634977 epoch total loss 1.40389872\n",
      "Trained batch 518 batch loss 1.33948874 epoch total loss 1.40377426\n",
      "Trained batch 519 batch loss 1.32574975 epoch total loss 1.40362394\n",
      "Trained batch 520 batch loss 1.32369483 epoch total loss 1.40347016\n",
      "Trained batch 521 batch loss 1.38901114 epoch total loss 1.4034425\n",
      "Trained batch 522 batch loss 1.35609508 epoch total loss 1.40335178\n",
      "Trained batch 523 batch loss 1.34122539 epoch total loss 1.40323305\n",
      "Trained batch 524 batch loss 1.36720574 epoch total loss 1.40316427\n",
      "Trained batch 525 batch loss 1.49808383 epoch total loss 1.40334511\n",
      "Trained batch 526 batch loss 1.49398112 epoch total loss 1.40351737\n",
      "Trained batch 527 batch loss 1.4898566 epoch total loss 1.40368116\n",
      "Trained batch 528 batch loss 1.38371694 epoch total loss 1.40364337\n",
      "Trained batch 529 batch loss 1.37823606 epoch total loss 1.40359533\n",
      "Trained batch 530 batch loss 1.38679469 epoch total loss 1.40356362\n",
      "Trained batch 531 batch loss 1.43916309 epoch total loss 1.40363061\n",
      "Trained batch 532 batch loss 1.49109101 epoch total loss 1.403795\n",
      "Trained batch 533 batch loss 1.59938049 epoch total loss 1.40416193\n",
      "Trained batch 534 batch loss 1.6078732 epoch total loss 1.4045434\n",
      "Trained batch 535 batch loss 1.50248981 epoch total loss 1.40472651\n",
      "Trained batch 536 batch loss 1.47804642 epoch total loss 1.40486324\n",
      "Trained batch 537 batch loss 1.41705966 epoch total loss 1.40488601\n",
      "Trained batch 538 batch loss 1.54179049 epoch total loss 1.40514052\n",
      "Trained batch 539 batch loss 1.52137709 epoch total loss 1.40535605\n",
      "Trained batch 540 batch loss 1.5786047 epoch total loss 1.40567696\n",
      "Trained batch 541 batch loss 1.43371987 epoch total loss 1.40572882\n",
      "Trained batch 542 batch loss 1.22232199 epoch total loss 1.40539038\n",
      "Trained batch 543 batch loss 1.44065237 epoch total loss 1.40545547\n",
      "Trained batch 544 batch loss 1.3823061 epoch total loss 1.40541291\n",
      "Trained batch 545 batch loss 1.32507157 epoch total loss 1.40526545\n",
      "Trained batch 546 batch loss 1.36449456 epoch total loss 1.40519083\n",
      "Trained batch 547 batch loss 1.3288939 epoch total loss 1.40505135\n",
      "Trained batch 548 batch loss 1.24108744 epoch total loss 1.40475214\n",
      "Trained batch 549 batch loss 1.30629659 epoch total loss 1.40457284\n",
      "Trained batch 550 batch loss 1.34787762 epoch total loss 1.40446973\n",
      "Trained batch 551 batch loss 1.33231568 epoch total loss 1.40433884\n",
      "Trained batch 552 batch loss 1.28591132 epoch total loss 1.40412426\n",
      "Trained batch 553 batch loss 1.28145421 epoch total loss 1.40390241\n",
      "Trained batch 554 batch loss 1.27204669 epoch total loss 1.40366435\n",
      "Trained batch 555 batch loss 1.32426047 epoch total loss 1.4035213\n",
      "Trained batch 556 batch loss 1.56564713 epoch total loss 1.403813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 557 batch loss 1.47721446 epoch total loss 1.40394485\n",
      "Trained batch 558 batch loss 1.44696915 epoch total loss 1.40402186\n",
      "Trained batch 559 batch loss 1.46996498 epoch total loss 1.40413988\n",
      "Trained batch 560 batch loss 1.53641236 epoch total loss 1.40437615\n",
      "Trained batch 561 batch loss 1.50913227 epoch total loss 1.40456283\n",
      "Trained batch 562 batch loss 1.40971899 epoch total loss 1.40457213\n",
      "Trained batch 563 batch loss 1.36124372 epoch total loss 1.40449512\n",
      "Trained batch 564 batch loss 1.2970463 epoch total loss 1.40430462\n",
      "Trained batch 565 batch loss 1.33885717 epoch total loss 1.40418887\n",
      "Trained batch 566 batch loss 1.37017965 epoch total loss 1.40412879\n",
      "Trained batch 567 batch loss 1.41552663 epoch total loss 1.40414882\n",
      "Trained batch 568 batch loss 1.33783054 epoch total loss 1.40403211\n",
      "Trained batch 569 batch loss 1.22897851 epoch total loss 1.40372455\n",
      "Trained batch 570 batch loss 1.30057693 epoch total loss 1.40354359\n",
      "Trained batch 571 batch loss 1.2849474 epoch total loss 1.40333593\n",
      "Trained batch 572 batch loss 1.29349363 epoch total loss 1.40314388\n",
      "Trained batch 573 batch loss 1.35216427 epoch total loss 1.40305495\n",
      "Trained batch 574 batch loss 1.33355141 epoch total loss 1.40293396\n",
      "Trained batch 575 batch loss 1.27281535 epoch total loss 1.40270758\n",
      "Trained batch 576 batch loss 1.33184958 epoch total loss 1.40258455\n",
      "Trained batch 577 batch loss 1.3129251 epoch total loss 1.40242922\n",
      "Trained batch 578 batch loss 1.38906169 epoch total loss 1.4024061\n",
      "Trained batch 579 batch loss 1.4312892 epoch total loss 1.40245593\n",
      "Trained batch 580 batch loss 1.45618153 epoch total loss 1.40254855\n",
      "Trained batch 581 batch loss 1.45370114 epoch total loss 1.40263653\n",
      "Trained batch 582 batch loss 1.35888052 epoch total loss 1.40256131\n",
      "Trained batch 583 batch loss 1.40575933 epoch total loss 1.40256679\n",
      "Trained batch 584 batch loss 1.37918377 epoch total loss 1.40252686\n",
      "Trained batch 585 batch loss 1.38409698 epoch total loss 1.40249538\n",
      "Trained batch 586 batch loss 1.32501304 epoch total loss 1.40236318\n",
      "Trained batch 587 batch loss 1.328596 epoch total loss 1.40223753\n",
      "Trained batch 588 batch loss 1.2969197 epoch total loss 1.40205836\n",
      "Trained batch 589 batch loss 1.28060079 epoch total loss 1.40185213\n",
      "Trained batch 590 batch loss 1.24831152 epoch total loss 1.4015919\n",
      "Trained batch 591 batch loss 1.27203357 epoch total loss 1.40137267\n",
      "Trained batch 592 batch loss 1.40867758 epoch total loss 1.40138507\n",
      "Trained batch 593 batch loss 1.26038933 epoch total loss 1.40114725\n",
      "Trained batch 594 batch loss 1.13767958 epoch total loss 1.40070367\n",
      "Trained batch 595 batch loss 1.13368 epoch total loss 1.40025496\n",
      "Trained batch 596 batch loss 1.09756052 epoch total loss 1.39974701\n",
      "Trained batch 597 batch loss 1.49898827 epoch total loss 1.39991319\n",
      "Trained batch 598 batch loss 1.49562371 epoch total loss 1.40007317\n",
      "Trained batch 599 batch loss 1.56397903 epoch total loss 1.40034676\n",
      "Trained batch 600 batch loss 1.35978746 epoch total loss 1.40027928\n",
      "Trained batch 601 batch loss 1.33408737 epoch total loss 1.40016913\n",
      "Trained batch 602 batch loss 1.37689221 epoch total loss 1.40013051\n",
      "Trained batch 603 batch loss 1.24990773 epoch total loss 1.39988124\n",
      "Trained batch 604 batch loss 1.29027724 epoch total loss 1.39969981\n",
      "Trained batch 605 batch loss 1.3283478 epoch total loss 1.39958191\n",
      "Trained batch 606 batch loss 1.39902866 epoch total loss 1.39958107\n",
      "Trained batch 607 batch loss 1.24294114 epoch total loss 1.39932299\n",
      "Trained batch 608 batch loss 1.29348385 epoch total loss 1.39914882\n",
      "Trained batch 609 batch loss 1.41305637 epoch total loss 1.39917171\n",
      "Trained batch 610 batch loss 1.47792745 epoch total loss 1.39930081\n",
      "Trained batch 611 batch loss 1.42836857 epoch total loss 1.39934838\n",
      "Trained batch 612 batch loss 1.4099803 epoch total loss 1.39936566\n",
      "Trained batch 613 batch loss 1.40732789 epoch total loss 1.39937866\n",
      "Trained batch 614 batch loss 1.39611173 epoch total loss 1.39937341\n",
      "Trained batch 615 batch loss 1.4050318 epoch total loss 1.39938259\n",
      "Trained batch 616 batch loss 1.44575608 epoch total loss 1.39945781\n",
      "Trained batch 617 batch loss 1.45725095 epoch total loss 1.39955151\n",
      "Trained batch 618 batch loss 1.28159332 epoch total loss 1.39936078\n",
      "Trained batch 619 batch loss 1.36847925 epoch total loss 1.39931083\n",
      "Trained batch 620 batch loss 1.50328255 epoch total loss 1.39947855\n",
      "Trained batch 621 batch loss 1.41159463 epoch total loss 1.39949811\n",
      "Trained batch 622 batch loss 1.41692615 epoch total loss 1.39952612\n",
      "Trained batch 623 batch loss 1.32550073 epoch total loss 1.39940727\n",
      "Trained batch 624 batch loss 1.36704135 epoch total loss 1.39935553\n",
      "Trained batch 625 batch loss 1.36156952 epoch total loss 1.39929497\n",
      "Trained batch 626 batch loss 1.42215359 epoch total loss 1.39933157\n",
      "Trained batch 627 batch loss 1.53772664 epoch total loss 1.39955235\n",
      "Trained batch 628 batch loss 1.4129045 epoch total loss 1.39957356\n",
      "Trained batch 629 batch loss 1.4329108 epoch total loss 1.39962661\n",
      "Trained batch 630 batch loss 1.39051604 epoch total loss 1.39961207\n",
      "Trained batch 631 batch loss 1.37834859 epoch total loss 1.39957845\n",
      "Trained batch 632 batch loss 1.40164518 epoch total loss 1.39958167\n",
      "Trained batch 633 batch loss 1.45226502 epoch total loss 1.399665\n",
      "Trained batch 634 batch loss 1.45715451 epoch total loss 1.3997556\n",
      "Trained batch 635 batch loss 1.35697007 epoch total loss 1.39968824\n",
      "Trained batch 636 batch loss 1.41469693 epoch total loss 1.39971185\n",
      "Trained batch 637 batch loss 1.44882536 epoch total loss 1.39978898\n",
      "Trained batch 638 batch loss 1.36159229 epoch total loss 1.39972913\n",
      "Trained batch 639 batch loss 1.39843464 epoch total loss 1.39972711\n",
      "Trained batch 640 batch loss 1.39640737 epoch total loss 1.39972186\n",
      "Trained batch 641 batch loss 1.35598898 epoch total loss 1.39965367\n",
      "Trained batch 642 batch loss 1.39550817 epoch total loss 1.39964724\n",
      "Trained batch 643 batch loss 1.33721316 epoch total loss 1.3995502\n",
      "Trained batch 644 batch loss 1.32569623 epoch total loss 1.39943552\n",
      "Trained batch 645 batch loss 1.20556152 epoch total loss 1.39913487\n",
      "Trained batch 646 batch loss 1.23574042 epoch total loss 1.39888191\n",
      "Trained batch 647 batch loss 1.36676979 epoch total loss 1.39883232\n",
      "Trained batch 648 batch loss 1.30905461 epoch total loss 1.3986938\n",
      "Trained batch 649 batch loss 1.22532713 epoch total loss 1.39842665\n",
      "Trained batch 650 batch loss 1.26609743 epoch total loss 1.39822316\n",
      "Trained batch 651 batch loss 1.29692924 epoch total loss 1.39806759\n",
      "Trained batch 652 batch loss 1.33304262 epoch total loss 1.39796782\n",
      "Trained batch 653 batch loss 1.34135723 epoch total loss 1.39788115\n",
      "Trained batch 654 batch loss 1.26210952 epoch total loss 1.39767349\n",
      "Trained batch 655 batch loss 1.21026886 epoch total loss 1.39738739\n",
      "Trained batch 656 batch loss 1.32585633 epoch total loss 1.39727843\n",
      "Trained batch 657 batch loss 1.27923656 epoch total loss 1.39709878\n",
      "Trained batch 658 batch loss 1.23687744 epoch total loss 1.39685524\n",
      "Trained batch 659 batch loss 1.34015465 epoch total loss 1.39676917\n",
      "Trained batch 660 batch loss 1.30544758 epoch total loss 1.39663076\n",
      "Trained batch 661 batch loss 1.43246925 epoch total loss 1.396685\n",
      "Trained batch 662 batch loss 1.34561539 epoch total loss 1.39660788\n",
      "Trained batch 663 batch loss 1.46154571 epoch total loss 1.39670587\n",
      "Trained batch 664 batch loss 1.18506157 epoch total loss 1.3963871\n",
      "Trained batch 665 batch loss 1.26810157 epoch total loss 1.39619422\n",
      "Trained batch 666 batch loss 1.26454365 epoch total loss 1.39599657\n",
      "Trained batch 667 batch loss 1.37213302 epoch total loss 1.39596081\n",
      "Trained batch 668 batch loss 1.51447737 epoch total loss 1.39613819\n",
      "Trained batch 669 batch loss 1.42569041 epoch total loss 1.39618242\n",
      "Trained batch 670 batch loss 1.3890729 epoch total loss 1.39617181\n",
      "Trained batch 671 batch loss 1.33753538 epoch total loss 1.39608443\n",
      "Trained batch 672 batch loss 1.22353649 epoch total loss 1.39582765\n",
      "Trained batch 673 batch loss 1.32975352 epoch total loss 1.39572942\n",
      "Trained batch 674 batch loss 1.31202865 epoch total loss 1.39560521\n",
      "Trained batch 675 batch loss 1.33690453 epoch total loss 1.3955183\n",
      "Trained batch 676 batch loss 1.39930916 epoch total loss 1.39552391\n",
      "Trained batch 677 batch loss 1.31196785 epoch total loss 1.3954004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 678 batch loss 1.20007169 epoch total loss 1.3951124\n",
      "Trained batch 679 batch loss 1.45076394 epoch total loss 1.39519429\n",
      "Trained batch 680 batch loss 1.42726874 epoch total loss 1.39524138\n",
      "Trained batch 681 batch loss 1.4441514 epoch total loss 1.39531326\n",
      "Trained batch 682 batch loss 1.26923513 epoch total loss 1.39512837\n",
      "Trained batch 683 batch loss 1.242625 epoch total loss 1.39490509\n",
      "Trained batch 684 batch loss 1.22476685 epoch total loss 1.3946563\n",
      "Trained batch 685 batch loss 1.21763742 epoch total loss 1.39439797\n",
      "Trained batch 686 batch loss 1.23156404 epoch total loss 1.39416063\n",
      "Trained batch 687 batch loss 1.17160118 epoch total loss 1.39383662\n",
      "Trained batch 688 batch loss 1.25912213 epoch total loss 1.39364088\n",
      "Trained batch 689 batch loss 1.29540968 epoch total loss 1.3934983\n",
      "Trained batch 690 batch loss 1.29730761 epoch total loss 1.39335883\n",
      "Trained batch 691 batch loss 1.40712535 epoch total loss 1.39337873\n",
      "Trained batch 692 batch loss 1.40994024 epoch total loss 1.39340258\n",
      "Trained batch 693 batch loss 1.3638041 epoch total loss 1.3933599\n",
      "Trained batch 694 batch loss 1.41026068 epoch total loss 1.39338434\n",
      "Trained batch 695 batch loss 1.42527866 epoch total loss 1.39343023\n",
      "Trained batch 696 batch loss 1.31874859 epoch total loss 1.39332294\n",
      "Trained batch 697 batch loss 1.35978317 epoch total loss 1.39327478\n",
      "Trained batch 698 batch loss 1.34862339 epoch total loss 1.39321089\n",
      "Trained batch 699 batch loss 1.41279292 epoch total loss 1.3932389\n",
      "Trained batch 700 batch loss 1.4185617 epoch total loss 1.39327502\n",
      "Trained batch 701 batch loss 1.27393961 epoch total loss 1.39310479\n",
      "Trained batch 702 batch loss 1.39133346 epoch total loss 1.39310229\n",
      "Trained batch 703 batch loss 1.48012173 epoch total loss 1.39322603\n",
      "Trained batch 704 batch loss 1.50354767 epoch total loss 1.39338279\n",
      "Trained batch 705 batch loss 1.57453394 epoch total loss 1.39363968\n",
      "Trained batch 706 batch loss 1.39051986 epoch total loss 1.39363527\n",
      "Trained batch 707 batch loss 1.39701164 epoch total loss 1.39364\n",
      "Trained batch 708 batch loss 1.28111541 epoch total loss 1.39348114\n",
      "Trained batch 709 batch loss 1.33447254 epoch total loss 1.39339793\n",
      "Trained batch 710 batch loss 1.4460547 epoch total loss 1.39347208\n",
      "Trained batch 711 batch loss 1.37884593 epoch total loss 1.39345145\n",
      "Trained batch 712 batch loss 1.32469869 epoch total loss 1.39335489\n",
      "Trained batch 713 batch loss 1.31062615 epoch total loss 1.3932389\n",
      "Trained batch 714 batch loss 1.42599058 epoch total loss 1.39328468\n",
      "Trained batch 715 batch loss 1.43328261 epoch total loss 1.39334071\n",
      "Trained batch 716 batch loss 1.44302464 epoch total loss 1.39341009\n",
      "Trained batch 717 batch loss 1.49431682 epoch total loss 1.39355087\n",
      "Trained batch 718 batch loss 1.47984505 epoch total loss 1.39367104\n",
      "Trained batch 719 batch loss 1.43902075 epoch total loss 1.3937341\n",
      "Trained batch 720 batch loss 1.31651235 epoch total loss 1.39362693\n",
      "Trained batch 721 batch loss 1.45024037 epoch total loss 1.39370549\n",
      "Trained batch 722 batch loss 1.3661226 epoch total loss 1.39366734\n",
      "Trained batch 723 batch loss 1.29455984 epoch total loss 1.39353025\n",
      "Trained batch 724 batch loss 1.32511485 epoch total loss 1.39343572\n",
      "Trained batch 725 batch loss 1.49761 epoch total loss 1.39357948\n",
      "Trained batch 726 batch loss 1.33726978 epoch total loss 1.39350188\n",
      "Trained batch 727 batch loss 1.34251702 epoch total loss 1.39343178\n",
      "Trained batch 728 batch loss 1.41568482 epoch total loss 1.39346242\n",
      "Trained batch 729 batch loss 1.3787725 epoch total loss 1.39344227\n",
      "Trained batch 730 batch loss 1.23786473 epoch total loss 1.39322913\n",
      "Trained batch 731 batch loss 1.36357355 epoch total loss 1.3931886\n",
      "Trained batch 732 batch loss 1.34658623 epoch total loss 1.39312482\n",
      "Trained batch 733 batch loss 1.28260922 epoch total loss 1.39297402\n",
      "Trained batch 734 batch loss 1.30377066 epoch total loss 1.39285254\n",
      "Trained batch 735 batch loss 1.31428874 epoch total loss 1.39274561\n",
      "Trained batch 736 batch loss 1.31451952 epoch total loss 1.39263928\n",
      "Trained batch 737 batch loss 1.38918495 epoch total loss 1.39263463\n",
      "Trained batch 738 batch loss 1.31015229 epoch total loss 1.39252293\n",
      "Trained batch 739 batch loss 1.28177404 epoch total loss 1.39237297\n",
      "Trained batch 740 batch loss 1.30117702 epoch total loss 1.3922497\n",
      "Trained batch 741 batch loss 1.29167771 epoch total loss 1.39211392\n",
      "Trained batch 742 batch loss 1.48513353 epoch total loss 1.39223921\n",
      "Trained batch 743 batch loss 1.44692039 epoch total loss 1.39231277\n",
      "Trained batch 744 batch loss 1.34066665 epoch total loss 1.39224339\n",
      "Trained batch 745 batch loss 1.30222774 epoch total loss 1.39212263\n",
      "Trained batch 746 batch loss 1.15673959 epoch total loss 1.39180708\n",
      "Trained batch 747 batch loss 1.24251628 epoch total loss 1.39160728\n",
      "Trained batch 748 batch loss 1.31795037 epoch total loss 1.39150882\n",
      "Trained batch 749 batch loss 1.31302357 epoch total loss 1.39140403\n",
      "Trained batch 750 batch loss 1.48606 epoch total loss 1.39153028\n",
      "Trained batch 751 batch loss 1.40742838 epoch total loss 1.39155149\n",
      "Trained batch 752 batch loss 1.36732078 epoch total loss 1.39151931\n",
      "Trained batch 753 batch loss 1.3599174 epoch total loss 1.39147723\n",
      "Trained batch 754 batch loss 1.35641599 epoch total loss 1.39143074\n",
      "Trained batch 755 batch loss 1.35599709 epoch total loss 1.39138377\n",
      "Trained batch 756 batch loss 1.39600062 epoch total loss 1.39138985\n",
      "Trained batch 757 batch loss 1.30887949 epoch total loss 1.39128077\n",
      "Trained batch 758 batch loss 1.37671912 epoch total loss 1.39126158\n",
      "Trained batch 759 batch loss 1.39867902 epoch total loss 1.39127135\n",
      "Trained batch 760 batch loss 1.28163028 epoch total loss 1.39112711\n",
      "Trained batch 761 batch loss 1.27738547 epoch total loss 1.39097762\n",
      "Trained batch 762 batch loss 1.29664278 epoch total loss 1.39085376\n",
      "Trained batch 763 batch loss 1.41559863 epoch total loss 1.39088631\n",
      "Trained batch 764 batch loss 1.40397 epoch total loss 1.39090335\n",
      "Trained batch 765 batch loss 1.40509415 epoch total loss 1.39092195\n",
      "Trained batch 766 batch loss 1.38486886 epoch total loss 1.39091408\n",
      "Trained batch 767 batch loss 1.378106 epoch total loss 1.39089727\n",
      "Trained batch 768 batch loss 1.37991822 epoch total loss 1.39088297\n",
      "Trained batch 769 batch loss 1.49229765 epoch total loss 1.39101481\n",
      "Trained batch 770 batch loss 1.38499439 epoch total loss 1.39100707\n",
      "Trained batch 771 batch loss 1.3326757 epoch total loss 1.39093137\n",
      "Trained batch 772 batch loss 1.32478106 epoch total loss 1.39084578\n",
      "Trained batch 773 batch loss 1.34238601 epoch total loss 1.39078307\n",
      "Trained batch 774 batch loss 1.3137548 epoch total loss 1.39068353\n",
      "Trained batch 775 batch loss 1.14185536 epoch total loss 1.39036238\n",
      "Trained batch 776 batch loss 1.469033 epoch total loss 1.39046371\n",
      "Trained batch 777 batch loss 1.32356596 epoch total loss 1.39037776\n",
      "Trained batch 778 batch loss 1.37924886 epoch total loss 1.39036345\n",
      "Trained batch 779 batch loss 1.26024759 epoch total loss 1.39019644\n",
      "Trained batch 780 batch loss 1.26062572 epoch total loss 1.39003026\n",
      "Trained batch 781 batch loss 1.2087568 epoch total loss 1.38979816\n",
      "Trained batch 782 batch loss 1.10482597 epoch total loss 1.38943374\n",
      "Trained batch 783 batch loss 1.11010289 epoch total loss 1.38907707\n",
      "Trained batch 784 batch loss 1.28723371 epoch total loss 1.38894713\n",
      "Trained batch 785 batch loss 1.5510993 epoch total loss 1.38915384\n",
      "Trained batch 786 batch loss 1.67703581 epoch total loss 1.38952\n",
      "Trained batch 787 batch loss 1.33960259 epoch total loss 1.38945651\n",
      "Trained batch 788 batch loss 1.37827182 epoch total loss 1.38944244\n",
      "Trained batch 789 batch loss 1.30808961 epoch total loss 1.38933933\n",
      "Trained batch 790 batch loss 1.45973873 epoch total loss 1.38942838\n",
      "Trained batch 791 batch loss 1.43070078 epoch total loss 1.38948059\n",
      "Trained batch 792 batch loss 1.36331797 epoch total loss 1.38944745\n",
      "Trained batch 793 batch loss 1.38104641 epoch total loss 1.38943696\n",
      "Trained batch 794 batch loss 1.47612274 epoch total loss 1.38954604\n",
      "Trained batch 795 batch loss 1.35244393 epoch total loss 1.38949931\n",
      "Trained batch 796 batch loss 1.37174726 epoch total loss 1.38947701\n",
      "Trained batch 797 batch loss 1.33390701 epoch total loss 1.38940716\n",
      "Trained batch 798 batch loss 1.30558777 epoch total loss 1.38930213\n",
      "Trained batch 799 batch loss 1.36153579 epoch total loss 1.38926744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 800 batch loss 1.3818419 epoch total loss 1.38925815\n",
      "Trained batch 801 batch loss 1.3100692 epoch total loss 1.3891592\n",
      "Trained batch 802 batch loss 1.17663658 epoch total loss 1.3888942\n",
      "Trained batch 803 batch loss 1.3211689 epoch total loss 1.38880992\n",
      "Trained batch 804 batch loss 1.34873903 epoch total loss 1.38876009\n",
      "Trained batch 805 batch loss 1.3586812 epoch total loss 1.38872266\n",
      "Trained batch 806 batch loss 1.32312238 epoch total loss 1.38864124\n",
      "Trained batch 807 batch loss 1.37940466 epoch total loss 1.38862979\n",
      "Trained batch 808 batch loss 1.39913678 epoch total loss 1.38864291\n",
      "Trained batch 809 batch loss 1.20527601 epoch total loss 1.38841629\n",
      "Trained batch 810 batch loss 1.36199152 epoch total loss 1.38838363\n",
      "Trained batch 811 batch loss 1.22899294 epoch total loss 1.38818705\n",
      "Trained batch 812 batch loss 1.23529017 epoch total loss 1.3879987\n",
      "Trained batch 813 batch loss 1.21059847 epoch total loss 1.38778043\n",
      "Trained batch 814 batch loss 1.3140074 epoch total loss 1.38768971\n",
      "Trained batch 815 batch loss 1.25451887 epoch total loss 1.38752639\n",
      "Trained batch 816 batch loss 1.31413293 epoch total loss 1.38743639\n",
      "Trained batch 817 batch loss 1.27736449 epoch total loss 1.38730156\n",
      "Trained batch 818 batch loss 1.30789769 epoch total loss 1.38720453\n",
      "Trained batch 819 batch loss 1.26864195 epoch total loss 1.38705981\n",
      "Trained batch 820 batch loss 1.34587574 epoch total loss 1.3870095\n",
      "Trained batch 821 batch loss 1.39305878 epoch total loss 1.38701689\n",
      "Trained batch 822 batch loss 1.33047104 epoch total loss 1.38694799\n",
      "Trained batch 823 batch loss 1.36773396 epoch total loss 1.38692462\n",
      "Trained batch 824 batch loss 1.26351178 epoch total loss 1.3867749\n",
      "Trained batch 825 batch loss 1.28664792 epoch total loss 1.38665354\n",
      "Trained batch 826 batch loss 1.25937045 epoch total loss 1.3864994\n",
      "Trained batch 827 batch loss 1.25538123 epoch total loss 1.38634086\n",
      "Trained batch 828 batch loss 1.28631032 epoch total loss 1.38622\n",
      "Trained batch 829 batch loss 1.37081122 epoch total loss 1.3862015\n",
      "Trained batch 830 batch loss 1.38401878 epoch total loss 1.38619888\n",
      "Trained batch 831 batch loss 1.39243388 epoch total loss 1.38620639\n",
      "Trained batch 832 batch loss 1.3765142 epoch total loss 1.38619471\n",
      "Trained batch 833 batch loss 1.24896336 epoch total loss 1.38603\n",
      "Trained batch 834 batch loss 1.51203704 epoch total loss 1.38618112\n",
      "Trained batch 835 batch loss 1.37871158 epoch total loss 1.38617218\n",
      "Trained batch 836 batch loss 1.34233558 epoch total loss 1.3861196\n",
      "Trained batch 837 batch loss 1.46246445 epoch total loss 1.38621092\n",
      "Trained batch 838 batch loss 1.41192484 epoch total loss 1.38624156\n",
      "Trained batch 839 batch loss 1.49042928 epoch total loss 1.38636577\n",
      "Trained batch 840 batch loss 1.36854911 epoch total loss 1.38634455\n",
      "Trained batch 841 batch loss 1.35795188 epoch total loss 1.3863107\n",
      "Trained batch 842 batch loss 1.43530202 epoch total loss 1.38636887\n",
      "Trained batch 843 batch loss 1.54992855 epoch total loss 1.38656294\n",
      "Trained batch 844 batch loss 1.53359926 epoch total loss 1.38673711\n",
      "Trained batch 845 batch loss 1.32567322 epoch total loss 1.38666487\n",
      "Trained batch 846 batch loss 1.36475158 epoch total loss 1.386639\n",
      "Trained batch 847 batch loss 1.37739897 epoch total loss 1.38662815\n",
      "Trained batch 848 batch loss 1.23123837 epoch total loss 1.38644481\n",
      "Trained batch 849 batch loss 1.3032794 epoch total loss 1.38634682\n",
      "Trained batch 850 batch loss 1.30160308 epoch total loss 1.38624716\n",
      "Trained batch 851 batch loss 1.3093245 epoch total loss 1.3861568\n",
      "Trained batch 852 batch loss 1.45239961 epoch total loss 1.38623452\n",
      "Trained batch 853 batch loss 1.4381299 epoch total loss 1.38629532\n",
      "Trained batch 854 batch loss 1.3236289 epoch total loss 1.38622189\n",
      "Trained batch 855 batch loss 1.30274284 epoch total loss 1.38612425\n",
      "Trained batch 856 batch loss 1.29237223 epoch total loss 1.3860147\n",
      "Trained batch 857 batch loss 1.340693 epoch total loss 1.38596177\n",
      "Trained batch 858 batch loss 1.36298466 epoch total loss 1.38593507\n",
      "Trained batch 859 batch loss 1.33088171 epoch total loss 1.38587105\n",
      "Trained batch 860 batch loss 1.33936715 epoch total loss 1.38581693\n",
      "Trained batch 861 batch loss 1.35822964 epoch total loss 1.38578498\n",
      "Trained batch 862 batch loss 1.39370728 epoch total loss 1.38579416\n",
      "Trained batch 863 batch loss 1.36672795 epoch total loss 1.38577199\n",
      "Trained batch 864 batch loss 1.40643549 epoch total loss 1.38579607\n",
      "Trained batch 865 batch loss 1.39274 epoch total loss 1.38580406\n",
      "Trained batch 866 batch loss 1.5317347 epoch total loss 1.3859725\n",
      "Trained batch 867 batch loss 1.34204125 epoch total loss 1.38592184\n",
      "Trained batch 868 batch loss 1.35470605 epoch total loss 1.38588595\n",
      "Trained batch 869 batch loss 1.29299903 epoch total loss 1.38577902\n",
      "Trained batch 870 batch loss 1.32117736 epoch total loss 1.38570476\n",
      "Trained batch 871 batch loss 1.38898742 epoch total loss 1.38570857\n",
      "Trained batch 872 batch loss 1.36878622 epoch total loss 1.38568914\n",
      "Trained batch 873 batch loss 1.46519136 epoch total loss 1.38578022\n",
      "Trained batch 874 batch loss 1.29271185 epoch total loss 1.38567376\n",
      "Trained batch 875 batch loss 1.3719188 epoch total loss 1.38565803\n",
      "Trained batch 876 batch loss 1.30760038 epoch total loss 1.38556898\n",
      "Trained batch 877 batch loss 1.2910217 epoch total loss 1.38546121\n",
      "Trained batch 878 batch loss 1.37319207 epoch total loss 1.38544714\n",
      "Trained batch 879 batch loss 1.44244468 epoch total loss 1.38551211\n",
      "Trained batch 880 batch loss 1.42647016 epoch total loss 1.38555872\n",
      "Trained batch 881 batch loss 1.35241306 epoch total loss 1.38552105\n",
      "Trained batch 882 batch loss 1.43059373 epoch total loss 1.38557208\n",
      "Trained batch 883 batch loss 1.46406507 epoch total loss 1.38566101\n",
      "Trained batch 884 batch loss 1.32285035 epoch total loss 1.38559\n",
      "Trained batch 885 batch loss 1.3598088 epoch total loss 1.38556099\n",
      "Trained batch 886 batch loss 1.3953681 epoch total loss 1.38557208\n",
      "Trained batch 887 batch loss 1.3507818 epoch total loss 1.38553286\n",
      "Trained batch 888 batch loss 1.2570219 epoch total loss 1.38538826\n",
      "Trained batch 889 batch loss 1.36032963 epoch total loss 1.38536\n",
      "Trained batch 890 batch loss 1.34311604 epoch total loss 1.38531256\n",
      "Trained batch 891 batch loss 1.27270067 epoch total loss 1.3851862\n",
      "Trained batch 892 batch loss 1.32026768 epoch total loss 1.38511348\n",
      "Trained batch 893 batch loss 1.27723205 epoch total loss 1.38499272\n",
      "Trained batch 894 batch loss 1.27500737 epoch total loss 1.38486969\n",
      "Trained batch 895 batch loss 1.35934591 epoch total loss 1.3848412\n",
      "Trained batch 896 batch loss 1.3891567 epoch total loss 1.38484597\n",
      "Trained batch 897 batch loss 1.43270314 epoch total loss 1.38489938\n",
      "Trained batch 898 batch loss 1.4664396 epoch total loss 1.38499022\n",
      "Trained batch 899 batch loss 1.41749048 epoch total loss 1.38502634\n",
      "Trained batch 900 batch loss 1.25011611 epoch total loss 1.38487649\n",
      "Trained batch 901 batch loss 1.29671109 epoch total loss 1.38477862\n",
      "Trained batch 902 batch loss 1.20189714 epoch total loss 1.38457584\n",
      "Trained batch 903 batch loss 1.30534697 epoch total loss 1.38448811\n",
      "Trained batch 904 batch loss 1.37515962 epoch total loss 1.38447773\n",
      "Trained batch 905 batch loss 1.43373203 epoch total loss 1.38453209\n",
      "Trained batch 906 batch loss 1.39244342 epoch total loss 1.38454092\n",
      "Trained batch 907 batch loss 1.32964218 epoch total loss 1.38448036\n",
      "Trained batch 908 batch loss 1.26072288 epoch total loss 1.38434398\n",
      "Trained batch 909 batch loss 1.31079936 epoch total loss 1.38426316\n",
      "Trained batch 910 batch loss 1.24188375 epoch total loss 1.38410676\n",
      "Trained batch 911 batch loss 1.27724528 epoch total loss 1.38398933\n",
      "Trained batch 912 batch loss 1.29928625 epoch total loss 1.38389659\n",
      "Trained batch 913 batch loss 1.43612623 epoch total loss 1.38395381\n",
      "Trained batch 914 batch loss 1.36120951 epoch total loss 1.38392889\n",
      "Trained batch 915 batch loss 1.43384457 epoch total loss 1.38398349\n",
      "Trained batch 916 batch loss 1.54940236 epoch total loss 1.38416409\n",
      "Trained batch 917 batch loss 1.62110686 epoch total loss 1.38442242\n",
      "Trained batch 918 batch loss 1.47479868 epoch total loss 1.38452101\n",
      "Trained batch 919 batch loss 1.27803707 epoch total loss 1.38440514\n",
      "Trained batch 920 batch loss 1.29665637 epoch total loss 1.38430977\n",
      "Trained batch 921 batch loss 1.34194458 epoch total loss 1.38426363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 922 batch loss 1.2871536 epoch total loss 1.38415837\n",
      "Trained batch 923 batch loss 1.32274115 epoch total loss 1.38409173\n",
      "Trained batch 924 batch loss 1.250489 epoch total loss 1.38394725\n",
      "Trained batch 925 batch loss 1.32069981 epoch total loss 1.38387883\n",
      "Trained batch 926 batch loss 1.26263654 epoch total loss 1.38374794\n",
      "Trained batch 927 batch loss 1.2327584 epoch total loss 1.3835851\n",
      "Trained batch 928 batch loss 1.24548638 epoch total loss 1.38343632\n",
      "Trained batch 929 batch loss 1.39818108 epoch total loss 1.38345218\n",
      "Trained batch 930 batch loss 1.25652528 epoch total loss 1.38331556\n",
      "Trained batch 931 batch loss 1.25087607 epoch total loss 1.38317335\n",
      "Trained batch 932 batch loss 1.20844257 epoch total loss 1.38298595\n",
      "Trained batch 933 batch loss 1.40087199 epoch total loss 1.38300514\n",
      "Trained batch 934 batch loss 1.33350575 epoch total loss 1.38295209\n",
      "Trained batch 935 batch loss 1.38906348 epoch total loss 1.38295865\n",
      "Trained batch 936 batch loss 1.4212358 epoch total loss 1.38299954\n",
      "Trained batch 937 batch loss 1.29701257 epoch total loss 1.38290775\n",
      "Trained batch 938 batch loss 1.27800751 epoch total loss 1.38279581\n",
      "Trained batch 939 batch loss 1.342507 epoch total loss 1.38275301\n",
      "Trained batch 940 batch loss 1.28300834 epoch total loss 1.3826468\n",
      "Trained batch 941 batch loss 1.28495038 epoch total loss 1.38254297\n",
      "Trained batch 942 batch loss 1.15317488 epoch total loss 1.38229942\n",
      "Trained batch 943 batch loss 1.16545224 epoch total loss 1.38206947\n",
      "Trained batch 944 batch loss 1.29001045 epoch total loss 1.38197196\n",
      "Trained batch 945 batch loss 1.29052401 epoch total loss 1.38187516\n",
      "Trained batch 946 batch loss 1.2708 epoch total loss 1.38175774\n",
      "Trained batch 947 batch loss 1.23310518 epoch total loss 1.38160086\n",
      "Trained batch 948 batch loss 1.30426979 epoch total loss 1.38151932\n",
      "Trained batch 949 batch loss 1.19766784 epoch total loss 1.38132548\n",
      "Trained batch 950 batch loss 1.632267 epoch total loss 1.38158977\n",
      "Trained batch 951 batch loss 1.44286537 epoch total loss 1.38165414\n",
      "Trained batch 952 batch loss 1.48557591 epoch total loss 1.38176334\n",
      "Trained batch 953 batch loss 1.39957356 epoch total loss 1.38178205\n",
      "Trained batch 954 batch loss 1.29885828 epoch total loss 1.38169503\n",
      "Trained batch 955 batch loss 1.16106474 epoch total loss 1.381464\n",
      "Trained batch 956 batch loss 1.09322226 epoch total loss 1.38116252\n",
      "Trained batch 957 batch loss 1.30209827 epoch total loss 1.38107991\n",
      "Trained batch 958 batch loss 1.25050092 epoch total loss 1.38094366\n",
      "Trained batch 959 batch loss 1.31230879 epoch total loss 1.38087201\n",
      "Trained batch 960 batch loss 1.34215271 epoch total loss 1.3808316\n",
      "Trained batch 961 batch loss 1.20578015 epoch total loss 1.38064957\n",
      "Trained batch 962 batch loss 1.30884171 epoch total loss 1.38057494\n",
      "Trained batch 963 batch loss 1.4089601 epoch total loss 1.38060439\n",
      "Trained batch 964 batch loss 1.42409265 epoch total loss 1.38064945\n",
      "Trained batch 965 batch loss 1.26740301 epoch total loss 1.38053215\n",
      "Trained batch 966 batch loss 1.14702725 epoch total loss 1.38029039\n",
      "Trained batch 967 batch loss 1.26577771 epoch total loss 1.3801719\n",
      "Trained batch 968 batch loss 1.29286206 epoch total loss 1.38008165\n",
      "Trained batch 969 batch loss 1.25954485 epoch total loss 1.37995732\n",
      "Trained batch 970 batch loss 1.40547991 epoch total loss 1.37998366\n",
      "Trained batch 971 batch loss 1.61731505 epoch total loss 1.38022804\n",
      "Trained batch 972 batch loss 1.55998993 epoch total loss 1.38041294\n",
      "Trained batch 973 batch loss 1.457497 epoch total loss 1.38049221\n",
      "Trained batch 974 batch loss 1.24093699 epoch total loss 1.38034892\n",
      "Trained batch 975 batch loss 1.25084043 epoch total loss 1.38021612\n",
      "Trained batch 976 batch loss 1.37212467 epoch total loss 1.38020778\n",
      "Trained batch 977 batch loss 1.55342424 epoch total loss 1.38038504\n",
      "Trained batch 978 batch loss 1.36308849 epoch total loss 1.3803674\n",
      "Trained batch 979 batch loss 1.412673 epoch total loss 1.38040042\n",
      "Trained batch 980 batch loss 1.49666762 epoch total loss 1.38051903\n",
      "Trained batch 981 batch loss 1.44374847 epoch total loss 1.38058352\n",
      "Trained batch 982 batch loss 1.4106282 epoch total loss 1.38061416\n",
      "Trained batch 983 batch loss 1.35077024 epoch total loss 1.38058376\n",
      "Trained batch 984 batch loss 1.26811802 epoch total loss 1.38046944\n",
      "Trained batch 985 batch loss 1.3157891 epoch total loss 1.38040376\n",
      "Trained batch 986 batch loss 1.37477303 epoch total loss 1.38039804\n",
      "Trained batch 987 batch loss 1.39902043 epoch total loss 1.38041699\n",
      "Trained batch 988 batch loss 1.44434893 epoch total loss 1.38048172\n",
      "Trained batch 989 batch loss 1.45843506 epoch total loss 1.3805604\n",
      "Trained batch 990 batch loss 1.39595079 epoch total loss 1.38057601\n",
      "Trained batch 991 batch loss 1.41443431 epoch total loss 1.38061023\n",
      "Trained batch 992 batch loss 1.31061959 epoch total loss 1.38053966\n",
      "Trained batch 993 batch loss 1.2955 epoch total loss 1.38045406\n",
      "Trained batch 994 batch loss 1.31792176 epoch total loss 1.38039112\n",
      "Trained batch 995 batch loss 1.26291525 epoch total loss 1.3802731\n",
      "Trained batch 996 batch loss 1.32082963 epoch total loss 1.38021338\n",
      "Trained batch 997 batch loss 1.33737707 epoch total loss 1.38017046\n",
      "Trained batch 998 batch loss 1.46904039 epoch total loss 1.38025939\n",
      "Trained batch 999 batch loss 1.43922877 epoch total loss 1.3803184\n",
      "Trained batch 1000 batch loss 1.24598408 epoch total loss 1.38018405\n",
      "Trained batch 1001 batch loss 1.18867254 epoch total loss 1.37999284\n",
      "Trained batch 1002 batch loss 1.2219739 epoch total loss 1.37983501\n",
      "Trained batch 1003 batch loss 1.39447939 epoch total loss 1.37984967\n",
      "Trained batch 1004 batch loss 1.34365821 epoch total loss 1.37981367\n",
      "Trained batch 1005 batch loss 1.37162852 epoch total loss 1.37980545\n",
      "Trained batch 1006 batch loss 1.41598535 epoch total loss 1.37984145\n",
      "Trained batch 1007 batch loss 1.34134948 epoch total loss 1.37980318\n",
      "Trained batch 1008 batch loss 1.39294982 epoch total loss 1.37981617\n",
      "Trained batch 1009 batch loss 1.27703691 epoch total loss 1.37971425\n",
      "Trained batch 1010 batch loss 1.34108889 epoch total loss 1.37967598\n",
      "Trained batch 1011 batch loss 1.40218091 epoch total loss 1.37969828\n",
      "Trained batch 1012 batch loss 1.24375045 epoch total loss 1.37956405\n",
      "Trained batch 1013 batch loss 1.34805584 epoch total loss 1.37953281\n",
      "Trained batch 1014 batch loss 1.3287077 epoch total loss 1.37948275\n",
      "Trained batch 1015 batch loss 1.34380841 epoch total loss 1.37944758\n",
      "Trained batch 1016 batch loss 1.37298441 epoch total loss 1.37944114\n",
      "Trained batch 1017 batch loss 1.36715364 epoch total loss 1.3794291\n",
      "Trained batch 1018 batch loss 1.37493718 epoch total loss 1.37942457\n",
      "Trained batch 1019 batch loss 1.36784041 epoch total loss 1.37941325\n",
      "Trained batch 1020 batch loss 1.40052283 epoch total loss 1.37943387\n",
      "Trained batch 1021 batch loss 1.24120069 epoch total loss 1.37929857\n",
      "Trained batch 1022 batch loss 1.37154031 epoch total loss 1.37929094\n",
      "Trained batch 1023 batch loss 1.28497529 epoch total loss 1.37919879\n",
      "Trained batch 1024 batch loss 1.35746837 epoch total loss 1.37917757\n",
      "Trained batch 1025 batch loss 1.35092497 epoch total loss 1.37915\n",
      "Trained batch 1026 batch loss 1.30730653 epoch total loss 1.37907994\n",
      "Trained batch 1027 batch loss 1.34715199 epoch total loss 1.37904882\n",
      "Trained batch 1028 batch loss 1.48963666 epoch total loss 1.37915647\n",
      "Trained batch 1029 batch loss 1.14601362 epoch total loss 1.37892985\n",
      "Trained batch 1030 batch loss 1.15291786 epoch total loss 1.37871051\n",
      "Trained batch 1031 batch loss 1.29120541 epoch total loss 1.37862563\n",
      "Trained batch 1032 batch loss 1.31970882 epoch total loss 1.37856853\n",
      "Trained batch 1033 batch loss 1.17095661 epoch total loss 1.37836754\n",
      "Trained batch 1034 batch loss 1.31786609 epoch total loss 1.37830901\n",
      "Trained batch 1035 batch loss 1.33917212 epoch total loss 1.3782711\n",
      "Trained batch 1036 batch loss 1.34903884 epoch total loss 1.37824285\n",
      "Trained batch 1037 batch loss 1.39786232 epoch total loss 1.3782618\n",
      "Trained batch 1038 batch loss 1.304703 epoch total loss 1.37819088\n",
      "Trained batch 1039 batch loss 1.57781339 epoch total loss 1.37838292\n",
      "Trained batch 1040 batch loss 1.62018526 epoch total loss 1.3786155\n",
      "Trained batch 1041 batch loss 1.46535993 epoch total loss 1.37869883\n",
      "Trained batch 1042 batch loss 1.15146911 epoch total loss 1.37848079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1043 batch loss 1.21746731 epoch total loss 1.3783263\n",
      "Trained batch 1044 batch loss 1.43619347 epoch total loss 1.37838173\n",
      "Trained batch 1045 batch loss 1.38873887 epoch total loss 1.37839174\n",
      "Trained batch 1046 batch loss 1.43464255 epoch total loss 1.37844551\n",
      "Trained batch 1047 batch loss 1.36887717 epoch total loss 1.37843645\n",
      "Trained batch 1048 batch loss 1.41263652 epoch total loss 1.37846899\n",
      "Trained batch 1049 batch loss 1.42433763 epoch total loss 1.37851274\n",
      "Trained batch 1050 batch loss 1.43170786 epoch total loss 1.3785634\n",
      "Trained batch 1051 batch loss 1.46291757 epoch total loss 1.37864363\n",
      "Trained batch 1052 batch loss 1.36766744 epoch total loss 1.37863326\n",
      "Trained batch 1053 batch loss 1.38797903 epoch total loss 1.37864208\n",
      "Trained batch 1054 batch loss 1.31178439 epoch total loss 1.37857866\n",
      "Trained batch 1055 batch loss 1.2708292 epoch total loss 1.3784765\n",
      "Trained batch 1056 batch loss 1.20692897 epoch total loss 1.37831402\n",
      "Trained batch 1057 batch loss 1.19081962 epoch total loss 1.37813663\n",
      "Trained batch 1058 batch loss 1.38968968 epoch total loss 1.37814748\n",
      "Trained batch 1059 batch loss 1.31912 epoch total loss 1.37809181\n",
      "Trained batch 1060 batch loss 1.43718922 epoch total loss 1.37814748\n",
      "Trained batch 1061 batch loss 1.38598394 epoch total loss 1.37815487\n",
      "Trained batch 1062 batch loss 1.38834095 epoch total loss 1.37816441\n",
      "Trained batch 1063 batch loss 1.37223077 epoch total loss 1.37815881\n",
      "Trained batch 1064 batch loss 1.38733768 epoch total loss 1.37816739\n",
      "Trained batch 1065 batch loss 1.30493343 epoch total loss 1.37809861\n",
      "Trained batch 1066 batch loss 1.30077577 epoch total loss 1.37802613\n",
      "Trained batch 1067 batch loss 1.33428979 epoch total loss 1.37798512\n",
      "Trained batch 1068 batch loss 1.47963548 epoch total loss 1.37808037\n",
      "Trained batch 1069 batch loss 1.35298729 epoch total loss 1.37805688\n",
      "Trained batch 1070 batch loss 1.35924315 epoch total loss 1.37803936\n",
      "Trained batch 1071 batch loss 1.34741712 epoch total loss 1.37801075\n",
      "Trained batch 1072 batch loss 1.17902088 epoch total loss 1.37782514\n",
      "Trained batch 1073 batch loss 1.33316624 epoch total loss 1.37778354\n",
      "Trained batch 1074 batch loss 1.36786199 epoch total loss 1.37777436\n",
      "Trained batch 1075 batch loss 1.46230698 epoch total loss 1.37785292\n",
      "Trained batch 1076 batch loss 1.3668263 epoch total loss 1.37784266\n",
      "Trained batch 1077 batch loss 1.36050296 epoch total loss 1.37782657\n",
      "Trained batch 1078 batch loss 1.26250899 epoch total loss 1.37771952\n",
      "Trained batch 1079 batch loss 1.4271884 epoch total loss 1.37776542\n",
      "Trained batch 1080 batch loss 1.24034941 epoch total loss 1.37763822\n",
      "Trained batch 1081 batch loss 1.27339149 epoch total loss 1.37754178\n",
      "Trained batch 1082 batch loss 1.32590497 epoch total loss 1.3774941\n",
      "Trained batch 1083 batch loss 1.42903852 epoch total loss 1.37754178\n",
      "Trained batch 1084 batch loss 1.34926724 epoch total loss 1.37751567\n",
      "Trained batch 1085 batch loss 1.2647934 epoch total loss 1.37741172\n",
      "Trained batch 1086 batch loss 1.31482434 epoch total loss 1.37735403\n",
      "Trained batch 1087 batch loss 1.24959195 epoch total loss 1.3772366\n",
      "Trained batch 1088 batch loss 1.25862384 epoch total loss 1.37712765\n",
      "Trained batch 1089 batch loss 1.39409184 epoch total loss 1.37714314\n",
      "Trained batch 1090 batch loss 1.33102441 epoch total loss 1.37710083\n",
      "Trained batch 1091 batch loss 1.58873808 epoch total loss 1.37729478\n",
      "Trained batch 1092 batch loss 1.55647683 epoch total loss 1.37745893\n",
      "Trained batch 1093 batch loss 1.44904304 epoch total loss 1.3775245\n",
      "Trained batch 1094 batch loss 1.36210632 epoch total loss 1.37751031\n",
      "Trained batch 1095 batch loss 1.4340688 epoch total loss 1.37756205\n",
      "Trained batch 1096 batch loss 1.37944412 epoch total loss 1.37756371\n",
      "Trained batch 1097 batch loss 1.35787845 epoch total loss 1.37754583\n",
      "Trained batch 1098 batch loss 1.36359 epoch total loss 1.37753308\n",
      "Trained batch 1099 batch loss 1.31494331 epoch total loss 1.37747622\n",
      "Trained batch 1100 batch loss 1.28031671 epoch total loss 1.37738776\n",
      "Trained batch 1101 batch loss 1.44884658 epoch total loss 1.37745273\n",
      "Trained batch 1102 batch loss 1.38017428 epoch total loss 1.37745512\n",
      "Trained batch 1103 batch loss 1.28458381 epoch total loss 1.37737095\n",
      "Trained batch 1104 batch loss 1.31486857 epoch total loss 1.37731421\n",
      "Trained batch 1105 batch loss 1.32661688 epoch total loss 1.37726843\n",
      "Trained batch 1106 batch loss 1.21682644 epoch total loss 1.37712336\n",
      "Trained batch 1107 batch loss 1.31045103 epoch total loss 1.37706304\n",
      "Trained batch 1108 batch loss 1.27878451 epoch total loss 1.37697434\n",
      "Trained batch 1109 batch loss 1.12400842 epoch total loss 1.3767463\n",
      "Trained batch 1110 batch loss 1.15047681 epoch total loss 1.37654245\n",
      "Trained batch 1111 batch loss 1.24665034 epoch total loss 1.37642562\n",
      "Trained batch 1112 batch loss 1.25003541 epoch total loss 1.3763119\n",
      "Trained batch 1113 batch loss 1.2907567 epoch total loss 1.37623513\n",
      "Trained batch 1114 batch loss 1.41015327 epoch total loss 1.37626553\n",
      "Trained batch 1115 batch loss 1.43521905 epoch total loss 1.37631834\n",
      "Trained batch 1116 batch loss 1.34294128 epoch total loss 1.37628841\n",
      "Trained batch 1117 batch loss 1.2818 epoch total loss 1.37620389\n",
      "Trained batch 1118 batch loss 1.2488941 epoch total loss 1.37609\n",
      "Trained batch 1119 batch loss 1.34297872 epoch total loss 1.37606049\n",
      "Trained batch 1120 batch loss 1.38204384 epoch total loss 1.37606585\n",
      "Trained batch 1121 batch loss 1.14315 epoch total loss 1.37585807\n",
      "Trained batch 1122 batch loss 1.04582858 epoch total loss 1.37556386\n",
      "Trained batch 1123 batch loss 1.16585219 epoch total loss 1.37537718\n",
      "Trained batch 1124 batch loss 1.19614518 epoch total loss 1.3752178\n",
      "Trained batch 1125 batch loss 1.48776019 epoch total loss 1.37531781\n",
      "Trained batch 1126 batch loss 1.53068233 epoch total loss 1.37545574\n",
      "Trained batch 1127 batch loss 1.49504948 epoch total loss 1.37556183\n",
      "Trained batch 1128 batch loss 1.44624639 epoch total loss 1.37562454\n",
      "Trained batch 1129 batch loss 1.41977859 epoch total loss 1.37566364\n",
      "Trained batch 1130 batch loss 1.31896579 epoch total loss 1.37561345\n",
      "Trained batch 1131 batch loss 1.14489019 epoch total loss 1.37540948\n",
      "Trained batch 1132 batch loss 1.21971786 epoch total loss 1.37527192\n",
      "Trained batch 1133 batch loss 1.10062933 epoch total loss 1.37502956\n",
      "Trained batch 1134 batch loss 1.5691644 epoch total loss 1.37520075\n",
      "Trained batch 1135 batch loss 1.45328009 epoch total loss 1.37526953\n",
      "Trained batch 1136 batch loss 1.3613143 epoch total loss 1.37525725\n",
      "Trained batch 1137 batch loss 1.2814523 epoch total loss 1.37517476\n",
      "Trained batch 1138 batch loss 1.36587799 epoch total loss 1.37516654\n",
      "Trained batch 1139 batch loss 1.24337864 epoch total loss 1.3750509\n",
      "Trained batch 1140 batch loss 1.29162729 epoch total loss 1.37497771\n",
      "Trained batch 1141 batch loss 1.29570651 epoch total loss 1.37490821\n",
      "Trained batch 1142 batch loss 1.30684853 epoch total loss 1.3748486\n",
      "Trained batch 1143 batch loss 1.31095219 epoch total loss 1.3747927\n",
      "Trained batch 1144 batch loss 1.43605578 epoch total loss 1.37484622\n",
      "Trained batch 1145 batch loss 1.32347786 epoch total loss 1.3748014\n",
      "Trained batch 1146 batch loss 1.32953668 epoch total loss 1.37476194\n",
      "Trained batch 1147 batch loss 1.2467221 epoch total loss 1.37465024\n",
      "Trained batch 1148 batch loss 1.24943674 epoch total loss 1.37454116\n",
      "Trained batch 1149 batch loss 1.26665533 epoch total loss 1.37444723\n",
      "Trained batch 1150 batch loss 1.35469353 epoch total loss 1.37443006\n",
      "Trained batch 1151 batch loss 1.36890435 epoch total loss 1.37442529\n",
      "Trained batch 1152 batch loss 1.37837291 epoch total loss 1.37442875\n",
      "Trained batch 1153 batch loss 1.31291151 epoch total loss 1.37437534\n",
      "Trained batch 1154 batch loss 1.48051417 epoch total loss 1.37446725\n",
      "Trained batch 1155 batch loss 1.56219792 epoch total loss 1.37462986\n",
      "Trained batch 1156 batch loss 1.35063481 epoch total loss 1.37460911\n",
      "Trained batch 1157 batch loss 1.35311794 epoch total loss 1.37459052\n",
      "Trained batch 1158 batch loss 1.33148885 epoch total loss 1.37455332\n",
      "Trained batch 1159 batch loss 1.34386969 epoch total loss 1.37452686\n",
      "Trained batch 1160 batch loss 1.33992362 epoch total loss 1.37449706\n",
      "Trained batch 1161 batch loss 1.39464688 epoch total loss 1.37451446\n",
      "Trained batch 1162 batch loss 1.35287237 epoch total loss 1.37449586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1163 batch loss 1.47787547 epoch total loss 1.37458479\n",
      "Trained batch 1164 batch loss 1.4692297 epoch total loss 1.37466609\n",
      "Trained batch 1165 batch loss 1.51797104 epoch total loss 1.37478912\n",
      "Trained batch 1166 batch loss 1.53330445 epoch total loss 1.37492502\n",
      "Trained batch 1167 batch loss 1.42222524 epoch total loss 1.37496555\n",
      "Trained batch 1168 batch loss 1.50904286 epoch total loss 1.37508035\n",
      "Trained batch 1169 batch loss 1.36939359 epoch total loss 1.37507546\n",
      "Trained batch 1170 batch loss 1.47816384 epoch total loss 1.37516356\n",
      "Trained batch 1171 batch loss 1.43356645 epoch total loss 1.3752135\n",
      "Trained batch 1172 batch loss 1.36228526 epoch total loss 1.37520254\n",
      "Trained batch 1173 batch loss 1.38812983 epoch total loss 1.3752135\n",
      "Trained batch 1174 batch loss 1.36008549 epoch total loss 1.37520063\n",
      "Trained batch 1175 batch loss 1.37470138 epoch total loss 1.37520027\n",
      "Trained batch 1176 batch loss 1.31582248 epoch total loss 1.37514973\n",
      "Trained batch 1177 batch loss 1.3241353 epoch total loss 1.37510645\n",
      "Trained batch 1178 batch loss 1.22725677 epoch total loss 1.37498093\n",
      "Trained batch 1179 batch loss 1.32115972 epoch total loss 1.37493527\n",
      "Trained batch 1180 batch loss 1.33254206 epoch total loss 1.37489939\n",
      "Trained batch 1181 batch loss 1.28338492 epoch total loss 1.37482178\n",
      "Trained batch 1182 batch loss 1.33959198 epoch total loss 1.37479198\n",
      "Trained batch 1183 batch loss 1.2404294 epoch total loss 1.37467849\n",
      "Trained batch 1184 batch loss 1.29834819 epoch total loss 1.374614\n",
      "Trained batch 1185 batch loss 1.33432364 epoch total loss 1.37458\n",
      "Trained batch 1186 batch loss 1.34263694 epoch total loss 1.37455308\n",
      "Trained batch 1187 batch loss 1.28969944 epoch total loss 1.37448156\n",
      "Trained batch 1188 batch loss 1.22052419 epoch total loss 1.3743521\n",
      "Trained batch 1189 batch loss 1.30143142 epoch total loss 1.3742907\n",
      "Trained batch 1190 batch loss 1.27478921 epoch total loss 1.37420702\n",
      "Trained batch 1191 batch loss 1.2477386 epoch total loss 1.3741008\n",
      "Trained batch 1192 batch loss 1.3978858 epoch total loss 1.37412071\n",
      "Trained batch 1193 batch loss 1.37130618 epoch total loss 1.37411845\n",
      "Trained batch 1194 batch loss 1.3206315 epoch total loss 1.37407362\n",
      "Trained batch 1195 batch loss 1.3263793 epoch total loss 1.37403381\n",
      "Trained batch 1196 batch loss 1.42736256 epoch total loss 1.37407839\n",
      "Trained batch 1197 batch loss 1.44687915 epoch total loss 1.37413919\n",
      "Trained batch 1198 batch loss 1.3384223 epoch total loss 1.37410939\n",
      "Trained batch 1199 batch loss 1.36357665 epoch total loss 1.37410057\n",
      "Trained batch 1200 batch loss 1.39108658 epoch total loss 1.37411463\n",
      "Trained batch 1201 batch loss 1.36129308 epoch total loss 1.37410402\n",
      "Trained batch 1202 batch loss 1.35777533 epoch total loss 1.37409043\n",
      "Trained batch 1203 batch loss 1.45471263 epoch total loss 1.37415743\n",
      "Trained batch 1204 batch loss 1.40977538 epoch total loss 1.37418711\n",
      "Trained batch 1205 batch loss 1.39791536 epoch total loss 1.37420678\n",
      "Trained batch 1206 batch loss 1.36173546 epoch total loss 1.37419641\n",
      "Trained batch 1207 batch loss 1.3156091 epoch total loss 1.37414789\n",
      "Trained batch 1208 batch loss 1.37552142 epoch total loss 1.37414896\n",
      "Trained batch 1209 batch loss 1.3366909 epoch total loss 1.37411797\n",
      "Trained batch 1210 batch loss 1.33359838 epoch total loss 1.37408447\n",
      "Trained batch 1211 batch loss 1.35374439 epoch total loss 1.37406766\n",
      "Trained batch 1212 batch loss 1.36631501 epoch total loss 1.37406135\n",
      "Trained batch 1213 batch loss 1.22090936 epoch total loss 1.3739351\n",
      "Trained batch 1214 batch loss 1.18363571 epoch total loss 1.37377834\n",
      "Trained batch 1215 batch loss 1.33160949 epoch total loss 1.37374365\n",
      "Trained batch 1216 batch loss 1.36062682 epoch total loss 1.37373281\n",
      "Trained batch 1217 batch loss 1.30175829 epoch total loss 1.37367368\n",
      "Trained batch 1218 batch loss 1.30694437 epoch total loss 1.37361884\n",
      "Trained batch 1219 batch loss 1.33807027 epoch total loss 1.37358963\n",
      "Trained batch 1220 batch loss 1.14969099 epoch total loss 1.37340605\n",
      "Trained batch 1221 batch loss 1.15356815 epoch total loss 1.37322605\n",
      "Trained batch 1222 batch loss 1.30083263 epoch total loss 1.3731668\n",
      "Trained batch 1223 batch loss 1.38322687 epoch total loss 1.37317491\n",
      "Trained batch 1224 batch loss 1.46210372 epoch total loss 1.37324762\n",
      "Trained batch 1225 batch loss 1.35243487 epoch total loss 1.37323058\n",
      "Trained batch 1226 batch loss 1.4898932 epoch total loss 1.37332571\n",
      "Trained batch 1227 batch loss 1.37358117 epoch total loss 1.37332594\n",
      "Trained batch 1228 batch loss 1.31929505 epoch total loss 1.37328196\n",
      "Trained batch 1229 batch loss 1.34048319 epoch total loss 1.37325525\n",
      "Trained batch 1230 batch loss 1.35335565 epoch total loss 1.37323916\n",
      "Trained batch 1231 batch loss 1.46295118 epoch total loss 1.373312\n",
      "Trained batch 1232 batch loss 1.31593263 epoch total loss 1.37326539\n",
      "Trained batch 1233 batch loss 1.41035533 epoch total loss 1.37329543\n",
      "Trained batch 1234 batch loss 1.423002 epoch total loss 1.37333572\n",
      "Trained batch 1235 batch loss 1.36598122 epoch total loss 1.37332976\n",
      "Trained batch 1236 batch loss 1.24290037 epoch total loss 1.37322426\n",
      "Trained batch 1237 batch loss 1.19985652 epoch total loss 1.37308407\n",
      "Trained batch 1238 batch loss 1.151214 epoch total loss 1.3729049\n",
      "Trained batch 1239 batch loss 1.43665743 epoch total loss 1.37295628\n",
      "Trained batch 1240 batch loss 1.4375751 epoch total loss 1.37300849\n",
      "Trained batch 1241 batch loss 1.34742308 epoch total loss 1.37298787\n",
      "Trained batch 1242 batch loss 1.45343101 epoch total loss 1.37305272\n",
      "Trained batch 1243 batch loss 1.35702705 epoch total loss 1.37303984\n",
      "Trained batch 1244 batch loss 1.30795407 epoch total loss 1.37298751\n",
      "Trained batch 1245 batch loss 1.23872805 epoch total loss 1.37287974\n",
      "Trained batch 1246 batch loss 1.29003787 epoch total loss 1.37281322\n",
      "Trained batch 1247 batch loss 1.33022785 epoch total loss 1.37277901\n",
      "Trained batch 1248 batch loss 1.35236549 epoch total loss 1.37276268\n",
      "Trained batch 1249 batch loss 1.38699329 epoch total loss 1.37277412\n",
      "Trained batch 1250 batch loss 1.34671319 epoch total loss 1.37275326\n",
      "Trained batch 1251 batch loss 1.35935092 epoch total loss 1.37274253\n",
      "Trained batch 1252 batch loss 1.38481927 epoch total loss 1.37275219\n",
      "Trained batch 1253 batch loss 1.43907332 epoch total loss 1.37280512\n",
      "Trained batch 1254 batch loss 1.45027709 epoch total loss 1.37286687\n",
      "Trained batch 1255 batch loss 1.42231297 epoch total loss 1.37290633\n",
      "Trained batch 1256 batch loss 1.49873 epoch total loss 1.37300658\n",
      "Trained batch 1257 batch loss 1.38571048 epoch total loss 1.37301672\n",
      "Trained batch 1258 batch loss 1.28397012 epoch total loss 1.3729459\n",
      "Trained batch 1259 batch loss 1.32540393 epoch total loss 1.37290812\n",
      "Trained batch 1260 batch loss 1.25049782 epoch total loss 1.37281096\n",
      "Trained batch 1261 batch loss 1.29377317 epoch total loss 1.37274837\n",
      "Trained batch 1262 batch loss 1.26206231 epoch total loss 1.37266064\n",
      "Trained batch 1263 batch loss 1.33626795 epoch total loss 1.37263191\n",
      "Trained batch 1264 batch loss 1.36554384 epoch total loss 1.3726263\n",
      "Trained batch 1265 batch loss 1.34083569 epoch total loss 1.37260115\n",
      "Trained batch 1266 batch loss 1.3220706 epoch total loss 1.37256122\n",
      "Trained batch 1267 batch loss 1.45000815 epoch total loss 1.37262225\n",
      "Trained batch 1268 batch loss 1.28996992 epoch total loss 1.37255704\n",
      "Trained batch 1269 batch loss 1.37789762 epoch total loss 1.37256134\n",
      "Trained batch 1270 batch loss 1.29782319 epoch total loss 1.37250245\n",
      "Trained batch 1271 batch loss 1.24183512 epoch total loss 1.37239969\n",
      "Trained batch 1272 batch loss 1.25715053 epoch total loss 1.37230909\n",
      "Trained batch 1273 batch loss 1.31470418 epoch total loss 1.37226379\n",
      "Trained batch 1274 batch loss 1.27711403 epoch total loss 1.37218916\n",
      "Trained batch 1275 batch loss 1.3887043 epoch total loss 1.37220204\n",
      "Trained batch 1276 batch loss 1.48796344 epoch total loss 1.37229276\n",
      "Trained batch 1277 batch loss 1.39650822 epoch total loss 1.37231171\n",
      "Trained batch 1278 batch loss 1.16119647 epoch total loss 1.37214649\n",
      "Trained batch 1279 batch loss 1.36220694 epoch total loss 1.37213874\n",
      "Trained batch 1280 batch loss 1.2497046 epoch total loss 1.37204313\n",
      "Trained batch 1281 batch loss 1.20335269 epoch total loss 1.37191141\n",
      "Trained batch 1282 batch loss 1.27664244 epoch total loss 1.37183714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1283 batch loss 1.4353956 epoch total loss 1.37188673\n",
      "Trained batch 1284 batch loss 1.32957041 epoch total loss 1.37185371\n",
      "Trained batch 1285 batch loss 1.37906766 epoch total loss 1.37185931\n",
      "Trained batch 1286 batch loss 1.30978417 epoch total loss 1.37181103\n",
      "Trained batch 1287 batch loss 1.31665707 epoch total loss 1.37176824\n",
      "Trained batch 1288 batch loss 1.22824156 epoch total loss 1.37165678\n",
      "Trained batch 1289 batch loss 1.33088481 epoch total loss 1.37162519\n",
      "Trained batch 1290 batch loss 1.27786756 epoch total loss 1.37155247\n",
      "Trained batch 1291 batch loss 1.25961041 epoch total loss 1.3714658\n",
      "Trained batch 1292 batch loss 1.26365125 epoch total loss 1.37138236\n",
      "Trained batch 1293 batch loss 1.20137072 epoch total loss 1.37125099\n",
      "Trained batch 1294 batch loss 1.41255784 epoch total loss 1.37128294\n",
      "Trained batch 1295 batch loss 1.38524568 epoch total loss 1.37129366\n",
      "Trained batch 1296 batch loss 1.29833567 epoch total loss 1.3712374\n",
      "Trained batch 1297 batch loss 1.21157265 epoch total loss 1.37111425\n",
      "Trained batch 1298 batch loss 1.15366149 epoch total loss 1.37094676\n",
      "Trained batch 1299 batch loss 1.09276807 epoch total loss 1.37073267\n",
      "Trained batch 1300 batch loss 1.21361899 epoch total loss 1.37061179\n",
      "Trained batch 1301 batch loss 1.14917612 epoch total loss 1.37044156\n",
      "Trained batch 1302 batch loss 1.2556721 epoch total loss 1.37035334\n",
      "Trained batch 1303 batch loss 1.25531888 epoch total loss 1.37026513\n",
      "Trained batch 1304 batch loss 1.23248315 epoch total loss 1.37015951\n",
      "Trained batch 1305 batch loss 1.35772955 epoch total loss 1.37015\n",
      "Trained batch 1306 batch loss 1.29962039 epoch total loss 1.37009597\n",
      "Trained batch 1307 batch loss 1.2213999 epoch total loss 1.36998224\n",
      "Trained batch 1308 batch loss 1.23254848 epoch total loss 1.3698771\n",
      "Trained batch 1309 batch loss 1.33112311 epoch total loss 1.36984754\n",
      "Trained batch 1310 batch loss 1.2637 epoch total loss 1.36976659\n",
      "Trained batch 1311 batch loss 1.36956024 epoch total loss 1.36976635\n",
      "Trained batch 1312 batch loss 1.30896521 epoch total loss 1.36972\n",
      "Trained batch 1313 batch loss 1.27917516 epoch total loss 1.36965108\n",
      "Trained batch 1314 batch loss 1.23466635 epoch total loss 1.36954832\n",
      "Trained batch 1315 batch loss 1.30578244 epoch total loss 1.3694998\n",
      "Trained batch 1316 batch loss 1.30513 epoch total loss 1.36945093\n",
      "Trained batch 1317 batch loss 1.37637103 epoch total loss 1.36945617\n",
      "Trained batch 1318 batch loss 1.34577084 epoch total loss 1.36943817\n",
      "Trained batch 1319 batch loss 1.20263231 epoch total loss 1.36931169\n",
      "Trained batch 1320 batch loss 1.24434638 epoch total loss 1.36921716\n",
      "Trained batch 1321 batch loss 1.2694819 epoch total loss 1.3691417\n",
      "Trained batch 1322 batch loss 1.21913826 epoch total loss 1.36902821\n",
      "Trained batch 1323 batch loss 1.2416482 epoch total loss 1.36893189\n",
      "Trained batch 1324 batch loss 1.41144753 epoch total loss 1.36896408\n",
      "Trained batch 1325 batch loss 1.42157495 epoch total loss 1.36900377\n",
      "Trained batch 1326 batch loss 1.25785446 epoch total loss 1.36892\n",
      "Trained batch 1327 batch loss 1.21152 epoch total loss 1.36880136\n",
      "Trained batch 1328 batch loss 1.19195592 epoch total loss 1.3686682\n",
      "Trained batch 1329 batch loss 1.19641113 epoch total loss 1.36853862\n",
      "Trained batch 1330 batch loss 1.1974349 epoch total loss 1.36841\n",
      "Trained batch 1331 batch loss 1.17282748 epoch total loss 1.36826301\n",
      "Trained batch 1332 batch loss 1.22117496 epoch total loss 1.36815262\n",
      "Trained batch 1333 batch loss 1.2411387 epoch total loss 1.36805725\n",
      "Trained batch 1334 batch loss 1.27782094 epoch total loss 1.36798966\n",
      "Trained batch 1335 batch loss 1.20026255 epoch total loss 1.36786401\n",
      "Trained batch 1336 batch loss 1.31360805 epoch total loss 1.36782348\n",
      "Trained batch 1337 batch loss 1.43140173 epoch total loss 1.36787093\n",
      "Trained batch 1338 batch loss 1.33278096 epoch total loss 1.3678447\n",
      "Trained batch 1339 batch loss 1.25642359 epoch total loss 1.36776161\n",
      "Trained batch 1340 batch loss 1.33159637 epoch total loss 1.36773455\n",
      "Trained batch 1341 batch loss 1.29187751 epoch total loss 1.36767793\n",
      "Trained batch 1342 batch loss 1.16612923 epoch total loss 1.36752784\n",
      "Trained batch 1343 batch loss 1.35539508 epoch total loss 1.36751866\n",
      "Trained batch 1344 batch loss 1.42352462 epoch total loss 1.36756039\n",
      "Trained batch 1345 batch loss 1.36208045 epoch total loss 1.36755633\n",
      "Trained batch 1346 batch loss 1.32543242 epoch total loss 1.3675251\n",
      "Trained batch 1347 batch loss 1.23422062 epoch total loss 1.36742616\n",
      "Trained batch 1348 batch loss 1.40609252 epoch total loss 1.36745477\n",
      "Trained batch 1349 batch loss 1.27396679 epoch total loss 1.36738551\n",
      "Trained batch 1350 batch loss 1.23911798 epoch total loss 1.3672905\n",
      "Trained batch 1351 batch loss 1.2709105 epoch total loss 1.36721909\n",
      "Trained batch 1352 batch loss 1.31921518 epoch total loss 1.36718357\n",
      "Trained batch 1353 batch loss 1.30408227 epoch total loss 1.36713696\n",
      "Trained batch 1354 batch loss 1.24191177 epoch total loss 1.36704457\n",
      "Trained batch 1355 batch loss 1.41927862 epoch total loss 1.36708307\n",
      "Trained batch 1356 batch loss 1.24741852 epoch total loss 1.36699486\n",
      "Trained batch 1357 batch loss 1.37101066 epoch total loss 1.36699784\n",
      "Trained batch 1358 batch loss 1.39445126 epoch total loss 1.36701798\n",
      "Trained batch 1359 batch loss 1.25731802 epoch total loss 1.36693728\n",
      "Trained batch 1360 batch loss 1.24537683 epoch total loss 1.36684787\n",
      "Trained batch 1361 batch loss 1.51006627 epoch total loss 1.36695302\n",
      "Trained batch 1362 batch loss 1.35182691 epoch total loss 1.36694193\n",
      "Trained batch 1363 batch loss 1.2965064 epoch total loss 1.36689019\n",
      "Trained batch 1364 batch loss 1.29053843 epoch total loss 1.36683428\n",
      "Trained batch 1365 batch loss 1.27829361 epoch total loss 1.36676943\n",
      "Trained batch 1366 batch loss 1.44294524 epoch total loss 1.36682522\n",
      "Trained batch 1367 batch loss 1.39495134 epoch total loss 1.36684573\n",
      "Trained batch 1368 batch loss 1.3005358 epoch total loss 1.36679733\n",
      "Trained batch 1369 batch loss 1.3330543 epoch total loss 1.36677265\n",
      "Trained batch 1370 batch loss 1.35134482 epoch total loss 1.36676133\n",
      "Trained batch 1371 batch loss 1.34193993 epoch total loss 1.36674321\n",
      "Trained batch 1372 batch loss 1.25624585 epoch total loss 1.36666262\n",
      "Trained batch 1373 batch loss 1.30428731 epoch total loss 1.3666172\n",
      "Trained batch 1374 batch loss 1.36214471 epoch total loss 1.36661398\n",
      "Trained batch 1375 batch loss 1.35155308 epoch total loss 1.36660302\n",
      "Trained batch 1376 batch loss 1.22555292 epoch total loss 1.36650062\n",
      "Trained batch 1377 batch loss 1.37119222 epoch total loss 1.36650395\n",
      "Trained batch 1378 batch loss 1.35823369 epoch total loss 1.36649799\n",
      "Trained batch 1379 batch loss 1.22406852 epoch total loss 1.36639476\n",
      "Trained batch 1380 batch loss 1.32098293 epoch total loss 1.36636186\n",
      "Trained batch 1381 batch loss 1.37831485 epoch total loss 1.36637044\n",
      "Trained batch 1382 batch loss 1.26116252 epoch total loss 1.36629426\n",
      "Trained batch 1383 batch loss 1.29947495 epoch total loss 1.36624599\n",
      "Trained batch 1384 batch loss 1.39835715 epoch total loss 1.36626911\n",
      "Trained batch 1385 batch loss 1.3141681 epoch total loss 1.36623156\n",
      "Trained batch 1386 batch loss 1.13133478 epoch total loss 1.36606205\n",
      "Trained batch 1387 batch loss 1.38305676 epoch total loss 1.36607432\n",
      "Trained batch 1388 batch loss 1.36085582 epoch total loss 1.36607063\n",
      "Epoch 2 train loss 1.3660706281661987\n",
      "Validated batch 1 batch loss 1.25390708\n",
      "Validated batch 2 batch loss 1.27841866\n",
      "Validated batch 3 batch loss 1.23354328\n",
      "Validated batch 4 batch loss 1.2133652\n",
      "Validated batch 5 batch loss 1.26147771\n",
      "Validated batch 6 batch loss 1.34853268\n",
      "Validated batch 7 batch loss 1.2914381\n",
      "Validated batch 8 batch loss 1.16132021\n",
      "Validated batch 9 batch loss 1.24937117\n",
      "Validated batch 10 batch loss 1.29044104\n",
      "Validated batch 11 batch loss 1.24041843\n",
      "Validated batch 12 batch loss 1.29654813\n",
      "Validated batch 13 batch loss 1.33687711\n",
      "Validated batch 14 batch loss 1.2573669\n",
      "Validated batch 15 batch loss 1.35850072\n",
      "Validated batch 16 batch loss 1.35343575\n",
      "Validated batch 17 batch loss 1.29255295\n",
      "Validated batch 18 batch loss 1.38029242\n",
      "Validated batch 19 batch loss 1.12376273\n",
      "Validated batch 20 batch loss 1.26320064\n",
      "Validated batch 21 batch loss 1.18929231\n",
      "Validated batch 22 batch loss 1.35055923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 23 batch loss 1.44425416\n",
      "Validated batch 24 batch loss 1.28862715\n",
      "Validated batch 25 batch loss 1.31505215\n",
      "Validated batch 26 batch loss 1.20278645\n",
      "Validated batch 27 batch loss 1.32408845\n",
      "Validated batch 28 batch loss 1.33003867\n",
      "Validated batch 29 batch loss 1.32350183\n",
      "Validated batch 30 batch loss 1.38981223\n",
      "Validated batch 31 batch loss 1.3561219\n",
      "Validated batch 32 batch loss 1.3585856\n",
      "Validated batch 33 batch loss 1.316504\n",
      "Validated batch 34 batch loss 1.39978743\n",
      "Validated batch 35 batch loss 1.29506731\n",
      "Validated batch 36 batch loss 1.30325818\n",
      "Validated batch 37 batch loss 1.3996886\n",
      "Validated batch 38 batch loss 1.38482773\n",
      "Validated batch 39 batch loss 1.30188274\n",
      "Validated batch 40 batch loss 1.45958281\n",
      "Validated batch 41 batch loss 1.16453052\n",
      "Validated batch 42 batch loss 1.3844924\n",
      "Validated batch 43 batch loss 1.12656629\n",
      "Validated batch 44 batch loss 1.32076967\n",
      "Validated batch 45 batch loss 1.3983798\n",
      "Validated batch 46 batch loss 1.21115661\n",
      "Validated batch 47 batch loss 1.36146569\n",
      "Validated batch 48 batch loss 1.32703876\n",
      "Validated batch 49 batch loss 1.25186849\n",
      "Validated batch 50 batch loss 1.25798583\n",
      "Validated batch 51 batch loss 1.33444881\n",
      "Validated batch 52 batch loss 1.32362473\n",
      "Validated batch 53 batch loss 1.27734947\n",
      "Validated batch 54 batch loss 1.27940106\n",
      "Validated batch 55 batch loss 1.30260408\n",
      "Validated batch 56 batch loss 1.36818647\n",
      "Validated batch 57 batch loss 1.23114645\n",
      "Validated batch 58 batch loss 1.17591357\n",
      "Validated batch 59 batch loss 1.35230112\n",
      "Validated batch 60 batch loss 1.36376321\n",
      "Validated batch 61 batch loss 1.39831567\n",
      "Validated batch 62 batch loss 1.41244245\n",
      "Validated batch 63 batch loss 1.23562407\n",
      "Validated batch 64 batch loss 1.45683312\n",
      "Validated batch 65 batch loss 1.27102315\n",
      "Validated batch 66 batch loss 1.36493242\n",
      "Validated batch 67 batch loss 1.34216571\n",
      "Validated batch 68 batch loss 1.10591674\n",
      "Validated batch 69 batch loss 1.31903613\n",
      "Validated batch 70 batch loss 1.20367503\n",
      "Validated batch 71 batch loss 1.34160197\n",
      "Validated batch 72 batch loss 1.3881197\n",
      "Validated batch 73 batch loss 1.23513174\n",
      "Validated batch 74 batch loss 1.3745873\n",
      "Validated batch 75 batch loss 1.45180988\n",
      "Validated batch 76 batch loss 1.15790856\n",
      "Validated batch 77 batch loss 1.3107605\n",
      "Validated batch 78 batch loss 1.27789617\n",
      "Validated batch 79 batch loss 1.35998452\n",
      "Validated batch 80 batch loss 1.34713554\n",
      "Validated batch 81 batch loss 1.22348571\n",
      "Validated batch 82 batch loss 1.13362575\n",
      "Validated batch 83 batch loss 1.28563237\n",
      "Validated batch 84 batch loss 1.27425241\n",
      "Validated batch 85 batch loss 1.24772978\n",
      "Validated batch 86 batch loss 1.3318063\n",
      "Validated batch 87 batch loss 1.24680734\n",
      "Validated batch 88 batch loss 1.30392432\n",
      "Validated batch 89 batch loss 1.36880398\n",
      "Validated batch 90 batch loss 1.34859431\n",
      "Validated batch 91 batch loss 1.29974914\n",
      "Validated batch 92 batch loss 1.20245242\n",
      "Validated batch 93 batch loss 1.41285443\n",
      "Validated batch 94 batch loss 1.2953608\n",
      "Validated batch 95 batch loss 1.30619347\n",
      "Validated batch 96 batch loss 1.26600146\n",
      "Validated batch 97 batch loss 1.21770513\n",
      "Validated batch 98 batch loss 1.32284868\n",
      "Validated batch 99 batch loss 1.29754245\n",
      "Validated batch 100 batch loss 1.26160789\n",
      "Validated batch 101 batch loss 1.28596461\n",
      "Validated batch 102 batch loss 1.30909312\n",
      "Validated batch 103 batch loss 1.32140064\n",
      "Validated batch 104 batch loss 1.3562746\n",
      "Validated batch 105 batch loss 1.29306698\n",
      "Validated batch 106 batch loss 1.24279428\n",
      "Validated batch 107 batch loss 1.23505425\n",
      "Validated batch 108 batch loss 1.30375719\n",
      "Validated batch 109 batch loss 1.26684642\n",
      "Validated batch 110 batch loss 1.33707213\n",
      "Validated batch 111 batch loss 1.36933851\n",
      "Validated batch 112 batch loss 1.51826286\n",
      "Validated batch 113 batch loss 1.44664609\n",
      "Validated batch 114 batch loss 1.30879486\n",
      "Validated batch 115 batch loss 1.21820652\n",
      "Validated batch 116 batch loss 1.23387623\n",
      "Validated batch 117 batch loss 1.22933757\n",
      "Validated batch 118 batch loss 1.29254675\n",
      "Validated batch 119 batch loss 1.23747075\n",
      "Validated batch 120 batch loss 1.26606321\n",
      "Validated batch 121 batch loss 1.41060615\n",
      "Validated batch 122 batch loss 1.29978156\n",
      "Validated batch 123 batch loss 1.36255622\n",
      "Validated batch 124 batch loss 1.39928424\n",
      "Validated batch 125 batch loss 1.35746944\n",
      "Validated batch 126 batch loss 1.31013751\n",
      "Validated batch 127 batch loss 1.43607092\n",
      "Validated batch 128 batch loss 1.33326983\n",
      "Validated batch 129 batch loss 1.3933022\n",
      "Validated batch 130 batch loss 1.38448048\n",
      "Validated batch 131 batch loss 1.45755792\n",
      "Validated batch 132 batch loss 1.37501371\n",
      "Validated batch 133 batch loss 1.25075531\n",
      "Validated batch 134 batch loss 1.31838942\n",
      "Validated batch 135 batch loss 1.3387208\n",
      "Validated batch 136 batch loss 1.37935889\n",
      "Validated batch 137 batch loss 1.27022099\n",
      "Validated batch 138 batch loss 1.30604804\n",
      "Validated batch 139 batch loss 1.30014277\n",
      "Validated batch 140 batch loss 1.27362132\n",
      "Validated batch 141 batch loss 1.29863191\n",
      "Validated batch 142 batch loss 1.25225222\n",
      "Validated batch 143 batch loss 1.32503331\n",
      "Validated batch 144 batch loss 1.3965807\n",
      "Validated batch 145 batch loss 1.18638349\n",
      "Validated batch 146 batch loss 1.32163143\n",
      "Validated batch 147 batch loss 1.19957447\n",
      "Validated batch 148 batch loss 1.3608129\n",
      "Validated batch 149 batch loss 1.26822472\n",
      "Validated batch 150 batch loss 1.2452085\n",
      "Validated batch 151 batch loss 1.34998548\n",
      "Validated batch 152 batch loss 1.33157647\n",
      "Validated batch 153 batch loss 1.37099659\n",
      "Validated batch 154 batch loss 1.28862298\n",
      "Validated batch 155 batch loss 1.31336403\n",
      "Validated batch 156 batch loss 1.29175174\n",
      "Validated batch 157 batch loss 1.23325562\n",
      "Validated batch 158 batch loss 1.31358767\n",
      "Validated batch 159 batch loss 1.27163601\n",
      "Validated batch 160 batch loss 1.28715384\n",
      "Validated batch 161 batch loss 1.31942189\n",
      "Validated batch 162 batch loss 1.34575677\n",
      "Validated batch 163 batch loss 1.21620154\n",
      "Validated batch 164 batch loss 1.31715035\n",
      "Validated batch 165 batch loss 1.25900733\n",
      "Validated batch 166 batch loss 1.22831535\n",
      "Validated batch 167 batch loss 1.32163739\n",
      "Validated batch 168 batch loss 1.25601065\n",
      "Validated batch 169 batch loss 1.30442595\n",
      "Validated batch 170 batch loss 1.39532173\n",
      "Validated batch 171 batch loss 1.21032166\n",
      "Validated batch 172 batch loss 1.39582777\n",
      "Validated batch 173 batch loss 1.35063744\n",
      "Validated batch 174 batch loss 1.1951952\n",
      "Validated batch 175 batch loss 1.29856396\n",
      "Validated batch 176 batch loss 1.34106493\n",
      "Validated batch 177 batch loss 1.27078199\n",
      "Validated batch 178 batch loss 1.39618731\n",
      "Validated batch 179 batch loss 1.33504319\n",
      "Validated batch 180 batch loss 1.37946701\n",
      "Validated batch 181 batch loss 1.26389599\n",
      "Validated batch 182 batch loss 1.35413039\n",
      "Validated batch 183 batch loss 1.2866199\n",
      "Validated batch 184 batch loss 1.23388743\n",
      "Validated batch 185 batch loss 1.38462758\n",
      "Epoch 2 val loss 1.3060938119888306\n",
      "Model .//model-epoch-2-loss-1.3061.h5 saved.\n",
      "Start epoch 3 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.25896072 epoch total loss 1.25896072\n",
      "Trained batch 2 batch loss 1.27090442 epoch total loss 1.26493263\n",
      "Trained batch 3 batch loss 1.39626598 epoch total loss 1.30871046\n",
      "Trained batch 4 batch loss 1.27596 epoch total loss 1.3005228\n",
      "Trained batch 5 batch loss 1.23591471 epoch total loss 1.28760123\n",
      "Trained batch 6 batch loss 1.24729156 epoch total loss 1.28088295\n",
      "Trained batch 7 batch loss 1.27322197 epoch total loss 1.27978837\n",
      "Trained batch 8 batch loss 1.34490168 epoch total loss 1.28792763\n",
      "Trained batch 9 batch loss 1.32531941 epoch total loss 1.29208231\n",
      "Trained batch 10 batch loss 1.44916749 epoch total loss 1.30779076\n",
      "Trained batch 11 batch loss 1.31862342 epoch total loss 1.30877554\n",
      "Trained batch 12 batch loss 1.26081812 epoch total loss 1.30477917\n",
      "Trained batch 13 batch loss 1.21696508 epoch total loss 1.2980243\n",
      "Trained batch 14 batch loss 1.32850754 epoch total loss 1.30020165\n",
      "Trained batch 15 batch loss 1.39311886 epoch total loss 1.30639625\n",
      "Trained batch 16 batch loss 1.39103436 epoch total loss 1.31168616\n",
      "Trained batch 17 batch loss 1.41388261 epoch total loss 1.31769776\n",
      "Trained batch 18 batch loss 1.43021536 epoch total loss 1.32394874\n",
      "Trained batch 19 batch loss 1.3732698 epoch total loss 1.32654464\n",
      "Trained batch 20 batch loss 1.42600453 epoch total loss 1.33151758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 21 batch loss 1.22492599 epoch total loss 1.32644176\n",
      "Trained batch 22 batch loss 1.36385369 epoch total loss 1.32814229\n",
      "Trained batch 23 batch loss 1.27648151 epoch total loss 1.32589626\n",
      "Trained batch 24 batch loss 1.28437901 epoch total loss 1.3241663\n",
      "Trained batch 25 batch loss 1.30852377 epoch total loss 1.32354069\n",
      "Trained batch 26 batch loss 1.35835183 epoch total loss 1.32487953\n",
      "Trained batch 27 batch loss 1.31917238 epoch total loss 1.32466817\n",
      "Trained batch 28 batch loss 1.34891617 epoch total loss 1.32553411\n",
      "Trained batch 29 batch loss 1.26091814 epoch total loss 1.32330596\n",
      "Trained batch 30 batch loss 1.23094702 epoch total loss 1.32022727\n",
      "Trained batch 31 batch loss 1.20216918 epoch total loss 1.31641889\n",
      "Trained batch 32 batch loss 1.28781462 epoch total loss 1.31552505\n",
      "Trained batch 33 batch loss 1.40692723 epoch total loss 1.31829488\n",
      "Trained batch 34 batch loss 1.4165225 epoch total loss 1.32118392\n",
      "Trained batch 35 batch loss 1.34304285 epoch total loss 1.32180846\n",
      "Trained batch 36 batch loss 1.32398129 epoch total loss 1.3218689\n",
      "Trained batch 37 batch loss 1.3644613 epoch total loss 1.32302\n",
      "Trained batch 38 batch loss 1.25820875 epoch total loss 1.32131445\n",
      "Trained batch 39 batch loss 1.38809133 epoch total loss 1.32302666\n",
      "Trained batch 40 batch loss 1.30308425 epoch total loss 1.32252812\n",
      "Trained batch 41 batch loss 1.26540792 epoch total loss 1.32113504\n",
      "Trained batch 42 batch loss 1.17286956 epoch total loss 1.3176049\n",
      "Trained batch 43 batch loss 1.3468827 epoch total loss 1.3182857\n",
      "Trained batch 44 batch loss 1.30272 epoch total loss 1.31793201\n",
      "Trained batch 45 batch loss 1.29811239 epoch total loss 1.31749153\n",
      "Trained batch 46 batch loss 1.32537 epoch total loss 1.31766284\n",
      "Trained batch 47 batch loss 1.43141413 epoch total loss 1.32008302\n",
      "Trained batch 48 batch loss 1.47430205 epoch total loss 1.32329595\n",
      "Trained batch 49 batch loss 1.43678832 epoch total loss 1.32561207\n",
      "Trained batch 50 batch loss 1.48524058 epoch total loss 1.32880461\n",
      "Trained batch 51 batch loss 1.39441967 epoch total loss 1.33009112\n",
      "Trained batch 52 batch loss 1.25178039 epoch total loss 1.32858515\n",
      "Trained batch 53 batch loss 1.31628084 epoch total loss 1.32835305\n",
      "Trained batch 54 batch loss 1.25606751 epoch total loss 1.32701433\n",
      "Trained batch 55 batch loss 1.21895099 epoch total loss 1.32504952\n",
      "Trained batch 56 batch loss 1.26431715 epoch total loss 1.32396507\n",
      "Trained batch 57 batch loss 1.19905639 epoch total loss 1.32177377\n",
      "Trained batch 58 batch loss 1.19225192 epoch total loss 1.31954062\n",
      "Trained batch 59 batch loss 1.17831695 epoch total loss 1.3171469\n",
      "Trained batch 60 batch loss 1.13629794 epoch total loss 1.31413281\n",
      "Trained batch 61 batch loss 1.2004379 epoch total loss 1.31226897\n",
      "Trained batch 62 batch loss 1.26417065 epoch total loss 1.31149316\n",
      "Trained batch 63 batch loss 1.30296922 epoch total loss 1.31135786\n",
      "Trained batch 64 batch loss 1.09468949 epoch total loss 1.30797243\n",
      "Trained batch 65 batch loss 1.21337986 epoch total loss 1.30651712\n",
      "Trained batch 66 batch loss 1.48276567 epoch total loss 1.30918753\n",
      "Trained batch 67 batch loss 1.23352075 epoch total loss 1.30805826\n",
      "Trained batch 68 batch loss 1.16365314 epoch total loss 1.30593455\n",
      "Trained batch 69 batch loss 1.07464719 epoch total loss 1.3025825\n",
      "Trained batch 70 batch loss 1.17524147 epoch total loss 1.30076337\n",
      "Trained batch 71 batch loss 1.25592792 epoch total loss 1.30013192\n",
      "Trained batch 72 batch loss 1.37773657 epoch total loss 1.30120981\n",
      "Trained batch 73 batch loss 1.5105145 epoch total loss 1.30407691\n",
      "Trained batch 74 batch loss 1.47395897 epoch total loss 1.30637264\n",
      "Trained batch 75 batch loss 1.3008827 epoch total loss 1.30629945\n",
      "Trained batch 76 batch loss 1.17606437 epoch total loss 1.30458581\n",
      "Trained batch 77 batch loss 1.37972212 epoch total loss 1.30556166\n",
      "Trained batch 78 batch loss 1.31676042 epoch total loss 1.30570519\n",
      "Trained batch 79 batch loss 1.43106866 epoch total loss 1.30729198\n",
      "Trained batch 80 batch loss 1.35859692 epoch total loss 1.30793333\n",
      "Trained batch 81 batch loss 1.50017524 epoch total loss 1.31030667\n",
      "Trained batch 82 batch loss 1.50328803 epoch total loss 1.3126601\n",
      "Trained batch 83 batch loss 1.39211464 epoch total loss 1.31361735\n",
      "Trained batch 84 batch loss 1.35592961 epoch total loss 1.31412101\n",
      "Trained batch 85 batch loss 1.2956115 epoch total loss 1.31390321\n",
      "Trained batch 86 batch loss 1.22333372 epoch total loss 1.31285012\n",
      "Trained batch 87 batch loss 1.25415301 epoch total loss 1.31217539\n",
      "Trained batch 88 batch loss 1.34536397 epoch total loss 1.31255257\n",
      "Trained batch 89 batch loss 1.40860665 epoch total loss 1.31363189\n",
      "Trained batch 90 batch loss 1.26674175 epoch total loss 1.31311083\n",
      "Trained batch 91 batch loss 1.20625401 epoch total loss 1.31193662\n",
      "Trained batch 92 batch loss 1.18615925 epoch total loss 1.31056941\n",
      "Trained batch 93 batch loss 1.27923989 epoch total loss 1.31023264\n",
      "Trained batch 94 batch loss 1.20810413 epoch total loss 1.30914617\n",
      "Trained batch 95 batch loss 1.26531112 epoch total loss 1.30868471\n",
      "Trained batch 96 batch loss 1.21168804 epoch total loss 1.30767429\n",
      "Trained batch 97 batch loss 1.23964906 epoch total loss 1.30697298\n",
      "Trained batch 98 batch loss 1.22593272 epoch total loss 1.30614614\n",
      "Trained batch 99 batch loss 1.1842041 epoch total loss 1.30491436\n",
      "Trained batch 100 batch loss 1.33356285 epoch total loss 1.30520082\n",
      "Trained batch 101 batch loss 1.36195838 epoch total loss 1.30576277\n",
      "Trained batch 102 batch loss 1.34417856 epoch total loss 1.30613935\n",
      "Trained batch 103 batch loss 1.38989496 epoch total loss 1.30695248\n",
      "Trained batch 104 batch loss 1.33248186 epoch total loss 1.30719805\n",
      "Trained batch 105 batch loss 1.42399442 epoch total loss 1.30831039\n",
      "Trained batch 106 batch loss 1.3626945 epoch total loss 1.30882347\n",
      "Trained batch 107 batch loss 1.30032778 epoch total loss 1.30874407\n",
      "Trained batch 108 batch loss 1.20278645 epoch total loss 1.30776298\n",
      "Trained batch 109 batch loss 1.04923606 epoch total loss 1.30539119\n",
      "Trained batch 110 batch loss 1.02832067 epoch total loss 1.30287242\n",
      "Trained batch 111 batch loss 1.14169168 epoch total loss 1.30142033\n",
      "Trained batch 112 batch loss 1.29988635 epoch total loss 1.30140662\n",
      "Trained batch 113 batch loss 1.4933579 epoch total loss 1.30310535\n",
      "Trained batch 114 batch loss 1.5975281 epoch total loss 1.30568802\n",
      "Trained batch 115 batch loss 1.26785409 epoch total loss 1.30535901\n",
      "Trained batch 116 batch loss 1.32316327 epoch total loss 1.30551255\n",
      "Trained batch 117 batch loss 1.32846165 epoch total loss 1.30570865\n",
      "Trained batch 118 batch loss 1.40946305 epoch total loss 1.30658805\n",
      "Trained batch 119 batch loss 1.37726116 epoch total loss 1.30718184\n",
      "Trained batch 120 batch loss 1.29073095 epoch total loss 1.30704474\n",
      "Trained batch 121 batch loss 1.293051 epoch total loss 1.30692899\n",
      "Trained batch 122 batch loss 1.34003305 epoch total loss 1.30720031\n",
      "Trained batch 123 batch loss 1.34497881 epoch total loss 1.30750751\n",
      "Trained batch 124 batch loss 1.36227047 epoch total loss 1.30794919\n",
      "Trained batch 125 batch loss 1.25440168 epoch total loss 1.30752075\n",
      "Trained batch 126 batch loss 1.20726395 epoch total loss 1.30672503\n",
      "Trained batch 127 batch loss 1.30336976 epoch total loss 1.30669868\n",
      "Trained batch 128 batch loss 1.26777673 epoch total loss 1.30639458\n",
      "Trained batch 129 batch loss 1.23173594 epoch total loss 1.30581582\n",
      "Trained batch 130 batch loss 1.19993615 epoch total loss 1.30500138\n",
      "Trained batch 131 batch loss 1.20713389 epoch total loss 1.30425429\n",
      "Trained batch 132 batch loss 1.26794696 epoch total loss 1.30397928\n",
      "Trained batch 133 batch loss 1.3259958 epoch total loss 1.30414474\n",
      "Trained batch 134 batch loss 1.31291497 epoch total loss 1.30421019\n",
      "Trained batch 135 batch loss 1.26223207 epoch total loss 1.30389929\n",
      "Trained batch 136 batch loss 1.13946402 epoch total loss 1.30269015\n",
      "Trained batch 137 batch loss 1.10679007 epoch total loss 1.30126023\n",
      "Trained batch 138 batch loss 1.33657575 epoch total loss 1.30151618\n",
      "Trained batch 139 batch loss 1.38114095 epoch total loss 1.30208898\n",
      "Trained batch 140 batch loss 1.32307303 epoch total loss 1.30223894\n",
      "Trained batch 141 batch loss 1.44315457 epoch total loss 1.30323839\n",
      "Trained batch 142 batch loss 1.49730372 epoch total loss 1.30460501\n",
      "Trained batch 143 batch loss 1.28778255 epoch total loss 1.30448735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 144 batch loss 1.33432412 epoch total loss 1.30469453\n",
      "Trained batch 145 batch loss 1.34593534 epoch total loss 1.30497885\n",
      "Trained batch 146 batch loss 1.37069798 epoch total loss 1.30542898\n",
      "Trained batch 147 batch loss 1.33979559 epoch total loss 1.30566287\n",
      "Trained batch 148 batch loss 1.3001858 epoch total loss 1.3056258\n",
      "Trained batch 149 batch loss 1.38039637 epoch total loss 1.30612767\n",
      "Trained batch 150 batch loss 1.35519 epoch total loss 1.30645478\n",
      "Trained batch 151 batch loss 1.29372156 epoch total loss 1.30637038\n",
      "Trained batch 152 batch loss 1.32323 epoch total loss 1.30648136\n",
      "Trained batch 153 batch loss 1.22105908 epoch total loss 1.30592299\n",
      "Trained batch 154 batch loss 1.22082019 epoch total loss 1.30537033\n",
      "Trained batch 155 batch loss 1.18843329 epoch total loss 1.30461597\n",
      "Trained batch 156 batch loss 1.2580235 epoch total loss 1.30431724\n",
      "Trained batch 157 batch loss 1.36811614 epoch total loss 1.30472362\n",
      "Trained batch 158 batch loss 1.38615704 epoch total loss 1.30523908\n",
      "Trained batch 159 batch loss 1.33695352 epoch total loss 1.30543852\n",
      "Trained batch 160 batch loss 1.18986547 epoch total loss 1.30471623\n",
      "Trained batch 161 batch loss 1.11104715 epoch total loss 1.30351329\n",
      "Trained batch 162 batch loss 1.22820568 epoch total loss 1.30304849\n",
      "Trained batch 163 batch loss 1.20642376 epoch total loss 1.30245566\n",
      "Trained batch 164 batch loss 1.294222 epoch total loss 1.30240548\n",
      "Trained batch 165 batch loss 1.38597822 epoch total loss 1.30291188\n",
      "Trained batch 166 batch loss 1.32998133 epoch total loss 1.30307508\n",
      "Trained batch 167 batch loss 1.3268069 epoch total loss 1.30321717\n",
      "Trained batch 168 batch loss 1.27530646 epoch total loss 1.30305099\n",
      "Trained batch 169 batch loss 1.24040961 epoch total loss 1.30268025\n",
      "Trained batch 170 batch loss 1.272367 epoch total loss 1.30250204\n",
      "Trained batch 171 batch loss 1.17320538 epoch total loss 1.30174589\n",
      "Trained batch 172 batch loss 1.36589181 epoch total loss 1.30211878\n",
      "Trained batch 173 batch loss 1.30620408 epoch total loss 1.30214238\n",
      "Trained batch 174 batch loss 1.32938576 epoch total loss 1.30229902\n",
      "Trained batch 175 batch loss 1.32920671 epoch total loss 1.30245268\n",
      "Trained batch 176 batch loss 1.41531551 epoch total loss 1.30309403\n",
      "Trained batch 177 batch loss 1.37229729 epoch total loss 1.30348504\n",
      "Trained batch 178 batch loss 1.24232161 epoch total loss 1.30314136\n",
      "Trained batch 179 batch loss 1.30800831 epoch total loss 1.30316865\n",
      "Trained batch 180 batch loss 1.35712409 epoch total loss 1.30346835\n",
      "Trained batch 181 batch loss 1.32215774 epoch total loss 1.30357158\n",
      "Trained batch 182 batch loss 1.29860985 epoch total loss 1.3035444\n",
      "Trained batch 183 batch loss 1.29191685 epoch total loss 1.30348086\n",
      "Trained batch 184 batch loss 1.36706161 epoch total loss 1.30382633\n",
      "Trained batch 185 batch loss 1.33955157 epoch total loss 1.30401945\n",
      "Trained batch 186 batch loss 1.43463206 epoch total loss 1.30472171\n",
      "Trained batch 187 batch loss 1.33559656 epoch total loss 1.30488682\n",
      "Trained batch 188 batch loss 1.32250965 epoch total loss 1.30498064\n",
      "Trained batch 189 batch loss 1.21784687 epoch total loss 1.30451953\n",
      "Trained batch 190 batch loss 1.18748569 epoch total loss 1.30390358\n",
      "Trained batch 191 batch loss 1.27967393 epoch total loss 1.30377674\n",
      "Trained batch 192 batch loss 1.20435798 epoch total loss 1.30325902\n",
      "Trained batch 193 batch loss 1.31645703 epoch total loss 1.30332732\n",
      "Trained batch 194 batch loss 1.24362803 epoch total loss 1.30301952\n",
      "Trained batch 195 batch loss 1.39712977 epoch total loss 1.3035022\n",
      "Trained batch 196 batch loss 1.33701134 epoch total loss 1.30367315\n",
      "Trained batch 197 batch loss 1.29065216 epoch total loss 1.30360699\n",
      "Trained batch 198 batch loss 1.29908681 epoch total loss 1.3035841\n",
      "Trained batch 199 batch loss 1.32364523 epoch total loss 1.30368483\n",
      "Trained batch 200 batch loss 1.45799196 epoch total loss 1.30445635\n",
      "Trained batch 201 batch loss 1.40939045 epoch total loss 1.30497837\n",
      "Trained batch 202 batch loss 1.34131694 epoch total loss 1.30515826\n",
      "Trained batch 203 batch loss 1.33733416 epoch total loss 1.30531681\n",
      "Trained batch 204 batch loss 1.31295288 epoch total loss 1.30535424\n",
      "Trained batch 205 batch loss 1.32926083 epoch total loss 1.30547082\n",
      "Trained batch 206 batch loss 1.33671975 epoch total loss 1.30562258\n",
      "Trained batch 207 batch loss 1.41838717 epoch total loss 1.30616736\n",
      "Trained batch 208 batch loss 1.53195488 epoch total loss 1.30725288\n",
      "Trained batch 209 batch loss 1.55776501 epoch total loss 1.30845153\n",
      "Trained batch 210 batch loss 1.38686502 epoch total loss 1.30882502\n",
      "Trained batch 211 batch loss 1.4046756 epoch total loss 1.3092792\n",
      "Trained batch 212 batch loss 1.39646804 epoch total loss 1.30969036\n",
      "Trained batch 213 batch loss 1.38647687 epoch total loss 1.31005085\n",
      "Trained batch 214 batch loss 1.52088559 epoch total loss 1.31103599\n",
      "Trained batch 215 batch loss 1.37035251 epoch total loss 1.31131196\n",
      "Trained batch 216 batch loss 1.14143515 epoch total loss 1.31052554\n",
      "Trained batch 217 batch loss 1.38806009 epoch total loss 1.31088281\n",
      "Trained batch 218 batch loss 1.31389093 epoch total loss 1.31089675\n",
      "Trained batch 219 batch loss 1.26416588 epoch total loss 1.31068325\n",
      "Trained batch 220 batch loss 1.28699136 epoch total loss 1.3105756\n",
      "Trained batch 221 batch loss 1.21603143 epoch total loss 1.31014776\n",
      "Trained batch 222 batch loss 1.39332581 epoch total loss 1.31052244\n",
      "Trained batch 223 batch loss 1.40970397 epoch total loss 1.31096709\n",
      "Trained batch 224 batch loss 1.41324794 epoch total loss 1.31142366\n",
      "Trained batch 225 batch loss 1.19928968 epoch total loss 1.31092525\n",
      "Trained batch 226 batch loss 1.36350179 epoch total loss 1.31115794\n",
      "Trained batch 227 batch loss 1.28828526 epoch total loss 1.31105721\n",
      "Trained batch 228 batch loss 1.28338027 epoch total loss 1.31093585\n",
      "Trained batch 229 batch loss 1.31221628 epoch total loss 1.31094146\n",
      "Trained batch 230 batch loss 1.32044601 epoch total loss 1.3109827\n",
      "Trained batch 231 batch loss 1.25358689 epoch total loss 1.31073439\n",
      "Trained batch 232 batch loss 1.30541503 epoch total loss 1.31071138\n",
      "Trained batch 233 batch loss 1.4221313 epoch total loss 1.31118953\n",
      "Trained batch 234 batch loss 1.30380774 epoch total loss 1.31115806\n",
      "Trained batch 235 batch loss 1.38881111 epoch total loss 1.31148851\n",
      "Trained batch 236 batch loss 1.39047873 epoch total loss 1.31182313\n",
      "Trained batch 237 batch loss 1.32412434 epoch total loss 1.3118751\n",
      "Trained batch 238 batch loss 1.26309526 epoch total loss 1.31167018\n",
      "Trained batch 239 batch loss 1.35328722 epoch total loss 1.31184435\n",
      "Trained batch 240 batch loss 1.49047422 epoch total loss 1.31258857\n",
      "Trained batch 241 batch loss 1.42149687 epoch total loss 1.31304061\n",
      "Trained batch 242 batch loss 1.31435418 epoch total loss 1.31304598\n",
      "Trained batch 243 batch loss 1.28214383 epoch total loss 1.31291878\n",
      "Trained batch 244 batch loss 1.34027827 epoch total loss 1.31303096\n",
      "Trained batch 245 batch loss 1.26688111 epoch total loss 1.31284249\n",
      "Trained batch 246 batch loss 1.25158262 epoch total loss 1.31259358\n",
      "Trained batch 247 batch loss 1.32108188 epoch total loss 1.31262791\n",
      "Trained batch 248 batch loss 1.34715986 epoch total loss 1.31276715\n",
      "Trained batch 249 batch loss 1.27603722 epoch total loss 1.31261957\n",
      "Trained batch 250 batch loss 1.25476193 epoch total loss 1.31238818\n",
      "Trained batch 251 batch loss 1.3445065 epoch total loss 1.31251621\n",
      "Trained batch 252 batch loss 1.25173259 epoch total loss 1.31227505\n",
      "Trained batch 253 batch loss 1.23705912 epoch total loss 1.31197774\n",
      "Trained batch 254 batch loss 1.24863601 epoch total loss 1.31172824\n",
      "Trained batch 255 batch loss 1.25511825 epoch total loss 1.31150627\n",
      "Trained batch 256 batch loss 1.2107327 epoch total loss 1.31111264\n",
      "Trained batch 257 batch loss 1.17875314 epoch total loss 1.31059754\n",
      "Trained batch 258 batch loss 1.18047857 epoch total loss 1.31009328\n",
      "Trained batch 259 batch loss 1.24905372 epoch total loss 1.30985761\n",
      "Trained batch 260 batch loss 1.37297285 epoch total loss 1.31010044\n",
      "Trained batch 261 batch loss 1.23681188 epoch total loss 1.30981958\n",
      "Trained batch 262 batch loss 1.22269166 epoch total loss 1.30948699\n",
      "Trained batch 263 batch loss 1.28192258 epoch total loss 1.3093822\n",
      "Trained batch 264 batch loss 1.24887395 epoch total loss 1.30915296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 265 batch loss 1.29728436 epoch total loss 1.30910814\n",
      "Trained batch 266 batch loss 1.3648926 epoch total loss 1.30931795\n",
      "Trained batch 267 batch loss 1.25820017 epoch total loss 1.3091265\n",
      "Trained batch 268 batch loss 1.39239788 epoch total loss 1.30943716\n",
      "Trained batch 269 batch loss 1.47661662 epoch total loss 1.31005871\n",
      "Trained batch 270 batch loss 1.52136993 epoch total loss 1.31084132\n",
      "Trained batch 271 batch loss 1.32025385 epoch total loss 1.31087601\n",
      "Trained batch 272 batch loss 1.20898306 epoch total loss 1.31050146\n",
      "Trained batch 273 batch loss 1.39232802 epoch total loss 1.31080115\n",
      "Trained batch 274 batch loss 1.33432555 epoch total loss 1.31088698\n",
      "Trained batch 275 batch loss 1.27892792 epoch total loss 1.31077087\n",
      "Trained batch 276 batch loss 1.32371926 epoch total loss 1.31081772\n",
      "Trained batch 277 batch loss 1.15437186 epoch total loss 1.3102529\n",
      "Trained batch 278 batch loss 1.22877061 epoch total loss 1.30995977\n",
      "Trained batch 279 batch loss 1.21750915 epoch total loss 1.30962837\n",
      "Trained batch 280 batch loss 1.17415333 epoch total loss 1.30914462\n",
      "Trained batch 281 batch loss 1.27613819 epoch total loss 1.30902708\n",
      "Trained batch 282 batch loss 1.23038328 epoch total loss 1.30874813\n",
      "Trained batch 283 batch loss 1.21967578 epoch total loss 1.30843341\n",
      "Trained batch 284 batch loss 1.24344444 epoch total loss 1.30820453\n",
      "Trained batch 285 batch loss 1.29711413 epoch total loss 1.30816567\n",
      "Trained batch 286 batch loss 1.1737802 epoch total loss 1.30769575\n",
      "Trained batch 287 batch loss 1.20477772 epoch total loss 1.30733716\n",
      "Trained batch 288 batch loss 1.27562845 epoch total loss 1.30722702\n",
      "Trained batch 289 batch loss 1.26221251 epoch total loss 1.30707121\n",
      "Trained batch 290 batch loss 1.26133382 epoch total loss 1.3069135\n",
      "Trained batch 291 batch loss 1.19194961 epoch total loss 1.30651844\n",
      "Trained batch 292 batch loss 1.30925727 epoch total loss 1.30652785\n",
      "Trained batch 293 batch loss 1.30892158 epoch total loss 1.30653608\n",
      "Trained batch 294 batch loss 1.25941229 epoch total loss 1.30637574\n",
      "Trained batch 295 batch loss 1.34061265 epoch total loss 1.30649173\n",
      "Trained batch 296 batch loss 1.19020963 epoch total loss 1.30609894\n",
      "Trained batch 297 batch loss 1.33257163 epoch total loss 1.30618811\n",
      "Trained batch 298 batch loss 1.37231421 epoch total loss 1.30641\n",
      "Trained batch 299 batch loss 1.45016921 epoch total loss 1.30689085\n",
      "Trained batch 300 batch loss 1.30467772 epoch total loss 1.30688345\n",
      "Trained batch 301 batch loss 1.24799681 epoch total loss 1.30668771\n",
      "Trained batch 302 batch loss 1.20931697 epoch total loss 1.30636537\n",
      "Trained batch 303 batch loss 1.32743 epoch total loss 1.30643487\n",
      "Trained batch 304 batch loss 1.22703838 epoch total loss 1.30617368\n",
      "Trained batch 305 batch loss 1.34153485 epoch total loss 1.30628967\n",
      "Trained batch 306 batch loss 1.13820744 epoch total loss 1.30574036\n",
      "Trained batch 307 batch loss 1.28323138 epoch total loss 1.30566704\n",
      "Trained batch 308 batch loss 1.42100203 epoch total loss 1.30604148\n",
      "Trained batch 309 batch loss 1.18126309 epoch total loss 1.30563772\n",
      "Trained batch 310 batch loss 1.2035917 epoch total loss 1.30530846\n",
      "Trained batch 311 batch loss 1.4860673 epoch total loss 1.30588961\n",
      "Trained batch 312 batch loss 1.35414088 epoch total loss 1.30604422\n",
      "Trained batch 313 batch loss 1.25712061 epoch total loss 1.30588794\n",
      "Trained batch 314 batch loss 1.15176988 epoch total loss 1.30539703\n",
      "Trained batch 315 batch loss 1.17780948 epoch total loss 1.30499196\n",
      "Trained batch 316 batch loss 1.15545893 epoch total loss 1.30451882\n",
      "Trained batch 317 batch loss 1.16938567 epoch total loss 1.30409241\n",
      "Trained batch 318 batch loss 1.09595132 epoch total loss 1.30343795\n",
      "Trained batch 319 batch loss 1.17399764 epoch total loss 1.30303216\n",
      "Trained batch 320 batch loss 1.25366926 epoch total loss 1.3028779\n",
      "Trained batch 321 batch loss 1.20903635 epoch total loss 1.3025856\n",
      "Trained batch 322 batch loss 1.24070644 epoch total loss 1.30239332\n",
      "Trained batch 323 batch loss 1.35563421 epoch total loss 1.30255818\n",
      "Trained batch 324 batch loss 1.41097319 epoch total loss 1.3028928\n",
      "Trained batch 325 batch loss 1.30366123 epoch total loss 1.30289519\n",
      "Trained batch 326 batch loss 1.30496049 epoch total loss 1.30290151\n",
      "Trained batch 327 batch loss 1.14060044 epoch total loss 1.30240512\n",
      "Trained batch 328 batch loss 1.30997562 epoch total loss 1.30242813\n",
      "Trained batch 329 batch loss 1.38484669 epoch total loss 1.3026787\n",
      "Trained batch 330 batch loss 1.47483563 epoch total loss 1.30320036\n",
      "Trained batch 331 batch loss 1.51246607 epoch total loss 1.30383253\n",
      "Trained batch 332 batch loss 1.38535154 epoch total loss 1.3040781\n",
      "Trained batch 333 batch loss 1.27717316 epoch total loss 1.30399728\n",
      "Trained batch 334 batch loss 1.29143476 epoch total loss 1.30395961\n",
      "Trained batch 335 batch loss 1.27655387 epoch total loss 1.30387783\n",
      "Trained batch 336 batch loss 1.30399108 epoch total loss 1.30387819\n",
      "Trained batch 337 batch loss 1.39212036 epoch total loss 1.30414\n",
      "Trained batch 338 batch loss 1.36676049 epoch total loss 1.30432522\n",
      "Trained batch 339 batch loss 1.45547092 epoch total loss 1.30477118\n",
      "Trained batch 340 batch loss 1.36284173 epoch total loss 1.30494201\n",
      "Trained batch 341 batch loss 1.48453736 epoch total loss 1.30546856\n",
      "Trained batch 342 batch loss 1.48140836 epoch total loss 1.30598307\n",
      "Trained batch 343 batch loss 1.39594305 epoch total loss 1.30624533\n",
      "Trained batch 344 batch loss 1.41588521 epoch total loss 1.30656409\n",
      "Trained batch 345 batch loss 1.41974163 epoch total loss 1.30689216\n",
      "Trained batch 346 batch loss 1.4170599 epoch total loss 1.30721045\n",
      "Trained batch 347 batch loss 1.52035284 epoch total loss 1.30782473\n",
      "Trained batch 348 batch loss 1.5715766 epoch total loss 1.30858266\n",
      "Trained batch 349 batch loss 1.40209866 epoch total loss 1.30885053\n",
      "Trained batch 350 batch loss 1.30515826 epoch total loss 1.30884\n",
      "Trained batch 351 batch loss 1.48958027 epoch total loss 1.3093549\n",
      "Trained batch 352 batch loss 1.3883369 epoch total loss 1.30957937\n",
      "Trained batch 353 batch loss 1.30169952 epoch total loss 1.30955696\n",
      "Trained batch 354 batch loss 1.35974383 epoch total loss 1.30969882\n",
      "Trained batch 355 batch loss 1.31918168 epoch total loss 1.30972552\n",
      "Trained batch 356 batch loss 1.26131344 epoch total loss 1.30958951\n",
      "Trained batch 357 batch loss 1.30821824 epoch total loss 1.30958569\n",
      "Trained batch 358 batch loss 1.28432679 epoch total loss 1.30951512\n",
      "Trained batch 359 batch loss 1.39453602 epoch total loss 1.30975199\n",
      "Trained batch 360 batch loss 1.33428526 epoch total loss 1.30982018\n",
      "Trained batch 361 batch loss 1.36068749 epoch total loss 1.30996108\n",
      "Trained batch 362 batch loss 1.39669371 epoch total loss 1.31020069\n",
      "Trained batch 363 batch loss 1.32059717 epoch total loss 1.3102293\n",
      "Trained batch 364 batch loss 1.21604466 epoch total loss 1.3099705\n",
      "Trained batch 365 batch loss 1.13852906 epoch total loss 1.30950069\n",
      "Trained batch 366 batch loss 1.23731911 epoch total loss 1.30930352\n",
      "Trained batch 367 batch loss 1.30953717 epoch total loss 1.30930412\n",
      "Trained batch 368 batch loss 1.24090886 epoch total loss 1.30911827\n",
      "Trained batch 369 batch loss 1.2952621 epoch total loss 1.30908072\n",
      "Trained batch 370 batch loss 1.24512887 epoch total loss 1.30890787\n",
      "Trained batch 371 batch loss 1.25113201 epoch total loss 1.30875206\n",
      "Trained batch 372 batch loss 1.37625551 epoch total loss 1.3089335\n",
      "Trained batch 373 batch loss 1.42732382 epoch total loss 1.30925095\n",
      "Trained batch 374 batch loss 1.30539763 epoch total loss 1.3092407\n",
      "Trained batch 375 batch loss 1.1757071 epoch total loss 1.30888462\n",
      "Trained batch 376 batch loss 1.12090707 epoch total loss 1.30838466\n",
      "Trained batch 377 batch loss 1.20714402 epoch total loss 1.3081162\n",
      "Trained batch 378 batch loss 1.22295165 epoch total loss 1.30789089\n",
      "Trained batch 379 batch loss 1.30442119 epoch total loss 1.30788171\n",
      "Trained batch 380 batch loss 1.28613925 epoch total loss 1.30782449\n",
      "Trained batch 381 batch loss 1.24952161 epoch total loss 1.30767143\n",
      "Trained batch 382 batch loss 1.15015531 epoch total loss 1.30725908\n",
      "Trained batch 383 batch loss 1.32470405 epoch total loss 1.30730462\n",
      "Trained batch 384 batch loss 1.3884995 epoch total loss 1.30751598\n",
      "Trained batch 385 batch loss 1.31590068 epoch total loss 1.30753779\n",
      "Trained batch 386 batch loss 1.33971548 epoch total loss 1.30762112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 387 batch loss 1.15446401 epoch total loss 1.30722535\n",
      "Trained batch 388 batch loss 1.05958593 epoch total loss 1.3065871\n",
      "Trained batch 389 batch loss 1.0235846 epoch total loss 1.30585968\n",
      "Trained batch 390 batch loss 1.07113969 epoch total loss 1.3052578\n",
      "Trained batch 391 batch loss 0.965631247 epoch total loss 1.30438924\n",
      "Trained batch 392 batch loss 1.15641737 epoch total loss 1.3040117\n",
      "Trained batch 393 batch loss 1.3945868 epoch total loss 1.30424213\n",
      "Trained batch 394 batch loss 1.40065575 epoch total loss 1.30448675\n",
      "Trained batch 395 batch loss 1.35366762 epoch total loss 1.30461121\n",
      "Trained batch 396 batch loss 1.41982698 epoch total loss 1.30490208\n",
      "Trained batch 397 batch loss 1.31708956 epoch total loss 1.30493271\n",
      "Trained batch 398 batch loss 1.28544712 epoch total loss 1.30488384\n",
      "Trained batch 399 batch loss 1.15529931 epoch total loss 1.30450881\n",
      "Trained batch 400 batch loss 1.20549881 epoch total loss 1.30426133\n",
      "Trained batch 401 batch loss 1.29327977 epoch total loss 1.30423391\n",
      "Trained batch 402 batch loss 1.28206086 epoch total loss 1.30417871\n",
      "Trained batch 403 batch loss 1.42577875 epoch total loss 1.30448043\n",
      "Trained batch 404 batch loss 1.29441643 epoch total loss 1.30445564\n",
      "Trained batch 405 batch loss 1.30731916 epoch total loss 1.30446267\n",
      "Trained batch 406 batch loss 1.36451507 epoch total loss 1.30461049\n",
      "Trained batch 407 batch loss 1.35646749 epoch total loss 1.30473793\n",
      "Trained batch 408 batch loss 1.49983263 epoch total loss 1.30521607\n",
      "Trained batch 409 batch loss 1.34052038 epoch total loss 1.30530238\n",
      "Trained batch 410 batch loss 1.45665097 epoch total loss 1.30567145\n",
      "Trained batch 411 batch loss 1.33245182 epoch total loss 1.30573666\n",
      "Trained batch 412 batch loss 1.31661618 epoch total loss 1.30576301\n",
      "Trained batch 413 batch loss 1.41786575 epoch total loss 1.30603445\n",
      "Trained batch 414 batch loss 1.26919246 epoch total loss 1.3059454\n",
      "Trained batch 415 batch loss 1.30453336 epoch total loss 1.30594194\n",
      "Trained batch 416 batch loss 1.331918 epoch total loss 1.30600429\n",
      "Trained batch 417 batch loss 1.29239595 epoch total loss 1.30597174\n",
      "Trained batch 418 batch loss 1.38962412 epoch total loss 1.30617189\n",
      "Trained batch 419 batch loss 1.28569531 epoch total loss 1.30612302\n",
      "Trained batch 420 batch loss 1.23748815 epoch total loss 1.3059597\n",
      "Trained batch 421 batch loss 1.2589258 epoch total loss 1.30584788\n",
      "Trained batch 422 batch loss 1.36108184 epoch total loss 1.30597878\n",
      "Trained batch 423 batch loss 1.32171106 epoch total loss 1.30601597\n",
      "Trained batch 424 batch loss 1.27928483 epoch total loss 1.30595303\n",
      "Trained batch 425 batch loss 1.33533549 epoch total loss 1.30602205\n",
      "Trained batch 426 batch loss 1.31416273 epoch total loss 1.30604112\n",
      "Trained batch 427 batch loss 1.2709167 epoch total loss 1.30595899\n",
      "Trained batch 428 batch loss 1.38255203 epoch total loss 1.30613792\n",
      "Trained batch 429 batch loss 1.24697471 epoch total loss 1.306\n",
      "Trained batch 430 batch loss 1.19068789 epoch total loss 1.30573177\n",
      "Trained batch 431 batch loss 1.27660847 epoch total loss 1.30566418\n",
      "Trained batch 432 batch loss 1.38793921 epoch total loss 1.30585468\n",
      "Trained batch 433 batch loss 1.26885939 epoch total loss 1.30576921\n",
      "Trained batch 434 batch loss 1.30808985 epoch total loss 1.30577457\n",
      "Trained batch 435 batch loss 1.32300806 epoch total loss 1.30581415\n",
      "Trained batch 436 batch loss 1.30330658 epoch total loss 1.30580842\n",
      "Trained batch 437 batch loss 1.16909111 epoch total loss 1.3054955\n",
      "Trained batch 438 batch loss 1.17586637 epoch total loss 1.3051995\n",
      "Trained batch 439 batch loss 1.09998631 epoch total loss 1.30473197\n",
      "Trained batch 440 batch loss 1.1514256 epoch total loss 1.30438352\n",
      "Trained batch 441 batch loss 1.35791838 epoch total loss 1.30450499\n",
      "Trained batch 442 batch loss 1.37574887 epoch total loss 1.30466604\n",
      "Trained batch 443 batch loss 1.5645541 epoch total loss 1.30525279\n",
      "Trained batch 444 batch loss 1.5012362 epoch total loss 1.30569422\n",
      "Trained batch 445 batch loss 1.36630964 epoch total loss 1.30583048\n",
      "Trained batch 446 batch loss 1.32606506 epoch total loss 1.30587578\n",
      "Trained batch 447 batch loss 1.27664208 epoch total loss 1.30581045\n",
      "Trained batch 448 batch loss 1.25187123 epoch total loss 1.30569\n",
      "Trained batch 449 batch loss 1.17239118 epoch total loss 1.3053931\n",
      "Trained batch 450 batch loss 1.27713776 epoch total loss 1.3053304\n",
      "Trained batch 451 batch loss 1.39175189 epoch total loss 1.30552197\n",
      "Trained batch 452 batch loss 1.30704343 epoch total loss 1.30552542\n",
      "Trained batch 453 batch loss 1.33453333 epoch total loss 1.30558944\n",
      "Trained batch 454 batch loss 1.38460684 epoch total loss 1.30576336\n",
      "Trained batch 455 batch loss 1.28519917 epoch total loss 1.3057183\n",
      "Trained batch 456 batch loss 1.09735143 epoch total loss 1.30526125\n",
      "Trained batch 457 batch loss 1.11166692 epoch total loss 1.3048377\n",
      "Trained batch 458 batch loss 1.19835353 epoch total loss 1.30460525\n",
      "Trained batch 459 batch loss 1.23352361 epoch total loss 1.30445039\n",
      "Trained batch 460 batch loss 1.23651898 epoch total loss 1.30430269\n",
      "Trained batch 461 batch loss 1.21667266 epoch total loss 1.30411267\n",
      "Trained batch 462 batch loss 1.19250107 epoch total loss 1.30387104\n",
      "Trained batch 463 batch loss 1.29102409 epoch total loss 1.30384326\n",
      "Trained batch 464 batch loss 1.27309549 epoch total loss 1.30377698\n",
      "Trained batch 465 batch loss 1.12515187 epoch total loss 1.30339277\n",
      "Trained batch 466 batch loss 1.41740513 epoch total loss 1.3036375\n",
      "Trained batch 467 batch loss 1.15519166 epoch total loss 1.30331957\n",
      "Trained batch 468 batch loss 1.22581553 epoch total loss 1.30315411\n",
      "Trained batch 469 batch loss 1.1318022 epoch total loss 1.30278862\n",
      "Trained batch 470 batch loss 1.2926271 epoch total loss 1.30276692\n",
      "Trained batch 471 batch loss 1.3221693 epoch total loss 1.30280817\n",
      "Trained batch 472 batch loss 1.51690626 epoch total loss 1.30326176\n",
      "Trained batch 473 batch loss 1.43877864 epoch total loss 1.30354822\n",
      "Trained batch 474 batch loss 1.33092976 epoch total loss 1.30360603\n",
      "Trained batch 475 batch loss 1.27984107 epoch total loss 1.30355597\n",
      "Trained batch 476 batch loss 1.18930376 epoch total loss 1.303316\n",
      "Trained batch 477 batch loss 1.14972162 epoch total loss 1.30299401\n",
      "Trained batch 478 batch loss 1.24412203 epoch total loss 1.30287087\n",
      "Trained batch 479 batch loss 1.25244331 epoch total loss 1.30276561\n",
      "Trained batch 480 batch loss 1.35754013 epoch total loss 1.30287969\n",
      "Trained batch 481 batch loss 1.35771859 epoch total loss 1.30299377\n",
      "Trained batch 482 batch loss 1.26345062 epoch total loss 1.30291164\n",
      "Trained batch 483 batch loss 1.32965755 epoch total loss 1.30296707\n",
      "Trained batch 484 batch loss 1.20509458 epoch total loss 1.30276477\n",
      "Trained batch 485 batch loss 1.22624028 epoch total loss 1.30260706\n",
      "Trained batch 486 batch loss 1.31447542 epoch total loss 1.30263138\n",
      "Trained batch 487 batch loss 1.28769779 epoch total loss 1.30260074\n",
      "Trained batch 488 batch loss 1.19323063 epoch total loss 1.30237663\n",
      "Trained batch 489 batch loss 1.2176007 epoch total loss 1.3022033\n",
      "Trained batch 490 batch loss 1.18946874 epoch total loss 1.30197322\n",
      "Trained batch 491 batch loss 1.29502857 epoch total loss 1.30195904\n",
      "Trained batch 492 batch loss 1.32027185 epoch total loss 1.30199623\n",
      "Trained batch 493 batch loss 1.32482219 epoch total loss 1.3020426\n",
      "Trained batch 494 batch loss 1.30813944 epoch total loss 1.302055\n",
      "Trained batch 495 batch loss 1.07018054 epoch total loss 1.30158651\n",
      "Trained batch 496 batch loss 1.08791256 epoch total loss 1.30115569\n",
      "Trained batch 497 batch loss 1.06065965 epoch total loss 1.30067182\n",
      "Trained batch 498 batch loss 1.2057395 epoch total loss 1.3004812\n",
      "Trained batch 499 batch loss 1.225986 epoch total loss 1.30033195\n",
      "Trained batch 500 batch loss 1.52184343 epoch total loss 1.30077505\n",
      "Trained batch 501 batch loss 1.28168035 epoch total loss 1.3007369\n",
      "Trained batch 502 batch loss 1.19414592 epoch total loss 1.30052459\n",
      "Trained batch 503 batch loss 1.31369829 epoch total loss 1.30055082\n",
      "Trained batch 504 batch loss 1.23198056 epoch total loss 1.3004148\n",
      "Trained batch 505 batch loss 1.17205107 epoch total loss 1.30016065\n",
      "Trained batch 506 batch loss 1.29609954 epoch total loss 1.30015254\n",
      "Trained batch 507 batch loss 1.38816357 epoch total loss 1.30032623\n",
      "Trained batch 508 batch loss 1.32450223 epoch total loss 1.30037379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 509 batch loss 1.26062059 epoch total loss 1.30029571\n",
      "Trained batch 510 batch loss 1.27233195 epoch total loss 1.30024087\n",
      "Trained batch 511 batch loss 1.17203593 epoch total loss 1.29999\n",
      "Trained batch 512 batch loss 1.22605085 epoch total loss 1.2998457\n",
      "Trained batch 513 batch loss 1.30343556 epoch total loss 1.29985261\n",
      "Trained batch 514 batch loss 1.19374919 epoch total loss 1.29964614\n",
      "Trained batch 515 batch loss 1.31561387 epoch total loss 1.29967713\n",
      "Trained batch 516 batch loss 1.25999427 epoch total loss 1.29960024\n",
      "Trained batch 517 batch loss 1.28009629 epoch total loss 1.29956257\n",
      "Trained batch 518 batch loss 1.26269674 epoch total loss 1.29949141\n",
      "Trained batch 519 batch loss 1.27504909 epoch total loss 1.2994442\n",
      "Trained batch 520 batch loss 1.18590188 epoch total loss 1.29922593\n",
      "Trained batch 521 batch loss 1.2112658 epoch total loss 1.29905701\n",
      "Trained batch 522 batch loss 1.28712785 epoch total loss 1.29903412\n",
      "Trained batch 523 batch loss 1.24299717 epoch total loss 1.29892695\n",
      "Trained batch 524 batch loss 1.231 epoch total loss 1.29879737\n",
      "Trained batch 525 batch loss 1.2655977 epoch total loss 1.29873419\n",
      "Trained batch 526 batch loss 1.20067608 epoch total loss 1.29854774\n",
      "Trained batch 527 batch loss 1.63923597 epoch total loss 1.29919422\n",
      "Trained batch 528 batch loss 1.55427527 epoch total loss 1.29967725\n",
      "Trained batch 529 batch loss 1.45948899 epoch total loss 1.29997933\n",
      "Trained batch 530 batch loss 1.35340035 epoch total loss 1.30008018\n",
      "Trained batch 531 batch loss 1.36146748 epoch total loss 1.30019569\n",
      "Trained batch 532 batch loss 1.33020699 epoch total loss 1.30025208\n",
      "Trained batch 533 batch loss 1.3466022 epoch total loss 1.3003391\n",
      "Trained batch 534 batch loss 1.38406396 epoch total loss 1.30049598\n",
      "Trained batch 535 batch loss 1.29216301 epoch total loss 1.30048037\n",
      "Trained batch 536 batch loss 1.21037197 epoch total loss 1.30031228\n",
      "Trained batch 537 batch loss 1.31898904 epoch total loss 1.30034709\n",
      "Trained batch 538 batch loss 1.35291553 epoch total loss 1.30044472\n",
      "Trained batch 539 batch loss 1.28106809 epoch total loss 1.30040884\n",
      "Trained batch 540 batch loss 1.24123931 epoch total loss 1.30029917\n",
      "Trained batch 541 batch loss 1.20861614 epoch total loss 1.30012977\n",
      "Trained batch 542 batch loss 1.24709404 epoch total loss 1.30003178\n",
      "Trained batch 543 batch loss 1.19696808 epoch total loss 1.299842\n",
      "Trained batch 544 batch loss 1.27773499 epoch total loss 1.29980135\n",
      "Trained batch 545 batch loss 1.36584055 epoch total loss 1.29992247\n",
      "Trained batch 546 batch loss 1.28159678 epoch total loss 1.29988897\n",
      "Trained batch 547 batch loss 1.17445421 epoch total loss 1.29965961\n",
      "Trained batch 548 batch loss 1.28324795 epoch total loss 1.29962969\n",
      "Trained batch 549 batch loss 1.16481638 epoch total loss 1.29938412\n",
      "Trained batch 550 batch loss 1.23820543 epoch total loss 1.29927289\n",
      "Trained batch 551 batch loss 1.28034651 epoch total loss 1.29923856\n",
      "Trained batch 552 batch loss 1.27152252 epoch total loss 1.29918838\n",
      "Trained batch 553 batch loss 1.21170497 epoch total loss 1.29903018\n",
      "Trained batch 554 batch loss 1.22839653 epoch total loss 1.29890275\n",
      "Trained batch 555 batch loss 1.18028736 epoch total loss 1.29868901\n",
      "Trained batch 556 batch loss 1.2608602 epoch total loss 1.29862094\n",
      "Trained batch 557 batch loss 1.30437481 epoch total loss 1.29863131\n",
      "Trained batch 558 batch loss 1.26313579 epoch total loss 1.29856765\n",
      "Trained batch 559 batch loss 1.14438736 epoch total loss 1.29829192\n",
      "Trained batch 560 batch loss 1.23297143 epoch total loss 1.29817522\n",
      "Trained batch 561 batch loss 1.26800025 epoch total loss 1.29812145\n",
      "Trained batch 562 batch loss 1.29701161 epoch total loss 1.29811954\n",
      "Trained batch 563 batch loss 1.3420763 epoch total loss 1.29819763\n",
      "Trained batch 564 batch loss 1.33021057 epoch total loss 1.29825437\n",
      "Trained batch 565 batch loss 1.2163471 epoch total loss 1.29810941\n",
      "Trained batch 566 batch loss 1.2718842 epoch total loss 1.29806316\n",
      "Trained batch 567 batch loss 1.30760288 epoch total loss 1.29808\n",
      "Trained batch 568 batch loss 1.33735526 epoch total loss 1.29814911\n",
      "Trained batch 569 batch loss 1.3257854 epoch total loss 1.29819775\n",
      "Trained batch 570 batch loss 1.33568144 epoch total loss 1.29826355\n",
      "Trained batch 571 batch loss 1.18669677 epoch total loss 1.29806817\n",
      "Trained batch 572 batch loss 1.18187809 epoch total loss 1.29786503\n",
      "Trained batch 573 batch loss 1.19613445 epoch total loss 1.29768741\n",
      "Trained batch 574 batch loss 1.11779225 epoch total loss 1.29737401\n",
      "Trained batch 575 batch loss 1.18915808 epoch total loss 1.29718578\n",
      "Trained batch 576 batch loss 1.27751684 epoch total loss 1.29715168\n",
      "Trained batch 577 batch loss 1.16535163 epoch total loss 1.29692328\n",
      "Trained batch 578 batch loss 1.16232789 epoch total loss 1.29669046\n",
      "Trained batch 579 batch loss 1.32929242 epoch total loss 1.29674673\n",
      "Trained batch 580 batch loss 1.22752142 epoch total loss 1.2966274\n",
      "Trained batch 581 batch loss 1.16947305 epoch total loss 1.29640853\n",
      "Trained batch 582 batch loss 1.35067117 epoch total loss 1.29650176\n",
      "Trained batch 583 batch loss 1.32404518 epoch total loss 1.29654896\n",
      "Trained batch 584 batch loss 1.27730703 epoch total loss 1.29651594\n",
      "Trained batch 585 batch loss 1.28176463 epoch total loss 1.29649079\n",
      "Trained batch 586 batch loss 1.25524867 epoch total loss 1.29642034\n",
      "Trained batch 587 batch loss 1.23762584 epoch total loss 1.2963202\n",
      "Trained batch 588 batch loss 1.29482746 epoch total loss 1.29631758\n",
      "Trained batch 589 batch loss 1.37186432 epoch total loss 1.29644585\n",
      "Trained batch 590 batch loss 1.2071507 epoch total loss 1.29629457\n",
      "Trained batch 591 batch loss 1.21612597 epoch total loss 1.29615891\n",
      "Trained batch 592 batch loss 1.19725347 epoch total loss 1.2959919\n",
      "Trained batch 593 batch loss 1.28792608 epoch total loss 1.29597819\n",
      "Trained batch 594 batch loss 1.21251249 epoch total loss 1.29583776\n",
      "Trained batch 595 batch loss 1.07495451 epoch total loss 1.29546642\n",
      "Trained batch 596 batch loss 1.13883138 epoch total loss 1.29520369\n",
      "Trained batch 597 batch loss 1.16029954 epoch total loss 1.29497766\n",
      "Trained batch 598 batch loss 1.1746552 epoch total loss 1.29477656\n",
      "Trained batch 599 batch loss 1.1755507 epoch total loss 1.29457748\n",
      "Trained batch 600 batch loss 1.24460268 epoch total loss 1.29449427\n",
      "Trained batch 601 batch loss 1.18360305 epoch total loss 1.29430974\n",
      "Trained batch 602 batch loss 1.22780943 epoch total loss 1.29419923\n",
      "Trained batch 603 batch loss 1.32319129 epoch total loss 1.29424727\n",
      "Trained batch 604 batch loss 1.43220174 epoch total loss 1.29447567\n",
      "Trained batch 605 batch loss 1.38525248 epoch total loss 1.29462564\n",
      "Trained batch 606 batch loss 1.32864738 epoch total loss 1.29468191\n",
      "Trained batch 607 batch loss 1.27405429 epoch total loss 1.29464781\n",
      "Trained batch 608 batch loss 1.26673663 epoch total loss 1.29460192\n",
      "Trained batch 609 batch loss 1.35624814 epoch total loss 1.29470325\n",
      "Trained batch 610 batch loss 1.50201738 epoch total loss 1.29504299\n",
      "Trained batch 611 batch loss 1.40109444 epoch total loss 1.29521668\n",
      "Trained batch 612 batch loss 1.30920291 epoch total loss 1.29523957\n",
      "Trained batch 613 batch loss 1.34837568 epoch total loss 1.29532623\n",
      "Trained batch 614 batch loss 1.25815177 epoch total loss 1.29526567\n",
      "Trained batch 615 batch loss 1.25613165 epoch total loss 1.29520202\n",
      "Trained batch 616 batch loss 1.25101578 epoch total loss 1.29513037\n",
      "Trained batch 617 batch loss 1.26759398 epoch total loss 1.29508567\n",
      "Trained batch 618 batch loss 1.3271867 epoch total loss 1.29513764\n",
      "Trained batch 619 batch loss 1.21168733 epoch total loss 1.29500282\n",
      "Trained batch 620 batch loss 1.1218946 epoch total loss 1.29472363\n",
      "Trained batch 621 batch loss 1.11713052 epoch total loss 1.29443765\n",
      "Trained batch 622 batch loss 1.17550111 epoch total loss 1.29424632\n",
      "Trained batch 623 batch loss 1.37514544 epoch total loss 1.29437613\n",
      "Trained batch 624 batch loss 1.54367781 epoch total loss 1.29477572\n",
      "Trained batch 625 batch loss 1.64779913 epoch total loss 1.29534066\n",
      "Trained batch 626 batch loss 1.37987566 epoch total loss 1.29547572\n",
      "Trained batch 627 batch loss 1.44430268 epoch total loss 1.29571295\n",
      "Trained batch 628 batch loss 1.56225455 epoch total loss 1.29613745\n",
      "Trained batch 629 batch loss 1.38683605 epoch total loss 1.29628158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 630 batch loss 1.45245886 epoch total loss 1.29652953\n",
      "Trained batch 631 batch loss 1.36301219 epoch total loss 1.29663491\n",
      "Trained batch 632 batch loss 1.16996145 epoch total loss 1.29643452\n",
      "Trained batch 633 batch loss 1.18894613 epoch total loss 1.29626477\n",
      "Trained batch 634 batch loss 1.31470299 epoch total loss 1.29629385\n",
      "Trained batch 635 batch loss 1.17628527 epoch total loss 1.29610479\n",
      "Trained batch 636 batch loss 1.22316611 epoch total loss 1.29599011\n",
      "Trained batch 637 batch loss 1.29345369 epoch total loss 1.29598606\n",
      "Trained batch 638 batch loss 1.4211483 epoch total loss 1.29618227\n",
      "Trained batch 639 batch loss 1.28067446 epoch total loss 1.29615808\n",
      "Trained batch 640 batch loss 1.2177695 epoch total loss 1.29603553\n",
      "Trained batch 641 batch loss 1.2777853 epoch total loss 1.29600704\n",
      "Trained batch 642 batch loss 1.38999736 epoch total loss 1.29615355\n",
      "Trained batch 643 batch loss 1.53576326 epoch total loss 1.29652619\n",
      "Trained batch 644 batch loss 1.58934164 epoch total loss 1.29698086\n",
      "Trained batch 645 batch loss 1.43373644 epoch total loss 1.29719281\n",
      "Trained batch 646 batch loss 1.17899394 epoch total loss 1.29701\n",
      "Trained batch 647 batch loss 1.21306109 epoch total loss 1.29688025\n",
      "Trained batch 648 batch loss 1.38059282 epoch total loss 1.29700947\n",
      "Trained batch 649 batch loss 1.46835423 epoch total loss 1.29727352\n",
      "Trained batch 650 batch loss 1.4290812 epoch total loss 1.29747629\n",
      "Trained batch 651 batch loss 1.3357439 epoch total loss 1.29753506\n",
      "Trained batch 652 batch loss 1.22008121 epoch total loss 1.29741621\n",
      "Trained batch 653 batch loss 1.21528208 epoch total loss 1.29729044\n",
      "Trained batch 654 batch loss 1.1367234 epoch total loss 1.29704499\n",
      "Trained batch 655 batch loss 1.17698336 epoch total loss 1.29686165\n",
      "Trained batch 656 batch loss 1.21730351 epoch total loss 1.29674041\n",
      "Trained batch 657 batch loss 1.34684777 epoch total loss 1.29681671\n",
      "Trained batch 658 batch loss 1.22835016 epoch total loss 1.29671264\n",
      "Trained batch 659 batch loss 1.28951693 epoch total loss 1.29670167\n",
      "Trained batch 660 batch loss 1.40141511 epoch total loss 1.29686034\n",
      "Trained batch 661 batch loss 1.29637897 epoch total loss 1.29685962\n",
      "Trained batch 662 batch loss 1.27393568 epoch total loss 1.29682493\n",
      "Trained batch 663 batch loss 1.257779 epoch total loss 1.29676604\n",
      "Trained batch 664 batch loss 1.2923696 epoch total loss 1.29675937\n",
      "Trained batch 665 batch loss 1.16922033 epoch total loss 1.29656768\n",
      "Trained batch 666 batch loss 1.08755708 epoch total loss 1.2962538\n",
      "Trained batch 667 batch loss 1.03709865 epoch total loss 1.2958653\n",
      "Trained batch 668 batch loss 1.12105358 epoch total loss 1.29560363\n",
      "Trained batch 669 batch loss 1.3203392 epoch total loss 1.29564047\n",
      "Trained batch 670 batch loss 1.51059079 epoch total loss 1.29596138\n",
      "Trained batch 671 batch loss 1.41076219 epoch total loss 1.29613245\n",
      "Trained batch 672 batch loss 1.39991212 epoch total loss 1.29628694\n",
      "Trained batch 673 batch loss 1.38265765 epoch total loss 1.29641521\n",
      "Trained batch 674 batch loss 1.29443586 epoch total loss 1.29641223\n",
      "Trained batch 675 batch loss 1.34842706 epoch total loss 1.29648936\n",
      "Trained batch 676 batch loss 1.3158921 epoch total loss 1.29651809\n",
      "Trained batch 677 batch loss 1.4107554 epoch total loss 1.29668689\n",
      "Trained batch 678 batch loss 1.31112409 epoch total loss 1.29670811\n",
      "Trained batch 679 batch loss 1.32511902 epoch total loss 1.29675\n",
      "Trained batch 680 batch loss 1.26882553 epoch total loss 1.29670882\n",
      "Trained batch 681 batch loss 1.43901753 epoch total loss 1.2969178\n",
      "Trained batch 682 batch loss 1.41400456 epoch total loss 1.29708958\n",
      "Trained batch 683 batch loss 1.28914 epoch total loss 1.29707789\n",
      "Trained batch 684 batch loss 1.33699441 epoch total loss 1.29713619\n",
      "Trained batch 685 batch loss 1.15578055 epoch total loss 1.29692984\n",
      "Trained batch 686 batch loss 1.27103877 epoch total loss 1.29689205\n",
      "Trained batch 687 batch loss 1.30022728 epoch total loss 1.29689693\n",
      "Trained batch 688 batch loss 1.26756382 epoch total loss 1.29685438\n",
      "Trained batch 689 batch loss 1.31351709 epoch total loss 1.29687858\n",
      "Trained batch 690 batch loss 1.17708147 epoch total loss 1.29670489\n",
      "Trained batch 691 batch loss 1.27365255 epoch total loss 1.29667163\n",
      "Trained batch 692 batch loss 1.19303012 epoch total loss 1.2965219\n",
      "Trained batch 693 batch loss 1.16014075 epoch total loss 1.29632509\n",
      "Trained batch 694 batch loss 1.23429251 epoch total loss 1.29623568\n",
      "Trained batch 695 batch loss 1.2612958 epoch total loss 1.29618549\n",
      "Trained batch 696 batch loss 1.42090619 epoch total loss 1.29636467\n",
      "Trained batch 697 batch loss 1.43277431 epoch total loss 1.29656041\n",
      "Trained batch 698 batch loss 1.15876544 epoch total loss 1.29636288\n",
      "Trained batch 699 batch loss 1.34556699 epoch total loss 1.29643333\n",
      "Trained batch 700 batch loss 1.32323539 epoch total loss 1.2964716\n",
      "Trained batch 701 batch loss 1.32343829 epoch total loss 1.2965101\n",
      "Trained batch 702 batch loss 1.35729623 epoch total loss 1.29659665\n",
      "Trained batch 703 batch loss 1.41001201 epoch total loss 1.29675806\n",
      "Trained batch 704 batch loss 1.3541429 epoch total loss 1.29683959\n",
      "Trained batch 705 batch loss 1.25230706 epoch total loss 1.29677641\n",
      "Trained batch 706 batch loss 1.34329796 epoch total loss 1.29684234\n",
      "Trained batch 707 batch loss 1.30275619 epoch total loss 1.29685068\n",
      "Trained batch 708 batch loss 1.339499 epoch total loss 1.29691088\n",
      "Trained batch 709 batch loss 1.23100483 epoch total loss 1.2968179\n",
      "Trained batch 710 batch loss 1.28450382 epoch total loss 1.29680061\n",
      "Trained batch 711 batch loss 1.29453933 epoch total loss 1.29679739\n",
      "Trained batch 712 batch loss 1.28619862 epoch total loss 1.29678249\n",
      "Trained batch 713 batch loss 1.36111414 epoch total loss 1.29687274\n",
      "Trained batch 714 batch loss 1.28234339 epoch total loss 1.29685235\n",
      "Trained batch 715 batch loss 1.25285423 epoch total loss 1.29679084\n",
      "Trained batch 716 batch loss 1.37750876 epoch total loss 1.29690361\n",
      "Trained batch 717 batch loss 1.40090215 epoch total loss 1.29704857\n",
      "Trained batch 718 batch loss 1.28676713 epoch total loss 1.29703426\n",
      "Trained batch 719 batch loss 1.31348658 epoch total loss 1.29705703\n",
      "Trained batch 720 batch loss 1.24829233 epoch total loss 1.29698932\n",
      "Trained batch 721 batch loss 1.23770034 epoch total loss 1.29690707\n",
      "Trained batch 722 batch loss 1.26017952 epoch total loss 1.29685628\n",
      "Trained batch 723 batch loss 1.17564201 epoch total loss 1.29668856\n",
      "Trained batch 724 batch loss 1.10343862 epoch total loss 1.29642165\n",
      "Trained batch 725 batch loss 1.16118956 epoch total loss 1.2962352\n",
      "Trained batch 726 batch loss 1.16577291 epoch total loss 1.29605544\n",
      "Trained batch 727 batch loss 1.13856959 epoch total loss 1.29583883\n",
      "Trained batch 728 batch loss 1.10101712 epoch total loss 1.29557121\n",
      "Trained batch 729 batch loss 1.15753698 epoch total loss 1.2953819\n",
      "Trained batch 730 batch loss 1.27715206 epoch total loss 1.29535687\n",
      "Trained batch 731 batch loss 1.28807187 epoch total loss 1.29534698\n",
      "Trained batch 732 batch loss 1.15140128 epoch total loss 1.2951504\n",
      "Trained batch 733 batch loss 1.13483846 epoch total loss 1.29493165\n",
      "Trained batch 734 batch loss 1.29634631 epoch total loss 1.29493356\n",
      "Trained batch 735 batch loss 1.3525281 epoch total loss 1.29501188\n",
      "Trained batch 736 batch loss 1.40758097 epoch total loss 1.29516482\n",
      "Trained batch 737 batch loss 1.20189178 epoch total loss 1.29503834\n",
      "Trained batch 738 batch loss 1.20109868 epoch total loss 1.29491103\n",
      "Trained batch 739 batch loss 1.41525805 epoch total loss 1.29507387\n",
      "Trained batch 740 batch loss 1.44300497 epoch total loss 1.29527378\n",
      "Trained batch 741 batch loss 1.40794313 epoch total loss 1.29542589\n",
      "Trained batch 742 batch loss 1.38325441 epoch total loss 1.29554427\n",
      "Trained batch 743 batch loss 1.39378583 epoch total loss 1.29567647\n",
      "Trained batch 744 batch loss 1.38662422 epoch total loss 1.29579866\n",
      "Trained batch 745 batch loss 1.22172976 epoch total loss 1.29569924\n",
      "Trained batch 746 batch loss 1.31765234 epoch total loss 1.29572868\n",
      "Trained batch 747 batch loss 1.32398558 epoch total loss 1.29576647\n",
      "Trained batch 748 batch loss 1.27512836 epoch total loss 1.29573894\n",
      "Trained batch 749 batch loss 1.19945991 epoch total loss 1.29561043\n",
      "Trained batch 750 batch loss 1.3061564 epoch total loss 1.29562438\n",
      "Trained batch 751 batch loss 1.18058848 epoch total loss 1.29547131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 752 batch loss 1.22806966 epoch total loss 1.29538167\n",
      "Trained batch 753 batch loss 1.19214427 epoch total loss 1.29524457\n",
      "Trained batch 754 batch loss 1.09528613 epoch total loss 1.29497933\n",
      "Trained batch 755 batch loss 1.15869713 epoch total loss 1.29479885\n",
      "Trained batch 756 batch loss 1.38276052 epoch total loss 1.2949152\n",
      "Trained batch 757 batch loss 1.4035635 epoch total loss 1.29505873\n",
      "Trained batch 758 batch loss 1.44207132 epoch total loss 1.29525268\n",
      "Trained batch 759 batch loss 1.44101882 epoch total loss 1.29544473\n",
      "Trained batch 760 batch loss 1.33692837 epoch total loss 1.29549932\n",
      "Trained batch 761 batch loss 1.27771962 epoch total loss 1.29547596\n",
      "Trained batch 762 batch loss 1.28729844 epoch total loss 1.29546523\n",
      "Trained batch 763 batch loss 1.33303773 epoch total loss 1.29551435\n",
      "Trained batch 764 batch loss 1.28401291 epoch total loss 1.29549932\n",
      "Trained batch 765 batch loss 1.20133281 epoch total loss 1.2953763\n",
      "Trained batch 766 batch loss 1.25836396 epoch total loss 1.2953279\n",
      "Trained batch 767 batch loss 1.22836876 epoch total loss 1.29524064\n",
      "Trained batch 768 batch loss 1.32430458 epoch total loss 1.29527843\n",
      "Trained batch 769 batch loss 1.42205822 epoch total loss 1.2954433\n",
      "Trained batch 770 batch loss 1.38559091 epoch total loss 1.29556048\n",
      "Trained batch 771 batch loss 1.48305106 epoch total loss 1.29580355\n",
      "Trained batch 772 batch loss 1.3323766 epoch total loss 1.29585099\n",
      "Trained batch 773 batch loss 1.2970258 epoch total loss 1.29585254\n",
      "Trained batch 774 batch loss 1.40930688 epoch total loss 1.29599905\n",
      "Trained batch 775 batch loss 1.34941411 epoch total loss 1.29606795\n",
      "Trained batch 776 batch loss 1.33908379 epoch total loss 1.2961235\n",
      "Trained batch 777 batch loss 1.20880485 epoch total loss 1.29601109\n",
      "Trained batch 778 batch loss 1.29186296 epoch total loss 1.29600573\n",
      "Trained batch 779 batch loss 1.1474843 epoch total loss 1.29581511\n",
      "Trained batch 780 batch loss 1.14069831 epoch total loss 1.29561615\n",
      "Trained batch 781 batch loss 1.26997948 epoch total loss 1.29558337\n",
      "Trained batch 782 batch loss 1.28138351 epoch total loss 1.29556513\n",
      "Trained batch 783 batch loss 1.14862418 epoch total loss 1.29537749\n",
      "Trained batch 784 batch loss 1.07918489 epoch total loss 1.29510176\n",
      "Trained batch 785 batch loss 1.28655303 epoch total loss 1.29509079\n",
      "Trained batch 786 batch loss 1.37352097 epoch total loss 1.29519057\n",
      "Trained batch 787 batch loss 1.3388201 epoch total loss 1.29524601\n",
      "Trained batch 788 batch loss 1.29626632 epoch total loss 1.29524732\n",
      "Trained batch 789 batch loss 1.24590731 epoch total loss 1.29518485\n",
      "Trained batch 790 batch loss 1.35758018 epoch total loss 1.29526377\n",
      "Trained batch 791 batch loss 1.43485188 epoch total loss 1.2954402\n",
      "Trained batch 792 batch loss 1.20673656 epoch total loss 1.29532826\n",
      "Trained batch 793 batch loss 1.21108401 epoch total loss 1.29522204\n",
      "Trained batch 794 batch loss 1.20430315 epoch total loss 1.2951076\n",
      "Trained batch 795 batch loss 1.28930771 epoch total loss 1.29510033\n",
      "Trained batch 796 batch loss 1.11231601 epoch total loss 1.29487062\n",
      "Trained batch 797 batch loss 1.23059189 epoch total loss 1.29479\n",
      "Trained batch 798 batch loss 1.33724046 epoch total loss 1.2948432\n",
      "Trained batch 799 batch loss 1.14321172 epoch total loss 1.29465342\n",
      "Trained batch 800 batch loss 1.22791529 epoch total loss 1.29457\n",
      "Trained batch 801 batch loss 1.14267159 epoch total loss 1.29438043\n",
      "Trained batch 802 batch loss 1.28721058 epoch total loss 1.29437149\n",
      "Trained batch 803 batch loss 1.35033333 epoch total loss 1.29444122\n",
      "Trained batch 804 batch loss 1.24048865 epoch total loss 1.29437411\n",
      "Trained batch 805 batch loss 1.14959049 epoch total loss 1.2941941\n",
      "Trained batch 806 batch loss 1.11327219 epoch total loss 1.29396975\n",
      "Trained batch 807 batch loss 1.08519292 epoch total loss 1.29371095\n",
      "Trained batch 808 batch loss 1.1415906 epoch total loss 1.29352272\n",
      "Trained batch 809 batch loss 1.48970675 epoch total loss 1.29376531\n",
      "Trained batch 810 batch loss 1.40083647 epoch total loss 1.29389751\n",
      "Trained batch 811 batch loss 1.21586907 epoch total loss 1.29380131\n",
      "Trained batch 812 batch loss 1.2460078 epoch total loss 1.29374242\n",
      "Trained batch 813 batch loss 1.25316358 epoch total loss 1.29369247\n",
      "Trained batch 814 batch loss 1.18184769 epoch total loss 1.29355514\n",
      "Trained batch 815 batch loss 1.23475707 epoch total loss 1.2934829\n",
      "Trained batch 816 batch loss 1.19587648 epoch total loss 1.29336333\n",
      "Trained batch 817 batch loss 1.22799265 epoch total loss 1.29328346\n",
      "Trained batch 818 batch loss 1.29472756 epoch total loss 1.29328513\n",
      "Trained batch 819 batch loss 1.30639648 epoch total loss 1.29330111\n",
      "Trained batch 820 batch loss 1.27775526 epoch total loss 1.29328215\n",
      "Trained batch 821 batch loss 1.27681112 epoch total loss 1.29326212\n",
      "Trained batch 822 batch loss 1.23826802 epoch total loss 1.29319525\n",
      "Trained batch 823 batch loss 1.1945312 epoch total loss 1.29307544\n",
      "Trained batch 824 batch loss 1.23105526 epoch total loss 1.2930001\n",
      "Trained batch 825 batch loss 1.33522189 epoch total loss 1.29305136\n",
      "Trained batch 826 batch loss 1.35507143 epoch total loss 1.29312646\n",
      "Trained batch 827 batch loss 1.31269419 epoch total loss 1.29315019\n",
      "Trained batch 828 batch loss 1.40806603 epoch total loss 1.29328895\n",
      "Trained batch 829 batch loss 1.31779432 epoch total loss 1.29331851\n",
      "Trained batch 830 batch loss 1.29222965 epoch total loss 1.2933172\n",
      "Trained batch 831 batch loss 1.38101673 epoch total loss 1.2934227\n",
      "Trained batch 832 batch loss 1.38996935 epoch total loss 1.29353881\n",
      "Trained batch 833 batch loss 1.2150526 epoch total loss 1.29344463\n",
      "Trained batch 834 batch loss 1.2432785 epoch total loss 1.29338443\n",
      "Trained batch 835 batch loss 1.26018858 epoch total loss 1.29334462\n",
      "Trained batch 836 batch loss 1.29356289 epoch total loss 1.29334486\n",
      "Trained batch 837 batch loss 1.21671855 epoch total loss 1.2932533\n",
      "Trained batch 838 batch loss 1.27522981 epoch total loss 1.29323184\n",
      "Trained batch 839 batch loss 1.15576446 epoch total loss 1.29306793\n",
      "Trained batch 840 batch loss 1.15014577 epoch total loss 1.29289782\n",
      "Trained batch 841 batch loss 1.22465324 epoch total loss 1.29281664\n",
      "Trained batch 842 batch loss 1.27499294 epoch total loss 1.29279554\n",
      "Trained batch 843 batch loss 1.26275587 epoch total loss 1.29275978\n",
      "Trained batch 844 batch loss 1.32551765 epoch total loss 1.29279864\n",
      "Trained batch 845 batch loss 1.3377316 epoch total loss 1.29285192\n",
      "Trained batch 846 batch loss 1.31901991 epoch total loss 1.2928828\n",
      "Trained batch 847 batch loss 1.26677132 epoch total loss 1.29285192\n",
      "Trained batch 848 batch loss 1.18883252 epoch total loss 1.29272926\n",
      "Trained batch 849 batch loss 1.21775413 epoch total loss 1.29264092\n",
      "Trained batch 850 batch loss 1.13452768 epoch total loss 1.29245496\n",
      "Trained batch 851 batch loss 1.25677621 epoch total loss 1.292413\n",
      "Trained batch 852 batch loss 1.29218137 epoch total loss 1.29241288\n",
      "Trained batch 853 batch loss 1.17883801 epoch total loss 1.29227972\n",
      "Trained batch 854 batch loss 1.20226622 epoch total loss 1.29217434\n",
      "Trained batch 855 batch loss 1.03669357 epoch total loss 1.2918756\n",
      "Trained batch 856 batch loss 1.00113118 epoch total loss 1.29153585\n",
      "Trained batch 857 batch loss 1.18719625 epoch total loss 1.29141414\n",
      "Trained batch 858 batch loss 1.37383246 epoch total loss 1.29151022\n",
      "Trained batch 859 batch loss 1.34653592 epoch total loss 1.29157424\n",
      "Trained batch 860 batch loss 1.27927029 epoch total loss 1.29155993\n",
      "Trained batch 861 batch loss 1.3336122 epoch total loss 1.29160881\n",
      "Trained batch 862 batch loss 1.27049 epoch total loss 1.29158437\n",
      "Trained batch 863 batch loss 1.14072824 epoch total loss 1.29140961\n",
      "Trained batch 864 batch loss 1.20352471 epoch total loss 1.29130781\n",
      "Trained batch 865 batch loss 1.26890123 epoch total loss 1.29128194\n",
      "Trained batch 866 batch loss 1.22652864 epoch total loss 1.29120719\n",
      "Trained batch 867 batch loss 1.31438851 epoch total loss 1.2912339\n",
      "Trained batch 868 batch loss 1.39222181 epoch total loss 1.29135025\n",
      "Trained batch 869 batch loss 1.4020164 epoch total loss 1.29147744\n",
      "Trained batch 870 batch loss 1.15732872 epoch total loss 1.2913233\n",
      "Trained batch 871 batch loss 1.16449058 epoch total loss 1.29117775\n",
      "Trained batch 872 batch loss 1.22020006 epoch total loss 1.29109645\n",
      "Trained batch 873 batch loss 1.3243804 epoch total loss 1.29113448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 874 batch loss 1.32149351 epoch total loss 1.29116929\n",
      "Trained batch 875 batch loss 1.34564614 epoch total loss 1.29123163\n",
      "Trained batch 876 batch loss 1.27521491 epoch total loss 1.29121339\n",
      "Trained batch 877 batch loss 1.33579266 epoch total loss 1.29126418\n",
      "Trained batch 878 batch loss 1.33908701 epoch total loss 1.29131877\n",
      "Trained batch 879 batch loss 1.26224518 epoch total loss 1.29128563\n",
      "Trained batch 880 batch loss 1.21304512 epoch total loss 1.2911967\n",
      "Trained batch 881 batch loss 1.25935316 epoch total loss 1.29116058\n",
      "Trained batch 882 batch loss 1.3147347 epoch total loss 1.29118729\n",
      "Trained batch 883 batch loss 1.24946129 epoch total loss 1.29114008\n",
      "Trained batch 884 batch loss 1.28741252 epoch total loss 1.29113579\n",
      "Trained batch 885 batch loss 1.23187757 epoch total loss 1.29106891\n",
      "Trained batch 886 batch loss 1.30715191 epoch total loss 1.29108703\n",
      "Trained batch 887 batch loss 1.38117361 epoch total loss 1.2911886\n",
      "Trained batch 888 batch loss 1.24496579 epoch total loss 1.29113662\n",
      "Trained batch 889 batch loss 1.27220821 epoch total loss 1.29111528\n",
      "Trained batch 890 batch loss 1.21353221 epoch total loss 1.29102814\n",
      "Trained batch 891 batch loss 1.13948202 epoch total loss 1.29085803\n",
      "Trained batch 892 batch loss 1.18959105 epoch total loss 1.29074454\n",
      "Trained batch 893 batch loss 1.21361613 epoch total loss 1.29065812\n",
      "Trained batch 894 batch loss 1.32121503 epoch total loss 1.29069233\n",
      "Trained batch 895 batch loss 1.23318374 epoch total loss 1.29062796\n",
      "Trained batch 896 batch loss 1.42732882 epoch total loss 1.29078066\n",
      "Trained batch 897 batch loss 1.47588801 epoch total loss 1.2909869\n",
      "Trained batch 898 batch loss 1.24652886 epoch total loss 1.29093742\n",
      "Trained batch 899 batch loss 1.22217953 epoch total loss 1.29086101\n",
      "Trained batch 900 batch loss 1.37916374 epoch total loss 1.29095912\n",
      "Trained batch 901 batch loss 1.32129633 epoch total loss 1.29099274\n",
      "Trained batch 902 batch loss 1.22942293 epoch total loss 1.29092443\n",
      "Trained batch 903 batch loss 1.37356591 epoch total loss 1.29101586\n",
      "Trained batch 904 batch loss 1.35709131 epoch total loss 1.29108894\n",
      "Trained batch 905 batch loss 1.28169978 epoch total loss 1.29107857\n",
      "Trained batch 906 batch loss 1.33314836 epoch total loss 1.29112506\n",
      "Trained batch 907 batch loss 1.31833553 epoch total loss 1.2911551\n",
      "Trained batch 908 batch loss 1.48025894 epoch total loss 1.29136324\n",
      "Trained batch 909 batch loss 1.34169161 epoch total loss 1.29141867\n",
      "Trained batch 910 batch loss 1.24807286 epoch total loss 1.29137099\n",
      "Trained batch 911 batch loss 1.37734771 epoch total loss 1.29146528\n",
      "Trained batch 912 batch loss 1.26650226 epoch total loss 1.29143786\n",
      "Trained batch 913 batch loss 1.29124451 epoch total loss 1.29143775\n",
      "Trained batch 914 batch loss 1.25692475 epoch total loss 1.2914\n",
      "Trained batch 915 batch loss 1.33828783 epoch total loss 1.29145122\n",
      "Trained batch 916 batch loss 1.35642052 epoch total loss 1.29152215\n",
      "Trained batch 917 batch loss 1.37703133 epoch total loss 1.29161549\n",
      "Trained batch 918 batch loss 1.24497008 epoch total loss 1.2915647\n",
      "Trained batch 919 batch loss 1.37059379 epoch total loss 1.29165065\n",
      "Trained batch 920 batch loss 1.21481991 epoch total loss 1.29156721\n",
      "Trained batch 921 batch loss 1.25036478 epoch total loss 1.2915225\n",
      "Trained batch 922 batch loss 1.32363677 epoch total loss 1.29155731\n",
      "Trained batch 923 batch loss 1.29416323 epoch total loss 1.29156017\n",
      "Trained batch 924 batch loss 1.3096416 epoch total loss 1.29157972\n",
      "Trained batch 925 batch loss 1.1772238 epoch total loss 1.2914561\n",
      "Trained batch 926 batch loss 1.21920276 epoch total loss 1.29137814\n",
      "Trained batch 927 batch loss 1.07941985 epoch total loss 1.2911495\n",
      "Trained batch 928 batch loss 1.14416051 epoch total loss 1.29099119\n",
      "Trained batch 929 batch loss 1.16680026 epoch total loss 1.29085743\n",
      "Trained batch 930 batch loss 1.1887579 epoch total loss 1.29074764\n",
      "Trained batch 931 batch loss 1.19734967 epoch total loss 1.29064727\n",
      "Trained batch 932 batch loss 1.12128913 epoch total loss 1.29046571\n",
      "Trained batch 933 batch loss 1.37367296 epoch total loss 1.29055476\n",
      "Trained batch 934 batch loss 1.22820961 epoch total loss 1.290488\n",
      "Trained batch 935 batch loss 1.34512639 epoch total loss 1.29054642\n",
      "Trained batch 936 batch loss 1.30788636 epoch total loss 1.29056489\n",
      "Trained batch 937 batch loss 1.22946537 epoch total loss 1.29049969\n",
      "Trained batch 938 batch loss 1.19712377 epoch total loss 1.29040015\n",
      "Trained batch 939 batch loss 1.11023164 epoch total loss 1.29020834\n",
      "Trained batch 940 batch loss 1.32275951 epoch total loss 1.29024291\n",
      "Trained batch 941 batch loss 1.24276888 epoch total loss 1.29019248\n",
      "Trained batch 942 batch loss 1.23661649 epoch total loss 1.29013562\n",
      "Trained batch 943 batch loss 1.28652501 epoch total loss 1.29013181\n",
      "Trained batch 944 batch loss 1.32248354 epoch total loss 1.29016602\n",
      "Trained batch 945 batch loss 1.16624784 epoch total loss 1.29003489\n",
      "Trained batch 946 batch loss 1.2507689 epoch total loss 1.28999341\n",
      "Trained batch 947 batch loss 1.25883114 epoch total loss 1.28996038\n",
      "Trained batch 948 batch loss 1.26632142 epoch total loss 1.28993559\n",
      "Trained batch 949 batch loss 1.27882481 epoch total loss 1.28992379\n",
      "Trained batch 950 batch loss 1.27142549 epoch total loss 1.28990436\n",
      "Trained batch 951 batch loss 1.25092018 epoch total loss 1.28986347\n",
      "Trained batch 952 batch loss 1.05761814 epoch total loss 1.28961957\n",
      "Trained batch 953 batch loss 1.25207055 epoch total loss 1.28958011\n",
      "Trained batch 954 batch loss 1.34395409 epoch total loss 1.28963721\n",
      "Trained batch 955 batch loss 1.45625865 epoch total loss 1.28981161\n",
      "Trained batch 956 batch loss 1.34052086 epoch total loss 1.28986478\n",
      "Trained batch 957 batch loss 1.33409417 epoch total loss 1.28991103\n",
      "Trained batch 958 batch loss 1.35033441 epoch total loss 1.28997409\n",
      "Trained batch 959 batch loss 1.35709548 epoch total loss 1.29004407\n",
      "Trained batch 960 batch loss 1.32109642 epoch total loss 1.29007638\n",
      "Trained batch 961 batch loss 1.23909461 epoch total loss 1.29002333\n",
      "Trained batch 962 batch loss 1.22555077 epoch total loss 1.28995633\n",
      "Trained batch 963 batch loss 1.13095772 epoch total loss 1.28979123\n",
      "Trained batch 964 batch loss 1.0837543 epoch total loss 1.28957748\n",
      "Trained batch 965 batch loss 1.10272062 epoch total loss 1.28938377\n",
      "Trained batch 966 batch loss 1.13952661 epoch total loss 1.28922868\n",
      "Trained batch 967 batch loss 1.18184519 epoch total loss 1.28911769\n",
      "Trained batch 968 batch loss 0.998802781 epoch total loss 1.28881776\n",
      "Trained batch 969 batch loss 0.980995536 epoch total loss 1.28850007\n",
      "Trained batch 970 batch loss 0.935658514 epoch total loss 1.28813624\n",
      "Trained batch 971 batch loss 1.20327449 epoch total loss 1.28804886\n",
      "Trained batch 972 batch loss 1.23676348 epoch total loss 1.28799617\n",
      "Trained batch 973 batch loss 1.28215671 epoch total loss 1.28799009\n",
      "Trained batch 974 batch loss 1.20547938 epoch total loss 1.28790534\n",
      "Trained batch 975 batch loss 1.23712051 epoch total loss 1.28785324\n",
      "Trained batch 976 batch loss 1.26917994 epoch total loss 1.28783405\n",
      "Trained batch 977 batch loss 1.29660821 epoch total loss 1.28784299\n",
      "Trained batch 978 batch loss 1.14217842 epoch total loss 1.2876941\n",
      "Trained batch 979 batch loss 1.21771073 epoch total loss 1.28762257\n",
      "Trained batch 980 batch loss 1.19899845 epoch total loss 1.28753209\n",
      "Trained batch 981 batch loss 1.19599307 epoch total loss 1.28743887\n",
      "Trained batch 982 batch loss 1.10120845 epoch total loss 1.28724921\n",
      "Trained batch 983 batch loss 1.15304244 epoch total loss 1.28711271\n",
      "Trained batch 984 batch loss 1.16970348 epoch total loss 1.28699338\n",
      "Trained batch 985 batch loss 1.15575111 epoch total loss 1.28686011\n",
      "Trained batch 986 batch loss 1.17735565 epoch total loss 1.28674912\n",
      "Trained batch 987 batch loss 1.20538068 epoch total loss 1.28666663\n",
      "Trained batch 988 batch loss 1.16802764 epoch total loss 1.28654647\n",
      "Trained batch 989 batch loss 1.20354426 epoch total loss 1.28646255\n",
      "Trained batch 990 batch loss 1.60705054 epoch total loss 1.28678632\n",
      "Trained batch 991 batch loss 1.23921633 epoch total loss 1.2867384\n",
      "Trained batch 992 batch loss 1.42738914 epoch total loss 1.28688014\n",
      "Trained batch 993 batch loss 1.4679234 epoch total loss 1.28706241\n",
      "Trained batch 994 batch loss 1.29907584 epoch total loss 1.28707445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 995 batch loss 1.34990561 epoch total loss 1.28713763\n",
      "Trained batch 996 batch loss 1.39443302 epoch total loss 1.28724527\n",
      "Trained batch 997 batch loss 1.14964795 epoch total loss 1.28710735\n",
      "Trained batch 998 batch loss 1.24915111 epoch total loss 1.2870692\n",
      "Trained batch 999 batch loss 1.40302527 epoch total loss 1.28718543\n",
      "Trained batch 1000 batch loss 1.26502562 epoch total loss 1.28716326\n",
      "Trained batch 1001 batch loss 1.2385118 epoch total loss 1.28711462\n",
      "Trained batch 1002 batch loss 1.33699167 epoch total loss 1.28716445\n",
      "Trained batch 1003 batch loss 1.15436327 epoch total loss 1.28703213\n",
      "Trained batch 1004 batch loss 1.18502176 epoch total loss 1.28693056\n",
      "Trained batch 1005 batch loss 1.20823276 epoch total loss 1.28685224\n",
      "Trained batch 1006 batch loss 1.11690915 epoch total loss 1.28668332\n",
      "Trained batch 1007 batch loss 1.22354913 epoch total loss 1.28662062\n",
      "Trained batch 1008 batch loss 1.1037246 epoch total loss 1.28643918\n",
      "Trained batch 1009 batch loss 1.11363077 epoch total loss 1.286268\n",
      "Trained batch 1010 batch loss 1.14696121 epoch total loss 1.28613007\n",
      "Trained batch 1011 batch loss 1.27656972 epoch total loss 1.28612065\n",
      "Trained batch 1012 batch loss 1.36939383 epoch total loss 1.28620291\n",
      "Trained batch 1013 batch loss 1.23529804 epoch total loss 1.28615272\n",
      "Trained batch 1014 batch loss 1.32289076 epoch total loss 1.28618896\n",
      "Trained batch 1015 batch loss 1.17172778 epoch total loss 1.28607619\n",
      "Trained batch 1016 batch loss 1.25642443 epoch total loss 1.28604698\n",
      "Trained batch 1017 batch loss 1.21072459 epoch total loss 1.28597295\n",
      "Trained batch 1018 batch loss 1.37673748 epoch total loss 1.28606212\n",
      "Trained batch 1019 batch loss 1.25474882 epoch total loss 1.28603137\n",
      "Trained batch 1020 batch loss 1.25334692 epoch total loss 1.2859993\n",
      "Trained batch 1021 batch loss 1.13890481 epoch total loss 1.28585517\n",
      "Trained batch 1022 batch loss 1.17845726 epoch total loss 1.28575015\n",
      "Trained batch 1023 batch loss 1.20171988 epoch total loss 1.2856679\n",
      "Trained batch 1024 batch loss 1.1615746 epoch total loss 1.28554678\n",
      "Trained batch 1025 batch loss 1.26507592 epoch total loss 1.28552687\n",
      "Trained batch 1026 batch loss 1.29523373 epoch total loss 1.28553641\n",
      "Trained batch 1027 batch loss 1.39842224 epoch total loss 1.28564632\n",
      "Trained batch 1028 batch loss 1.40595269 epoch total loss 1.28576338\n",
      "Trained batch 1029 batch loss 1.29225993 epoch total loss 1.2857697\n",
      "Trained batch 1030 batch loss 1.22422409 epoch total loss 1.28571\n",
      "Trained batch 1031 batch loss 1.1851052 epoch total loss 1.28561234\n",
      "Trained batch 1032 batch loss 1.13191557 epoch total loss 1.28546345\n",
      "Trained batch 1033 batch loss 1.18318439 epoch total loss 1.28536451\n",
      "Trained batch 1034 batch loss 1.22935045 epoch total loss 1.28531027\n",
      "Trained batch 1035 batch loss 1.4633882 epoch total loss 1.28548241\n",
      "Trained batch 1036 batch loss 1.28633082 epoch total loss 1.28548324\n",
      "Trained batch 1037 batch loss 1.25223088 epoch total loss 1.28545117\n",
      "Trained batch 1038 batch loss 1.22755885 epoch total loss 1.28539538\n",
      "Trained batch 1039 batch loss 1.30165982 epoch total loss 1.285411\n",
      "Trained batch 1040 batch loss 1.12475801 epoch total loss 1.28525651\n",
      "Trained batch 1041 batch loss 1.15179133 epoch total loss 1.28512824\n",
      "Trained batch 1042 batch loss 1.24216199 epoch total loss 1.28508699\n",
      "Trained batch 1043 batch loss 1.20943093 epoch total loss 1.28501451\n",
      "Trained batch 1044 batch loss 1.201033 epoch total loss 1.28493404\n",
      "Trained batch 1045 batch loss 1.27085972 epoch total loss 1.28492069\n",
      "Trained batch 1046 batch loss 1.25413775 epoch total loss 1.28489125\n",
      "Trained batch 1047 batch loss 1.22003174 epoch total loss 1.28482938\n",
      "Trained batch 1048 batch loss 1.14582586 epoch total loss 1.2846967\n",
      "Trained batch 1049 batch loss 1.25263739 epoch total loss 1.28466618\n",
      "Trained batch 1050 batch loss 1.24630499 epoch total loss 1.2846297\n",
      "Trained batch 1051 batch loss 1.306674 epoch total loss 1.28465068\n",
      "Trained batch 1052 batch loss 1.2522229 epoch total loss 1.28461981\n",
      "Trained batch 1053 batch loss 1.15572488 epoch total loss 1.2844975\n",
      "Trained batch 1054 batch loss 1.27362227 epoch total loss 1.28448713\n",
      "Trained batch 1055 batch loss 1.23915458 epoch total loss 1.28444421\n",
      "Trained batch 1056 batch loss 1.29972875 epoch total loss 1.28445864\n",
      "Trained batch 1057 batch loss 1.30762064 epoch total loss 1.28448057\n",
      "Trained batch 1058 batch loss 1.27385879 epoch total loss 1.28447044\n",
      "Trained batch 1059 batch loss 1.1930635 epoch total loss 1.28438413\n",
      "Trained batch 1060 batch loss 1.1622653 epoch total loss 1.28426898\n",
      "Trained batch 1061 batch loss 1.25485158 epoch total loss 1.2842412\n",
      "Trained batch 1062 batch loss 1.06554282 epoch total loss 1.28403533\n",
      "Trained batch 1063 batch loss 1.43787229 epoch total loss 1.28418\n",
      "Trained batch 1064 batch loss 1.51024389 epoch total loss 1.28439248\n",
      "Trained batch 1065 batch loss 1.35121119 epoch total loss 1.28445518\n",
      "Trained batch 1066 batch loss 1.21689415 epoch total loss 1.28439188\n",
      "Trained batch 1067 batch loss 1.16462779 epoch total loss 1.2842797\n",
      "Trained batch 1068 batch loss 1.25706875 epoch total loss 1.28425419\n",
      "Trained batch 1069 batch loss 1.3301959 epoch total loss 1.28429723\n",
      "Trained batch 1070 batch loss 1.31357634 epoch total loss 1.28432453\n",
      "Trained batch 1071 batch loss 1.18899083 epoch total loss 1.28423548\n",
      "Trained batch 1072 batch loss 1.29909134 epoch total loss 1.28424942\n",
      "Trained batch 1073 batch loss 1.41937709 epoch total loss 1.28437531\n",
      "Trained batch 1074 batch loss 1.3723948 epoch total loss 1.28445733\n",
      "Trained batch 1075 batch loss 1.39965677 epoch total loss 1.2845645\n",
      "Trained batch 1076 batch loss 1.28494215 epoch total loss 1.28456485\n",
      "Trained batch 1077 batch loss 1.2003653 epoch total loss 1.28448665\n",
      "Trained batch 1078 batch loss 1.2647332 epoch total loss 1.28446829\n",
      "Trained batch 1079 batch loss 1.29873276 epoch total loss 1.28448153\n",
      "Trained batch 1080 batch loss 1.25875807 epoch total loss 1.28445768\n",
      "Trained batch 1081 batch loss 1.23654222 epoch total loss 1.28441346\n",
      "Trained batch 1082 batch loss 1.19229794 epoch total loss 1.28432822\n",
      "Trained batch 1083 batch loss 1.16324925 epoch total loss 1.2842164\n",
      "Trained batch 1084 batch loss 1.29990578 epoch total loss 1.28423095\n",
      "Trained batch 1085 batch loss 1.20454156 epoch total loss 1.28415751\n",
      "Trained batch 1086 batch loss 1.23467159 epoch total loss 1.28411186\n",
      "Trained batch 1087 batch loss 1.25620937 epoch total loss 1.28408623\n",
      "Trained batch 1088 batch loss 1.10049653 epoch total loss 1.28391743\n",
      "Trained batch 1089 batch loss 1.11279702 epoch total loss 1.28376031\n",
      "Trained batch 1090 batch loss 1.20680642 epoch total loss 1.28368974\n",
      "Trained batch 1091 batch loss 1.28010333 epoch total loss 1.28368652\n",
      "Trained batch 1092 batch loss 1.26022339 epoch total loss 1.28366506\n",
      "Trained batch 1093 batch loss 1.25341868 epoch total loss 1.2836374\n",
      "Trained batch 1094 batch loss 1.26534474 epoch total loss 1.28362072\n",
      "Trained batch 1095 batch loss 1.21609437 epoch total loss 1.28355896\n",
      "Trained batch 1096 batch loss 1.26431406 epoch total loss 1.28354132\n",
      "Trained batch 1097 batch loss 1.31850147 epoch total loss 1.28357327\n",
      "Trained batch 1098 batch loss 1.30660474 epoch total loss 1.28359425\n",
      "Trained batch 1099 batch loss 1.27152944 epoch total loss 1.28358316\n",
      "Trained batch 1100 batch loss 1.21656084 epoch total loss 1.28352225\n",
      "Trained batch 1101 batch loss 1.18461537 epoch total loss 1.28343236\n",
      "Trained batch 1102 batch loss 1.18679237 epoch total loss 1.28334475\n",
      "Trained batch 1103 batch loss 1.22035336 epoch total loss 1.28328753\n",
      "Trained batch 1104 batch loss 1.33145666 epoch total loss 1.28333116\n",
      "Trained batch 1105 batch loss 1.24171078 epoch total loss 1.28329349\n",
      "Trained batch 1106 batch loss 1.12077045 epoch total loss 1.2831465\n",
      "Trained batch 1107 batch loss 1.23942244 epoch total loss 1.28310692\n",
      "Trained batch 1108 batch loss 1.28913856 epoch total loss 1.28311241\n",
      "Trained batch 1109 batch loss 1.45480049 epoch total loss 1.28326726\n",
      "Trained batch 1110 batch loss 1.44480681 epoch total loss 1.28341281\n",
      "Trained batch 1111 batch loss 1.50270629 epoch total loss 1.28361022\n",
      "Trained batch 1112 batch loss 1.37517464 epoch total loss 1.28369248\n",
      "Trained batch 1113 batch loss 1.39701056 epoch total loss 1.28379428\n",
      "Trained batch 1114 batch loss 1.24154115 epoch total loss 1.28375638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1115 batch loss 1.24708569 epoch total loss 1.28372347\n",
      "Trained batch 1116 batch loss 1.15396857 epoch total loss 1.28360713\n",
      "Trained batch 1117 batch loss 1.30571675 epoch total loss 1.28362691\n",
      "Trained batch 1118 batch loss 1.292835 epoch total loss 1.28363514\n",
      "Trained batch 1119 batch loss 1.31142688 epoch total loss 1.28365993\n",
      "Trained batch 1120 batch loss 1.29901147 epoch total loss 1.28367376\n",
      "Trained batch 1121 batch loss 1.14207602 epoch total loss 1.2835474\n",
      "Trained batch 1122 batch loss 1.17497718 epoch total loss 1.2834506\n",
      "Trained batch 1123 batch loss 1.09645164 epoch total loss 1.28328407\n",
      "Trained batch 1124 batch loss 1.18815374 epoch total loss 1.28319943\n",
      "Trained batch 1125 batch loss 1.19223702 epoch total loss 1.28311861\n",
      "Trained batch 1126 batch loss 1.17636275 epoch total loss 1.28302383\n",
      "Trained batch 1127 batch loss 1.24560928 epoch total loss 1.28299057\n",
      "Trained batch 1128 batch loss 1.18053508 epoch total loss 1.28289974\n",
      "Trained batch 1129 batch loss 1.24872744 epoch total loss 1.28286958\n",
      "Trained batch 1130 batch loss 1.11703062 epoch total loss 1.28272283\n",
      "Trained batch 1131 batch loss 1.22083604 epoch total loss 1.28266811\n",
      "Trained batch 1132 batch loss 1.17134643 epoch total loss 1.28256977\n",
      "Trained batch 1133 batch loss 1.24519503 epoch total loss 1.28253686\n",
      "Trained batch 1134 batch loss 1.21804523 epoch total loss 1.28248\n",
      "Trained batch 1135 batch loss 1.19267547 epoch total loss 1.28240085\n",
      "Trained batch 1136 batch loss 1.17228818 epoch total loss 1.28230381\n",
      "Trained batch 1137 batch loss 1.12054491 epoch total loss 1.28216159\n",
      "Trained batch 1138 batch loss 1.20505881 epoch total loss 1.28209388\n",
      "Trained batch 1139 batch loss 1.18510151 epoch total loss 1.28200865\n",
      "Trained batch 1140 batch loss 1.10680342 epoch total loss 1.28185499\n",
      "Trained batch 1141 batch loss 1.15854144 epoch total loss 1.28174698\n",
      "Trained batch 1142 batch loss 1.24881029 epoch total loss 1.28171813\n",
      "Trained batch 1143 batch loss 1.22210979 epoch total loss 1.28166592\n",
      "Trained batch 1144 batch loss 1.26421177 epoch total loss 1.28165066\n",
      "Trained batch 1145 batch loss 1.2852385 epoch total loss 1.28165388\n",
      "Trained batch 1146 batch loss 1.30829489 epoch total loss 1.28167713\n",
      "Trained batch 1147 batch loss 1.33313954 epoch total loss 1.28172195\n",
      "Trained batch 1148 batch loss 1.38024437 epoch total loss 1.28180778\n",
      "Trained batch 1149 batch loss 1.22107744 epoch total loss 1.28175497\n",
      "Trained batch 1150 batch loss 1.13345361 epoch total loss 1.28162599\n",
      "Trained batch 1151 batch loss 1.06951392 epoch total loss 1.28144169\n",
      "Trained batch 1152 batch loss 1.31195736 epoch total loss 1.28146815\n",
      "Trained batch 1153 batch loss 1.30814075 epoch total loss 1.28149128\n",
      "Trained batch 1154 batch loss 1.32554364 epoch total loss 1.28152943\n",
      "Trained batch 1155 batch loss 1.31775582 epoch total loss 1.28156078\n",
      "Trained batch 1156 batch loss 1.31963933 epoch total loss 1.28159368\n",
      "Trained batch 1157 batch loss 1.30662537 epoch total loss 1.28161538\n",
      "Trained batch 1158 batch loss 1.33457863 epoch total loss 1.28166115\n",
      "Trained batch 1159 batch loss 1.3086766 epoch total loss 1.28168452\n",
      "Trained batch 1160 batch loss 1.37131882 epoch total loss 1.28176177\n",
      "Trained batch 1161 batch loss 1.29393935 epoch total loss 1.28177226\n",
      "Trained batch 1162 batch loss 1.19617617 epoch total loss 1.28169858\n",
      "Trained batch 1163 batch loss 1.17186379 epoch total loss 1.28160417\n",
      "Trained batch 1164 batch loss 1.15185571 epoch total loss 1.28149271\n",
      "Trained batch 1165 batch loss 1.09780622 epoch total loss 1.281335\n",
      "Trained batch 1166 batch loss 1.23264289 epoch total loss 1.28129327\n",
      "Trained batch 1167 batch loss 1.33801091 epoch total loss 1.28134179\n",
      "Trained batch 1168 batch loss 1.24508953 epoch total loss 1.2813108\n",
      "Trained batch 1169 batch loss 1.36828125 epoch total loss 1.28138518\n",
      "Trained batch 1170 batch loss 1.31673133 epoch total loss 1.28141546\n",
      "Trained batch 1171 batch loss 1.2924633 epoch total loss 1.28142488\n",
      "Trained batch 1172 batch loss 1.27646899 epoch total loss 1.28142071\n",
      "Trained batch 1173 batch loss 1.28504658 epoch total loss 1.28142381\n",
      "Trained batch 1174 batch loss 1.22593474 epoch total loss 1.2813766\n",
      "Trained batch 1175 batch loss 1.21162927 epoch total loss 1.28131723\n",
      "Trained batch 1176 batch loss 1.22070336 epoch total loss 1.28126562\n",
      "Trained batch 1177 batch loss 1.24081182 epoch total loss 1.28123128\n",
      "Trained batch 1178 batch loss 1.23881602 epoch total loss 1.28119528\n",
      "Trained batch 1179 batch loss 1.26309097 epoch total loss 1.2811799\n",
      "Trained batch 1180 batch loss 1.25194561 epoch total loss 1.28115511\n",
      "Trained batch 1181 batch loss 1.35950446 epoch total loss 1.28122151\n",
      "Trained batch 1182 batch loss 1.32039249 epoch total loss 1.28125465\n",
      "Trained batch 1183 batch loss 1.27845764 epoch total loss 1.28125226\n",
      "Trained batch 1184 batch loss 1.4867655 epoch total loss 1.28142583\n",
      "Trained batch 1185 batch loss 1.3143363 epoch total loss 1.28145361\n",
      "Trained batch 1186 batch loss 1.25951576 epoch total loss 1.28143513\n",
      "Trained batch 1187 batch loss 1.28657174 epoch total loss 1.28143954\n",
      "Trained batch 1188 batch loss 1.24062 epoch total loss 1.28140521\n",
      "Trained batch 1189 batch loss 1.21083522 epoch total loss 1.28134584\n",
      "Trained batch 1190 batch loss 1.20862532 epoch total loss 1.28128469\n",
      "Trained batch 1191 batch loss 1.07858634 epoch total loss 1.28111446\n",
      "Trained batch 1192 batch loss 1.2220695 epoch total loss 1.28106499\n",
      "Trained batch 1193 batch loss 1.22573745 epoch total loss 1.2810185\n",
      "Trained batch 1194 batch loss 1.15024722 epoch total loss 1.28090906\n",
      "Trained batch 1195 batch loss 1.33118367 epoch total loss 1.28095114\n",
      "Trained batch 1196 batch loss 1.35804713 epoch total loss 1.28101552\n",
      "Trained batch 1197 batch loss 1.35814333 epoch total loss 1.28108\n",
      "Trained batch 1198 batch loss 1.2850095 epoch total loss 1.28108335\n",
      "Trained batch 1199 batch loss 1.28868341 epoch total loss 1.28108966\n",
      "Trained batch 1200 batch loss 1.28754926 epoch total loss 1.28109503\n",
      "Trained batch 1201 batch loss 1.34004533 epoch total loss 1.28114414\n",
      "Trained batch 1202 batch loss 1.25638318 epoch total loss 1.28112352\n",
      "Trained batch 1203 batch loss 1.27827644 epoch total loss 1.28112125\n",
      "Trained batch 1204 batch loss 1.35923 epoch total loss 1.2811861\n",
      "Trained batch 1205 batch loss 1.23254716 epoch total loss 1.28114581\n",
      "Trained batch 1206 batch loss 1.30499744 epoch total loss 1.2811656\n",
      "Trained batch 1207 batch loss 1.27489042 epoch total loss 1.28116035\n",
      "Trained batch 1208 batch loss 1.23528194 epoch total loss 1.28112233\n",
      "Trained batch 1209 batch loss 1.22565472 epoch total loss 1.28107655\n",
      "Trained batch 1210 batch loss 1.35066819 epoch total loss 1.28113413\n",
      "Trained batch 1211 batch loss 1.33995187 epoch total loss 1.28118265\n",
      "Trained batch 1212 batch loss 1.2751255 epoch total loss 1.28117764\n",
      "Trained batch 1213 batch loss 1.12551308 epoch total loss 1.28104937\n",
      "Trained batch 1214 batch loss 1.16688323 epoch total loss 1.28095531\n",
      "Trained batch 1215 batch loss 1.08476925 epoch total loss 1.28079379\n",
      "Trained batch 1216 batch loss 1.16946828 epoch total loss 1.28070223\n",
      "Trained batch 1217 batch loss 1.15333784 epoch total loss 1.28059757\n",
      "Trained batch 1218 batch loss 1.08516371 epoch total loss 1.28043711\n",
      "Trained batch 1219 batch loss 1.24410057 epoch total loss 1.28040731\n",
      "Trained batch 1220 batch loss 1.3210758 epoch total loss 1.28044069\n",
      "Trained batch 1221 batch loss 1.27170706 epoch total loss 1.28043354\n",
      "Trained batch 1222 batch loss 1.26053429 epoch total loss 1.2804172\n",
      "Trained batch 1223 batch loss 1.31251276 epoch total loss 1.28044343\n",
      "Trained batch 1224 batch loss 1.33262253 epoch total loss 1.28048611\n",
      "Trained batch 1225 batch loss 1.26565766 epoch total loss 1.28047395\n",
      "Trained batch 1226 batch loss 1.32712197 epoch total loss 1.28051198\n",
      "Trained batch 1227 batch loss 1.21219409 epoch total loss 1.2804563\n",
      "Trained batch 1228 batch loss 1.14682496 epoch total loss 1.28034747\n",
      "Trained batch 1229 batch loss 1.30852103 epoch total loss 1.28037035\n",
      "Trained batch 1230 batch loss 1.21494412 epoch total loss 1.28031719\n",
      "Trained batch 1231 batch loss 1.35425878 epoch total loss 1.28037727\n",
      "Trained batch 1232 batch loss 1.36494207 epoch total loss 1.28044593\n",
      "Trained batch 1233 batch loss 1.40640926 epoch total loss 1.2805481\n",
      "Trained batch 1234 batch loss 1.18506479 epoch total loss 1.28047073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1235 batch loss 1.22680855 epoch total loss 1.28042722\n",
      "Trained batch 1236 batch loss 1.27269864 epoch total loss 1.28042102\n",
      "Trained batch 1237 batch loss 1.26186371 epoch total loss 1.280406\n",
      "Trained batch 1238 batch loss 1.3967824 epoch total loss 1.28049994\n",
      "Trained batch 1239 batch loss 1.0765785 epoch total loss 1.28033531\n",
      "Trained batch 1240 batch loss 1.06763351 epoch total loss 1.28016376\n",
      "Trained batch 1241 batch loss 1.14190638 epoch total loss 1.2800523\n",
      "Trained batch 1242 batch loss 1.16273427 epoch total loss 1.27995789\n",
      "Trained batch 1243 batch loss 1.14605737 epoch total loss 1.27985013\n",
      "Trained batch 1244 batch loss 1.24964213 epoch total loss 1.27982593\n",
      "Trained batch 1245 batch loss 1.11017287 epoch total loss 1.27968967\n",
      "Trained batch 1246 batch loss 1.31249392 epoch total loss 1.27971601\n",
      "Trained batch 1247 batch loss 1.29540718 epoch total loss 1.27972853\n",
      "Trained batch 1248 batch loss 1.31744564 epoch total loss 1.27975881\n",
      "Trained batch 1249 batch loss 1.428671 epoch total loss 1.27987814\n",
      "Trained batch 1250 batch loss 1.14409363 epoch total loss 1.27976942\n",
      "Trained batch 1251 batch loss 1.18060982 epoch total loss 1.27969027\n",
      "Trained batch 1252 batch loss 1.2252425 epoch total loss 1.27964675\n",
      "Trained batch 1253 batch loss 1.29344881 epoch total loss 1.27965772\n",
      "Trained batch 1254 batch loss 1.40505171 epoch total loss 1.27975774\n",
      "Trained batch 1255 batch loss 1.36413717 epoch total loss 1.27982497\n",
      "Trained batch 1256 batch loss 1.30155265 epoch total loss 1.27984226\n",
      "Trained batch 1257 batch loss 1.11781859 epoch total loss 1.27971327\n",
      "Trained batch 1258 batch loss 1.21667635 epoch total loss 1.27966321\n",
      "Trained batch 1259 batch loss 1.25300956 epoch total loss 1.27964211\n",
      "Trained batch 1260 batch loss 1.28624797 epoch total loss 1.27964735\n",
      "Trained batch 1261 batch loss 1.25959015 epoch total loss 1.27963138\n",
      "Trained batch 1262 batch loss 1.3339622 epoch total loss 1.27967453\n",
      "Trained batch 1263 batch loss 1.32103837 epoch total loss 1.27970731\n",
      "Trained batch 1264 batch loss 1.32851255 epoch total loss 1.27974582\n",
      "Trained batch 1265 batch loss 1.26824832 epoch total loss 1.27973676\n",
      "Trained batch 1266 batch loss 1.22762346 epoch total loss 1.27969563\n",
      "Trained batch 1267 batch loss 1.29670167 epoch total loss 1.2797091\n",
      "Trained batch 1268 batch loss 1.05743539 epoch total loss 1.27953374\n",
      "Trained batch 1269 batch loss 1.22580659 epoch total loss 1.27949142\n",
      "Trained batch 1270 batch loss 1.18861949 epoch total loss 1.2794199\n",
      "Trained batch 1271 batch loss 1.32787061 epoch total loss 1.27945805\n",
      "Trained batch 1272 batch loss 1.51887083 epoch total loss 1.27964628\n",
      "Trained batch 1273 batch loss 1.44040191 epoch total loss 1.27977264\n",
      "Trained batch 1274 batch loss 1.41406512 epoch total loss 1.27987802\n",
      "Trained batch 1275 batch loss 1.30062437 epoch total loss 1.27989435\n",
      "Trained batch 1276 batch loss 1.15980852 epoch total loss 1.27980018\n",
      "Trained batch 1277 batch loss 1.10874033 epoch total loss 1.27966619\n",
      "Trained batch 1278 batch loss 0.983837664 epoch total loss 1.2794348\n",
      "Trained batch 1279 batch loss 1.06244636 epoch total loss 1.27926517\n",
      "Trained batch 1280 batch loss 1.19449973 epoch total loss 1.27919888\n",
      "Trained batch 1281 batch loss 1.24918866 epoch total loss 1.27917552\n",
      "Trained batch 1282 batch loss 1.22634351 epoch total loss 1.27913427\n",
      "Trained batch 1283 batch loss 1.44129443 epoch total loss 1.27926064\n",
      "Trained batch 1284 batch loss 1.11904371 epoch total loss 1.27913582\n",
      "Trained batch 1285 batch loss 1.27085233 epoch total loss 1.27912939\n",
      "Trained batch 1286 batch loss 1.29782701 epoch total loss 1.27914393\n",
      "Trained batch 1287 batch loss 1.21465254 epoch total loss 1.27909386\n",
      "Trained batch 1288 batch loss 1.26186347 epoch total loss 1.27908039\n",
      "Trained batch 1289 batch loss 1.25027442 epoch total loss 1.27905798\n",
      "Trained batch 1290 batch loss 1.09335709 epoch total loss 1.27891409\n",
      "Trained batch 1291 batch loss 1.08810437 epoch total loss 1.27876627\n",
      "Trained batch 1292 batch loss 1.15371084 epoch total loss 1.27866948\n",
      "Trained batch 1293 batch loss 1.25784767 epoch total loss 1.27865338\n",
      "Trained batch 1294 batch loss 1.16324258 epoch total loss 1.27856421\n",
      "Trained batch 1295 batch loss 1.22275436 epoch total loss 1.27852106\n",
      "Trained batch 1296 batch loss 1.27212191 epoch total loss 1.27851617\n",
      "Trained batch 1297 batch loss 1.33277285 epoch total loss 1.27855802\n",
      "Trained batch 1298 batch loss 1.29243493 epoch total loss 1.27856874\n",
      "Trained batch 1299 batch loss 1.44553328 epoch total loss 1.27869725\n",
      "Trained batch 1300 batch loss 1.4666214 epoch total loss 1.27884185\n",
      "Trained batch 1301 batch loss 1.47203112 epoch total loss 1.27899039\n",
      "Trained batch 1302 batch loss 1.44128442 epoch total loss 1.27911496\n",
      "Trained batch 1303 batch loss 1.32918429 epoch total loss 1.27915347\n",
      "Trained batch 1304 batch loss 1.29615009 epoch total loss 1.27916646\n",
      "Trained batch 1305 batch loss 1.27884078 epoch total loss 1.27916622\n",
      "Trained batch 1306 batch loss 1.24217379 epoch total loss 1.27913785\n",
      "Trained batch 1307 batch loss 1.27531588 epoch total loss 1.27913487\n",
      "Trained batch 1308 batch loss 1.33112299 epoch total loss 1.27917469\n",
      "Trained batch 1309 batch loss 1.24723 epoch total loss 1.27915025\n",
      "Trained batch 1310 batch loss 1.41264558 epoch total loss 1.27925217\n",
      "Trained batch 1311 batch loss 1.30716372 epoch total loss 1.27927339\n",
      "Trained batch 1312 batch loss 1.39423394 epoch total loss 1.27936113\n",
      "Trained batch 1313 batch loss 1.31794286 epoch total loss 1.27939045\n",
      "Trained batch 1314 batch loss 1.36452293 epoch total loss 1.2794553\n",
      "Trained batch 1315 batch loss 1.21020675 epoch total loss 1.27940261\n",
      "Trained batch 1316 batch loss 1.22318721 epoch total loss 1.27935982\n",
      "Trained batch 1317 batch loss 1.23239911 epoch total loss 1.27932417\n",
      "Trained batch 1318 batch loss 1.2468245 epoch total loss 1.27929962\n",
      "Trained batch 1319 batch loss 1.27202511 epoch total loss 1.27929401\n",
      "Trained batch 1320 batch loss 1.34394944 epoch total loss 1.27934301\n",
      "Trained batch 1321 batch loss 1.24919391 epoch total loss 1.27932012\n",
      "Trained batch 1322 batch loss 1.31397772 epoch total loss 1.27934635\n",
      "Trained batch 1323 batch loss 1.16101527 epoch total loss 1.27925694\n",
      "Trained batch 1324 batch loss 1.17416573 epoch total loss 1.27917755\n",
      "Trained batch 1325 batch loss 1.34184313 epoch total loss 1.27922487\n",
      "Trained batch 1326 batch loss 1.50075102 epoch total loss 1.27939188\n",
      "Trained batch 1327 batch loss 1.24500728 epoch total loss 1.27936602\n",
      "Trained batch 1328 batch loss 1.11928821 epoch total loss 1.27924538\n",
      "Trained batch 1329 batch loss 1.13842249 epoch total loss 1.2791394\n",
      "Trained batch 1330 batch loss 1.18793571 epoch total loss 1.27907085\n",
      "Trained batch 1331 batch loss 1.34664094 epoch total loss 1.27912176\n",
      "Trained batch 1332 batch loss 1.34202886 epoch total loss 1.27916896\n",
      "Trained batch 1333 batch loss 1.21367669 epoch total loss 1.27911973\n",
      "Trained batch 1334 batch loss 1.34170771 epoch total loss 1.2791667\n",
      "Trained batch 1335 batch loss 1.21533966 epoch total loss 1.2791189\n",
      "Trained batch 1336 batch loss 1.24106801 epoch total loss 1.2790904\n",
      "Trained batch 1337 batch loss 1.19066453 epoch total loss 1.27902424\n",
      "Trained batch 1338 batch loss 1.25686097 epoch total loss 1.27900767\n",
      "Trained batch 1339 batch loss 1.42278373 epoch total loss 1.27911496\n",
      "Trained batch 1340 batch loss 1.38764501 epoch total loss 1.27919602\n",
      "Trained batch 1341 batch loss 1.18646801 epoch total loss 1.27912688\n",
      "Trained batch 1342 batch loss 1.17973506 epoch total loss 1.27905285\n",
      "Trained batch 1343 batch loss 1.20749593 epoch total loss 1.27899957\n",
      "Trained batch 1344 batch loss 1.30323458 epoch total loss 1.27901757\n",
      "Trained batch 1345 batch loss 1.16744435 epoch total loss 1.2789346\n",
      "Trained batch 1346 batch loss 1.2863481 epoch total loss 1.2789402\n",
      "Trained batch 1347 batch loss 1.28682232 epoch total loss 1.27894604\n",
      "Trained batch 1348 batch loss 1.31425261 epoch total loss 1.27897227\n",
      "Trained batch 1349 batch loss 1.31086075 epoch total loss 1.27899587\n",
      "Trained batch 1350 batch loss 1.47302079 epoch total loss 1.27913964\n",
      "Trained batch 1351 batch loss 1.41027951 epoch total loss 1.27923667\n",
      "Trained batch 1352 batch loss 1.30774105 epoch total loss 1.27925777\n",
      "Trained batch 1353 batch loss 1.16298282 epoch total loss 1.27917182\n",
      "Trained batch 1354 batch loss 1.07013321 epoch total loss 1.27901745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1355 batch loss 1.04246688 epoch total loss 1.27884293\n",
      "Trained batch 1356 batch loss 1.19288838 epoch total loss 1.27877951\n",
      "Trained batch 1357 batch loss 1.14544272 epoch total loss 1.27868116\n",
      "Trained batch 1358 batch loss 0.935520232 epoch total loss 1.27842855\n",
      "Trained batch 1359 batch loss 0.979919076 epoch total loss 1.27820885\n",
      "Trained batch 1360 batch loss 1.11478961 epoch total loss 1.27808869\n",
      "Trained batch 1361 batch loss 1.20695782 epoch total loss 1.27803636\n",
      "Trained batch 1362 batch loss 1.12956798 epoch total loss 1.27792728\n",
      "Trained batch 1363 batch loss 1.32216215 epoch total loss 1.2779597\n",
      "Trained batch 1364 batch loss 1.23052025 epoch total loss 1.2779249\n",
      "Trained batch 1365 batch loss 1.32589912 epoch total loss 1.27796006\n",
      "Trained batch 1366 batch loss 1.22875047 epoch total loss 1.27792406\n",
      "Trained batch 1367 batch loss 1.20647049 epoch total loss 1.27787173\n",
      "Trained batch 1368 batch loss 1.29337573 epoch total loss 1.27788305\n",
      "Trained batch 1369 batch loss 1.45362 epoch total loss 1.27801144\n",
      "Trained batch 1370 batch loss 1.28260481 epoch total loss 1.27801478\n",
      "Trained batch 1371 batch loss 1.36838508 epoch total loss 1.2780807\n",
      "Trained batch 1372 batch loss 1.35229588 epoch total loss 1.27813482\n",
      "Trained batch 1373 batch loss 1.30231667 epoch total loss 1.27815247\n",
      "Trained batch 1374 batch loss 1.28109384 epoch total loss 1.27815461\n",
      "Trained batch 1375 batch loss 1.20545936 epoch total loss 1.27810168\n",
      "Trained batch 1376 batch loss 1.36729813 epoch total loss 1.27816653\n",
      "Trained batch 1377 batch loss 1.41912556 epoch total loss 1.27826893\n",
      "Trained batch 1378 batch loss 1.32211494 epoch total loss 1.27830076\n",
      "Trained batch 1379 batch loss 1.25041068 epoch total loss 1.2782805\n",
      "Trained batch 1380 batch loss 1.35131431 epoch total loss 1.27833343\n",
      "Trained batch 1381 batch loss 1.26751578 epoch total loss 1.27832556\n",
      "Trained batch 1382 batch loss 1.25692761 epoch total loss 1.27831006\n",
      "Trained batch 1383 batch loss 1.36020565 epoch total loss 1.27836931\n",
      "Trained batch 1384 batch loss 1.39393687 epoch total loss 1.27845275\n",
      "Trained batch 1385 batch loss 1.41575289 epoch total loss 1.27855194\n",
      "Trained batch 1386 batch loss 1.26587605 epoch total loss 1.27854276\n",
      "Trained batch 1387 batch loss 1.13554478 epoch total loss 1.27843964\n",
      "Trained batch 1388 batch loss 1.06002915 epoch total loss 1.27828228\n",
      "Epoch 3 train loss 1.2782822847366333\n",
      "Validated batch 1 batch loss 1.3681705\n",
      "Validated batch 2 batch loss 1.3199029\n",
      "Validated batch 3 batch loss 1.20330501\n",
      "Validated batch 4 batch loss 1.27317095\n",
      "Validated batch 5 batch loss 1.18029404\n",
      "Validated batch 6 batch loss 1.32606602\n",
      "Validated batch 7 batch loss 1.31927979\n",
      "Validated batch 8 batch loss 1.17003226\n",
      "Validated batch 9 batch loss 1.27871335\n",
      "Validated batch 10 batch loss 1.24468946\n",
      "Validated batch 11 batch loss 1.30420065\n",
      "Validated batch 12 batch loss 1.26803398\n",
      "Validated batch 13 batch loss 1.30883574\n",
      "Validated batch 14 batch loss 1.2633059\n",
      "Validated batch 15 batch loss 1.18390989\n",
      "Validated batch 16 batch loss 1.30437553\n",
      "Validated batch 17 batch loss 1.24455571\n",
      "Validated batch 18 batch loss 1.36094809\n",
      "Validated batch 19 batch loss 1.37190783\n",
      "Validated batch 20 batch loss 1.55380809\n",
      "Validated batch 21 batch loss 1.348207\n",
      "Validated batch 22 batch loss 1.27937508\n",
      "Validated batch 23 batch loss 1.13920891\n",
      "Validated batch 24 batch loss 1.2968632\n",
      "Validated batch 25 batch loss 1.32181704\n",
      "Validated batch 26 batch loss 1.23993051\n",
      "Validated batch 27 batch loss 1.24758101\n",
      "Validated batch 28 batch loss 1.30700409\n",
      "Validated batch 29 batch loss 1.45366263\n",
      "Validated batch 30 batch loss 1.24150312\n",
      "Validated batch 31 batch loss 1.37766778\n",
      "Validated batch 32 batch loss 1.30778265\n",
      "Validated batch 33 batch loss 1.32048738\n",
      "Validated batch 34 batch loss 1.29173911\n",
      "Validated batch 35 batch loss 1.15172577\n",
      "Validated batch 36 batch loss 1.43389344\n",
      "Validated batch 37 batch loss 1.22617626\n",
      "Validated batch 38 batch loss 1.36315346\n",
      "Validated batch 39 batch loss 1.30158806\n",
      "Validated batch 40 batch loss 1.37980258\n",
      "Validated batch 41 batch loss 1.15360796\n",
      "Validated batch 42 batch loss 1.25268984\n",
      "Validated batch 43 batch loss 1.2348218\n",
      "Validated batch 44 batch loss 1.31756604\n",
      "Validated batch 45 batch loss 1.29739738\n",
      "Validated batch 46 batch loss 1.27818596\n",
      "Validated batch 47 batch loss 1.26277173\n",
      "Validated batch 48 batch loss 1.2584877\n",
      "Validated batch 49 batch loss 1.23667693\n",
      "Validated batch 50 batch loss 1.18292475\n",
      "Validated batch 51 batch loss 1.21408629\n",
      "Validated batch 52 batch loss 1.35342026\n",
      "Validated batch 53 batch loss 1.2151792\n",
      "Validated batch 54 batch loss 1.0756942\n",
      "Validated batch 55 batch loss 1.20868599\n",
      "Validated batch 56 batch loss 1.22710598\n",
      "Validated batch 57 batch loss 1.17522144\n",
      "Validated batch 58 batch loss 1.24152887\n",
      "Validated batch 59 batch loss 1.26645565\n",
      "Validated batch 60 batch loss 1.2103076\n",
      "Validated batch 61 batch loss 1.31307137\n",
      "Validated batch 62 batch loss 1.3672123\n",
      "Validated batch 63 batch loss 1.20035291\n",
      "Validated batch 64 batch loss 1.37543595\n",
      "Validated batch 65 batch loss 1.06042063\n",
      "Validated batch 66 batch loss 1.25515962\n",
      "Validated batch 67 batch loss 1.17076993\n",
      "Validated batch 68 batch loss 1.24759793\n",
      "Validated batch 69 batch loss 1.40955877\n",
      "Validated batch 70 batch loss 1.23506153\n",
      "Validated batch 71 batch loss 1.20417845\n",
      "Validated batch 72 batch loss 1.26437891\n",
      "Validated batch 73 batch loss 1.16363156\n",
      "Validated batch 74 batch loss 1.23134136\n",
      "Validated batch 75 batch loss 1.28820395\n",
      "Validated batch 76 batch loss 1.27471387\n",
      "Validated batch 77 batch loss 1.37165499\n",
      "Validated batch 78 batch loss 1.32492971\n",
      "Validated batch 79 batch loss 1.29758024\n",
      "Validated batch 80 batch loss 1.26743078\n",
      "Validated batch 81 batch loss 1.4226625\n",
      "Validated batch 82 batch loss 1.33522201\n",
      "Validated batch 83 batch loss 1.34695506\n",
      "Validated batch 84 batch loss 1.36364973\n",
      "Validated batch 85 batch loss 1.36428952\n",
      "Validated batch 86 batch loss 1.33818233\n",
      "Validated batch 87 batch loss 1.18329835\n",
      "Validated batch 88 batch loss 1.22935641\n",
      "Validated batch 89 batch loss 1.35160637\n",
      "Validated batch 90 batch loss 1.34375048\n",
      "Validated batch 91 batch loss 1.27665257\n",
      "Validated batch 92 batch loss 1.28302681\n",
      "Validated batch 93 batch loss 1.30834365\n",
      "Validated batch 94 batch loss 1.33516312\n",
      "Validated batch 95 batch loss 1.15943742\n",
      "Validated batch 96 batch loss 1.19203901\n",
      "Validated batch 97 batch loss 1.27129936\n",
      "Validated batch 98 batch loss 1.22582102\n",
      "Validated batch 99 batch loss 1.25510895\n",
      "Validated batch 100 batch loss 1.27898061\n",
      "Validated batch 101 batch loss 1.18534124\n",
      "Validated batch 102 batch loss 1.36743116\n",
      "Validated batch 103 batch loss 1.19511569\n",
      "Validated batch 104 batch loss 1.1721915\n",
      "Validated batch 105 batch loss 1.25024128\n",
      "Validated batch 106 batch loss 1.40520406\n",
      "Validated batch 107 batch loss 1.350945\n",
      "Validated batch 108 batch loss 1.42670941\n",
      "Validated batch 109 batch loss 1.22483718\n",
      "Validated batch 110 batch loss 1.40993643\n",
      "Validated batch 111 batch loss 1.24878883\n",
      "Validated batch 112 batch loss 1.32311654\n",
      "Validated batch 113 batch loss 1.30829382\n",
      "Validated batch 114 batch loss 1.0176692\n",
      "Validated batch 115 batch loss 1.18652773\n",
      "Validated batch 116 batch loss 1.27140951\n",
      "Validated batch 117 batch loss 1.15938044\n",
      "Validated batch 118 batch loss 1.26936185\n",
      "Validated batch 119 batch loss 1.20259452\n",
      "Validated batch 120 batch loss 1.1784296\n",
      "Validated batch 121 batch loss 1.28332889\n",
      "Validated batch 122 batch loss 1.25462389\n",
      "Validated batch 123 batch loss 1.26185167\n",
      "Validated batch 124 batch loss 1.32309961\n",
      "Validated batch 125 batch loss 1.19894123\n",
      "Validated batch 126 batch loss 1.34407508\n",
      "Validated batch 127 batch loss 1.30779147\n",
      "Validated batch 128 batch loss 1.1611166\n",
      "Validated batch 129 batch loss 1.25275064\n",
      "Validated batch 130 batch loss 1.28605056\n",
      "Validated batch 131 batch loss 1.23564982\n",
      "Validated batch 132 batch loss 1.37137437\n",
      "Validated batch 133 batch loss 1.29902053\n",
      "Validated batch 134 batch loss 1.28597105\n",
      "Validated batch 135 batch loss 1.26285315\n",
      "Validated batch 136 batch loss 1.25243878\n",
      "Validated batch 137 batch loss 1.24648547\n",
      "Validated batch 138 batch loss 1.20373297\n",
      "Validated batch 139 batch loss 1.21769047\n",
      "Validated batch 140 batch loss 1.34584963\n",
      "Validated batch 141 batch loss 1.24378264\n",
      "Validated batch 142 batch loss 1.20545352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 143 batch loss 1.31025\n",
      "Validated batch 144 batch loss 1.28344655\n",
      "Validated batch 145 batch loss 1.3585012\n",
      "Validated batch 146 batch loss 1.36269164\n",
      "Validated batch 147 batch loss 1.31206489\n",
      "Validated batch 148 batch loss 1.2038238\n",
      "Validated batch 149 batch loss 1.29450428\n",
      "Validated batch 150 batch loss 1.31614912\n",
      "Validated batch 151 batch loss 1.26428604\n",
      "Validated batch 152 batch loss 1.3759625\n",
      "Validated batch 153 batch loss 1.37180018\n",
      "Validated batch 154 batch loss 1.29122448\n",
      "Validated batch 155 batch loss 1.40465856\n",
      "Validated batch 156 batch loss 1.2254101\n",
      "Validated batch 157 batch loss 1.20213842\n",
      "Validated batch 158 batch loss 1.23980057\n",
      "Validated batch 159 batch loss 1.15261269\n",
      "Validated batch 160 batch loss 1.34597957\n",
      "Validated batch 161 batch loss 1.19585979\n",
      "Validated batch 162 batch loss 1.21891963\n",
      "Validated batch 163 batch loss 1.22059846\n",
      "Validated batch 164 batch loss 1.35083508\n",
      "Validated batch 165 batch loss 1.25801849\n",
      "Validated batch 166 batch loss 1.27121019\n",
      "Validated batch 167 batch loss 1.45109\n",
      "Validated batch 168 batch loss 1.17573702\n",
      "Validated batch 169 batch loss 1.28952348\n",
      "Validated batch 170 batch loss 1.28710854\n",
      "Validated batch 171 batch loss 1.30747306\n",
      "Validated batch 172 batch loss 1.33219445\n",
      "Validated batch 173 batch loss 1.24594402\n",
      "Validated batch 174 batch loss 0.983671427\n",
      "Validated batch 175 batch loss 1.27868414\n",
      "Validated batch 176 batch loss 1.21781492\n",
      "Validated batch 177 batch loss 1.26357007\n",
      "Validated batch 178 batch loss 1.30859697\n",
      "Validated batch 179 batch loss 1.14320862\n",
      "Validated batch 180 batch loss 1.33039021\n",
      "Validated batch 181 batch loss 1.3767308\n",
      "Validated batch 182 batch loss 1.30581236\n",
      "Validated batch 183 batch loss 1.30673015\n",
      "Validated batch 184 batch loss 1.15190482\n",
      "Validated batch 185 batch loss 1.24708021\n",
      "Epoch 3 val loss 1.2732893228530884\n",
      "Model .//model-epoch-3-loss-1.2733.h5 saved.\n",
      "Start epoch 4 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.28029561 epoch total loss 1.28029561\n",
      "Trained batch 2 batch loss 1.36227858 epoch total loss 1.32128716\n",
      "Trained batch 3 batch loss 1.4502629 epoch total loss 1.36427915\n",
      "Trained batch 4 batch loss 1.42107892 epoch total loss 1.378479\n",
      "Trained batch 5 batch loss 1.37446404 epoch total loss 1.37767601\n",
      "Trained batch 6 batch loss 1.26718462 epoch total loss 1.35926068\n",
      "Trained batch 7 batch loss 1.35174704 epoch total loss 1.35818732\n",
      "Trained batch 8 batch loss 1.12960196 epoch total loss 1.32961416\n",
      "Trained batch 9 batch loss 1.3147701 epoch total loss 1.32796478\n",
      "Trained batch 10 batch loss 1.19820499 epoch total loss 1.31498885\n",
      "Trained batch 11 batch loss 1.22967398 epoch total loss 1.30723298\n",
      "Trained batch 12 batch loss 1.14494061 epoch total loss 1.29370856\n",
      "Trained batch 13 batch loss 1.06748438 epoch total loss 1.27630675\n",
      "Trained batch 14 batch loss 1.06350684 epoch total loss 1.26110673\n",
      "Trained batch 15 batch loss 1.13043869 epoch total loss 1.25239551\n",
      "Trained batch 16 batch loss 1.42515481 epoch total loss 1.26319301\n",
      "Trained batch 17 batch loss 1.49470961 epoch total loss 1.2768116\n",
      "Trained batch 18 batch loss 1.47774065 epoch total loss 1.28797436\n",
      "Trained batch 19 batch loss 1.37964368 epoch total loss 1.29279912\n",
      "Trained batch 20 batch loss 1.33263516 epoch total loss 1.29479098\n",
      "Trained batch 21 batch loss 1.43607354 epoch total loss 1.30151868\n",
      "Trained batch 22 batch loss 1.37469673 epoch total loss 1.30484498\n",
      "Trained batch 23 batch loss 1.33096623 epoch total loss 1.30598068\n",
      "Trained batch 24 batch loss 1.29813361 epoch total loss 1.30565369\n",
      "Trained batch 25 batch loss 1.13333869 epoch total loss 1.29876113\n",
      "Trained batch 26 batch loss 1.19628584 epoch total loss 1.29481971\n",
      "Trained batch 27 batch loss 1.27931595 epoch total loss 1.2942456\n",
      "Trained batch 28 batch loss 1.31996131 epoch total loss 1.29516399\n",
      "Trained batch 29 batch loss 1.23164034 epoch total loss 1.29297352\n",
      "Trained batch 30 batch loss 1.2606976 epoch total loss 1.29189754\n",
      "Trained batch 31 batch loss 1.28168476 epoch total loss 1.29156816\n",
      "Trained batch 32 batch loss 1.32969284 epoch total loss 1.29275954\n",
      "Trained batch 33 batch loss 1.33984256 epoch total loss 1.29418635\n",
      "Trained batch 34 batch loss 1.31980145 epoch total loss 1.29493976\n",
      "Trained batch 35 batch loss 1.33676922 epoch total loss 1.29613483\n",
      "Trained batch 36 batch loss 1.32989085 epoch total loss 1.29707253\n",
      "Trained batch 37 batch loss 1.17703414 epoch total loss 1.29382825\n",
      "Trained batch 38 batch loss 1.22699034 epoch total loss 1.29206932\n",
      "Trained batch 39 batch loss 1.15718555 epoch total loss 1.2886107\n",
      "Trained batch 40 batch loss 1.09525633 epoch total loss 1.28377688\n",
      "Trained batch 41 batch loss 1.10843074 epoch total loss 1.27950013\n",
      "Trained batch 42 batch loss 1.13465166 epoch total loss 1.27605128\n",
      "Trained batch 43 batch loss 1.24219966 epoch total loss 1.27526402\n",
      "Trained batch 44 batch loss 1.35984337 epoch total loss 1.27718627\n",
      "Trained batch 45 batch loss 1.26509261 epoch total loss 1.27691758\n",
      "Trained batch 46 batch loss 1.21780396 epoch total loss 1.2756325\n",
      "Trained batch 47 batch loss 1.34145761 epoch total loss 1.27703297\n",
      "Trained batch 48 batch loss 1.19074297 epoch total loss 1.2752353\n",
      "Trained batch 49 batch loss 1.10028267 epoch total loss 1.27166486\n",
      "Trained batch 50 batch loss 1.24074602 epoch total loss 1.2710464\n",
      "Trained batch 51 batch loss 1.23760676 epoch total loss 1.27039087\n",
      "Trained batch 52 batch loss 1.23295116 epoch total loss 1.26967072\n",
      "Trained batch 53 batch loss 1.25397539 epoch total loss 1.26937461\n",
      "Trained batch 54 batch loss 1.23481369 epoch total loss 1.26873469\n",
      "Trained batch 55 batch loss 1.25909853 epoch total loss 1.26855958\n",
      "Trained batch 56 batch loss 1.25720513 epoch total loss 1.26835668\n",
      "Trained batch 57 batch loss 1.11782336 epoch total loss 1.26571572\n",
      "Trained batch 58 batch loss 1.18775487 epoch total loss 1.26437151\n",
      "Trained batch 59 batch loss 1.28274238 epoch total loss 1.26468301\n",
      "Trained batch 60 batch loss 1.30415726 epoch total loss 1.26534092\n",
      "Trained batch 61 batch loss 1.23722744 epoch total loss 1.26488006\n",
      "Trained batch 62 batch loss 1.16202641 epoch total loss 1.26322114\n",
      "Trained batch 63 batch loss 1.10888052 epoch total loss 1.26077127\n",
      "Trained batch 64 batch loss 1.23951268 epoch total loss 1.26043904\n",
      "Trained batch 65 batch loss 1.22615862 epoch total loss 1.25991166\n",
      "Trained batch 66 batch loss 1.21066868 epoch total loss 1.25916553\n",
      "Trained batch 67 batch loss 1.10728359 epoch total loss 1.25689864\n",
      "Trained batch 68 batch loss 1.09895992 epoch total loss 1.25457609\n",
      "Trained batch 69 batch loss 1.08856559 epoch total loss 1.25217009\n",
      "Trained batch 70 batch loss 1.26199722 epoch total loss 1.2523104\n",
      "Trained batch 71 batch loss 1.20280504 epoch total loss 1.25161314\n",
      "Trained batch 72 batch loss 1.22029781 epoch total loss 1.25117826\n",
      "Trained batch 73 batch loss 1.29121935 epoch total loss 1.25172675\n",
      "Trained batch 74 batch loss 1.35903835 epoch total loss 1.25317693\n",
      "Trained batch 75 batch loss 1.19238389 epoch total loss 1.2523663\n",
      "Trained batch 76 batch loss 1.21407187 epoch total loss 1.25186253\n",
      "Trained batch 77 batch loss 1.20829463 epoch total loss 1.25129676\n",
      "Trained batch 78 batch loss 1.3104161 epoch total loss 1.25205469\n",
      "Trained batch 79 batch loss 1.19230771 epoch total loss 1.25129831\n",
      "Trained batch 80 batch loss 1.12692475 epoch total loss 1.2497437\n",
      "Trained batch 81 batch loss 0.900151849 epoch total loss 1.24542773\n",
      "Trained batch 82 batch loss 1.02413917 epoch total loss 1.24272907\n",
      "Trained batch 83 batch loss 1.1938355 epoch total loss 1.24213994\n",
      "Trained batch 84 batch loss 1.37308407 epoch total loss 1.24369884\n",
      "Trained batch 85 batch loss 1.35072529 epoch total loss 1.24495792\n",
      "Trained batch 86 batch loss 1.35987496 epoch total loss 1.24629426\n",
      "Trained batch 87 batch loss 1.23336387 epoch total loss 1.24614561\n",
      "Trained batch 88 batch loss 1.19254923 epoch total loss 1.24553657\n",
      "Trained batch 89 batch loss 1.21892929 epoch total loss 1.24523759\n",
      "Trained batch 90 batch loss 1.13306618 epoch total loss 1.24399114\n",
      "Trained batch 91 batch loss 1.14897072 epoch total loss 1.24294698\n",
      "Trained batch 92 batch loss 1.09772277 epoch total loss 1.24136853\n",
      "Trained batch 93 batch loss 1.09693 epoch total loss 1.23981547\n",
      "Trained batch 94 batch loss 1.14242136 epoch total loss 1.23877931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 95 batch loss 1.18728852 epoch total loss 1.23823726\n",
      "Trained batch 96 batch loss 1.10567212 epoch total loss 1.23685634\n",
      "Trained batch 97 batch loss 1.07049894 epoch total loss 1.23514128\n",
      "Trained batch 98 batch loss 1.15263951 epoch total loss 1.23429954\n",
      "Trained batch 99 batch loss 1.03322983 epoch total loss 1.23226857\n",
      "Trained batch 100 batch loss 1.32531512 epoch total loss 1.233199\n",
      "Trained batch 101 batch loss 1.5080843 epoch total loss 1.23592067\n",
      "Trained batch 102 batch loss 1.19590044 epoch total loss 1.23552835\n",
      "Trained batch 103 batch loss 1.42408454 epoch total loss 1.23735893\n",
      "Trained batch 104 batch loss 1.39453757 epoch total loss 1.23887038\n",
      "Trained batch 105 batch loss 1.20688283 epoch total loss 1.23856568\n",
      "Trained batch 106 batch loss 1.19735336 epoch total loss 1.23817694\n",
      "Trained batch 107 batch loss 1.28088045 epoch total loss 1.23857605\n",
      "Trained batch 108 batch loss 1.07465947 epoch total loss 1.23705828\n",
      "Trained batch 109 batch loss 1.18840337 epoch total loss 1.23661184\n",
      "Trained batch 110 batch loss 1.12765777 epoch total loss 1.23562133\n",
      "Trained batch 111 batch loss 1.12564886 epoch total loss 1.2346307\n",
      "Trained batch 112 batch loss 1.19591427 epoch total loss 1.234285\n",
      "Trained batch 113 batch loss 1.20016432 epoch total loss 1.23398304\n",
      "Trained batch 114 batch loss 1.34861946 epoch total loss 1.23498857\n",
      "Trained batch 115 batch loss 1.3896184 epoch total loss 1.23633313\n",
      "Trained batch 116 batch loss 1.40486205 epoch total loss 1.23778605\n",
      "Trained batch 117 batch loss 1.43364799 epoch total loss 1.23946011\n",
      "Trained batch 118 batch loss 1.32873666 epoch total loss 1.24021661\n",
      "Trained batch 119 batch loss 1.29880214 epoch total loss 1.24070895\n",
      "Trained batch 120 batch loss 1.1831795 epoch total loss 1.24022949\n",
      "Trained batch 121 batch loss 1.24747849 epoch total loss 1.24028945\n",
      "Trained batch 122 batch loss 1.35677683 epoch total loss 1.24124432\n",
      "Trained batch 123 batch loss 1.23299599 epoch total loss 1.24117732\n",
      "Trained batch 124 batch loss 1.21577442 epoch total loss 1.2409724\n",
      "Trained batch 125 batch loss 1.23562419 epoch total loss 1.24092972\n",
      "Trained batch 126 batch loss 1.15909934 epoch total loss 1.24028027\n",
      "Trained batch 127 batch loss 1.12998247 epoch total loss 1.23941183\n",
      "Trained batch 128 batch loss 1.20114088 epoch total loss 1.23911285\n",
      "Trained batch 129 batch loss 1.17995346 epoch total loss 1.23865426\n",
      "Trained batch 130 batch loss 1.32133186 epoch total loss 1.23929024\n",
      "Trained batch 131 batch loss 1.41240466 epoch total loss 1.24061167\n",
      "Trained batch 132 batch loss 1.27308631 epoch total loss 1.24085772\n",
      "Trained batch 133 batch loss 1.19623017 epoch total loss 1.24052215\n",
      "Trained batch 134 batch loss 1.14573646 epoch total loss 1.23981476\n",
      "Trained batch 135 batch loss 1.29881823 epoch total loss 1.24025178\n",
      "Trained batch 136 batch loss 1.29578 epoch total loss 1.24066007\n",
      "Trained batch 137 batch loss 1.38143039 epoch total loss 1.24168754\n",
      "Trained batch 138 batch loss 1.31834304 epoch total loss 1.24224305\n",
      "Trained batch 139 batch loss 1.18778074 epoch total loss 1.24185109\n",
      "Trained batch 140 batch loss 1.31845522 epoch total loss 1.24239826\n",
      "Trained batch 141 batch loss 1.23529446 epoch total loss 1.24234784\n",
      "Trained batch 142 batch loss 1.3252449 epoch total loss 1.2429316\n",
      "Trained batch 143 batch loss 1.13565981 epoch total loss 1.24218154\n",
      "Trained batch 144 batch loss 1.21299136 epoch total loss 1.24197888\n",
      "Trained batch 145 batch loss 1.23044121 epoch total loss 1.24189925\n",
      "Trained batch 146 batch loss 1.2438 epoch total loss 1.24191236\n",
      "Trained batch 147 batch loss 1.26650357 epoch total loss 1.24207962\n",
      "Trained batch 148 batch loss 1.2674408 epoch total loss 1.24225104\n",
      "Trained batch 149 batch loss 1.2728374 epoch total loss 1.24245632\n",
      "Trained batch 150 batch loss 1.29940355 epoch total loss 1.242836\n",
      "Trained batch 151 batch loss 1.31654012 epoch total loss 1.24332416\n",
      "Trained batch 152 batch loss 1.27070796 epoch total loss 1.24350429\n",
      "Trained batch 153 batch loss 1.24579883 epoch total loss 1.24351931\n",
      "Trained batch 154 batch loss 1.24711633 epoch total loss 1.24354267\n",
      "Trained batch 155 batch loss 1.12735546 epoch total loss 1.24279308\n",
      "Trained batch 156 batch loss 1.1508559 epoch total loss 1.24220359\n",
      "Trained batch 157 batch loss 1.14209867 epoch total loss 1.24156606\n",
      "Trained batch 158 batch loss 1.14876413 epoch total loss 1.24097872\n",
      "Trained batch 159 batch loss 1.13036442 epoch total loss 1.24028301\n",
      "Trained batch 160 batch loss 1.15487301 epoch total loss 1.23974919\n",
      "Trained batch 161 batch loss 1.20513141 epoch total loss 1.23953414\n",
      "Trained batch 162 batch loss 1.0797981 epoch total loss 1.23854816\n",
      "Trained batch 163 batch loss 1.20318186 epoch total loss 1.2383312\n",
      "Trained batch 164 batch loss 1.06394577 epoch total loss 1.23726797\n",
      "Trained batch 165 batch loss 1.27044713 epoch total loss 1.23746908\n",
      "Trained batch 166 batch loss 1.24597394 epoch total loss 1.23752022\n",
      "Trained batch 167 batch loss 1.29471 epoch total loss 1.23786271\n",
      "Trained batch 168 batch loss 1.37006056 epoch total loss 1.23864961\n",
      "Trained batch 169 batch loss 1.23488355 epoch total loss 1.2386272\n",
      "Trained batch 170 batch loss 1.294927 epoch total loss 1.23895836\n",
      "Trained batch 171 batch loss 1.25294125 epoch total loss 1.23904014\n",
      "Trained batch 172 batch loss 1.20391762 epoch total loss 1.23883593\n",
      "Trained batch 173 batch loss 1.13322008 epoch total loss 1.23822546\n",
      "Trained batch 174 batch loss 1.36786425 epoch total loss 1.23897052\n",
      "Trained batch 175 batch loss 1.34032035 epoch total loss 1.23954964\n",
      "Trained batch 176 batch loss 1.34563684 epoch total loss 1.24015248\n",
      "Trained batch 177 batch loss 1.21397233 epoch total loss 1.24000454\n",
      "Trained batch 178 batch loss 1.13680947 epoch total loss 1.23942482\n",
      "Trained batch 179 batch loss 1.18012011 epoch total loss 1.23909342\n",
      "Trained batch 180 batch loss 1.22597218 epoch total loss 1.23902059\n",
      "Trained batch 181 batch loss 1.30498528 epoch total loss 1.23938501\n",
      "Trained batch 182 batch loss 1.18730736 epoch total loss 1.23909891\n",
      "Trained batch 183 batch loss 1.23988712 epoch total loss 1.2391032\n",
      "Trained batch 184 batch loss 1.20435941 epoch total loss 1.23891437\n",
      "Trained batch 185 batch loss 1.1564914 epoch total loss 1.23846877\n",
      "Trained batch 186 batch loss 1.24711919 epoch total loss 1.23851526\n",
      "Trained batch 187 batch loss 1.17570484 epoch total loss 1.23817945\n",
      "Trained batch 188 batch loss 1.22297311 epoch total loss 1.2380985\n",
      "Trained batch 189 batch loss 1.1350615 epoch total loss 1.23755336\n",
      "Trained batch 190 batch loss 1.30933487 epoch total loss 1.23793113\n",
      "Trained batch 191 batch loss 1.2403748 epoch total loss 1.23794401\n",
      "Trained batch 192 batch loss 1.17469847 epoch total loss 1.23761451\n",
      "Trained batch 193 batch loss 1.25740516 epoch total loss 1.23771703\n",
      "Trained batch 194 batch loss 1.14753985 epoch total loss 1.23725224\n",
      "Trained batch 195 batch loss 1.05965292 epoch total loss 1.23634148\n",
      "Trained batch 196 batch loss 1.15188015 epoch total loss 1.23591053\n",
      "Trained batch 197 batch loss 1.30131769 epoch total loss 1.23624253\n",
      "Trained batch 198 batch loss 1.43064559 epoch total loss 1.23722434\n",
      "Trained batch 199 batch loss 1.27357852 epoch total loss 1.23740709\n",
      "Trained batch 200 batch loss 1.31872106 epoch total loss 1.23781359\n",
      "Trained batch 201 batch loss 1.25425458 epoch total loss 1.23789549\n",
      "Trained batch 202 batch loss 1.29989994 epoch total loss 1.23820233\n",
      "Trained batch 203 batch loss 1.24791312 epoch total loss 1.23825026\n",
      "Trained batch 204 batch loss 1.36906505 epoch total loss 1.23889148\n",
      "Trained batch 205 batch loss 1.41918814 epoch total loss 1.23977101\n",
      "Trained batch 206 batch loss 1.24236763 epoch total loss 1.23978353\n",
      "Trained batch 207 batch loss 1.23918366 epoch total loss 1.23978078\n",
      "Trained batch 208 batch loss 1.34632432 epoch total loss 1.24029291\n",
      "Trained batch 209 batch loss 1.20089674 epoch total loss 1.24010444\n",
      "Trained batch 210 batch loss 1.36726713 epoch total loss 1.24071\n",
      "Trained batch 211 batch loss 1.19969273 epoch total loss 1.24051571\n",
      "Trained batch 212 batch loss 1.19598472 epoch total loss 1.24030566\n",
      "Trained batch 213 batch loss 1.27324831 epoch total loss 1.24046028\n",
      "Trained batch 214 batch loss 1.26046801 epoch total loss 1.24055386\n",
      "Trained batch 215 batch loss 1.30783403 epoch total loss 1.24086678\n",
      "Trained batch 216 batch loss 1.29165292 epoch total loss 1.24110186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 217 batch loss 1.18614006 epoch total loss 1.24084854\n",
      "Trained batch 218 batch loss 1.25457633 epoch total loss 1.24091148\n",
      "Trained batch 219 batch loss 1.25392342 epoch total loss 1.24097097\n",
      "Trained batch 220 batch loss 1.14850247 epoch total loss 1.24055064\n",
      "Trained batch 221 batch loss 1.14666569 epoch total loss 1.24012589\n",
      "Trained batch 222 batch loss 1.21933532 epoch total loss 1.2400322\n",
      "Trained batch 223 batch loss 1.13184762 epoch total loss 1.23954701\n",
      "Trained batch 224 batch loss 1.15435457 epoch total loss 1.23916662\n",
      "Trained batch 225 batch loss 1.18992209 epoch total loss 1.23894775\n",
      "Trained batch 226 batch loss 1.17505932 epoch total loss 1.23866498\n",
      "Trained batch 227 batch loss 1.20309711 epoch total loss 1.23850834\n",
      "Trained batch 228 batch loss 1.34360504 epoch total loss 1.23896921\n",
      "Trained batch 229 batch loss 1.3132894 epoch total loss 1.23929381\n",
      "Trained batch 230 batch loss 1.38209093 epoch total loss 1.23991466\n",
      "Trained batch 231 batch loss 1.43806231 epoch total loss 1.24077237\n",
      "Trained batch 232 batch loss 1.50610065 epoch total loss 1.24191594\n",
      "Trained batch 233 batch loss 1.20860088 epoch total loss 1.24177301\n",
      "Trained batch 234 batch loss 1.2492367 epoch total loss 1.24180484\n",
      "Trained batch 235 batch loss 1.19614911 epoch total loss 1.24161053\n",
      "Trained batch 236 batch loss 1.44185138 epoch total loss 1.24245906\n",
      "Trained batch 237 batch loss 1.35805106 epoch total loss 1.24294686\n",
      "Trained batch 238 batch loss 1.39397824 epoch total loss 1.24358141\n",
      "Trained batch 239 batch loss 1.22822034 epoch total loss 1.24351716\n",
      "Trained batch 240 batch loss 1.22786617 epoch total loss 1.24345195\n",
      "Trained batch 241 batch loss 1.33518875 epoch total loss 1.24383259\n",
      "Trained batch 242 batch loss 1.38800287 epoch total loss 1.24442828\n",
      "Trained batch 243 batch loss 1.42131162 epoch total loss 1.24515629\n",
      "Trained batch 244 batch loss 1.37358701 epoch total loss 1.2456826\n",
      "Trained batch 245 batch loss 1.39093864 epoch total loss 1.24627554\n",
      "Trained batch 246 batch loss 1.34477687 epoch total loss 1.24667597\n",
      "Trained batch 247 batch loss 1.20617843 epoch total loss 1.24651194\n",
      "Trained batch 248 batch loss 1.15648603 epoch total loss 1.24614894\n",
      "Trained batch 249 batch loss 1.21618676 epoch total loss 1.24602866\n",
      "Trained batch 250 batch loss 1.31852221 epoch total loss 1.24631858\n",
      "Trained batch 251 batch loss 1.37360454 epoch total loss 1.2468257\n",
      "Trained batch 252 batch loss 1.25534284 epoch total loss 1.24685943\n",
      "Trained batch 253 batch loss 1.16366446 epoch total loss 1.24653065\n",
      "Trained batch 254 batch loss 1.15251851 epoch total loss 1.24616051\n",
      "Trained batch 255 batch loss 1.12546611 epoch total loss 1.24568725\n",
      "Trained batch 256 batch loss 1.0744698 epoch total loss 1.24501836\n",
      "Trained batch 257 batch loss 1.19235861 epoch total loss 1.24481344\n",
      "Trained batch 258 batch loss 1.11511493 epoch total loss 1.24431074\n",
      "Trained batch 259 batch loss 1.17721498 epoch total loss 1.24405169\n",
      "Trained batch 260 batch loss 1.24754071 epoch total loss 1.24406505\n",
      "Trained batch 261 batch loss 1.35808325 epoch total loss 1.24450195\n",
      "Trained batch 262 batch loss 1.26587367 epoch total loss 1.24458349\n",
      "Trained batch 263 batch loss 1.14349544 epoch total loss 1.24419916\n",
      "Trained batch 264 batch loss 1.29747033 epoch total loss 1.24440098\n",
      "Trained batch 265 batch loss 1.27845609 epoch total loss 1.24452937\n",
      "Trained batch 266 batch loss 1.29511487 epoch total loss 1.24471951\n",
      "Trained batch 267 batch loss 1.20672095 epoch total loss 1.24457729\n",
      "Trained batch 268 batch loss 1.32936394 epoch total loss 1.24489367\n",
      "Trained batch 269 batch loss 1.30529273 epoch total loss 1.24511826\n",
      "Trained batch 270 batch loss 1.16674805 epoch total loss 1.24482799\n",
      "Trained batch 271 batch loss 1.27593541 epoch total loss 1.24494278\n",
      "Trained batch 272 batch loss 1.28587842 epoch total loss 1.24509335\n",
      "Trained batch 273 batch loss 1.25090909 epoch total loss 1.24511456\n",
      "Trained batch 274 batch loss 1.23886764 epoch total loss 1.2450918\n",
      "Trained batch 275 batch loss 1.0940119 epoch total loss 1.24454248\n",
      "Trained batch 276 batch loss 1.119223 epoch total loss 1.24408841\n",
      "Trained batch 277 batch loss 1.08952594 epoch total loss 1.24353051\n",
      "Trained batch 278 batch loss 1.11981654 epoch total loss 1.2430855\n",
      "Trained batch 279 batch loss 1.22685242 epoch total loss 1.24302733\n",
      "Trained batch 280 batch loss 1.14201939 epoch total loss 1.2426666\n",
      "Trained batch 281 batch loss 1.11322939 epoch total loss 1.24220598\n",
      "Trained batch 282 batch loss 1.12557364 epoch total loss 1.24179244\n",
      "Trained batch 283 batch loss 1.12933671 epoch total loss 1.241395\n",
      "Trained batch 284 batch loss 1.07379603 epoch total loss 1.24080491\n",
      "Trained batch 285 batch loss 1.01923633 epoch total loss 1.24002743\n",
      "Trained batch 286 batch loss 1.28035092 epoch total loss 1.24016845\n",
      "Trained batch 287 batch loss 1.39125717 epoch total loss 1.24069488\n",
      "Trained batch 288 batch loss 1.36807621 epoch total loss 1.24113715\n",
      "Trained batch 289 batch loss 1.25888062 epoch total loss 1.24119854\n",
      "Trained batch 290 batch loss 1.23400342 epoch total loss 1.24117374\n",
      "Trained batch 291 batch loss 1.21554744 epoch total loss 1.24108577\n",
      "Trained batch 292 batch loss 1.35285711 epoch total loss 1.24146843\n",
      "Trained batch 293 batch loss 1.21299934 epoch total loss 1.24137139\n",
      "Trained batch 294 batch loss 1.19991183 epoch total loss 1.24123037\n",
      "Trained batch 295 batch loss 1.10166955 epoch total loss 1.24075735\n",
      "Trained batch 296 batch loss 1.2568891 epoch total loss 1.24081182\n",
      "Trained batch 297 batch loss 1.11020553 epoch total loss 1.24037206\n",
      "Trained batch 298 batch loss 1.11946476 epoch total loss 1.23996639\n",
      "Trained batch 299 batch loss 1.28283465 epoch total loss 1.2401098\n",
      "Trained batch 300 batch loss 1.26496017 epoch total loss 1.24019253\n",
      "Trained batch 301 batch loss 1.17864311 epoch total loss 1.23998809\n",
      "Trained batch 302 batch loss 1.09154212 epoch total loss 1.23949659\n",
      "Trained batch 303 batch loss 1.14159167 epoch total loss 1.23917353\n",
      "Trained batch 304 batch loss 1.28568065 epoch total loss 1.23932648\n",
      "Trained batch 305 batch loss 1.23431838 epoch total loss 1.23931\n",
      "Trained batch 306 batch loss 1.15674269 epoch total loss 1.23904026\n",
      "Trained batch 307 batch loss 1.12905335 epoch total loss 1.23868191\n",
      "Trained batch 308 batch loss 1.36571062 epoch total loss 1.23909438\n",
      "Trained batch 309 batch loss 1.37998247 epoch total loss 1.23955035\n",
      "Trained batch 310 batch loss 1.35508847 epoch total loss 1.23992312\n",
      "Trained batch 311 batch loss 1.2986927 epoch total loss 1.24011207\n",
      "Trained batch 312 batch loss 1.25279689 epoch total loss 1.24015284\n",
      "Trained batch 313 batch loss 1.17985725 epoch total loss 1.23996019\n",
      "Trained batch 314 batch loss 1.24971509 epoch total loss 1.23999131\n",
      "Trained batch 315 batch loss 1.25946188 epoch total loss 1.24005306\n",
      "Trained batch 316 batch loss 1.45514953 epoch total loss 1.24073374\n",
      "Trained batch 317 batch loss 1.51885712 epoch total loss 1.24161112\n",
      "Trained batch 318 batch loss 1.36334896 epoch total loss 1.2419939\n",
      "Trained batch 319 batch loss 1.36154425 epoch total loss 1.2423687\n",
      "Trained batch 320 batch loss 1.4036262 epoch total loss 1.2428726\n",
      "Trained batch 321 batch loss 1.28648376 epoch total loss 1.24300849\n",
      "Trained batch 322 batch loss 1.31260431 epoch total loss 1.24322462\n",
      "Trained batch 323 batch loss 1.33332598 epoch total loss 1.24350357\n",
      "Trained batch 324 batch loss 1.2181145 epoch total loss 1.24342513\n",
      "Trained batch 325 batch loss 1.14755106 epoch total loss 1.24313021\n",
      "Trained batch 326 batch loss 1.243258 epoch total loss 1.24313056\n",
      "Trained batch 327 batch loss 1.23474729 epoch total loss 1.24310493\n",
      "Trained batch 328 batch loss 1.25315309 epoch total loss 1.24313545\n",
      "Trained batch 329 batch loss 1.25448227 epoch total loss 1.24317\n",
      "Trained batch 330 batch loss 1.42099535 epoch total loss 1.24370885\n",
      "Trained batch 331 batch loss 1.34272873 epoch total loss 1.24400806\n",
      "Trained batch 332 batch loss 1.21949828 epoch total loss 1.24393427\n",
      "Trained batch 333 batch loss 1.4203732 epoch total loss 1.24446416\n",
      "Trained batch 334 batch loss 1.2667706 epoch total loss 1.24453092\n",
      "Trained batch 335 batch loss 1.33374739 epoch total loss 1.24479723\n",
      "Trained batch 336 batch loss 1.26732945 epoch total loss 1.24486434\n",
      "Trained batch 337 batch loss 1.23482597 epoch total loss 1.24483454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 338 batch loss 1.2032373 epoch total loss 1.24471152\n",
      "Trained batch 339 batch loss 1.22527575 epoch total loss 1.24465418\n",
      "Trained batch 340 batch loss 1.26846576 epoch total loss 1.24472427\n",
      "Trained batch 341 batch loss 1.23742533 epoch total loss 1.24470282\n",
      "Trained batch 342 batch loss 1.25736916 epoch total loss 1.24473977\n",
      "Trained batch 343 batch loss 1.21369445 epoch total loss 1.24464929\n",
      "Trained batch 344 batch loss 1.22332704 epoch total loss 1.2445873\n",
      "Trained batch 345 batch loss 1.16572833 epoch total loss 1.24435878\n",
      "Trained batch 346 batch loss 1.15562034 epoch total loss 1.24410224\n",
      "Trained batch 347 batch loss 1.22371376 epoch total loss 1.24404359\n",
      "Trained batch 348 batch loss 1.08910048 epoch total loss 1.24359834\n",
      "Trained batch 349 batch loss 1.16319871 epoch total loss 1.24336803\n",
      "Trained batch 350 batch loss 1.08108962 epoch total loss 1.24290431\n",
      "Trained batch 351 batch loss 1.25660634 epoch total loss 1.24294329\n",
      "Trained batch 352 batch loss 1.31114554 epoch total loss 1.24313712\n",
      "Trained batch 353 batch loss 1.26694083 epoch total loss 1.24320447\n",
      "Trained batch 354 batch loss 1.16931331 epoch total loss 1.24299574\n",
      "Trained batch 355 batch loss 1.20552063 epoch total loss 1.24289012\n",
      "Trained batch 356 batch loss 1.33535743 epoch total loss 1.24314988\n",
      "Trained batch 357 batch loss 1.26721811 epoch total loss 1.24321735\n",
      "Trained batch 358 batch loss 1.29341233 epoch total loss 1.24335754\n",
      "Trained batch 359 batch loss 1.27529049 epoch total loss 1.24344659\n",
      "Trained batch 360 batch loss 1.139166 epoch total loss 1.24315691\n",
      "Trained batch 361 batch loss 1.18669391 epoch total loss 1.24300051\n",
      "Trained batch 362 batch loss 1.21108603 epoch total loss 1.24291229\n",
      "Trained batch 363 batch loss 1.24825263 epoch total loss 1.24292707\n",
      "Trained batch 364 batch loss 1.231583 epoch total loss 1.24289596\n",
      "Trained batch 365 batch loss 1.36323202 epoch total loss 1.24322557\n",
      "Trained batch 366 batch loss 1.30053 epoch total loss 1.24338222\n",
      "Trained batch 367 batch loss 1.18814075 epoch total loss 1.24323165\n",
      "Trained batch 368 batch loss 1.23959649 epoch total loss 1.24322176\n",
      "Trained batch 369 batch loss 1.07723963 epoch total loss 1.24277198\n",
      "Trained batch 370 batch loss 0.946603894 epoch total loss 1.24197149\n",
      "Trained batch 371 batch loss 0.992616296 epoch total loss 1.24129939\n",
      "Trained batch 372 batch loss 1.2530148 epoch total loss 1.24133086\n",
      "Trained batch 373 batch loss 1.37367857 epoch total loss 1.24168575\n",
      "Trained batch 374 batch loss 1.60321808 epoch total loss 1.24265242\n",
      "Trained batch 375 batch loss 1.27212226 epoch total loss 1.24273098\n",
      "Trained batch 376 batch loss 1.21715 epoch total loss 1.24266291\n",
      "Trained batch 377 batch loss 1.19652724 epoch total loss 1.2425406\n",
      "Trained batch 378 batch loss 1.22622669 epoch total loss 1.24249744\n",
      "Trained batch 379 batch loss 1.31273472 epoch total loss 1.24268281\n",
      "Trained batch 380 batch loss 1.2827574 epoch total loss 1.2427882\n",
      "Trained batch 381 batch loss 1.27968419 epoch total loss 1.24288511\n",
      "Trained batch 382 batch loss 1.28056049 epoch total loss 1.2429837\n",
      "Trained batch 383 batch loss 1.25072491 epoch total loss 1.24300396\n",
      "Trained batch 384 batch loss 1.23742735 epoch total loss 1.24298942\n",
      "Trained batch 385 batch loss 1.22672725 epoch total loss 1.2429471\n",
      "Trained batch 386 batch loss 1.13917041 epoch total loss 1.24267828\n",
      "Trained batch 387 batch loss 1.24068511 epoch total loss 1.24267316\n",
      "Trained batch 388 batch loss 1.23864484 epoch total loss 1.24266267\n",
      "Trained batch 389 batch loss 1.16098452 epoch total loss 1.24245274\n",
      "Trained batch 390 batch loss 1.01865637 epoch total loss 1.24187887\n",
      "Trained batch 391 batch loss 1.16050124 epoch total loss 1.24167073\n",
      "Trained batch 392 batch loss 1.11463857 epoch total loss 1.2413466\n",
      "Trained batch 393 batch loss 1.33092225 epoch total loss 1.24157465\n",
      "Trained batch 394 batch loss 1.27433574 epoch total loss 1.24165773\n",
      "Trained batch 395 batch loss 1.25557566 epoch total loss 1.24169302\n",
      "Trained batch 396 batch loss 1.04844356 epoch total loss 1.24120498\n",
      "Trained batch 397 batch loss 1.3937062 epoch total loss 1.24158907\n",
      "Trained batch 398 batch loss 1.28489602 epoch total loss 1.24169779\n",
      "Trained batch 399 batch loss 1.26183367 epoch total loss 1.24174833\n",
      "Trained batch 400 batch loss 1.11901975 epoch total loss 1.24144149\n",
      "Trained batch 401 batch loss 1.03341138 epoch total loss 1.24092269\n",
      "Trained batch 402 batch loss 1.06871009 epoch total loss 1.24049425\n",
      "Trained batch 403 batch loss 1.02383339 epoch total loss 1.23995674\n",
      "Trained batch 404 batch loss 1.04779589 epoch total loss 1.23948097\n",
      "Trained batch 405 batch loss 1.08019674 epoch total loss 1.2390877\n",
      "Trained batch 406 batch loss 1.09696436 epoch total loss 1.2387377\n",
      "Trained batch 407 batch loss 1.15063727 epoch total loss 1.23852122\n",
      "Trained batch 408 batch loss 1.17667603 epoch total loss 1.23836958\n",
      "Trained batch 409 batch loss 1.21817017 epoch total loss 1.23832023\n",
      "Trained batch 410 batch loss 1.34483671 epoch total loss 1.23858\n",
      "Trained batch 411 batch loss 1.27469385 epoch total loss 1.23866785\n",
      "Trained batch 412 batch loss 1.24418664 epoch total loss 1.23868132\n",
      "Trained batch 413 batch loss 1.08835423 epoch total loss 1.23831725\n",
      "Trained batch 414 batch loss 1.13592076 epoch total loss 1.23807\n",
      "Trained batch 415 batch loss 1.22478223 epoch total loss 1.23803794\n",
      "Trained batch 416 batch loss 1.12320185 epoch total loss 1.23776197\n",
      "Trained batch 417 batch loss 1.16305065 epoch total loss 1.2375828\n",
      "Trained batch 418 batch loss 1.1981883 epoch total loss 1.23748851\n",
      "Trained batch 419 batch loss 1.39657724 epoch total loss 1.23786831\n",
      "Trained batch 420 batch loss 1.25641 epoch total loss 1.23791242\n",
      "Trained batch 421 batch loss 1.27839196 epoch total loss 1.2380085\n",
      "Trained batch 422 batch loss 1.26194394 epoch total loss 1.23806536\n",
      "Trained batch 423 batch loss 1.1585263 epoch total loss 1.23787725\n",
      "Trained batch 424 batch loss 1.16884649 epoch total loss 1.23771441\n",
      "Trained batch 425 batch loss 1.11952066 epoch total loss 1.23743629\n",
      "Trained batch 426 batch loss 1.18147409 epoch total loss 1.23730481\n",
      "Trained batch 427 batch loss 1.33551645 epoch total loss 1.23753476\n",
      "Trained batch 428 batch loss 1.22903895 epoch total loss 1.23751497\n",
      "Trained batch 429 batch loss 1.22197104 epoch total loss 1.23747885\n",
      "Trained batch 430 batch loss 1.25530708 epoch total loss 1.23752034\n",
      "Trained batch 431 batch loss 1.18855035 epoch total loss 1.23740661\n",
      "Trained batch 432 batch loss 1.24734139 epoch total loss 1.23742962\n",
      "Trained batch 433 batch loss 1.09018588 epoch total loss 1.23708963\n",
      "Trained batch 434 batch loss 1.17934012 epoch total loss 1.23695648\n",
      "Trained batch 435 batch loss 1.13165915 epoch total loss 1.23671436\n",
      "Trained batch 436 batch loss 1.09327614 epoch total loss 1.23638535\n",
      "Trained batch 437 batch loss 1.22516596 epoch total loss 1.23635972\n",
      "Trained batch 438 batch loss 1.29762113 epoch total loss 1.23649955\n",
      "Trained batch 439 batch loss 1.25914073 epoch total loss 1.23655117\n",
      "Trained batch 440 batch loss 1.33086443 epoch total loss 1.2367655\n",
      "Trained batch 441 batch loss 1.23582125 epoch total loss 1.23676336\n",
      "Trained batch 442 batch loss 1.04998112 epoch total loss 1.23634088\n",
      "Trained batch 443 batch loss 1.06782579 epoch total loss 1.23596036\n",
      "Trained batch 444 batch loss 1.17168248 epoch total loss 1.23581564\n",
      "Trained batch 445 batch loss 1.19818091 epoch total loss 1.23573112\n",
      "Trained batch 446 batch loss 1.331568 epoch total loss 1.23594594\n",
      "Trained batch 447 batch loss 1.30501926 epoch total loss 1.23610032\n",
      "Trained batch 448 batch loss 1.18823075 epoch total loss 1.2359935\n",
      "Trained batch 449 batch loss 1.21958339 epoch total loss 1.23595703\n",
      "Trained batch 450 batch loss 1.21376431 epoch total loss 1.23590767\n",
      "Trained batch 451 batch loss 1.24777842 epoch total loss 1.23593402\n",
      "Trained batch 452 batch loss 1.21334493 epoch total loss 1.23588395\n",
      "Trained batch 453 batch loss 1.18427217 epoch total loss 1.23577011\n",
      "Trained batch 454 batch loss 1.23125553 epoch total loss 1.23576009\n",
      "Trained batch 455 batch loss 1.33679593 epoch total loss 1.23598218\n",
      "Trained batch 456 batch loss 1.34332836 epoch total loss 1.23621762\n",
      "Trained batch 457 batch loss 1.43966317 epoch total loss 1.23666263\n",
      "Trained batch 458 batch loss 1.36922944 epoch total loss 1.23695207\n",
      "Trained batch 459 batch loss 1.28164244 epoch total loss 1.23704934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 460 batch loss 1.17218804 epoch total loss 1.23690832\n",
      "Trained batch 461 batch loss 1.23519599 epoch total loss 1.23690462\n",
      "Trained batch 462 batch loss 1.17679358 epoch total loss 1.23677456\n",
      "Trained batch 463 batch loss 1.16291225 epoch total loss 1.23661494\n",
      "Trained batch 464 batch loss 1.26648152 epoch total loss 1.23667932\n",
      "Trained batch 465 batch loss 1.38808203 epoch total loss 1.23700488\n",
      "Trained batch 466 batch loss 1.22815847 epoch total loss 1.23698592\n",
      "Trained batch 467 batch loss 1.2509861 epoch total loss 1.23701584\n",
      "Trained batch 468 batch loss 1.24175549 epoch total loss 1.23702598\n",
      "Trained batch 469 batch loss 1.23361218 epoch total loss 1.23701882\n",
      "Trained batch 470 batch loss 1.13589609 epoch total loss 1.23680365\n",
      "Trained batch 471 batch loss 1.20850372 epoch total loss 1.23674357\n",
      "Trained batch 472 batch loss 1.15312481 epoch total loss 1.23656642\n",
      "Trained batch 473 batch loss 1.14959037 epoch total loss 1.2363826\n",
      "Trained batch 474 batch loss 1.11877823 epoch total loss 1.23613441\n",
      "Trained batch 475 batch loss 1.08660245 epoch total loss 1.2358197\n",
      "Trained batch 476 batch loss 1.2085731 epoch total loss 1.23576236\n",
      "Trained batch 477 batch loss 1.26359129 epoch total loss 1.23582077\n",
      "Trained batch 478 batch loss 1.26867366 epoch total loss 1.23588955\n",
      "Trained batch 479 batch loss 1.18676674 epoch total loss 1.23578691\n",
      "Trained batch 480 batch loss 1.15568614 epoch total loss 1.23562014\n",
      "Trained batch 481 batch loss 1.1416682 epoch total loss 1.23542476\n",
      "Trained batch 482 batch loss 1.30267048 epoch total loss 1.23556435\n",
      "Trained batch 483 batch loss 1.34127617 epoch total loss 1.2357831\n",
      "Trained batch 484 batch loss 1.41595113 epoch total loss 1.23615539\n",
      "Trained batch 485 batch loss 1.22000074 epoch total loss 1.23612201\n",
      "Trained batch 486 batch loss 1.394907 epoch total loss 1.23644865\n",
      "Trained batch 487 batch loss 1.28010988 epoch total loss 1.23653829\n",
      "Trained batch 488 batch loss 1.35744762 epoch total loss 1.23678601\n",
      "Trained batch 489 batch loss 1.28010416 epoch total loss 1.23687458\n",
      "Trained batch 490 batch loss 1.22058713 epoch total loss 1.23684132\n",
      "Trained batch 491 batch loss 1.31092799 epoch total loss 1.23699212\n",
      "Trained batch 492 batch loss 1.15835631 epoch total loss 1.23683238\n",
      "Trained batch 493 batch loss 1.21496093 epoch total loss 1.23678803\n",
      "Trained batch 494 batch loss 1.31242716 epoch total loss 1.23694122\n",
      "Trained batch 495 batch loss 1.21872461 epoch total loss 1.2369045\n",
      "Trained batch 496 batch loss 1.17250538 epoch total loss 1.23677456\n",
      "Trained batch 497 batch loss 1.22303355 epoch total loss 1.23674691\n",
      "Trained batch 498 batch loss 1.19411743 epoch total loss 1.2366612\n",
      "Trained batch 499 batch loss 1.25256634 epoch total loss 1.23669314\n",
      "Trained batch 500 batch loss 1.28758085 epoch total loss 1.23679495\n",
      "Trained batch 501 batch loss 1.22685564 epoch total loss 1.23677516\n",
      "Trained batch 502 batch loss 1.22312641 epoch total loss 1.23674798\n",
      "Trained batch 503 batch loss 1.26881218 epoch total loss 1.23681164\n",
      "Trained batch 504 batch loss 1.27648211 epoch total loss 1.23689044\n",
      "Trained batch 505 batch loss 1.26969266 epoch total loss 1.2369554\n",
      "Trained batch 506 batch loss 1.16933942 epoch total loss 1.23682177\n",
      "Trained batch 507 batch loss 1.24118423 epoch total loss 1.23683035\n",
      "Trained batch 508 batch loss 1.41679788 epoch total loss 1.23718464\n",
      "Trained batch 509 batch loss 1.42656457 epoch total loss 1.2375567\n",
      "Trained batch 510 batch loss 1.32411897 epoch total loss 1.23772645\n",
      "Trained batch 511 batch loss 1.25369096 epoch total loss 1.23775756\n",
      "Trained batch 512 batch loss 1.29121566 epoch total loss 1.23786199\n",
      "Trained batch 513 batch loss 1.16579807 epoch total loss 1.23772144\n",
      "Trained batch 514 batch loss 1.2110852 epoch total loss 1.23766959\n",
      "Trained batch 515 batch loss 1.19390321 epoch total loss 1.23758459\n",
      "Trained batch 516 batch loss 1.23887897 epoch total loss 1.23758709\n",
      "Trained batch 517 batch loss 1.2890234 epoch total loss 1.23768663\n",
      "Trained batch 518 batch loss 1.31738842 epoch total loss 1.23784041\n",
      "Trained batch 519 batch loss 1.15408134 epoch total loss 1.237679\n",
      "Trained batch 520 batch loss 1.28739262 epoch total loss 1.23777461\n",
      "Trained batch 521 batch loss 1.24752355 epoch total loss 1.23779333\n",
      "Trained batch 522 batch loss 1.34698534 epoch total loss 1.23800254\n",
      "Trained batch 523 batch loss 1.24508464 epoch total loss 1.23801601\n",
      "Trained batch 524 batch loss 1.19054449 epoch total loss 1.23792541\n",
      "Trained batch 525 batch loss 1.22029686 epoch total loss 1.23789179\n",
      "Trained batch 526 batch loss 1.13492441 epoch total loss 1.23769605\n",
      "Trained batch 527 batch loss 1.11174417 epoch total loss 1.23745716\n",
      "Trained batch 528 batch loss 1.09410548 epoch total loss 1.2371856\n",
      "Trained batch 529 batch loss 1.0861485 epoch total loss 1.23690009\n",
      "Trained batch 530 batch loss 1.1489563 epoch total loss 1.23673415\n",
      "Trained batch 531 batch loss 1.12079692 epoch total loss 1.23651588\n",
      "Trained batch 532 batch loss 1.07630515 epoch total loss 1.23621464\n",
      "Trained batch 533 batch loss 1.14110875 epoch total loss 1.23603618\n",
      "Trained batch 534 batch loss 1.17895007 epoch total loss 1.23592937\n",
      "Trained batch 535 batch loss 1.24237275 epoch total loss 1.23594141\n",
      "Trained batch 536 batch loss 1.34499717 epoch total loss 1.23614478\n",
      "Trained batch 537 batch loss 1.31382751 epoch total loss 1.2362895\n",
      "Trained batch 538 batch loss 1.19653904 epoch total loss 1.23621559\n",
      "Trained batch 539 batch loss 1.13771033 epoch total loss 1.23603284\n",
      "Trained batch 540 batch loss 1.11419678 epoch total loss 1.23580718\n",
      "Trained batch 541 batch loss 1.28471541 epoch total loss 1.23589766\n",
      "Trained batch 542 batch loss 1.37597573 epoch total loss 1.23615611\n",
      "Trained batch 543 batch loss 1.16697705 epoch total loss 1.23602867\n",
      "Trained batch 544 batch loss 1.14308977 epoch total loss 1.23585784\n",
      "Trained batch 545 batch loss 1.18580663 epoch total loss 1.23576593\n",
      "Trained batch 546 batch loss 1.25885415 epoch total loss 1.23580825\n",
      "Trained batch 547 batch loss 1.17821908 epoch total loss 1.23570299\n",
      "Trained batch 548 batch loss 1.22143054 epoch total loss 1.23567688\n",
      "Trained batch 549 batch loss 1.12588668 epoch total loss 1.23547697\n",
      "Trained batch 550 batch loss 1.21257496 epoch total loss 1.23543537\n",
      "Trained batch 551 batch loss 1.11244631 epoch total loss 1.23521209\n",
      "Trained batch 552 batch loss 1.24513876 epoch total loss 1.23523009\n",
      "Trained batch 553 batch loss 1.17545474 epoch total loss 1.23512197\n",
      "Trained batch 554 batch loss 1.11547744 epoch total loss 1.23490608\n",
      "Trained batch 555 batch loss 1.14265835 epoch total loss 1.23473978\n",
      "Trained batch 556 batch loss 1.21456921 epoch total loss 1.23470354\n",
      "Trained batch 557 batch loss 1.24788344 epoch total loss 1.23472714\n",
      "Trained batch 558 batch loss 1.30942309 epoch total loss 1.23486114\n",
      "Trained batch 559 batch loss 1.22778404 epoch total loss 1.23484838\n",
      "Trained batch 560 batch loss 1.17348742 epoch total loss 1.23473883\n",
      "Trained batch 561 batch loss 1.06461871 epoch total loss 1.23443556\n",
      "Trained batch 562 batch loss 1.17172456 epoch total loss 1.2343241\n",
      "Trained batch 563 batch loss 1.10805333 epoch total loss 1.23409975\n",
      "Trained batch 564 batch loss 1.17493844 epoch total loss 1.23399484\n",
      "Trained batch 565 batch loss 1.20042551 epoch total loss 1.23393548\n",
      "Trained batch 566 batch loss 1.1630404 epoch total loss 1.23381019\n",
      "Trained batch 567 batch loss 1.29436278 epoch total loss 1.233917\n",
      "Trained batch 568 batch loss 1.15332794 epoch total loss 1.23377502\n",
      "Trained batch 569 batch loss 1.1941359 epoch total loss 1.2337054\n",
      "Trained batch 570 batch loss 1.18701446 epoch total loss 1.2336235\n",
      "Trained batch 571 batch loss 1.22395825 epoch total loss 1.23360658\n",
      "Trained batch 572 batch loss 1.17234576 epoch total loss 1.23349953\n",
      "Trained batch 573 batch loss 1.07961965 epoch total loss 1.23323083\n",
      "Trained batch 574 batch loss 1.02340531 epoch total loss 1.23286533\n",
      "Trained batch 575 batch loss 1.09151411 epoch total loss 1.2326194\n",
      "Trained batch 576 batch loss 1.12933934 epoch total loss 1.23244011\n",
      "Trained batch 577 batch loss 1.10359311 epoch total loss 1.23221672\n",
      "Trained batch 578 batch loss 1.07341647 epoch total loss 1.23194206\n",
      "Trained batch 579 batch loss 1.1086396 epoch total loss 1.23172903\n",
      "Trained batch 580 batch loss 1.19414496 epoch total loss 1.2316643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 581 batch loss 1.27625048 epoch total loss 1.23174107\n",
      "Trained batch 582 batch loss 1.38151801 epoch total loss 1.23199844\n",
      "Trained batch 583 batch loss 1.24143493 epoch total loss 1.23201466\n",
      "Trained batch 584 batch loss 1.31220198 epoch total loss 1.23215187\n",
      "Trained batch 585 batch loss 1.26684213 epoch total loss 1.23221123\n",
      "Trained batch 586 batch loss 1.32685089 epoch total loss 1.23237276\n",
      "Trained batch 587 batch loss 1.24515653 epoch total loss 1.23239458\n",
      "Trained batch 588 batch loss 1.24865746 epoch total loss 1.23242223\n",
      "Trained batch 589 batch loss 1.24529791 epoch total loss 1.23244405\n",
      "Trained batch 590 batch loss 1.18098855 epoch total loss 1.23235679\n",
      "Trained batch 591 batch loss 1.16933894 epoch total loss 1.23225009\n",
      "Trained batch 592 batch loss 1.01926827 epoch total loss 1.23189044\n",
      "Trained batch 593 batch loss 1.2242378 epoch total loss 1.23187745\n",
      "Trained batch 594 batch loss 1.25695682 epoch total loss 1.23191977\n",
      "Trained batch 595 batch loss 1.30722487 epoch total loss 1.23204637\n",
      "Trained batch 596 batch loss 1.33596969 epoch total loss 1.23222077\n",
      "Trained batch 597 batch loss 1.35942209 epoch total loss 1.2324338\n",
      "Trained batch 598 batch loss 1.33510816 epoch total loss 1.23260546\n",
      "Trained batch 599 batch loss 1.24940753 epoch total loss 1.23263347\n",
      "Trained batch 600 batch loss 1.35158849 epoch total loss 1.23283172\n",
      "Trained batch 601 batch loss 1.21997333 epoch total loss 1.23281038\n",
      "Trained batch 602 batch loss 1.21271372 epoch total loss 1.23277688\n",
      "Trained batch 603 batch loss 1.36822653 epoch total loss 1.23300159\n",
      "Trained batch 604 batch loss 1.27351427 epoch total loss 1.23306859\n",
      "Trained batch 605 batch loss 1.29894185 epoch total loss 1.23317754\n",
      "Trained batch 606 batch loss 1.31333327 epoch total loss 1.23330986\n",
      "Trained batch 607 batch loss 1.30015349 epoch total loss 1.23342\n",
      "Trained batch 608 batch loss 1.15410316 epoch total loss 1.23328948\n",
      "Trained batch 609 batch loss 1.35230947 epoch total loss 1.23348498\n",
      "Trained batch 610 batch loss 1.21025503 epoch total loss 1.23344684\n",
      "Trained batch 611 batch loss 1.12512803 epoch total loss 1.23326957\n",
      "Trained batch 612 batch loss 1.24239099 epoch total loss 1.23328447\n",
      "Trained batch 613 batch loss 1.31926823 epoch total loss 1.23342478\n",
      "Trained batch 614 batch loss 1.39302647 epoch total loss 1.23368466\n",
      "Trained batch 615 batch loss 1.16425729 epoch total loss 1.23357177\n",
      "Trained batch 616 batch loss 1.24698162 epoch total loss 1.23359358\n",
      "Trained batch 617 batch loss 1.33101678 epoch total loss 1.23375142\n",
      "Trained batch 618 batch loss 1.20149159 epoch total loss 1.2336992\n",
      "Trained batch 619 batch loss 1.18828487 epoch total loss 1.23362577\n",
      "Trained batch 620 batch loss 1.24751341 epoch total loss 1.23364818\n",
      "Trained batch 621 batch loss 1.18608379 epoch total loss 1.23357165\n",
      "Trained batch 622 batch loss 1.3199023 epoch total loss 1.23371041\n",
      "Trained batch 623 batch loss 1.30396688 epoch total loss 1.23382318\n",
      "Trained batch 624 batch loss 1.31815267 epoch total loss 1.23395836\n",
      "Trained batch 625 batch loss 1.35242653 epoch total loss 1.23414791\n",
      "Trained batch 626 batch loss 1.28701735 epoch total loss 1.23423231\n",
      "Trained batch 627 batch loss 1.32411253 epoch total loss 1.2343756\n",
      "Trained batch 628 batch loss 1.23404717 epoch total loss 1.23437512\n",
      "Trained batch 629 batch loss 1.23509574 epoch total loss 1.23437631\n",
      "Trained batch 630 batch loss 1.30194342 epoch total loss 1.23448348\n",
      "Trained batch 631 batch loss 1.15751076 epoch total loss 1.23436153\n",
      "Trained batch 632 batch loss 1.17216456 epoch total loss 1.23426318\n",
      "Trained batch 633 batch loss 0.999156117 epoch total loss 1.23389173\n",
      "Trained batch 634 batch loss 1.08495057 epoch total loss 1.23365688\n",
      "Trained batch 635 batch loss 1.23657882 epoch total loss 1.23366141\n",
      "Trained batch 636 batch loss 1.20760179 epoch total loss 1.23362041\n",
      "Trained batch 637 batch loss 1.43986177 epoch total loss 1.23394418\n",
      "Trained batch 638 batch loss 1.24568987 epoch total loss 1.23396254\n",
      "Trained batch 639 batch loss 1.1773746 epoch total loss 1.23387396\n",
      "Trained batch 640 batch loss 1.11741972 epoch total loss 1.23369205\n",
      "Trained batch 641 batch loss 1.17462218 epoch total loss 1.2335999\n",
      "Trained batch 642 batch loss 1.36666632 epoch total loss 1.23380709\n",
      "Trained batch 643 batch loss 1.28640521 epoch total loss 1.23388886\n",
      "Trained batch 644 batch loss 1.27645671 epoch total loss 1.23395491\n",
      "Trained batch 645 batch loss 1.39225125 epoch total loss 1.23420036\n",
      "Trained batch 646 batch loss 1.35686755 epoch total loss 1.23439026\n",
      "Trained batch 647 batch loss 1.28731799 epoch total loss 1.23447204\n",
      "Trained batch 648 batch loss 1.2082603 epoch total loss 1.23443162\n",
      "Trained batch 649 batch loss 1.14666128 epoch total loss 1.23429632\n",
      "Trained batch 650 batch loss 1.22705317 epoch total loss 1.23428524\n",
      "Trained batch 651 batch loss 1.42189431 epoch total loss 1.23457336\n",
      "Trained batch 652 batch loss 1.22313881 epoch total loss 1.23455584\n",
      "Trained batch 653 batch loss 1.28711128 epoch total loss 1.23463631\n",
      "Trained batch 654 batch loss 1.26415431 epoch total loss 1.23468149\n",
      "Trained batch 655 batch loss 1.10405684 epoch total loss 1.23448205\n",
      "Trained batch 656 batch loss 1.21692979 epoch total loss 1.23445535\n",
      "Trained batch 657 batch loss 1.28286266 epoch total loss 1.2345289\n",
      "Trained batch 658 batch loss 1.17960715 epoch total loss 1.23444545\n",
      "Trained batch 659 batch loss 1.03117466 epoch total loss 1.23413706\n",
      "Trained batch 660 batch loss 1.088081 epoch total loss 1.23391581\n",
      "Trained batch 661 batch loss 1.06425834 epoch total loss 1.23365915\n",
      "Trained batch 662 batch loss 1.17224145 epoch total loss 1.23356628\n",
      "Trained batch 663 batch loss 1.11696792 epoch total loss 1.23339045\n",
      "Trained batch 664 batch loss 1.08923399 epoch total loss 1.23317337\n",
      "Trained batch 665 batch loss 1.21257055 epoch total loss 1.23314238\n",
      "Trained batch 666 batch loss 1.23019016 epoch total loss 1.23313785\n",
      "Trained batch 667 batch loss 1.18459272 epoch total loss 1.23306513\n",
      "Trained batch 668 batch loss 1.03957391 epoch total loss 1.23277533\n",
      "Trained batch 669 batch loss 1.21698427 epoch total loss 1.23275173\n",
      "Trained batch 670 batch loss 1.36439693 epoch total loss 1.23294818\n",
      "Trained batch 671 batch loss 1.23884201 epoch total loss 1.23295701\n",
      "Trained batch 672 batch loss 1.17923629 epoch total loss 1.23287714\n",
      "Trained batch 673 batch loss 1.25702286 epoch total loss 1.23291302\n",
      "Trained batch 674 batch loss 1.21054292 epoch total loss 1.23287988\n",
      "Trained batch 675 batch loss 1.33345985 epoch total loss 1.23302877\n",
      "Trained batch 676 batch loss 1.31436181 epoch total loss 1.23314917\n",
      "Trained batch 677 batch loss 1.21457791 epoch total loss 1.23312175\n",
      "Trained batch 678 batch loss 1.2141459 epoch total loss 1.23309386\n",
      "Trained batch 679 batch loss 1.40748703 epoch total loss 1.23335063\n",
      "Trained batch 680 batch loss 1.23669255 epoch total loss 1.23335552\n",
      "Trained batch 681 batch loss 1.27697289 epoch total loss 1.23341954\n",
      "Trained batch 682 batch loss 1.26310754 epoch total loss 1.23346317\n",
      "Trained batch 683 batch loss 1.22762656 epoch total loss 1.23345459\n",
      "Trained batch 684 batch loss 1.37498665 epoch total loss 1.23366153\n",
      "Trained batch 685 batch loss 1.37038815 epoch total loss 1.23386109\n",
      "Trained batch 686 batch loss 1.2528131 epoch total loss 1.23388863\n",
      "Trained batch 687 batch loss 1.22647798 epoch total loss 1.2338779\n",
      "Trained batch 688 batch loss 1.17699146 epoch total loss 1.23379529\n",
      "Trained batch 689 batch loss 1.06535 epoch total loss 1.23355079\n",
      "Trained batch 690 batch loss 1.12290311 epoch total loss 1.23339045\n",
      "Trained batch 691 batch loss 1.16616392 epoch total loss 1.23329318\n",
      "Trained batch 692 batch loss 1.21486676 epoch total loss 1.23326647\n",
      "Trained batch 693 batch loss 1.26454115 epoch total loss 1.23331165\n",
      "Trained batch 694 batch loss 1.36984563 epoch total loss 1.23350835\n",
      "Trained batch 695 batch loss 1.29798865 epoch total loss 1.23360109\n",
      "Trained batch 696 batch loss 1.27705252 epoch total loss 1.23366356\n",
      "Trained batch 697 batch loss 1.27219296 epoch total loss 1.23371887\n",
      "Trained batch 698 batch loss 1.07274556 epoch total loss 1.2334882\n",
      "Trained batch 699 batch loss 1.03683555 epoch total loss 1.23320699\n",
      "Trained batch 700 batch loss 1.08530927 epoch total loss 1.23299575\n",
      "Trained batch 701 batch loss 1.23457837 epoch total loss 1.23299789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 702 batch loss 1.20236564 epoch total loss 1.23295438\n",
      "Trained batch 703 batch loss 1.47114587 epoch total loss 1.23329318\n",
      "Trained batch 704 batch loss 1.43846762 epoch total loss 1.23358452\n",
      "Trained batch 705 batch loss 1.41947329 epoch total loss 1.23384833\n",
      "Trained batch 706 batch loss 1.34867144 epoch total loss 1.23401093\n",
      "Trained batch 707 batch loss 1.27013385 epoch total loss 1.23406208\n",
      "Trained batch 708 batch loss 1.11497235 epoch total loss 1.23389387\n",
      "Trained batch 709 batch loss 1.1439265 epoch total loss 1.23376691\n",
      "Trained batch 710 batch loss 1.2377094 epoch total loss 1.23377252\n",
      "Trained batch 711 batch loss 1.2054 epoch total loss 1.23373258\n",
      "Trained batch 712 batch loss 1.19484508 epoch total loss 1.23367798\n",
      "Trained batch 713 batch loss 1.29396176 epoch total loss 1.2337625\n",
      "Trained batch 714 batch loss 1.24273312 epoch total loss 1.23377502\n",
      "Trained batch 715 batch loss 1.20581508 epoch total loss 1.23373592\n",
      "Trained batch 716 batch loss 1.29163158 epoch total loss 1.23381686\n",
      "Trained batch 717 batch loss 1.28840661 epoch total loss 1.23389292\n",
      "Trained batch 718 batch loss 1.34917951 epoch total loss 1.23405349\n",
      "Trained batch 719 batch loss 1.29239595 epoch total loss 1.23413467\n",
      "Trained batch 720 batch loss 1.26466298 epoch total loss 1.23417711\n",
      "Trained batch 721 batch loss 1.33225679 epoch total loss 1.23431313\n",
      "Trained batch 722 batch loss 1.25259066 epoch total loss 1.2343384\n",
      "Trained batch 723 batch loss 1.13918376 epoch total loss 1.2342068\n",
      "Trained batch 724 batch loss 1.21272659 epoch total loss 1.23417711\n",
      "Trained batch 725 batch loss 1.3017633 epoch total loss 1.23427022\n",
      "Trained batch 726 batch loss 1.28359783 epoch total loss 1.23433816\n",
      "Trained batch 727 batch loss 1.32971811 epoch total loss 1.23446941\n",
      "Trained batch 728 batch loss 1.33180523 epoch total loss 1.23460305\n",
      "Trained batch 729 batch loss 1.18858612 epoch total loss 1.23454\n",
      "Trained batch 730 batch loss 1.25873053 epoch total loss 1.23457313\n",
      "Trained batch 731 batch loss 1.18684769 epoch total loss 1.2345078\n",
      "Trained batch 732 batch loss 1.37839508 epoch total loss 1.23470438\n",
      "Trained batch 733 batch loss 1.33029771 epoch total loss 1.23483479\n",
      "Trained batch 734 batch loss 1.31094587 epoch total loss 1.2349385\n",
      "Trained batch 735 batch loss 1.32755804 epoch total loss 1.23506463\n",
      "Trained batch 736 batch loss 1.11399055 epoch total loss 1.23490012\n",
      "Trained batch 737 batch loss 1.1075176 epoch total loss 1.23472726\n",
      "Trained batch 738 batch loss 1.09968412 epoch total loss 1.23454428\n",
      "Trained batch 739 batch loss 1.13951778 epoch total loss 1.23441577\n",
      "Trained batch 740 batch loss 1.16128492 epoch total loss 1.23431683\n",
      "Trained batch 741 batch loss 1.15635419 epoch total loss 1.23421168\n",
      "Trained batch 742 batch loss 1.21146131 epoch total loss 1.23418105\n",
      "Trained batch 743 batch loss 1.16156209 epoch total loss 1.23408329\n",
      "Trained batch 744 batch loss 1.29353046 epoch total loss 1.23416317\n",
      "Trained batch 745 batch loss 1.12007833 epoch total loss 1.23401\n",
      "Trained batch 746 batch loss 1.06963921 epoch total loss 1.23378968\n",
      "Trained batch 747 batch loss 1.04811227 epoch total loss 1.23354113\n",
      "Trained batch 748 batch loss 0.970980883 epoch total loss 1.23319006\n",
      "Trained batch 749 batch loss 1.28828073 epoch total loss 1.23326361\n",
      "Trained batch 750 batch loss 1.20154715 epoch total loss 1.23322141\n",
      "Trained batch 751 batch loss 1.22142303 epoch total loss 1.23320568\n",
      "Trained batch 752 batch loss 1.25479603 epoch total loss 1.23323441\n",
      "Trained batch 753 batch loss 1.20636249 epoch total loss 1.23319876\n",
      "Trained batch 754 batch loss 1.10419619 epoch total loss 1.23302758\n",
      "Trained batch 755 batch loss 1.1370523 epoch total loss 1.2329005\n",
      "Trained batch 756 batch loss 1.07971942 epoch total loss 1.23269784\n",
      "Trained batch 757 batch loss 1.19854224 epoch total loss 1.23265266\n",
      "Trained batch 758 batch loss 1.27757192 epoch total loss 1.23271203\n",
      "Trained batch 759 batch loss 1.11733937 epoch total loss 1.23255992\n",
      "Trained batch 760 batch loss 1.17786145 epoch total loss 1.23248792\n",
      "Trained batch 761 batch loss 1.1731112 epoch total loss 1.23241\n",
      "Trained batch 762 batch loss 1.23876345 epoch total loss 1.2324183\n",
      "Trained batch 763 batch loss 1.24333191 epoch total loss 1.2324326\n",
      "Trained batch 764 batch loss 1.17682409 epoch total loss 1.23235977\n",
      "Trained batch 765 batch loss 1.19930899 epoch total loss 1.23231661\n",
      "Trained batch 766 batch loss 1.19418383 epoch total loss 1.23226678\n",
      "Trained batch 767 batch loss 1.18762016 epoch total loss 1.23220861\n",
      "Trained batch 768 batch loss 1.18626845 epoch total loss 1.23214877\n",
      "Trained batch 769 batch loss 1.25116897 epoch total loss 1.23217356\n",
      "Trained batch 770 batch loss 1.11324739 epoch total loss 1.23201907\n",
      "Trained batch 771 batch loss 1.09936881 epoch total loss 1.23184705\n",
      "Trained batch 772 batch loss 1.18746305 epoch total loss 1.23178947\n",
      "Trained batch 773 batch loss 1.13613009 epoch total loss 1.23166573\n",
      "Trained batch 774 batch loss 1.3570888 epoch total loss 1.23182774\n",
      "Trained batch 775 batch loss 1.20043242 epoch total loss 1.23178732\n",
      "Trained batch 776 batch loss 1.21692908 epoch total loss 1.23176813\n",
      "Trained batch 777 batch loss 0.967888117 epoch total loss 1.2314285\n",
      "Trained batch 778 batch loss 1.00106478 epoch total loss 1.23113239\n",
      "Trained batch 779 batch loss 0.973522425 epoch total loss 1.2308017\n",
      "Trained batch 780 batch loss 1.12693822 epoch total loss 1.23066854\n",
      "Trained batch 781 batch loss 1.37660563 epoch total loss 1.23085535\n",
      "Trained batch 782 batch loss 1.30529642 epoch total loss 1.23095059\n",
      "Trained batch 783 batch loss 1.3459549 epoch total loss 1.23109746\n",
      "Trained batch 784 batch loss 1.32999218 epoch total loss 1.23122358\n",
      "Trained batch 785 batch loss 1.25329232 epoch total loss 1.23125172\n",
      "Trained batch 786 batch loss 1.41477895 epoch total loss 1.23148525\n",
      "Trained batch 787 batch loss 1.26480973 epoch total loss 1.23152757\n",
      "Trained batch 788 batch loss 1.1678884 epoch total loss 1.23144686\n",
      "Trained batch 789 batch loss 1.12983561 epoch total loss 1.23131812\n",
      "Trained batch 790 batch loss 1.17528284 epoch total loss 1.23124719\n",
      "Trained batch 791 batch loss 1.27684557 epoch total loss 1.23130476\n",
      "Trained batch 792 batch loss 1.33809876 epoch total loss 1.23143959\n",
      "Trained batch 793 batch loss 1.27418661 epoch total loss 1.23149347\n",
      "Trained batch 794 batch loss 1.36409307 epoch total loss 1.23166049\n",
      "Trained batch 795 batch loss 1.22900152 epoch total loss 1.23165715\n",
      "Trained batch 796 batch loss 1.38398111 epoch total loss 1.23184848\n",
      "Trained batch 797 batch loss 1.14459729 epoch total loss 1.23173904\n",
      "Trained batch 798 batch loss 1.07965875 epoch total loss 1.23154843\n",
      "Trained batch 799 batch loss 1.12522721 epoch total loss 1.23141539\n",
      "Trained batch 800 batch loss 1.29063988 epoch total loss 1.23148942\n",
      "Trained batch 801 batch loss 1.24176145 epoch total loss 1.23150218\n",
      "Trained batch 802 batch loss 1.11746585 epoch total loss 1.23136008\n",
      "Trained batch 803 batch loss 1.07142973 epoch total loss 1.23116088\n",
      "Trained batch 804 batch loss 0.962598145 epoch total loss 1.23082685\n",
      "Trained batch 805 batch loss 1.14418435 epoch total loss 1.23071921\n",
      "Trained batch 806 batch loss 1.15921152 epoch total loss 1.23063052\n",
      "Trained batch 807 batch loss 1.18071401 epoch total loss 1.23056865\n",
      "Trained batch 808 batch loss 1.12261248 epoch total loss 1.23043501\n",
      "Trained batch 809 batch loss 1.11181939 epoch total loss 1.23028839\n",
      "Trained batch 810 batch loss 1.0888207 epoch total loss 1.23011374\n",
      "Trained batch 811 batch loss 1.2766757 epoch total loss 1.2301712\n",
      "Trained batch 812 batch loss 1.14058733 epoch total loss 1.23006082\n",
      "Trained batch 813 batch loss 1.27558827 epoch total loss 1.23011684\n",
      "Trained batch 814 batch loss 1.12230706 epoch total loss 1.2299844\n",
      "Trained batch 815 batch loss 1.04373884 epoch total loss 1.22975588\n",
      "Trained batch 816 batch loss 1.09590578 epoch total loss 1.22959185\n",
      "Trained batch 817 batch loss 1.16289914 epoch total loss 1.22951019\n",
      "Trained batch 818 batch loss 1.2460134 epoch total loss 1.22953033\n",
      "Trained batch 819 batch loss 1.44706726 epoch total loss 1.22979605\n",
      "Trained batch 820 batch loss 1.39368832 epoch total loss 1.22999585\n",
      "Trained batch 821 batch loss 1.309237 epoch total loss 1.23009241\n",
      "Trained batch 822 batch loss 1.36272097 epoch total loss 1.23025382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 823 batch loss 1.12175488 epoch total loss 1.23012197\n",
      "Trained batch 824 batch loss 1.21539259 epoch total loss 1.23010409\n",
      "Trained batch 825 batch loss 1.24655986 epoch total loss 1.23012412\n",
      "Trained batch 826 batch loss 1.14931333 epoch total loss 1.23002625\n",
      "Trained batch 827 batch loss 1.21350312 epoch total loss 1.23000622\n",
      "Trained batch 828 batch loss 1.19067049 epoch total loss 1.22995877\n",
      "Trained batch 829 batch loss 1.11911321 epoch total loss 1.22982502\n",
      "Trained batch 830 batch loss 1.27108121 epoch total loss 1.22987473\n",
      "Trained batch 831 batch loss 1.17804992 epoch total loss 1.22981238\n",
      "Trained batch 832 batch loss 1.14615154 epoch total loss 1.22971177\n",
      "Trained batch 833 batch loss 1.16151714 epoch total loss 1.22963\n",
      "Trained batch 834 batch loss 1.16899574 epoch total loss 1.22955716\n",
      "Trained batch 835 batch loss 1.11071396 epoch total loss 1.22941482\n",
      "Trained batch 836 batch loss 1.10470057 epoch total loss 1.22926569\n",
      "Trained batch 837 batch loss 1.15024102 epoch total loss 1.22917128\n",
      "Trained batch 838 batch loss 1.17884207 epoch total loss 1.22911131\n",
      "Trained batch 839 batch loss 1.15620029 epoch total loss 1.22902441\n",
      "Trained batch 840 batch loss 1.20935154 epoch total loss 1.22900105\n",
      "Trained batch 841 batch loss 1.22550702 epoch total loss 1.22899675\n",
      "Trained batch 842 batch loss 1.32279432 epoch total loss 1.2291081\n",
      "Trained batch 843 batch loss 1.26611114 epoch total loss 1.22915208\n",
      "Trained batch 844 batch loss 1.31055498 epoch total loss 1.22924852\n",
      "Trained batch 845 batch loss 1.28126836 epoch total loss 1.22931\n",
      "Trained batch 846 batch loss 1.12655807 epoch total loss 1.22918856\n",
      "Trained batch 847 batch loss 0.970189869 epoch total loss 1.22888279\n",
      "Trained batch 848 batch loss 1.28569627 epoch total loss 1.22894979\n",
      "Trained batch 849 batch loss 1.26816845 epoch total loss 1.22899604\n",
      "Trained batch 850 batch loss 1.3048209 epoch total loss 1.22908521\n",
      "Trained batch 851 batch loss 1.36443865 epoch total loss 1.22924423\n",
      "Trained batch 852 batch loss 1.32611179 epoch total loss 1.22935796\n",
      "Trained batch 853 batch loss 1.18736529 epoch total loss 1.22930872\n",
      "Trained batch 854 batch loss 1.10814357 epoch total loss 1.22916687\n",
      "Trained batch 855 batch loss 1.0602262 epoch total loss 1.22896922\n",
      "Trained batch 856 batch loss 1.24260974 epoch total loss 1.22898507\n",
      "Trained batch 857 batch loss 1.20886922 epoch total loss 1.22896159\n",
      "Trained batch 858 batch loss 1.24387515 epoch total loss 1.22897899\n",
      "Trained batch 859 batch loss 1.34446943 epoch total loss 1.22911346\n",
      "Trained batch 860 batch loss 1.32176054 epoch total loss 1.22922122\n",
      "Trained batch 861 batch loss 1.2548281 epoch total loss 1.22925103\n",
      "Trained batch 862 batch loss 1.3946259 epoch total loss 1.22944295\n",
      "Trained batch 863 batch loss 1.32770777 epoch total loss 1.2295568\n",
      "Trained batch 864 batch loss 1.27411222 epoch total loss 1.22960842\n",
      "Trained batch 865 batch loss 1.35017788 epoch total loss 1.22974789\n",
      "Trained batch 866 batch loss 1.3940053 epoch total loss 1.22993767\n",
      "Trained batch 867 batch loss 1.22417831 epoch total loss 1.22993088\n",
      "Trained batch 868 batch loss 1.45472932 epoch total loss 1.23018992\n",
      "Trained batch 869 batch loss 1.46954823 epoch total loss 1.23046541\n",
      "Trained batch 870 batch loss 1.31429017 epoch total loss 1.23056173\n",
      "Trained batch 871 batch loss 1.21360993 epoch total loss 1.2305423\n",
      "Trained batch 872 batch loss 1.11752582 epoch total loss 1.23041272\n",
      "Trained batch 873 batch loss 1.17809534 epoch total loss 1.23035288\n",
      "Trained batch 874 batch loss 1.23769104 epoch total loss 1.23036122\n",
      "Trained batch 875 batch loss 1.25572419 epoch total loss 1.23039019\n",
      "Trained batch 876 batch loss 1.24183297 epoch total loss 1.2304033\n",
      "Trained batch 877 batch loss 1.25577593 epoch total loss 1.23043215\n",
      "Trained batch 878 batch loss 1.31512916 epoch total loss 1.23052871\n",
      "Trained batch 879 batch loss 1.41269755 epoch total loss 1.2307359\n",
      "Trained batch 880 batch loss 1.34008789 epoch total loss 1.23086023\n",
      "Trained batch 881 batch loss 1.34098589 epoch total loss 1.23098516\n",
      "Trained batch 882 batch loss 1.13887835 epoch total loss 1.23088074\n",
      "Trained batch 883 batch loss 1.19724476 epoch total loss 1.23084271\n",
      "Trained batch 884 batch loss 1.27771783 epoch total loss 1.23089576\n",
      "Trained batch 885 batch loss 1.19585085 epoch total loss 1.23085606\n",
      "Trained batch 886 batch loss 1.1325922 epoch total loss 1.23074508\n",
      "Trained batch 887 batch loss 1.27185464 epoch total loss 1.23079145\n",
      "Trained batch 888 batch loss 1.13974392 epoch total loss 1.23068893\n",
      "Trained batch 889 batch loss 1.29423666 epoch total loss 1.23076046\n",
      "Trained batch 890 batch loss 1.24136376 epoch total loss 1.23077226\n",
      "Trained batch 891 batch loss 1.29793179 epoch total loss 1.23084772\n",
      "Trained batch 892 batch loss 1.19710279 epoch total loss 1.23080993\n",
      "Trained batch 893 batch loss 1.28021789 epoch total loss 1.23086536\n",
      "Trained batch 894 batch loss 1.28105104 epoch total loss 1.23092139\n",
      "Trained batch 895 batch loss 1.23394036 epoch total loss 1.23092473\n",
      "Trained batch 896 batch loss 1.28462338 epoch total loss 1.23098469\n",
      "Trained batch 897 batch loss 1.31000173 epoch total loss 1.23107278\n",
      "Trained batch 898 batch loss 1.16745901 epoch total loss 1.23100197\n",
      "Trained batch 899 batch loss 1.18954933 epoch total loss 1.23095596\n",
      "Trained batch 900 batch loss 1.18204618 epoch total loss 1.2309016\n",
      "Trained batch 901 batch loss 1.22901309 epoch total loss 1.23089945\n",
      "Trained batch 902 batch loss 1.20351481 epoch total loss 1.23086905\n",
      "Trained batch 903 batch loss 1.169047 epoch total loss 1.23080063\n",
      "Trained batch 904 batch loss 1.07659 epoch total loss 1.23062992\n",
      "Trained batch 905 batch loss 1.12974048 epoch total loss 1.23051858\n",
      "Trained batch 906 batch loss 1.11969626 epoch total loss 1.23039627\n",
      "Trained batch 907 batch loss 1.15063369 epoch total loss 1.23030829\n",
      "Trained batch 908 batch loss 1.24278915 epoch total loss 1.23032212\n",
      "Trained batch 909 batch loss 1.209023 epoch total loss 1.23029864\n",
      "Trained batch 910 batch loss 1.25214219 epoch total loss 1.23032272\n",
      "Trained batch 911 batch loss 1.29074156 epoch total loss 1.230389\n",
      "Trained batch 912 batch loss 1.35025954 epoch total loss 1.23052037\n",
      "Trained batch 913 batch loss 1.36330211 epoch total loss 1.2306658\n",
      "Trained batch 914 batch loss 1.19170094 epoch total loss 1.23062313\n",
      "Trained batch 915 batch loss 1.33757532 epoch total loss 1.23074\n",
      "Trained batch 916 batch loss 1.2946738 epoch total loss 1.23080981\n",
      "Trained batch 917 batch loss 1.23855591 epoch total loss 1.23081815\n",
      "Trained batch 918 batch loss 1.1552794 epoch total loss 1.2307359\n",
      "Trained batch 919 batch loss 1.15848517 epoch total loss 1.23065722\n",
      "Trained batch 920 batch loss 1.07176375 epoch total loss 1.23048449\n",
      "Trained batch 921 batch loss 1.02263284 epoch total loss 1.23025882\n",
      "Trained batch 922 batch loss 1.08871949 epoch total loss 1.23010528\n",
      "Trained batch 923 batch loss 1.12092602 epoch total loss 1.22998703\n",
      "Trained batch 924 batch loss 0.954280257 epoch total loss 1.22968864\n",
      "Trained batch 925 batch loss 0.956116319 epoch total loss 1.22939301\n",
      "Trained batch 926 batch loss 0.956339836 epoch total loss 1.22909808\n",
      "Trained batch 927 batch loss 1.04200912 epoch total loss 1.22889614\n",
      "Trained batch 928 batch loss 1.07521939 epoch total loss 1.22873056\n",
      "Trained batch 929 batch loss 1.22284603 epoch total loss 1.22872424\n",
      "Trained batch 930 batch loss 1.13804376 epoch total loss 1.22862685\n",
      "Trained batch 931 batch loss 1.18589962 epoch total loss 1.22858095\n",
      "Trained batch 932 batch loss 1.20464909 epoch total loss 1.2285552\n",
      "Trained batch 933 batch loss 1.13503015 epoch total loss 1.22845495\n",
      "Trained batch 934 batch loss 1.1287173 epoch total loss 1.22834802\n",
      "Trained batch 935 batch loss 1.27597153 epoch total loss 1.22839904\n",
      "Trained batch 936 batch loss 1.13412261 epoch total loss 1.22829831\n",
      "Trained batch 937 batch loss 1.09397435 epoch total loss 1.22815502\n",
      "Trained batch 938 batch loss 1.18217421 epoch total loss 1.2281059\n",
      "Trained batch 939 batch loss 1.16612124 epoch total loss 1.22804\n",
      "Trained batch 940 batch loss 1.24872684 epoch total loss 1.22806203\n",
      "Trained batch 941 batch loss 1.13036025 epoch total loss 1.2279582\n",
      "Trained batch 942 batch loss 1.12965918 epoch total loss 1.22785378\n",
      "Trained batch 943 batch loss 1.32266629 epoch total loss 1.22795439\n",
      "Trained batch 944 batch loss 1.14198112 epoch total loss 1.22786319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 945 batch loss 1.17790759 epoch total loss 1.22781038\n",
      "Trained batch 946 batch loss 1.27826881 epoch total loss 1.22786367\n",
      "Trained batch 947 batch loss 1.20840144 epoch total loss 1.22784317\n",
      "Trained batch 948 batch loss 1.18363762 epoch total loss 1.22779644\n",
      "Trained batch 949 batch loss 1.33985877 epoch total loss 1.22791457\n",
      "Trained batch 950 batch loss 1.31420779 epoch total loss 1.22800541\n",
      "Trained batch 951 batch loss 1.25532961 epoch total loss 1.22803414\n",
      "Trained batch 952 batch loss 1.23245287 epoch total loss 1.22803879\n",
      "Trained batch 953 batch loss 1.14663649 epoch total loss 1.22795331\n",
      "Trained batch 954 batch loss 1.18133879 epoch total loss 1.22790456\n",
      "Trained batch 955 batch loss 1.13891792 epoch total loss 1.22781134\n",
      "Trained batch 956 batch loss 1.32376242 epoch total loss 1.22791171\n",
      "Trained batch 957 batch loss 1.33880019 epoch total loss 1.22802746\n",
      "Trained batch 958 batch loss 1.19118011 epoch total loss 1.22798896\n",
      "Trained batch 959 batch loss 1.14359164 epoch total loss 1.22790098\n",
      "Trained batch 960 batch loss 1.09352398 epoch total loss 1.22776091\n",
      "Trained batch 961 batch loss 1.13403964 epoch total loss 1.2276634\n",
      "Trained batch 962 batch loss 1.15308487 epoch total loss 1.22758591\n",
      "Trained batch 963 batch loss 1.32244849 epoch total loss 1.22768438\n",
      "Trained batch 964 batch loss 1.28530478 epoch total loss 1.2277441\n",
      "Trained batch 965 batch loss 1.24420273 epoch total loss 1.22776115\n",
      "Trained batch 966 batch loss 1.21464312 epoch total loss 1.22774756\n",
      "Trained batch 967 batch loss 1.25002074 epoch total loss 1.22777057\n",
      "Trained batch 968 batch loss 1.12726283 epoch total loss 1.22766685\n",
      "Trained batch 969 batch loss 1.21130204 epoch total loss 1.22764993\n",
      "Trained batch 970 batch loss 1.30225849 epoch total loss 1.22772682\n",
      "Trained batch 971 batch loss 1.28709531 epoch total loss 1.22778797\n",
      "Trained batch 972 batch loss 1.29742694 epoch total loss 1.22785974\n",
      "Trained batch 973 batch loss 1.10336816 epoch total loss 1.22773182\n",
      "Trained batch 974 batch loss 1.17202222 epoch total loss 1.22767448\n",
      "Trained batch 975 batch loss 1.0799 epoch total loss 1.22752309\n",
      "Trained batch 976 batch loss 1.10605 epoch total loss 1.22739863\n",
      "Trained batch 977 batch loss 1.10907936 epoch total loss 1.22727752\n",
      "Trained batch 978 batch loss 1.15014315 epoch total loss 1.22719872\n",
      "Trained batch 979 batch loss 1.32477736 epoch total loss 1.22729838\n",
      "Trained batch 980 batch loss 1.31727386 epoch total loss 1.22739017\n",
      "Trained batch 981 batch loss 1.32481396 epoch total loss 1.22748959\n",
      "Trained batch 982 batch loss 1.25223231 epoch total loss 1.22751474\n",
      "Trained batch 983 batch loss 1.41154218 epoch total loss 1.2277019\n",
      "Trained batch 984 batch loss 1.21513188 epoch total loss 1.22768903\n",
      "Trained batch 985 batch loss 1.24279 epoch total loss 1.22770441\n",
      "Trained batch 986 batch loss 1.30720961 epoch total loss 1.22778511\n",
      "Trained batch 987 batch loss 1.331828 epoch total loss 1.22789049\n",
      "Trained batch 988 batch loss 1.20835865 epoch total loss 1.2278707\n",
      "Trained batch 989 batch loss 1.11581969 epoch total loss 1.22775745\n",
      "Trained batch 990 batch loss 1.0598309 epoch total loss 1.22758782\n",
      "Trained batch 991 batch loss 1.04540837 epoch total loss 1.22740388\n",
      "Trained batch 992 batch loss 1.1896472 epoch total loss 1.22736597\n",
      "Trained batch 993 batch loss 1.15717018 epoch total loss 1.22729528\n",
      "Trained batch 994 batch loss 1.27852678 epoch total loss 1.2273469\n",
      "Trained batch 995 batch loss 1.16057658 epoch total loss 1.22727966\n",
      "Trained batch 996 batch loss 1.38775396 epoch total loss 1.22744071\n",
      "Trained batch 997 batch loss 1.24368072 epoch total loss 1.22745705\n",
      "Trained batch 998 batch loss 1.21466815 epoch total loss 1.22744429\n",
      "Trained batch 999 batch loss 1.20279717 epoch total loss 1.22741961\n",
      "Trained batch 1000 batch loss 1.29376149 epoch total loss 1.2274859\n",
      "Trained batch 1001 batch loss 1.17778659 epoch total loss 1.22743618\n",
      "Trained batch 1002 batch loss 1.34443116 epoch total loss 1.22755301\n",
      "Trained batch 1003 batch loss 1.34720027 epoch total loss 1.22767222\n",
      "Trained batch 1004 batch loss 1.41349339 epoch total loss 1.22785723\n",
      "Trained batch 1005 batch loss 1.26195478 epoch total loss 1.22789121\n",
      "Trained batch 1006 batch loss 1.19448614 epoch total loss 1.22785795\n",
      "Trained batch 1007 batch loss 0.99806875 epoch total loss 1.22762978\n",
      "Trained batch 1008 batch loss 1.0024904 epoch total loss 1.22740638\n",
      "Trained batch 1009 batch loss 1.23592174 epoch total loss 1.22741485\n",
      "Trained batch 1010 batch loss 0.975421309 epoch total loss 1.22716534\n",
      "Trained batch 1011 batch loss 0.982746363 epoch total loss 1.22692358\n",
      "Trained batch 1012 batch loss 0.931463599 epoch total loss 1.22663176\n",
      "Trained batch 1013 batch loss 1.03266251 epoch total loss 1.22644031\n",
      "Trained batch 1014 batch loss 1.14445388 epoch total loss 1.22635937\n",
      "Trained batch 1015 batch loss 1.13026798 epoch total loss 1.22626472\n",
      "Trained batch 1016 batch loss 1.20991933 epoch total loss 1.22624862\n",
      "Trained batch 1017 batch loss 1.23899722 epoch total loss 1.22626126\n",
      "Trained batch 1018 batch loss 1.27140915 epoch total loss 1.22630548\n",
      "Trained batch 1019 batch loss 1.23782325 epoch total loss 1.22631681\n",
      "Trained batch 1020 batch loss 1.10726166 epoch total loss 1.2262001\n",
      "Trained batch 1021 batch loss 1.20629418 epoch total loss 1.22618067\n",
      "Trained batch 1022 batch loss 1.10752738 epoch total loss 1.22606456\n",
      "Trained batch 1023 batch loss 0.960176229 epoch total loss 1.22580469\n",
      "Trained batch 1024 batch loss 1.05476487 epoch total loss 1.22563767\n",
      "Trained batch 1025 batch loss 1.09887242 epoch total loss 1.22551405\n",
      "Trained batch 1026 batch loss 1.12963438 epoch total loss 1.22542059\n",
      "Trained batch 1027 batch loss 1.12081456 epoch total loss 1.22531879\n",
      "Trained batch 1028 batch loss 1.19400764 epoch total loss 1.22528827\n",
      "Trained batch 1029 batch loss 1.22859585 epoch total loss 1.22529149\n",
      "Trained batch 1030 batch loss 1.33235872 epoch total loss 1.22539544\n",
      "Trained batch 1031 batch loss 1.2500174 epoch total loss 1.2254194\n",
      "Trained batch 1032 batch loss 1.34436631 epoch total loss 1.22553456\n",
      "Trained batch 1033 batch loss 1.20234895 epoch total loss 1.22551215\n",
      "Trained batch 1034 batch loss 1.07774138 epoch total loss 1.22536933\n",
      "Trained batch 1035 batch loss 1.1852752 epoch total loss 1.22533059\n",
      "Trained batch 1036 batch loss 1.28174806 epoch total loss 1.22538507\n",
      "Trained batch 1037 batch loss 1.3583107 epoch total loss 1.22551322\n",
      "Trained batch 1038 batch loss 1.2468853 epoch total loss 1.22553372\n",
      "Trained batch 1039 batch loss 1.26351523 epoch total loss 1.22557032\n",
      "Trained batch 1040 batch loss 1.14013863 epoch total loss 1.22548819\n",
      "Trained batch 1041 batch loss 1.16697598 epoch total loss 1.22543192\n",
      "Trained batch 1042 batch loss 1.14039373 epoch total loss 1.22535038\n",
      "Trained batch 1043 batch loss 1.22497809 epoch total loss 1.22535\n",
      "Trained batch 1044 batch loss 1.27548432 epoch total loss 1.22539806\n",
      "Trained batch 1045 batch loss 1.27228427 epoch total loss 1.22544301\n",
      "Trained batch 1046 batch loss 1.25215042 epoch total loss 1.22546852\n",
      "Trained batch 1047 batch loss 1.26441336 epoch total loss 1.22550571\n",
      "Trained batch 1048 batch loss 1.20861673 epoch total loss 1.22548962\n",
      "Trained batch 1049 batch loss 1.28816748 epoch total loss 1.22554934\n",
      "Trained batch 1050 batch loss 1.29109812 epoch total loss 1.22561181\n",
      "Trained batch 1051 batch loss 1.29432464 epoch total loss 1.22567725\n",
      "Trained batch 1052 batch loss 1.3446244 epoch total loss 1.22579026\n",
      "Trained batch 1053 batch loss 1.23076975 epoch total loss 1.22579491\n",
      "Trained batch 1054 batch loss 1.28937793 epoch total loss 1.22585535\n",
      "Trained batch 1055 batch loss 1.30752158 epoch total loss 1.22593272\n",
      "Trained batch 1056 batch loss 1.16057563 epoch total loss 1.22587073\n",
      "Trained batch 1057 batch loss 1.24126911 epoch total loss 1.22588527\n",
      "Trained batch 1058 batch loss 1.27801967 epoch total loss 1.22593462\n",
      "Trained batch 1059 batch loss 1.28931439 epoch total loss 1.22599447\n",
      "Trained batch 1060 batch loss 1.24255157 epoch total loss 1.22601008\n",
      "Trained batch 1061 batch loss 1.1590656 epoch total loss 1.22594702\n",
      "Trained batch 1062 batch loss 1.07690346 epoch total loss 1.22580659\n",
      "Trained batch 1063 batch loss 1.1206 epoch total loss 1.22570765\n",
      "Trained batch 1064 batch loss 1.29907238 epoch total loss 1.22577667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1065 batch loss 1.16941845 epoch total loss 1.22572374\n",
      "Trained batch 1066 batch loss 1.23914385 epoch total loss 1.22573626\n",
      "Trained batch 1067 batch loss 1.29365611 epoch total loss 1.2258\n",
      "Trained batch 1068 batch loss 1.22052741 epoch total loss 1.22579515\n",
      "Trained batch 1069 batch loss 1.14197731 epoch total loss 1.22571671\n",
      "Trained batch 1070 batch loss 1.1544745 epoch total loss 1.22565007\n",
      "Trained batch 1071 batch loss 1.13258648 epoch total loss 1.22556317\n",
      "Trained batch 1072 batch loss 1.04257691 epoch total loss 1.22539246\n",
      "Trained batch 1073 batch loss 1.05924094 epoch total loss 1.22523761\n",
      "Trained batch 1074 batch loss 1.11043131 epoch total loss 1.22513068\n",
      "Trained batch 1075 batch loss 1.11572266 epoch total loss 1.22502899\n",
      "Trained batch 1076 batch loss 1.18235588 epoch total loss 1.2249893\n",
      "Trained batch 1077 batch loss 1.12108111 epoch total loss 1.22489285\n",
      "Trained batch 1078 batch loss 1.12623525 epoch total loss 1.2248013\n",
      "Trained batch 1079 batch loss 1.18338954 epoch total loss 1.22476292\n",
      "Trained batch 1080 batch loss 1.06715178 epoch total loss 1.224617\n",
      "Trained batch 1081 batch loss 1.30019057 epoch total loss 1.22468686\n",
      "Trained batch 1082 batch loss 1.25813246 epoch total loss 1.22471774\n",
      "Trained batch 1083 batch loss 1.24922419 epoch total loss 1.22474051\n",
      "Trained batch 1084 batch loss 1.22570384 epoch total loss 1.22474134\n",
      "Trained batch 1085 batch loss 1.20735455 epoch total loss 1.22472537\n",
      "Trained batch 1086 batch loss 1.10345745 epoch total loss 1.22461379\n",
      "Trained batch 1087 batch loss 1.21032619 epoch total loss 1.22460067\n",
      "Trained batch 1088 batch loss 1.24990404 epoch total loss 1.2246238\n",
      "Trained batch 1089 batch loss 1.1973058 epoch total loss 1.22459877\n",
      "Trained batch 1090 batch loss 1.28835392 epoch total loss 1.22465718\n",
      "Trained batch 1091 batch loss 1.09602475 epoch total loss 1.22453928\n",
      "Trained batch 1092 batch loss 1.25535083 epoch total loss 1.22456753\n",
      "Trained batch 1093 batch loss 1.30790794 epoch total loss 1.22464383\n",
      "Trained batch 1094 batch loss 1.27094018 epoch total loss 1.22468615\n",
      "Trained batch 1095 batch loss 1.28815055 epoch total loss 1.2247442\n",
      "Trained batch 1096 batch loss 1.13687921 epoch total loss 1.22466397\n",
      "Trained batch 1097 batch loss 1.24023318 epoch total loss 1.22467816\n",
      "Trained batch 1098 batch loss 1.21023309 epoch total loss 1.22466493\n",
      "Trained batch 1099 batch loss 1.24159217 epoch total loss 1.2246803\n",
      "Trained batch 1100 batch loss 1.31477404 epoch total loss 1.22476232\n",
      "Trained batch 1101 batch loss 1.24920523 epoch total loss 1.22478449\n",
      "Trained batch 1102 batch loss 1.37716365 epoch total loss 1.22492278\n",
      "Trained batch 1103 batch loss 1.30626559 epoch total loss 1.22499645\n",
      "Trained batch 1104 batch loss 1.33514869 epoch total loss 1.22509634\n",
      "Trained batch 1105 batch loss 1.19096875 epoch total loss 1.22506535\n",
      "Trained batch 1106 batch loss 1.17845941 epoch total loss 1.22502327\n",
      "Trained batch 1107 batch loss 1.20407987 epoch total loss 1.22500432\n",
      "Trained batch 1108 batch loss 1.24602747 epoch total loss 1.22502327\n",
      "Trained batch 1109 batch loss 1.21870172 epoch total loss 1.22501767\n",
      "Trained batch 1110 batch loss 1.20984983 epoch total loss 1.22500396\n",
      "Trained batch 1111 batch loss 1.30600405 epoch total loss 1.22507691\n",
      "Trained batch 1112 batch loss 1.23313963 epoch total loss 1.22508419\n",
      "Trained batch 1113 batch loss 1.22247946 epoch total loss 1.22508192\n",
      "Trained batch 1114 batch loss 1.10767937 epoch total loss 1.22497642\n",
      "Trained batch 1115 batch loss 1.26839304 epoch total loss 1.2250154\n",
      "Trained batch 1116 batch loss 1.11372328 epoch total loss 1.22491574\n",
      "Trained batch 1117 batch loss 1.15447104 epoch total loss 1.22485268\n",
      "Trained batch 1118 batch loss 1.14010036 epoch total loss 1.22477686\n",
      "Trained batch 1119 batch loss 1.29041672 epoch total loss 1.22483552\n",
      "Trained batch 1120 batch loss 1.4535023 epoch total loss 1.22503972\n",
      "Trained batch 1121 batch loss 1.44353151 epoch total loss 1.22523451\n",
      "Trained batch 1122 batch loss 1.34525156 epoch total loss 1.22534144\n",
      "Trained batch 1123 batch loss 1.34768295 epoch total loss 1.2254504\n",
      "Trained batch 1124 batch loss 1.03369606 epoch total loss 1.22527981\n",
      "Trained batch 1125 batch loss 1.05611062 epoch total loss 1.22512949\n",
      "Trained batch 1126 batch loss 0.976831853 epoch total loss 1.22490895\n",
      "Trained batch 1127 batch loss 0.989456952 epoch total loss 1.22470009\n",
      "Trained batch 1128 batch loss 1.11007762 epoch total loss 1.22459841\n",
      "Trained batch 1129 batch loss 1.00713193 epoch total loss 1.22440577\n",
      "Trained batch 1130 batch loss 1.08806753 epoch total loss 1.22428513\n",
      "Trained batch 1131 batch loss 1.05644906 epoch total loss 1.22413659\n",
      "Trained batch 1132 batch loss 1.23172772 epoch total loss 1.22414327\n",
      "Trained batch 1133 batch loss 1.24653256 epoch total loss 1.22416306\n",
      "Trained batch 1134 batch loss 1.27034616 epoch total loss 1.22420382\n",
      "Trained batch 1135 batch loss 1.22064948 epoch total loss 1.22420084\n",
      "Trained batch 1136 batch loss 1.33014047 epoch total loss 1.22429407\n",
      "Trained batch 1137 batch loss 1.33448851 epoch total loss 1.22439098\n",
      "Trained batch 1138 batch loss 1.2272104 epoch total loss 1.22439349\n",
      "Trained batch 1139 batch loss 1.18859303 epoch total loss 1.22436202\n",
      "Trained batch 1140 batch loss 1.17007291 epoch total loss 1.22431433\n",
      "Trained batch 1141 batch loss 1.30072629 epoch total loss 1.22438145\n",
      "Trained batch 1142 batch loss 1.33663416 epoch total loss 1.22447968\n",
      "Trained batch 1143 batch loss 1.32167852 epoch total loss 1.22456479\n",
      "Trained batch 1144 batch loss 1.1907872 epoch total loss 1.22453523\n",
      "Trained batch 1145 batch loss 1.31998718 epoch total loss 1.22461855\n",
      "Trained batch 1146 batch loss 1.38794315 epoch total loss 1.22476101\n",
      "Trained batch 1147 batch loss 1.3900249 epoch total loss 1.22490513\n",
      "Trained batch 1148 batch loss 1.22048116 epoch total loss 1.22490132\n",
      "Trained batch 1149 batch loss 1.19596279 epoch total loss 1.22487605\n",
      "Trained batch 1150 batch loss 1.22255588 epoch total loss 1.22487402\n",
      "Trained batch 1151 batch loss 1.13050163 epoch total loss 1.224792\n",
      "Trained batch 1152 batch loss 1.25669348 epoch total loss 1.22481966\n",
      "Trained batch 1153 batch loss 1.462201 epoch total loss 1.22502553\n",
      "Trained batch 1154 batch loss 1.21146095 epoch total loss 1.22501373\n",
      "Trained batch 1155 batch loss 1.15838754 epoch total loss 1.22495615\n",
      "Trained batch 1156 batch loss 1.16593707 epoch total loss 1.22490501\n",
      "Trained batch 1157 batch loss 1.12807083 epoch total loss 1.22482133\n",
      "Trained batch 1158 batch loss 1.04577899 epoch total loss 1.22466671\n",
      "Trained batch 1159 batch loss 1.17369866 epoch total loss 1.22462273\n",
      "Trained batch 1160 batch loss 1.26015449 epoch total loss 1.22465336\n",
      "Trained batch 1161 batch loss 1.13815856 epoch total loss 1.22457886\n",
      "Trained batch 1162 batch loss 1.12086725 epoch total loss 1.22448957\n",
      "Trained batch 1163 batch loss 1.1183567 epoch total loss 1.22439837\n",
      "Trained batch 1164 batch loss 1.25952649 epoch total loss 1.22442853\n",
      "Trained batch 1165 batch loss 1.33286822 epoch total loss 1.22452164\n",
      "Trained batch 1166 batch loss 1.20019078 epoch total loss 1.22450078\n",
      "Trained batch 1167 batch loss 1.17258692 epoch total loss 1.22445631\n",
      "Trained batch 1168 batch loss 1.06050074 epoch total loss 1.224316\n",
      "Trained batch 1169 batch loss 0.958836198 epoch total loss 1.22408891\n",
      "Trained batch 1170 batch loss 0.862879634 epoch total loss 1.22378027\n",
      "Trained batch 1171 batch loss 1.08783567 epoch total loss 1.22366416\n",
      "Trained batch 1172 batch loss 1.18620765 epoch total loss 1.22363222\n",
      "Trained batch 1173 batch loss 1.21286702 epoch total loss 1.22362304\n",
      "Trained batch 1174 batch loss 1.23282528 epoch total loss 1.22363079\n",
      "Trained batch 1175 batch loss 1.29701579 epoch total loss 1.22369325\n",
      "Trained batch 1176 batch loss 1.29421139 epoch total loss 1.22375321\n",
      "Trained batch 1177 batch loss 1.38571095 epoch total loss 1.2238909\n",
      "Trained batch 1178 batch loss 1.22282839 epoch total loss 1.22389\n",
      "Trained batch 1179 batch loss 1.30361354 epoch total loss 1.22395754\n",
      "Trained batch 1180 batch loss 1.271492 epoch total loss 1.22399783\n",
      "Trained batch 1181 batch loss 1.19984031 epoch total loss 1.22397733\n",
      "Trained batch 1182 batch loss 1.25849104 epoch total loss 1.22400653\n",
      "Trained batch 1183 batch loss 1.27656686 epoch total loss 1.224051\n",
      "Trained batch 1184 batch loss 1.31045246 epoch total loss 1.22412395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1185 batch loss 1.37664151 epoch total loss 1.2242527\n",
      "Trained batch 1186 batch loss 1.36578166 epoch total loss 1.22437191\n",
      "Trained batch 1187 batch loss 1.43768048 epoch total loss 1.22455156\n",
      "Trained batch 1188 batch loss 1.43854 epoch total loss 1.2247318\n",
      "Trained batch 1189 batch loss 1.34181643 epoch total loss 1.22483027\n",
      "Trained batch 1190 batch loss 1.41370022 epoch total loss 1.22498894\n",
      "Trained batch 1191 batch loss 1.34955907 epoch total loss 1.2250936\n",
      "Trained batch 1192 batch loss 1.31252384 epoch total loss 1.22516692\n",
      "Trained batch 1193 batch loss 1.35024452 epoch total loss 1.2252717\n",
      "Trained batch 1194 batch loss 1.12417865 epoch total loss 1.22518706\n",
      "Trained batch 1195 batch loss 1.29281151 epoch total loss 1.22524369\n",
      "Trained batch 1196 batch loss 1.32258987 epoch total loss 1.22532511\n",
      "Trained batch 1197 batch loss 1.20503926 epoch total loss 1.22530818\n",
      "Trained batch 1198 batch loss 1.25835562 epoch total loss 1.22533572\n",
      "Trained batch 1199 batch loss 1.24114645 epoch total loss 1.22534883\n",
      "Trained batch 1200 batch loss 1.25422347 epoch total loss 1.22537291\n",
      "Trained batch 1201 batch loss 1.22798932 epoch total loss 1.22537518\n",
      "Trained batch 1202 batch loss 1.15991735 epoch total loss 1.2253207\n",
      "Trained batch 1203 batch loss 1.09147501 epoch total loss 1.22520936\n",
      "Trained batch 1204 batch loss 1.08943141 epoch total loss 1.22509658\n",
      "Trained batch 1205 batch loss 1.17199922 epoch total loss 1.2250526\n",
      "Trained batch 1206 batch loss 1.1355803 epoch total loss 1.22497845\n",
      "Trained batch 1207 batch loss 1.13536668 epoch total loss 1.22490418\n",
      "Trained batch 1208 batch loss 1.18665707 epoch total loss 1.22487247\n",
      "Trained batch 1209 batch loss 1.27317619 epoch total loss 1.22491252\n",
      "Trained batch 1210 batch loss 1.36087465 epoch total loss 1.22502482\n",
      "Trained batch 1211 batch loss 1.22025609 epoch total loss 1.22502089\n",
      "Trained batch 1212 batch loss 1.30611217 epoch total loss 1.22508776\n",
      "Trained batch 1213 batch loss 1.49914861 epoch total loss 1.22531366\n",
      "Trained batch 1214 batch loss 1.46104586 epoch total loss 1.22550786\n",
      "Trained batch 1215 batch loss 1.31170404 epoch total loss 1.22557878\n",
      "Trained batch 1216 batch loss 1.22074175 epoch total loss 1.22557485\n",
      "Trained batch 1217 batch loss 1.25357485 epoch total loss 1.22559774\n",
      "Trained batch 1218 batch loss 1.15116811 epoch total loss 1.22553658\n",
      "Trained batch 1219 batch loss 1.30424643 epoch total loss 1.2256012\n",
      "Trained batch 1220 batch loss 1.22539532 epoch total loss 1.22560096\n",
      "Trained batch 1221 batch loss 1.10229743 epoch total loss 1.2255\n",
      "Trained batch 1222 batch loss 1.15689778 epoch total loss 1.22544384\n",
      "Trained batch 1223 batch loss 1.24695933 epoch total loss 1.22546136\n",
      "Trained batch 1224 batch loss 1.33606064 epoch total loss 1.22555172\n",
      "Trained batch 1225 batch loss 1.28311574 epoch total loss 1.22559869\n",
      "Trained batch 1226 batch loss 1.39215875 epoch total loss 1.22573459\n",
      "Trained batch 1227 batch loss 1.26618218 epoch total loss 1.22576761\n",
      "Trained batch 1228 batch loss 1.32941628 epoch total loss 1.22585201\n",
      "Trained batch 1229 batch loss 1.3131361 epoch total loss 1.22592306\n",
      "Trained batch 1230 batch loss 1.27726555 epoch total loss 1.22596478\n",
      "Trained batch 1231 batch loss 1.19462657 epoch total loss 1.22593927\n",
      "Trained batch 1232 batch loss 1.21612811 epoch total loss 1.22593129\n",
      "Trained batch 1233 batch loss 1.19564617 epoch total loss 1.22590685\n",
      "Trained batch 1234 batch loss 1.08575463 epoch total loss 1.22579324\n",
      "Trained batch 1235 batch loss 1.18370986 epoch total loss 1.22575915\n",
      "Trained batch 1236 batch loss 1.11869168 epoch total loss 1.2256726\n",
      "Trained batch 1237 batch loss 1.14981794 epoch total loss 1.22561121\n",
      "Trained batch 1238 batch loss 1.17802691 epoch total loss 1.22557271\n",
      "Trained batch 1239 batch loss 1.10503018 epoch total loss 1.22547543\n",
      "Trained batch 1240 batch loss 1.14381027 epoch total loss 1.22540951\n",
      "Trained batch 1241 batch loss 1.13749421 epoch total loss 1.2253387\n",
      "Trained batch 1242 batch loss 1.18931901 epoch total loss 1.22530961\n",
      "Trained batch 1243 batch loss 1.23691308 epoch total loss 1.22531903\n",
      "Trained batch 1244 batch loss 1.32009876 epoch total loss 1.2253952\n",
      "Trained batch 1245 batch loss 1.17837667 epoch total loss 1.22535741\n",
      "Trained batch 1246 batch loss 1.06582785 epoch total loss 1.22522938\n",
      "Trained batch 1247 batch loss 1.18788505 epoch total loss 1.22519934\n",
      "Trained batch 1248 batch loss 1.28485537 epoch total loss 1.22524726\n",
      "Trained batch 1249 batch loss 1.24018776 epoch total loss 1.22525918\n",
      "Trained batch 1250 batch loss 1.20441568 epoch total loss 1.22524261\n",
      "Trained batch 1251 batch loss 1.28139067 epoch total loss 1.22528744\n",
      "Trained batch 1252 batch loss 1.14866889 epoch total loss 1.22522628\n",
      "Trained batch 1253 batch loss 1.16340423 epoch total loss 1.22517693\n",
      "Trained batch 1254 batch loss 1.07815361 epoch total loss 1.22505975\n",
      "Trained batch 1255 batch loss 1.12942958 epoch total loss 1.22498345\n",
      "Trained batch 1256 batch loss 1.24549139 epoch total loss 1.22499979\n",
      "Trained batch 1257 batch loss 1.22211838 epoch total loss 1.22499752\n",
      "Trained batch 1258 batch loss 1.18643105 epoch total loss 1.22496688\n",
      "Trained batch 1259 batch loss 1.16140163 epoch total loss 1.22491634\n",
      "Trained batch 1260 batch loss 1.27353477 epoch total loss 1.22495496\n",
      "Trained batch 1261 batch loss 1.25188756 epoch total loss 1.2249763\n",
      "Trained batch 1262 batch loss 1.16067815 epoch total loss 1.22492528\n",
      "Trained batch 1263 batch loss 1.18555105 epoch total loss 1.22489417\n",
      "Trained batch 1264 batch loss 1.0994966 epoch total loss 1.22479486\n",
      "Trained batch 1265 batch loss 1.07810521 epoch total loss 1.22467899\n",
      "Trained batch 1266 batch loss 1.1761322 epoch total loss 1.22464061\n",
      "Trained batch 1267 batch loss 1.14380896 epoch total loss 1.22457683\n",
      "Trained batch 1268 batch loss 1.15301061 epoch total loss 1.22452033\n",
      "Trained batch 1269 batch loss 1.22141182 epoch total loss 1.22451794\n",
      "Trained batch 1270 batch loss 1.22947764 epoch total loss 1.22452176\n",
      "Trained batch 1271 batch loss 1.10161161 epoch total loss 1.22442508\n",
      "Trained batch 1272 batch loss 1.16567481 epoch total loss 1.22437882\n",
      "Trained batch 1273 batch loss 1.20104063 epoch total loss 1.22436059\n",
      "Trained batch 1274 batch loss 1.20015168 epoch total loss 1.22434151\n",
      "Trained batch 1275 batch loss 1.1016742 epoch total loss 1.22424531\n",
      "Trained batch 1276 batch loss 1.14214277 epoch total loss 1.22418094\n",
      "Trained batch 1277 batch loss 1.17239809 epoch total loss 1.22414041\n",
      "Trained batch 1278 batch loss 1.22451782 epoch total loss 1.22414064\n",
      "Trained batch 1279 batch loss 1.23306358 epoch total loss 1.22414768\n",
      "Trained batch 1280 batch loss 1.2474947 epoch total loss 1.2241658\n",
      "Trained batch 1281 batch loss 1.2555604 epoch total loss 1.22419035\n",
      "Trained batch 1282 batch loss 1.19932508 epoch total loss 1.22417104\n",
      "Trained batch 1283 batch loss 1.29552019 epoch total loss 1.22422659\n",
      "Trained batch 1284 batch loss 1.17759037 epoch total loss 1.22419035\n",
      "Trained batch 1285 batch loss 1.20434725 epoch total loss 1.22417486\n",
      "Trained batch 1286 batch loss 1.26867187 epoch total loss 1.22420943\n",
      "Trained batch 1287 batch loss 1.21275055 epoch total loss 1.22420061\n",
      "Trained batch 1288 batch loss 1.09037411 epoch total loss 1.22409666\n",
      "Trained batch 1289 batch loss 1.16400445 epoch total loss 1.22405\n",
      "Trained batch 1290 batch loss 1.21295476 epoch total loss 1.22404146\n",
      "Trained batch 1291 batch loss 1.2280457 epoch total loss 1.22404456\n",
      "Trained batch 1292 batch loss 1.30785453 epoch total loss 1.22410953\n",
      "Trained batch 1293 batch loss 1.21960139 epoch total loss 1.22410595\n",
      "Trained batch 1294 batch loss 1.22071457 epoch total loss 1.22410333\n",
      "Trained batch 1295 batch loss 1.15284538 epoch total loss 1.22404838\n",
      "Trained batch 1296 batch loss 1.23384511 epoch total loss 1.22405589\n",
      "Trained batch 1297 batch loss 1.11085987 epoch total loss 1.22396863\n",
      "Trained batch 1298 batch loss 1.19496381 epoch total loss 1.22394621\n",
      "Trained batch 1299 batch loss 1.20638347 epoch total loss 1.22393274\n",
      "Trained batch 1300 batch loss 1.31407881 epoch total loss 1.22400212\n",
      "Trained batch 1301 batch loss 1.16252697 epoch total loss 1.2239548\n",
      "Trained batch 1302 batch loss 1.24997115 epoch total loss 1.22397482\n",
      "Trained batch 1303 batch loss 1.26152897 epoch total loss 1.22400367\n",
      "Trained batch 1304 batch loss 1.33019567 epoch total loss 1.22408509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1305 batch loss 1.39847159 epoch total loss 1.22421861\n",
      "Trained batch 1306 batch loss 1.36776555 epoch total loss 1.22432864\n",
      "Trained batch 1307 batch loss 1.33755815 epoch total loss 1.22441518\n",
      "Trained batch 1308 batch loss 1.28600752 epoch total loss 1.22446227\n",
      "Trained batch 1309 batch loss 1.24741042 epoch total loss 1.22447979\n",
      "Trained batch 1310 batch loss 1.21270311 epoch total loss 1.22447085\n",
      "Trained batch 1311 batch loss 1.23604798 epoch total loss 1.22447968\n",
      "Trained batch 1312 batch loss 1.25468564 epoch total loss 1.22450268\n",
      "Trained batch 1313 batch loss 1.25386703 epoch total loss 1.22452509\n",
      "Trained batch 1314 batch loss 1.19774652 epoch total loss 1.22450471\n",
      "Trained batch 1315 batch loss 1.0813967 epoch total loss 1.22439587\n",
      "Trained batch 1316 batch loss 1.12538064 epoch total loss 1.22432065\n",
      "Trained batch 1317 batch loss 1.11320031 epoch total loss 1.22423625\n",
      "Trained batch 1318 batch loss 1.00161505 epoch total loss 1.22406733\n",
      "Trained batch 1319 batch loss 1.20702672 epoch total loss 1.22405434\n",
      "Trained batch 1320 batch loss 1.21989107 epoch total loss 1.22405124\n",
      "Trained batch 1321 batch loss 1.3685925 epoch total loss 1.22416067\n",
      "Trained batch 1322 batch loss 1.3559171 epoch total loss 1.22426033\n",
      "Trained batch 1323 batch loss 1.27108037 epoch total loss 1.22429574\n",
      "Trained batch 1324 batch loss 1.15080416 epoch total loss 1.22424018\n",
      "Trained batch 1325 batch loss 1.29933929 epoch total loss 1.22429693\n",
      "Trained batch 1326 batch loss 1.25381613 epoch total loss 1.2243191\n",
      "Trained batch 1327 batch loss 1.35216498 epoch total loss 1.22441542\n",
      "Trained batch 1328 batch loss 1.41175497 epoch total loss 1.22455657\n",
      "Trained batch 1329 batch loss 1.40639782 epoch total loss 1.2246933\n",
      "Trained batch 1330 batch loss 1.4048661 epoch total loss 1.22482884\n",
      "Trained batch 1331 batch loss 1.17732739 epoch total loss 1.2247932\n",
      "Trained batch 1332 batch loss 1.16243505 epoch total loss 1.22474635\n",
      "Trained batch 1333 batch loss 1.18424463 epoch total loss 1.22471595\n",
      "Trained batch 1334 batch loss 1.17293501 epoch total loss 1.22467721\n",
      "Trained batch 1335 batch loss 1.11799359 epoch total loss 1.22459733\n",
      "Trained batch 1336 batch loss 1.1423285 epoch total loss 1.2245357\n",
      "Trained batch 1337 batch loss 1.15562594 epoch total loss 1.22448421\n",
      "Trained batch 1338 batch loss 1.06930673 epoch total loss 1.22436821\n",
      "Trained batch 1339 batch loss 1.02559423 epoch total loss 1.2242198\n",
      "Trained batch 1340 batch loss 1.01586771 epoch total loss 1.22406435\n",
      "Trained batch 1341 batch loss 1.16647828 epoch total loss 1.22402143\n",
      "Trained batch 1342 batch loss 1.13055193 epoch total loss 1.2239517\n",
      "Trained batch 1343 batch loss 1.13691044 epoch total loss 1.22388697\n",
      "Trained batch 1344 batch loss 1.03763664 epoch total loss 1.22374833\n",
      "Trained batch 1345 batch loss 1.25644088 epoch total loss 1.22377264\n",
      "Trained batch 1346 batch loss 1.16247082 epoch total loss 1.22372711\n",
      "Trained batch 1347 batch loss 1.19551396 epoch total loss 1.22370625\n",
      "Trained batch 1348 batch loss 1.27899241 epoch total loss 1.22374725\n",
      "Trained batch 1349 batch loss 1.22729695 epoch total loss 1.22374988\n",
      "Trained batch 1350 batch loss 1.16644096 epoch total loss 1.22370744\n",
      "Trained batch 1351 batch loss 1.24158788 epoch total loss 1.22372067\n",
      "Trained batch 1352 batch loss 1.15668619 epoch total loss 1.22367108\n",
      "Trained batch 1353 batch loss 1.13373208 epoch total loss 1.22360468\n",
      "Trained batch 1354 batch loss 1.11952865 epoch total loss 1.22352779\n",
      "Trained batch 1355 batch loss 1.15058053 epoch total loss 1.22347403\n",
      "Trained batch 1356 batch loss 1.29285741 epoch total loss 1.22352517\n",
      "Trained batch 1357 batch loss 1.2903192 epoch total loss 1.2235744\n",
      "Trained batch 1358 batch loss 1.22227371 epoch total loss 1.22357345\n",
      "Trained batch 1359 batch loss 1.28727818 epoch total loss 1.2236203\n",
      "Trained batch 1360 batch loss 1.26140249 epoch total loss 1.22364795\n",
      "Trained batch 1361 batch loss 1.31529582 epoch total loss 1.22371531\n",
      "Trained batch 1362 batch loss 1.20579743 epoch total loss 1.22370219\n",
      "Trained batch 1363 batch loss 1.19214201 epoch total loss 1.22367907\n",
      "Trained batch 1364 batch loss 1.22246659 epoch total loss 1.22367811\n",
      "Trained batch 1365 batch loss 1.05991185 epoch total loss 1.22355819\n",
      "Trained batch 1366 batch loss 1.20787942 epoch total loss 1.22354662\n",
      "Trained batch 1367 batch loss 1.35425603 epoch total loss 1.22364223\n",
      "Trained batch 1368 batch loss 1.25912619 epoch total loss 1.22366822\n",
      "Trained batch 1369 batch loss 1.13677835 epoch total loss 1.22360468\n",
      "Trained batch 1370 batch loss 1.10050201 epoch total loss 1.2235148\n",
      "Trained batch 1371 batch loss 1.01388907 epoch total loss 1.22336197\n",
      "Trained batch 1372 batch loss 1.23762655 epoch total loss 1.22337234\n",
      "Trained batch 1373 batch loss 1.31131387 epoch total loss 1.22343647\n",
      "Trained batch 1374 batch loss 1.27692199 epoch total loss 1.22347534\n",
      "Trained batch 1375 batch loss 1.11778212 epoch total loss 1.22339857\n",
      "Trained batch 1376 batch loss 1.16155243 epoch total loss 1.22335351\n",
      "Trained batch 1377 batch loss 1.19717801 epoch total loss 1.22333455\n",
      "Trained batch 1378 batch loss 1.2357204 epoch total loss 1.22334349\n",
      "Trained batch 1379 batch loss 1.13523507 epoch total loss 1.2232796\n",
      "Trained batch 1380 batch loss 1.15364778 epoch total loss 1.22322917\n",
      "Trained batch 1381 batch loss 1.18844366 epoch total loss 1.22320402\n",
      "Trained batch 1382 batch loss 1.17265427 epoch total loss 1.22316742\n",
      "Trained batch 1383 batch loss 1.22929466 epoch total loss 1.22317183\n",
      "Trained batch 1384 batch loss 1.21954179 epoch total loss 1.22316909\n",
      "Trained batch 1385 batch loss 1.15607083 epoch total loss 1.22312069\n",
      "Trained batch 1386 batch loss 1.17996669 epoch total loss 1.22308958\n",
      "Trained batch 1387 batch loss 1.11278379 epoch total loss 1.22301006\n",
      "Trained batch 1388 batch loss 1.23254311 epoch total loss 1.22301698\n",
      "Epoch 4 train loss 1.2230169773101807\n",
      "Validated batch 1 batch loss 1.26199424\n",
      "Validated batch 2 batch loss 1.18518043\n",
      "Validated batch 3 batch loss 1.19631159\n",
      "Validated batch 4 batch loss 1.17570758\n",
      "Validated batch 5 batch loss 1.330217\n",
      "Validated batch 6 batch loss 1.33335066\n",
      "Validated batch 7 batch loss 1.16468585\n",
      "Validated batch 8 batch loss 1.23045862\n",
      "Validated batch 9 batch loss 1.24480939\n",
      "Validated batch 10 batch loss 1.22157538\n",
      "Validated batch 11 batch loss 1.27070427\n",
      "Validated batch 12 batch loss 1.10704374\n",
      "Validated batch 13 batch loss 1.36525714\n",
      "Validated batch 14 batch loss 1.16226387\n",
      "Validated batch 15 batch loss 1.2588166\n",
      "Validated batch 16 batch loss 1.26414239\n",
      "Validated batch 17 batch loss 1.31660628\n",
      "Validated batch 18 batch loss 1.08171976\n",
      "Validated batch 19 batch loss 1.26322138\n",
      "Validated batch 20 batch loss 1.12492108\n",
      "Validated batch 21 batch loss 1.25274229\n",
      "Validated batch 22 batch loss 1.19672108\n",
      "Validated batch 23 batch loss 1.23898482\n",
      "Validated batch 24 batch loss 1.22264636\n",
      "Validated batch 25 batch loss 1.11102927\n",
      "Validated batch 26 batch loss 1.21413469\n",
      "Validated batch 27 batch loss 1.11761427\n",
      "Validated batch 28 batch loss 1.19255161\n",
      "Validated batch 29 batch loss 1.22951519\n",
      "Validated batch 30 batch loss 1.21674335\n",
      "Validated batch 31 batch loss 1.335186\n",
      "Validated batch 32 batch loss 1.25830913\n",
      "Validated batch 33 batch loss 1.19980884\n",
      "Validated batch 34 batch loss 1.22686434\n",
      "Validated batch 35 batch loss 1.29309869\n",
      "Validated batch 36 batch loss 1.32936966\n",
      "Validated batch 37 batch loss 1.31007695\n",
      "Validated batch 38 batch loss 1.27528548\n",
      "Validated batch 39 batch loss 1.34601426\n",
      "Validated batch 40 batch loss 1.30729985\n",
      "Validated batch 41 batch loss 1.12325597\n",
      "Validated batch 42 batch loss 1.23237896\n",
      "Validated batch 43 batch loss 1.30553138\n",
      "Validated batch 44 batch loss 1.23563993\n",
      "Validated batch 45 batch loss 1.23291373\n",
      "Validated batch 46 batch loss 1.19467759\n",
      "Validated batch 47 batch loss 1.27104437\n",
      "Validated batch 48 batch loss 1.21846914\n",
      "Validated batch 49 batch loss 1.17018688\n",
      "Validated batch 50 batch loss 1.19075227\n",
      "Validated batch 51 batch loss 1.20518875\n",
      "Validated batch 52 batch loss 1.23351812\n",
      "Validated batch 53 batch loss 1.229491\n",
      "Validated batch 54 batch loss 1.17845023\n",
      "Validated batch 55 batch loss 1.23322272\n",
      "Validated batch 56 batch loss 1.26598406\n",
      "Validated batch 57 batch loss 1.12644386\n",
      "Validated batch 58 batch loss 1.086761\n",
      "Validated batch 59 batch loss 1.29031098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 60 batch loss 1.24097478\n",
      "Validated batch 61 batch loss 1.35178828\n",
      "Validated batch 62 batch loss 1.32204151\n",
      "Validated batch 63 batch loss 1.16877234\n",
      "Validated batch 64 batch loss 1.35045528\n",
      "Validated batch 65 batch loss 1.18167496\n",
      "Validated batch 66 batch loss 1.26170433\n",
      "Validated batch 67 batch loss 1.2913636\n",
      "Validated batch 68 batch loss 0.984722\n",
      "Validated batch 69 batch loss 1.1994586\n",
      "Validated batch 70 batch loss 1.34147108\n",
      "Validated batch 71 batch loss 1.20002723\n",
      "Validated batch 72 batch loss 1.16136146\n",
      "Validated batch 73 batch loss 1.19624209\n",
      "Validated batch 74 batch loss 1.13169491\n",
      "Validated batch 75 batch loss 1.21809983\n",
      "Validated batch 76 batch loss 1.2194519\n",
      "Validated batch 77 batch loss 1.15538061\n",
      "Validated batch 78 batch loss 1.16984677\n",
      "Validated batch 79 batch loss 1.20017183\n",
      "Validated batch 80 batch loss 1.2355305\n",
      "Validated batch 81 batch loss 1.26996219\n",
      "Validated batch 82 batch loss 1.22863221\n",
      "Validated batch 83 batch loss 1.13441801\n",
      "Validated batch 84 batch loss 1.14746475\n",
      "Validated batch 85 batch loss 1.2407515\n",
      "Validated batch 86 batch loss 1.16107559\n",
      "Validated batch 87 batch loss 1.23251557\n",
      "Validated batch 88 batch loss 1.23943472\n",
      "Validated batch 89 batch loss 1.45265174\n",
      "Validated batch 90 batch loss 1.30115414\n",
      "Validated batch 91 batch loss 1.22508502\n",
      "Validated batch 92 batch loss 1.12917042\n",
      "Validated batch 93 batch loss 1.06205034\n",
      "Validated batch 94 batch loss 1.19822466\n",
      "Validated batch 95 batch loss 1.28659034\n",
      "Validated batch 96 batch loss 1.14932299\n",
      "Validated batch 97 batch loss 1.30435991\n",
      "Validated batch 98 batch loss 1.38855302\n",
      "Validated batch 99 batch loss 1.0758\n",
      "Validated batch 100 batch loss 1.16791797\n",
      "Validated batch 101 batch loss 1.20418489\n",
      "Validated batch 102 batch loss 1.29182696\n",
      "Validated batch 103 batch loss 1.27706432\n",
      "Validated batch 104 batch loss 1.1169188\n",
      "Validated batch 105 batch loss 1.02737236\n",
      "Validated batch 106 batch loss 1.16510236\n",
      "Validated batch 107 batch loss 1.10251868\n",
      "Validated batch 108 batch loss 1.19937277\n",
      "Validated batch 109 batch loss 1.25541687\n",
      "Validated batch 110 batch loss 1.09715891\n",
      "Validated batch 111 batch loss 1.21426833\n",
      "Validated batch 112 batch loss 1.29972529\n",
      "Validated batch 113 batch loss 1.23422885\n",
      "Validated batch 114 batch loss 1.19967675\n",
      "Validated batch 115 batch loss 1.07395923\n",
      "Validated batch 116 batch loss 1.18785286\n",
      "Validated batch 117 batch loss 1.23659909\n",
      "Validated batch 118 batch loss 1.18756866\n",
      "Validated batch 119 batch loss 1.11394095\n",
      "Validated batch 120 batch loss 1.1293838\n",
      "Validated batch 121 batch loss 1.3054055\n",
      "Validated batch 122 batch loss 1.1065712\n",
      "Validated batch 123 batch loss 1.07137442\n",
      "Validated batch 124 batch loss 1.18561029\n",
      "Validated batch 125 batch loss 1.17704368\n",
      "Validated batch 126 batch loss 1.08964014\n",
      "Validated batch 127 batch loss 1.22074342\n",
      "Validated batch 128 batch loss 1.16504538\n",
      "Validated batch 129 batch loss 1.11984\n",
      "Validated batch 130 batch loss 1.24481642\n",
      "Validated batch 131 batch loss 1.336573\n",
      "Validated batch 132 batch loss 1.10949922\n",
      "Validated batch 133 batch loss 1.32832491\n",
      "Validated batch 134 batch loss 1.00386453\n",
      "Validated batch 135 batch loss 1.12664294\n",
      "Validated batch 136 batch loss 1.14267564\n",
      "Validated batch 137 batch loss 1.19182062\n",
      "Validated batch 138 batch loss 1.32988095\n",
      "Validated batch 139 batch loss 1.21050262\n",
      "Validated batch 140 batch loss 1.27225757\n",
      "Validated batch 141 batch loss 1.17123449\n",
      "Validated batch 142 batch loss 1.09896243\n",
      "Validated batch 143 batch loss 1.20314527\n",
      "Validated batch 144 batch loss 1.20316279\n",
      "Validated batch 145 batch loss 1.29430699\n",
      "Validated batch 146 batch loss 1.3329587\n",
      "Validated batch 147 batch loss 1.2684586\n",
      "Validated batch 148 batch loss 1.17735362\n",
      "Validated batch 149 batch loss 1.21610069\n",
      "Validated batch 150 batch loss 1.22664762\n",
      "Validated batch 151 batch loss 1.2175281\n",
      "Validated batch 152 batch loss 1.31431913\n",
      "Validated batch 153 batch loss 1.33213365\n",
      "Validated batch 154 batch loss 1.22684419\n",
      "Validated batch 155 batch loss 1.3330313\n",
      "Validated batch 156 batch loss 1.18174815\n",
      "Validated batch 157 batch loss 1.18243527\n",
      "Validated batch 158 batch loss 1.1811173\n",
      "Validated batch 159 batch loss 1.11228466\n",
      "Validated batch 160 batch loss 1.29810214\n",
      "Validated batch 161 batch loss 1.19585359\n",
      "Validated batch 162 batch loss 1.16971266\n",
      "Validated batch 163 batch loss 1.10670471\n",
      "Validated batch 164 batch loss 1.21411169\n",
      "Validated batch 165 batch loss 1.17834878\n",
      "Validated batch 166 batch loss 1.12911892\n",
      "Validated batch 167 batch loss 1.21125817\n",
      "Validated batch 168 batch loss 1.1550175\n",
      "Validated batch 169 batch loss 1.24095631\n",
      "Validated batch 170 batch loss 1.31917071\n",
      "Validated batch 171 batch loss 1.05304193\n",
      "Validated batch 172 batch loss 1.28275597\n",
      "Validated batch 173 batch loss 1.25449181\n",
      "Validated batch 174 batch loss 1.09699631\n",
      "Validated batch 175 batch loss 1.23200548\n",
      "Validated batch 176 batch loss 1.20963466\n",
      "Validated batch 177 batch loss 1.13732219\n",
      "Validated batch 178 batch loss 1.31010211\n",
      "Validated batch 179 batch loss 1.21892834\n",
      "Validated batch 180 batch loss 1.25263774\n",
      "Validated batch 181 batch loss 1.14011478\n",
      "Validated batch 182 batch loss 1.23281789\n",
      "Validated batch 183 batch loss 1.2074368\n",
      "Validated batch 184 batch loss 1.11717439\n",
      "Validated batch 185 batch loss 1.23962164\n",
      "Epoch 4 val loss 1.212238073348999\n",
      "Model .//model-epoch-4-loss-1.2122.h5 saved.\n",
      "Start epoch 5 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.2834568 epoch total loss 1.2834568\n",
      "Trained batch 2 batch loss 1.24033237 epoch total loss 1.26189458\n",
      "Trained batch 3 batch loss 1.2177937 epoch total loss 1.24719429\n",
      "Trained batch 4 batch loss 1.21880877 epoch total loss 1.24009788\n",
      "Trained batch 5 batch loss 1.14679253 epoch total loss 1.22143674\n",
      "Trained batch 6 batch loss 1.19818842 epoch total loss 1.21756208\n",
      "Trained batch 7 batch loss 1.29488015 epoch total loss 1.22860742\n",
      "Trained batch 8 batch loss 1.24131751 epoch total loss 1.23019624\n",
      "Trained batch 9 batch loss 1.28776526 epoch total loss 1.23659277\n",
      "Trained batch 10 batch loss 1.28615117 epoch total loss 1.24154866\n",
      "Trained batch 11 batch loss 1.38113356 epoch total loss 1.25423813\n",
      "Trained batch 12 batch loss 1.33042169 epoch total loss 1.26058674\n",
      "Trained batch 13 batch loss 1.12981343 epoch total loss 1.25052726\n",
      "Trained batch 14 batch loss 1.12992072 epoch total loss 1.2419126\n",
      "Trained batch 15 batch loss 1.18855774 epoch total loss 1.23835564\n",
      "Trained batch 16 batch loss 1.19299173 epoch total loss 1.23552036\n",
      "Trained batch 17 batch loss 1.22169471 epoch total loss 1.23470712\n",
      "Trained batch 18 batch loss 1.27688336 epoch total loss 1.23705029\n",
      "Trained batch 19 batch loss 1.21966445 epoch total loss 1.23613513\n",
      "Trained batch 20 batch loss 1.23434675 epoch total loss 1.23604572\n",
      "Trained batch 21 batch loss 1.19285107 epoch total loss 1.23398876\n",
      "Trained batch 22 batch loss 1.09359944 epoch total loss 1.22760749\n",
      "Trained batch 23 batch loss 1.07232726 epoch total loss 1.22085607\n",
      "Trained batch 24 batch loss 1.20195627 epoch total loss 1.22006857\n",
      "Trained batch 25 batch loss 1.22723722 epoch total loss 1.22035539\n",
      "Trained batch 26 batch loss 1.37656045 epoch total loss 1.2263633\n",
      "Trained batch 27 batch loss 1.24640107 epoch total loss 1.2271055\n",
      "Trained batch 28 batch loss 1.22520638 epoch total loss 1.22703767\n",
      "Trained batch 29 batch loss 1.2175889 epoch total loss 1.22671199\n",
      "Trained batch 30 batch loss 1.01295877 epoch total loss 1.21958685\n",
      "Trained batch 31 batch loss 1.10900247 epoch total loss 1.21601951\n",
      "Trained batch 32 batch loss 1.19531274 epoch total loss 1.21537244\n",
      "Trained batch 33 batch loss 1.15902424 epoch total loss 1.21366489\n",
      "Trained batch 34 batch loss 1.22562575 epoch total loss 1.21401668\n",
      "Trained batch 35 batch loss 1.29440188 epoch total loss 1.21631336\n",
      "Trained batch 36 batch loss 1.29205275 epoch total loss 1.21841729\n",
      "Trained batch 37 batch loss 1.16269362 epoch total loss 1.2169112\n",
      "Trained batch 38 batch loss 1.20142567 epoch total loss 1.21650374\n",
      "Trained batch 39 batch loss 1.3680284 epoch total loss 1.22038901\n",
      "Trained batch 40 batch loss 1.3235383 epoch total loss 1.22296774\n",
      "Trained batch 41 batch loss 1.33534098 epoch total loss 1.2257086\n",
      "Trained batch 42 batch loss 1.32628036 epoch total loss 1.22810304\n",
      "Trained batch 43 batch loss 1.21663678 epoch total loss 1.22783649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 44 batch loss 1.15803111 epoch total loss 1.22624993\n",
      "Trained batch 45 batch loss 1.31363916 epoch total loss 1.22819197\n",
      "Trained batch 46 batch loss 1.01823723 epoch total loss 1.22362781\n",
      "Trained batch 47 batch loss 1.0761435 epoch total loss 1.22048986\n",
      "Trained batch 48 batch loss 0.946277559 epoch total loss 1.21477711\n",
      "Trained batch 49 batch loss 1.05582666 epoch total loss 1.21153319\n",
      "Trained batch 50 batch loss 1.15808415 epoch total loss 1.21046424\n",
      "Trained batch 51 batch loss 1.05562675 epoch total loss 1.20742822\n",
      "Trained batch 52 batch loss 1.17597449 epoch total loss 1.20682335\n",
      "Trained batch 53 batch loss 1.13474464 epoch total loss 1.20546329\n",
      "Trained batch 54 batch loss 1.26329958 epoch total loss 1.20653439\n",
      "Trained batch 55 batch loss 1.1786809 epoch total loss 1.20602787\n",
      "Trained batch 56 batch loss 1.25021052 epoch total loss 1.20681691\n",
      "Trained batch 57 batch loss 1.17500401 epoch total loss 1.20625877\n",
      "Trained batch 58 batch loss 1.06016946 epoch total loss 1.20374012\n",
      "Trained batch 59 batch loss 1.11745512 epoch total loss 1.20227766\n",
      "Trained batch 60 batch loss 1.25772417 epoch total loss 1.20320165\n",
      "Trained batch 61 batch loss 1.24958277 epoch total loss 1.20396197\n",
      "Trained batch 62 batch loss 1.30585098 epoch total loss 1.20560527\n",
      "Trained batch 63 batch loss 1.20119524 epoch total loss 1.20553529\n",
      "Trained batch 64 batch loss 1.13249874 epoch total loss 1.2043941\n",
      "Trained batch 65 batch loss 1.25823069 epoch total loss 1.20522237\n",
      "Trained batch 66 batch loss 1.44111633 epoch total loss 1.2087965\n",
      "Trained batch 67 batch loss 1.2634356 epoch total loss 1.20961201\n",
      "Trained batch 68 batch loss 1.19873834 epoch total loss 1.20945215\n",
      "Trained batch 69 batch loss 1.31758726 epoch total loss 1.21101928\n",
      "Trained batch 70 batch loss 1.37289226 epoch total loss 1.21333182\n",
      "Trained batch 71 batch loss 1.26121187 epoch total loss 1.21400619\n",
      "Trained batch 72 batch loss 1.14241409 epoch total loss 1.21301186\n",
      "Trained batch 73 batch loss 1.18003237 epoch total loss 1.21256\n",
      "Trained batch 74 batch loss 1.17312646 epoch total loss 1.21202719\n",
      "Trained batch 75 batch loss 1.24134374 epoch total loss 1.21241796\n",
      "Trained batch 76 batch loss 1.2266326 epoch total loss 1.212605\n",
      "Trained batch 77 batch loss 1.24108827 epoch total loss 1.21297491\n",
      "Trained batch 78 batch loss 1.29751444 epoch total loss 1.21405876\n",
      "Trained batch 79 batch loss 1.17649508 epoch total loss 1.21358335\n",
      "Trained batch 80 batch loss 1.27769613 epoch total loss 1.21438479\n",
      "Trained batch 81 batch loss 1.17438352 epoch total loss 1.21389091\n",
      "Trained batch 82 batch loss 1.07499051 epoch total loss 1.21219695\n",
      "Trained batch 83 batch loss 1.14022541 epoch total loss 1.21132994\n",
      "Trained batch 84 batch loss 1.15030241 epoch total loss 1.21060336\n",
      "Trained batch 85 batch loss 1.19205487 epoch total loss 1.21038508\n",
      "Trained batch 86 batch loss 1.1731385 epoch total loss 1.20995212\n",
      "Trained batch 87 batch loss 1.2643913 epoch total loss 1.21057773\n",
      "Trained batch 88 batch loss 1.19142783 epoch total loss 1.21036017\n",
      "Trained batch 89 batch loss 1.28591418 epoch total loss 1.21120906\n",
      "Trained batch 90 batch loss 1.17071116 epoch total loss 1.21075904\n",
      "Trained batch 91 batch loss 1.23229134 epoch total loss 1.21099567\n",
      "Trained batch 92 batch loss 1.35986495 epoch total loss 1.21261382\n",
      "Trained batch 93 batch loss 1.20664954 epoch total loss 1.21254969\n",
      "Trained batch 94 batch loss 1.33063734 epoch total loss 1.21380591\n",
      "Trained batch 95 batch loss 1.34697127 epoch total loss 1.21520758\n",
      "Trained batch 96 batch loss 1.29289079 epoch total loss 1.21601689\n",
      "Trained batch 97 batch loss 1.2663759 epoch total loss 1.21653593\n",
      "Trained batch 98 batch loss 1.29847026 epoch total loss 1.21737206\n",
      "Trained batch 99 batch loss 1.09872746 epoch total loss 1.21617353\n",
      "Trained batch 100 batch loss 1.10392272 epoch total loss 1.21505105\n",
      "Trained batch 101 batch loss 1.21868253 epoch total loss 1.21508694\n",
      "Trained batch 102 batch loss 1.25291097 epoch total loss 1.2154578\n",
      "Trained batch 103 batch loss 1.21033 epoch total loss 1.21540809\n",
      "Trained batch 104 batch loss 1.21908176 epoch total loss 1.21544337\n",
      "Trained batch 105 batch loss 1.16280651 epoch total loss 1.21494198\n",
      "Trained batch 106 batch loss 1.17033553 epoch total loss 1.21452117\n",
      "Trained batch 107 batch loss 1.18928504 epoch total loss 1.21428525\n",
      "Trained batch 108 batch loss 1.15364146 epoch total loss 1.21372378\n",
      "Trained batch 109 batch loss 1.13478315 epoch total loss 1.21299958\n",
      "Trained batch 110 batch loss 1.29682362 epoch total loss 1.21376157\n",
      "Trained batch 111 batch loss 1.42098713 epoch total loss 1.2156285\n",
      "Trained batch 112 batch loss 1.16553307 epoch total loss 1.21518123\n",
      "Trained batch 113 batch loss 1.14066243 epoch total loss 1.21452165\n",
      "Trained batch 114 batch loss 1.10013962 epoch total loss 1.21351838\n",
      "Trained batch 115 batch loss 1.05174494 epoch total loss 1.21211159\n",
      "Trained batch 116 batch loss 1.15967155 epoch total loss 1.21165955\n",
      "Trained batch 117 batch loss 1.31271696 epoch total loss 1.21252322\n",
      "Trained batch 118 batch loss 1.17729509 epoch total loss 1.21222472\n",
      "Trained batch 119 batch loss 1.2160455 epoch total loss 1.21225679\n",
      "Trained batch 120 batch loss 1.19031072 epoch total loss 1.21207392\n",
      "Trained batch 121 batch loss 1.15269172 epoch total loss 1.21158314\n",
      "Trained batch 122 batch loss 1.10452843 epoch total loss 1.21070564\n",
      "Trained batch 123 batch loss 1.20615721 epoch total loss 1.21066868\n",
      "Trained batch 124 batch loss 1.24003541 epoch total loss 1.21090555\n",
      "Trained batch 125 batch loss 1.3001852 epoch total loss 1.21161973\n",
      "Trained batch 126 batch loss 1.14015126 epoch total loss 1.21105254\n",
      "Trained batch 127 batch loss 1.20163846 epoch total loss 1.21097851\n",
      "Trained batch 128 batch loss 1.09071612 epoch total loss 1.2100389\n",
      "Trained batch 129 batch loss 1.13068187 epoch total loss 1.20942366\n",
      "Trained batch 130 batch loss 1.31947136 epoch total loss 1.21027017\n",
      "Trained batch 131 batch loss 1.1635139 epoch total loss 1.20991325\n",
      "Trained batch 132 batch loss 1.22458529 epoch total loss 1.21002436\n",
      "Trained batch 133 batch loss 1.25753701 epoch total loss 1.21038163\n",
      "Trained batch 134 batch loss 1.19731915 epoch total loss 1.21028423\n",
      "Trained batch 135 batch loss 1.11835682 epoch total loss 1.20960331\n",
      "Trained batch 136 batch loss 1.08016133 epoch total loss 1.20865154\n",
      "Trained batch 137 batch loss 1.05896151 epoch total loss 1.20755887\n",
      "Trained batch 138 batch loss 1.12568474 epoch total loss 1.20696557\n",
      "Trained batch 139 batch loss 1.05135846 epoch total loss 1.20584607\n",
      "Trained batch 140 batch loss 1.07191479 epoch total loss 1.20488942\n",
      "Trained batch 141 batch loss 1.07315695 epoch total loss 1.20395517\n",
      "Trained batch 142 batch loss 1.02109432 epoch total loss 1.20266736\n",
      "Trained batch 143 batch loss 1.21982718 epoch total loss 1.2027874\n",
      "Trained batch 144 batch loss 1.18356633 epoch total loss 1.20265388\n",
      "Trained batch 145 batch loss 1.17262769 epoch total loss 1.20244682\n",
      "Trained batch 146 batch loss 1.14142072 epoch total loss 1.20202875\n",
      "Trained batch 147 batch loss 1.33374488 epoch total loss 1.20292473\n",
      "Trained batch 148 batch loss 1.21195269 epoch total loss 1.20298576\n",
      "Trained batch 149 batch loss 1.07168806 epoch total loss 1.20210457\n",
      "Trained batch 150 batch loss 1.15994048 epoch total loss 1.20182347\n",
      "Trained batch 151 batch loss 1.35174513 epoch total loss 1.20281637\n",
      "Trained batch 152 batch loss 1.18621349 epoch total loss 1.20270717\n",
      "Trained batch 153 batch loss 1.17414784 epoch total loss 1.20252049\n",
      "Trained batch 154 batch loss 1.18482864 epoch total loss 1.20240569\n",
      "Trained batch 155 batch loss 1.14769852 epoch total loss 1.20205271\n",
      "Trained batch 156 batch loss 1.00757718 epoch total loss 1.20080614\n",
      "Trained batch 157 batch loss 1.00363064 epoch total loss 1.19955027\n",
      "Trained batch 158 batch loss 1.0970732 epoch total loss 1.19890165\n",
      "Trained batch 159 batch loss 1.26936221 epoch total loss 1.19934487\n",
      "Trained batch 160 batch loss 1.31889653 epoch total loss 1.20009208\n",
      "Trained batch 161 batch loss 1.26449251 epoch total loss 1.20049202\n",
      "Trained batch 162 batch loss 1.40185308 epoch total loss 1.20173502\n",
      "Trained batch 163 batch loss 1.32345927 epoch total loss 1.20248175\n",
      "Trained batch 164 batch loss 1.35008907 epoch total loss 1.20338178\n",
      "Trained batch 165 batch loss 1.17800951 epoch total loss 1.203228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 166 batch loss 1.06263328 epoch total loss 1.20238113\n",
      "Trained batch 167 batch loss 1.1695559 epoch total loss 1.20218456\n",
      "Trained batch 168 batch loss 1.19630837 epoch total loss 1.20214951\n",
      "Trained batch 169 batch loss 1.18106699 epoch total loss 1.2020247\n",
      "Trained batch 170 batch loss 1.1650784 epoch total loss 1.2018075\n",
      "Trained batch 171 batch loss 1.27933538 epoch total loss 1.20226085\n",
      "Trained batch 172 batch loss 1.15883923 epoch total loss 1.20200849\n",
      "Trained batch 173 batch loss 1.15660346 epoch total loss 1.20174599\n",
      "Trained batch 174 batch loss 1.19304812 epoch total loss 1.20169604\n",
      "Trained batch 175 batch loss 1.21171141 epoch total loss 1.20175326\n",
      "Trained batch 176 batch loss 1.32356203 epoch total loss 1.20244539\n",
      "Trained batch 177 batch loss 1.38066041 epoch total loss 1.20345223\n",
      "Trained batch 178 batch loss 1.40546727 epoch total loss 1.20458722\n",
      "Trained batch 179 batch loss 1.35426652 epoch total loss 1.20542336\n",
      "Trained batch 180 batch loss 1.29183447 epoch total loss 1.20590341\n",
      "Trained batch 181 batch loss 1.17593789 epoch total loss 1.20573783\n",
      "Trained batch 182 batch loss 1.16701508 epoch total loss 1.20552516\n",
      "Trained batch 183 batch loss 1.07119489 epoch total loss 1.20479107\n",
      "Trained batch 184 batch loss 1.15020013 epoch total loss 1.20449448\n",
      "Trained batch 185 batch loss 1.25925899 epoch total loss 1.20479047\n",
      "Trained batch 186 batch loss 1.21323442 epoch total loss 1.20483601\n",
      "Trained batch 187 batch loss 1.23519087 epoch total loss 1.20499825\n",
      "Trained batch 188 batch loss 1.1487242 epoch total loss 1.20469892\n",
      "Trained batch 189 batch loss 1.1348598 epoch total loss 1.20432937\n",
      "Trained batch 190 batch loss 1.00967622 epoch total loss 1.20330489\n",
      "Trained batch 191 batch loss 1.08295977 epoch total loss 1.20267487\n",
      "Trained batch 192 batch loss 1.10721385 epoch total loss 1.20217764\n",
      "Trained batch 193 batch loss 1.06998169 epoch total loss 1.20149267\n",
      "Trained batch 194 batch loss 1.12489009 epoch total loss 1.20109773\n",
      "Trained batch 195 batch loss 1.10218024 epoch total loss 1.20059049\n",
      "Trained batch 196 batch loss 1.01539648 epoch total loss 1.19964564\n",
      "Trained batch 197 batch loss 1.10458171 epoch total loss 1.19916308\n",
      "Trained batch 198 batch loss 1.18661094 epoch total loss 1.19909966\n",
      "Trained batch 199 batch loss 1.2200278 epoch total loss 1.19920492\n",
      "Trained batch 200 batch loss 1.28557372 epoch total loss 1.1996367\n",
      "Trained batch 201 batch loss 1.19415152 epoch total loss 1.1996094\n",
      "Trained batch 202 batch loss 1.32376552 epoch total loss 1.20022404\n",
      "Trained batch 203 batch loss 1.27921617 epoch total loss 1.20061314\n",
      "Trained batch 204 batch loss 1.15847874 epoch total loss 1.20040667\n",
      "Trained batch 205 batch loss 1.25573564 epoch total loss 1.20067656\n",
      "Trained batch 206 batch loss 1.38741589 epoch total loss 1.20158303\n",
      "Trained batch 207 batch loss 1.25963068 epoch total loss 1.20186341\n",
      "Trained batch 208 batch loss 1.05442095 epoch total loss 1.20115459\n",
      "Trained batch 209 batch loss 1.12286043 epoch total loss 1.20078\n",
      "Trained batch 210 batch loss 1.03362417 epoch total loss 1.19998407\n",
      "Trained batch 211 batch loss 1.02894771 epoch total loss 1.19917345\n",
      "Trained batch 212 batch loss 1.14949369 epoch total loss 1.19893909\n",
      "Trained batch 213 batch loss 1.1481508 epoch total loss 1.19870067\n",
      "Trained batch 214 batch loss 1.29012346 epoch total loss 1.19912791\n",
      "Trained batch 215 batch loss 1.30644965 epoch total loss 1.19962716\n",
      "Trained batch 216 batch loss 1.2442019 epoch total loss 1.19983351\n",
      "Trained batch 217 batch loss 1.20996916 epoch total loss 1.19988012\n",
      "Trained batch 218 batch loss 1.21478915 epoch total loss 1.19994855\n",
      "Trained batch 219 batch loss 1.13421166 epoch total loss 1.19964838\n",
      "Trained batch 220 batch loss 1.33449638 epoch total loss 1.20026135\n",
      "Trained batch 221 batch loss 1.05690157 epoch total loss 1.19961274\n",
      "Trained batch 222 batch loss 1.21112156 epoch total loss 1.19966459\n",
      "Trained batch 223 batch loss 1.20294952 epoch total loss 1.19967926\n",
      "Trained batch 224 batch loss 1.27339375 epoch total loss 1.20000839\n",
      "Trained batch 225 batch loss 1.29169154 epoch total loss 1.20041585\n",
      "Trained batch 226 batch loss 1.05327451 epoch total loss 1.19976485\n",
      "Trained batch 227 batch loss 1.08647716 epoch total loss 1.19926584\n",
      "Trained batch 228 batch loss 1.05086482 epoch total loss 1.19861495\n",
      "Trained batch 229 batch loss 1.12986386 epoch total loss 1.19831467\n",
      "Trained batch 230 batch loss 1.12496018 epoch total loss 1.19799578\n",
      "Trained batch 231 batch loss 1.18518019 epoch total loss 1.19794035\n",
      "Trained batch 232 batch loss 1.16370523 epoch total loss 1.19779277\n",
      "Trained batch 233 batch loss 1.15227568 epoch total loss 1.19759738\n",
      "Trained batch 234 batch loss 1.27338576 epoch total loss 1.19792128\n",
      "Trained batch 235 batch loss 1.27658188 epoch total loss 1.1982559\n",
      "Trained batch 236 batch loss 1.28069961 epoch total loss 1.1986053\n",
      "Trained batch 237 batch loss 1.33233237 epoch total loss 1.19916952\n",
      "Trained batch 238 batch loss 1.43007362 epoch total loss 1.20013976\n",
      "Trained batch 239 batch loss 1.42024863 epoch total loss 1.20106077\n",
      "Trained batch 240 batch loss 1.39849305 epoch total loss 1.20188344\n",
      "Trained batch 241 batch loss 1.24819076 epoch total loss 1.2020756\n",
      "Trained batch 242 batch loss 1.1991508 epoch total loss 1.20206356\n",
      "Trained batch 243 batch loss 1.09654701 epoch total loss 1.2016294\n",
      "Trained batch 244 batch loss 1.2617979 epoch total loss 1.20187604\n",
      "Trained batch 245 batch loss 1.24423218 epoch total loss 1.2020489\n",
      "Trained batch 246 batch loss 1.09236407 epoch total loss 1.20160306\n",
      "Trained batch 247 batch loss 1.10303319 epoch total loss 1.20120394\n",
      "Trained batch 248 batch loss 1.00753641 epoch total loss 1.20042312\n",
      "Trained batch 249 batch loss 1.09502244 epoch total loss 1.19999981\n",
      "Trained batch 250 batch loss 1.14504445 epoch total loss 1.19978\n",
      "Trained batch 251 batch loss 1.02455688 epoch total loss 1.19908202\n",
      "Trained batch 252 batch loss 1.06851923 epoch total loss 1.19856381\n",
      "Trained batch 253 batch loss 1.18292499 epoch total loss 1.19850206\n",
      "Trained batch 254 batch loss 1.18265688 epoch total loss 1.1984396\n",
      "Trained batch 255 batch loss 1.16456544 epoch total loss 1.19830668\n",
      "Trained batch 256 batch loss 1.07530856 epoch total loss 1.19782627\n",
      "Trained batch 257 batch loss 1.1834265 epoch total loss 1.19777024\n",
      "Trained batch 258 batch loss 1.2734828 epoch total loss 1.19806373\n",
      "Trained batch 259 batch loss 1.29647017 epoch total loss 1.19844365\n",
      "Trained batch 260 batch loss 1.17507017 epoch total loss 1.19835377\n",
      "Trained batch 261 batch loss 1.12471247 epoch total loss 1.19807172\n",
      "Trained batch 262 batch loss 1.19248855 epoch total loss 1.19805038\n",
      "Trained batch 263 batch loss 1.29596317 epoch total loss 1.19842267\n",
      "Trained batch 264 batch loss 1.17087162 epoch total loss 1.19831824\n",
      "Trained batch 265 batch loss 1.26879144 epoch total loss 1.1985842\n",
      "Trained batch 266 batch loss 1.21826303 epoch total loss 1.19865823\n",
      "Trained batch 267 batch loss 1.11166 epoch total loss 1.19833231\n",
      "Trained batch 268 batch loss 1.0860517 epoch total loss 1.19791341\n",
      "Trained batch 269 batch loss 1.03048515 epoch total loss 1.19729102\n",
      "Trained batch 270 batch loss 1.05341983 epoch total loss 1.19675815\n",
      "Trained batch 271 batch loss 1.08431292 epoch total loss 1.19634318\n",
      "Trained batch 272 batch loss 1.0853883 epoch total loss 1.19593525\n",
      "Trained batch 273 batch loss 1.20646417 epoch total loss 1.19597387\n",
      "Trained batch 274 batch loss 1.09090853 epoch total loss 1.19559038\n",
      "Trained batch 275 batch loss 1.06152511 epoch total loss 1.19510293\n",
      "Trained batch 276 batch loss 1.25708663 epoch total loss 1.1953274\n",
      "Trained batch 277 batch loss 1.14717662 epoch total loss 1.19515359\n",
      "Trained batch 278 batch loss 1.09336901 epoch total loss 1.1947875\n",
      "Trained batch 279 batch loss 1.21512175 epoch total loss 1.19486046\n",
      "Trained batch 280 batch loss 1.26925135 epoch total loss 1.19512618\n",
      "Trained batch 281 batch loss 1.21340811 epoch total loss 1.19519114\n",
      "Trained batch 282 batch loss 1.17680299 epoch total loss 1.19512594\n",
      "Trained batch 283 batch loss 1.24246943 epoch total loss 1.19529319\n",
      "Trained batch 284 batch loss 1.19066346 epoch total loss 1.19527698\n",
      "Trained batch 285 batch loss 1.2043699 epoch total loss 1.1953088\n",
      "Trained batch 286 batch loss 1.10127544 epoch total loss 1.19498014\n",
      "Trained batch 287 batch loss 1.05831242 epoch total loss 1.1945039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 288 batch loss 1.00170398 epoch total loss 1.19383454\n",
      "Trained batch 289 batch loss 1.03475952 epoch total loss 1.19328403\n",
      "Trained batch 290 batch loss 1.10890818 epoch total loss 1.19299316\n",
      "Trained batch 291 batch loss 1.02201486 epoch total loss 1.19240558\n",
      "Trained batch 292 batch loss 1.14575899 epoch total loss 1.19224584\n",
      "Trained batch 293 batch loss 1.14360869 epoch total loss 1.19207978\n",
      "Trained batch 294 batch loss 1.05235863 epoch total loss 1.19160461\n",
      "Trained batch 295 batch loss 1.09675181 epoch total loss 1.19128299\n",
      "Trained batch 296 batch loss 1.3492254 epoch total loss 1.19181657\n",
      "Trained batch 297 batch loss 1.15519869 epoch total loss 1.19169331\n",
      "Trained batch 298 batch loss 1.38089716 epoch total loss 1.19232821\n",
      "Trained batch 299 batch loss 1.21486831 epoch total loss 1.19240367\n",
      "Trained batch 300 batch loss 1.1962719 epoch total loss 1.19241643\n",
      "Trained batch 301 batch loss 1.18715525 epoch total loss 1.19239902\n",
      "Trained batch 302 batch loss 1.1551919 epoch total loss 1.19227576\n",
      "Trained batch 303 batch loss 1.0863924 epoch total loss 1.19192636\n",
      "Trained batch 304 batch loss 1.35355306 epoch total loss 1.19245803\n",
      "Trained batch 305 batch loss 1.14270079 epoch total loss 1.19229484\n",
      "Trained batch 306 batch loss 1.20837271 epoch total loss 1.19234741\n",
      "Trained batch 307 batch loss 1.14852047 epoch total loss 1.19220471\n",
      "Trained batch 308 batch loss 1.35953021 epoch total loss 1.19274795\n",
      "Trained batch 309 batch loss 1.24383485 epoch total loss 1.19291329\n",
      "Trained batch 310 batch loss 1.27502847 epoch total loss 1.19317818\n",
      "Trained batch 311 batch loss 1.22314346 epoch total loss 1.1932745\n",
      "Trained batch 312 batch loss 1.23344612 epoch total loss 1.19340324\n",
      "Trained batch 313 batch loss 1.19761109 epoch total loss 1.19341671\n",
      "Trained batch 314 batch loss 1.16670275 epoch total loss 1.19333172\n",
      "Trained batch 315 batch loss 1.19715142 epoch total loss 1.19334376\n",
      "Trained batch 316 batch loss 1.30305696 epoch total loss 1.19369102\n",
      "Trained batch 317 batch loss 1.29627895 epoch total loss 1.19401455\n",
      "Trained batch 318 batch loss 1.09580326 epoch total loss 1.19370568\n",
      "Trained batch 319 batch loss 1.09645438 epoch total loss 1.19340086\n",
      "Trained batch 320 batch loss 0.970949531 epoch total loss 1.19270575\n",
      "Trained batch 321 batch loss 1.04188681 epoch total loss 1.19223595\n",
      "Trained batch 322 batch loss 1.12826681 epoch total loss 1.19203722\n",
      "Trained batch 323 batch loss 1.17172849 epoch total loss 1.1919744\n",
      "Trained batch 324 batch loss 1.05134559 epoch total loss 1.19154024\n",
      "Trained batch 325 batch loss 1.12713432 epoch total loss 1.19134212\n",
      "Trained batch 326 batch loss 1.2944926 epoch total loss 1.1916585\n",
      "Trained batch 327 batch loss 1.21273124 epoch total loss 1.19172299\n",
      "Trained batch 328 batch loss 1.15429747 epoch total loss 1.19160891\n",
      "Trained batch 329 batch loss 1.03671312 epoch total loss 1.19113803\n",
      "Trained batch 330 batch loss 1.15149343 epoch total loss 1.19101799\n",
      "Trained batch 331 batch loss 1.29852 epoch total loss 1.19134271\n",
      "Trained batch 332 batch loss 1.32188845 epoch total loss 1.19173598\n",
      "Trained batch 333 batch loss 1.23988342 epoch total loss 1.19188058\n",
      "Trained batch 334 batch loss 1.20116198 epoch total loss 1.19190848\n",
      "Trained batch 335 batch loss 1.20389938 epoch total loss 1.19194412\n",
      "Trained batch 336 batch loss 1.26248586 epoch total loss 1.19215417\n",
      "Trained batch 337 batch loss 1.28632307 epoch total loss 1.19243348\n",
      "Trained batch 338 batch loss 1.17588353 epoch total loss 1.19238448\n",
      "Trained batch 339 batch loss 1.16562927 epoch total loss 1.19230556\n",
      "Trained batch 340 batch loss 1.15253723 epoch total loss 1.19218862\n",
      "Trained batch 341 batch loss 1.22441459 epoch total loss 1.19228315\n",
      "Trained batch 342 batch loss 1.19339561 epoch total loss 1.19228637\n",
      "Trained batch 343 batch loss 1.20947051 epoch total loss 1.19233644\n",
      "Trained batch 344 batch loss 1.19090009 epoch total loss 1.19233227\n",
      "Trained batch 345 batch loss 1.22189116 epoch total loss 1.19241798\n",
      "Trained batch 346 batch loss 1.25905919 epoch total loss 1.1926105\n",
      "Trained batch 347 batch loss 1.28240943 epoch total loss 1.19286931\n",
      "Trained batch 348 batch loss 1.19831514 epoch total loss 1.19288492\n",
      "Trained batch 349 batch loss 1.09179831 epoch total loss 1.19259524\n",
      "Trained batch 350 batch loss 1.02732038 epoch total loss 1.19212306\n",
      "Trained batch 351 batch loss 1.06696045 epoch total loss 1.1917665\n",
      "Trained batch 352 batch loss 1.14883518 epoch total loss 1.19164455\n",
      "Trained batch 353 batch loss 1.22501218 epoch total loss 1.19173896\n",
      "Trained batch 354 batch loss 1.18591762 epoch total loss 1.19172251\n",
      "Trained batch 355 batch loss 1.23887014 epoch total loss 1.19185531\n",
      "Trained batch 356 batch loss 1.31779838 epoch total loss 1.19220912\n",
      "Trained batch 357 batch loss 1.21416903 epoch total loss 1.19227064\n",
      "Trained batch 358 batch loss 1.19257855 epoch total loss 1.19227147\n",
      "Trained batch 359 batch loss 1.12250793 epoch total loss 1.19207716\n",
      "Trained batch 360 batch loss 1.12053323 epoch total loss 1.19187844\n",
      "Trained batch 361 batch loss 1.30744267 epoch total loss 1.19219851\n",
      "Trained batch 362 batch loss 1.0069257 epoch total loss 1.19168675\n",
      "Trained batch 363 batch loss 0.876218736 epoch total loss 1.19081771\n",
      "Trained batch 364 batch loss 0.94664818 epoch total loss 1.19014692\n",
      "Trained batch 365 batch loss 1.26123524 epoch total loss 1.19034159\n",
      "Trained batch 366 batch loss 1.27630007 epoch total loss 1.19057655\n",
      "Trained batch 367 batch loss 1.37881863 epoch total loss 1.19108939\n",
      "Trained batch 368 batch loss 1.32360506 epoch total loss 1.19144952\n",
      "Trained batch 369 batch loss 1.21649373 epoch total loss 1.19151735\n",
      "Trained batch 370 batch loss 1.12565601 epoch total loss 1.19133937\n",
      "Trained batch 371 batch loss 1.067204 epoch total loss 1.19100475\n",
      "Trained batch 372 batch loss 1.15400863 epoch total loss 1.19090533\n",
      "Trained batch 373 batch loss 1.21565819 epoch total loss 1.19097173\n",
      "Trained batch 374 batch loss 1.15242577 epoch total loss 1.19086874\n",
      "Trained batch 375 batch loss 1.30058396 epoch total loss 1.19116127\n",
      "Trained batch 376 batch loss 1.32338488 epoch total loss 1.19151294\n",
      "Trained batch 377 batch loss 1.23390615 epoch total loss 1.19162548\n",
      "Trained batch 378 batch loss 1.18301511 epoch total loss 1.19160271\n",
      "Trained batch 379 batch loss 1.30457377 epoch total loss 1.19190073\n",
      "Trained batch 380 batch loss 1.152583 epoch total loss 1.19179726\n",
      "Trained batch 381 batch loss 1.20373631 epoch total loss 1.19182861\n",
      "Trained batch 382 batch loss 1.22326279 epoch total loss 1.19191086\n",
      "Trained batch 383 batch loss 1.27216804 epoch total loss 1.19212043\n",
      "Trained batch 384 batch loss 1.20746684 epoch total loss 1.19216037\n",
      "Trained batch 385 batch loss 1.21846914 epoch total loss 1.19222867\n",
      "Trained batch 386 batch loss 1.26583874 epoch total loss 1.19241941\n",
      "Trained batch 387 batch loss 1.37520027 epoch total loss 1.19289172\n",
      "Trained batch 388 batch loss 1.23769379 epoch total loss 1.19300723\n",
      "Trained batch 389 batch loss 1.19825315 epoch total loss 1.1930207\n",
      "Trained batch 390 batch loss 1.20457268 epoch total loss 1.19305027\n",
      "Trained batch 391 batch loss 1.27966595 epoch total loss 1.19327176\n",
      "Trained batch 392 batch loss 1.19515431 epoch total loss 1.19327664\n",
      "Trained batch 393 batch loss 1.3038367 epoch total loss 1.19355798\n",
      "Trained batch 394 batch loss 1.23596513 epoch total loss 1.1936655\n",
      "Trained batch 395 batch loss 1.27635527 epoch total loss 1.19387496\n",
      "Trained batch 396 batch loss 1.3045541 epoch total loss 1.1941545\n",
      "Trained batch 397 batch loss 1.17932844 epoch total loss 1.19411707\n",
      "Trained batch 398 batch loss 1.11488175 epoch total loss 1.19391799\n",
      "Trained batch 399 batch loss 1.2380867 epoch total loss 1.19402874\n",
      "Trained batch 400 batch loss 1.21783948 epoch total loss 1.19408822\n",
      "Trained batch 401 batch loss 1.36222637 epoch total loss 1.19450748\n",
      "Trained batch 402 batch loss 1.29396033 epoch total loss 1.19475484\n",
      "Trained batch 403 batch loss 1.15041876 epoch total loss 1.19464481\n",
      "Trained batch 404 batch loss 1.24786758 epoch total loss 1.19477654\n",
      "Trained batch 405 batch loss 1.24514985 epoch total loss 1.19490087\n",
      "Trained batch 406 batch loss 1.14121127 epoch total loss 1.19476867\n",
      "Trained batch 407 batch loss 1.30574846 epoch total loss 1.19504142\n",
      "Trained batch 408 batch loss 1.39833164 epoch total loss 1.19553971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 409 batch loss 1.269701 epoch total loss 1.19572103\n",
      "Trained batch 410 batch loss 1.21353281 epoch total loss 1.19576442\n",
      "Trained batch 411 batch loss 1.02332008 epoch total loss 1.19534492\n",
      "Trained batch 412 batch loss 1.16850495 epoch total loss 1.19527972\n",
      "Trained batch 413 batch loss 1.34768987 epoch total loss 1.19564879\n",
      "Trained batch 414 batch loss 1.26458085 epoch total loss 1.19581532\n",
      "Trained batch 415 batch loss 1.23725414 epoch total loss 1.1959151\n",
      "Trained batch 416 batch loss 1.25592768 epoch total loss 1.19605935\n",
      "Trained batch 417 batch loss 1.28165936 epoch total loss 1.19626462\n",
      "Trained batch 418 batch loss 1.24325 epoch total loss 1.19637704\n",
      "Trained batch 419 batch loss 1.15462422 epoch total loss 1.19627738\n",
      "Trained batch 420 batch loss 1.14166021 epoch total loss 1.19614732\n",
      "Trained batch 421 batch loss 1.04526079 epoch total loss 1.19578898\n",
      "Trained batch 422 batch loss 1.04012179 epoch total loss 1.19542015\n",
      "Trained batch 423 batch loss 0.985916853 epoch total loss 1.19492483\n",
      "Trained batch 424 batch loss 1.14222217 epoch total loss 1.1948005\n",
      "Trained batch 425 batch loss 0.989440322 epoch total loss 1.19431734\n",
      "Trained batch 426 batch loss 0.872206926 epoch total loss 1.1935612\n",
      "Trained batch 427 batch loss 0.908715487 epoch total loss 1.1928941\n",
      "Trained batch 428 batch loss 0.889732122 epoch total loss 1.19218576\n",
      "Trained batch 429 batch loss 1.04223108 epoch total loss 1.19183624\n",
      "Trained batch 430 batch loss 1.18979442 epoch total loss 1.19183147\n",
      "Trained batch 431 batch loss 1.15258348 epoch total loss 1.19174051\n",
      "Trained batch 432 batch loss 1.1263814 epoch total loss 1.19158924\n",
      "Trained batch 433 batch loss 1.12769401 epoch total loss 1.19144166\n",
      "Trained batch 434 batch loss 1.16951656 epoch total loss 1.19139111\n",
      "Trained batch 435 batch loss 1.23064566 epoch total loss 1.19148135\n",
      "Trained batch 436 batch loss 1.09417236 epoch total loss 1.19125819\n",
      "Trained batch 437 batch loss 1.04060555 epoch total loss 1.19091332\n",
      "Trained batch 438 batch loss 0.91746819 epoch total loss 1.19028914\n",
      "Trained batch 439 batch loss 1.02407038 epoch total loss 1.18991041\n",
      "Trained batch 440 batch loss 1.32630503 epoch total loss 1.19022036\n",
      "Trained batch 441 batch loss 1.22939837 epoch total loss 1.19030917\n",
      "Trained batch 442 batch loss 1.2915771 epoch total loss 1.19053817\n",
      "Trained batch 443 batch loss 1.26902807 epoch total loss 1.19071543\n",
      "Trained batch 444 batch loss 1.15819085 epoch total loss 1.19064224\n",
      "Trained batch 445 batch loss 1.0909977 epoch total loss 1.19041836\n",
      "Trained batch 446 batch loss 1.1578089 epoch total loss 1.19034529\n",
      "Trained batch 447 batch loss 1.16708922 epoch total loss 1.19029331\n",
      "Trained batch 448 batch loss 1.25805962 epoch total loss 1.19044459\n",
      "Trained batch 449 batch loss 1.26968634 epoch total loss 1.19062114\n",
      "Trained batch 450 batch loss 1.31209373 epoch total loss 1.19089103\n",
      "Trained batch 451 batch loss 1.3902812 epoch total loss 1.19133306\n",
      "Trained batch 452 batch loss 1.27578568 epoch total loss 1.19151986\n",
      "Trained batch 453 batch loss 1.131953 epoch total loss 1.19138837\n",
      "Trained batch 454 batch loss 1.19579971 epoch total loss 1.19139802\n",
      "Trained batch 455 batch loss 1.11763668 epoch total loss 1.1912359\n",
      "Trained batch 456 batch loss 1.14347196 epoch total loss 1.19113123\n",
      "Trained batch 457 batch loss 1.11922693 epoch total loss 1.19097376\n",
      "Trained batch 458 batch loss 1.23730147 epoch total loss 1.19107497\n",
      "Trained batch 459 batch loss 1.17776453 epoch total loss 1.19104588\n",
      "Trained batch 460 batch loss 1.1760968 epoch total loss 1.19101334\n",
      "Trained batch 461 batch loss 1.19452834 epoch total loss 1.19102097\n",
      "Trained batch 462 batch loss 1.14644885 epoch total loss 1.19092441\n",
      "Trained batch 463 batch loss 1.16553438 epoch total loss 1.19086957\n",
      "Trained batch 464 batch loss 1.15843368 epoch total loss 1.19079971\n",
      "Trained batch 465 batch loss 1.26991653 epoch total loss 1.19096982\n",
      "Trained batch 466 batch loss 1.34055746 epoch total loss 1.19129086\n",
      "Trained batch 467 batch loss 1.2131387 epoch total loss 1.19133759\n",
      "Trained batch 468 batch loss 1.3718915 epoch total loss 1.19172347\n",
      "Trained batch 469 batch loss 1.23895359 epoch total loss 1.19182408\n",
      "Trained batch 470 batch loss 1.19902599 epoch total loss 1.19183946\n",
      "Trained batch 471 batch loss 1.3078444 epoch total loss 1.19208574\n",
      "Trained batch 472 batch loss 1.1779511 epoch total loss 1.19205594\n",
      "Trained batch 473 batch loss 1.21227622 epoch total loss 1.19209862\n",
      "Trained batch 474 batch loss 1.10088468 epoch total loss 1.19190621\n",
      "Trained batch 475 batch loss 1.1038909 epoch total loss 1.19172096\n",
      "Trained batch 476 batch loss 0.974225521 epoch total loss 1.19126403\n",
      "Trained batch 477 batch loss 1.17189264 epoch total loss 1.19122338\n",
      "Trained batch 478 batch loss 1.14711428 epoch total loss 1.19113111\n",
      "Trained batch 479 batch loss 1.12982917 epoch total loss 1.19100308\n",
      "Trained batch 480 batch loss 1.14812505 epoch total loss 1.1909138\n",
      "Trained batch 481 batch loss 1.18981552 epoch total loss 1.19091153\n",
      "Trained batch 482 batch loss 1.17719126 epoch total loss 1.19088304\n",
      "Trained batch 483 batch loss 1.19197035 epoch total loss 1.19088519\n",
      "Trained batch 484 batch loss 1.14600468 epoch total loss 1.19079244\n",
      "Trained batch 485 batch loss 1.23662972 epoch total loss 1.19088697\n",
      "Trained batch 486 batch loss 1.26697326 epoch total loss 1.1910435\n",
      "Trained batch 487 batch loss 1.22936285 epoch total loss 1.19112229\n",
      "Trained batch 488 batch loss 1.31222272 epoch total loss 1.19137037\n",
      "Trained batch 489 batch loss 1.19146645 epoch total loss 1.19137061\n",
      "Trained batch 490 batch loss 1.30344069 epoch total loss 1.19159937\n",
      "Trained batch 491 batch loss 1.35292923 epoch total loss 1.19192779\n",
      "Trained batch 492 batch loss 1.11163795 epoch total loss 1.19176459\n",
      "Trained batch 493 batch loss 1.16566944 epoch total loss 1.19171166\n",
      "Trained batch 494 batch loss 1.19456422 epoch total loss 1.19171751\n",
      "Trained batch 495 batch loss 1.1381743 epoch total loss 1.19160938\n",
      "Trained batch 496 batch loss 1.05635679 epoch total loss 1.19133663\n",
      "Trained batch 497 batch loss 1.11605358 epoch total loss 1.19118512\n",
      "Trained batch 498 batch loss 1.11413598 epoch total loss 1.19103038\n",
      "Trained batch 499 batch loss 1.08997214 epoch total loss 1.19082785\n",
      "Trained batch 500 batch loss 1.19800472 epoch total loss 1.19084215\n",
      "Trained batch 501 batch loss 1.29554975 epoch total loss 1.19105113\n",
      "Trained batch 502 batch loss 1.19800115 epoch total loss 1.19106495\n",
      "Trained batch 503 batch loss 1.2348752 epoch total loss 1.1911521\n",
      "Trained batch 504 batch loss 1.36294663 epoch total loss 1.19149292\n",
      "Trained batch 505 batch loss 1.23300457 epoch total loss 1.19157517\n",
      "Trained batch 506 batch loss 1.13536143 epoch total loss 1.19146419\n",
      "Trained batch 507 batch loss 1.2932744 epoch total loss 1.19166493\n",
      "Trained batch 508 batch loss 1.45556033 epoch total loss 1.19218445\n",
      "Trained batch 509 batch loss 1.27406025 epoch total loss 1.19234526\n",
      "Trained batch 510 batch loss 1.19793129 epoch total loss 1.19235623\n",
      "Trained batch 511 batch loss 1.26469612 epoch total loss 1.19249785\n",
      "Trained batch 512 batch loss 1.18310881 epoch total loss 1.19247949\n",
      "Trained batch 513 batch loss 1.21631145 epoch total loss 1.19252598\n",
      "Trained batch 514 batch loss 1.22945213 epoch total loss 1.19259775\n",
      "Trained batch 515 batch loss 1.212888 epoch total loss 1.19263709\n",
      "Trained batch 516 batch loss 1.14122915 epoch total loss 1.19253755\n",
      "Trained batch 517 batch loss 1.1644814 epoch total loss 1.19248331\n",
      "Trained batch 518 batch loss 1.30112517 epoch total loss 1.19269311\n",
      "Trained batch 519 batch loss 1.27713466 epoch total loss 1.19285583\n",
      "Trained batch 520 batch loss 1.18958712 epoch total loss 1.19284952\n",
      "Trained batch 521 batch loss 1.14950323 epoch total loss 1.19276619\n",
      "Trained batch 522 batch loss 1.22998798 epoch total loss 1.19283748\n",
      "Trained batch 523 batch loss 1.21738768 epoch total loss 1.19288456\n",
      "Trained batch 524 batch loss 1.1076895 epoch total loss 1.19272184\n",
      "Trained batch 525 batch loss 1.32016873 epoch total loss 1.19296467\n",
      "Trained batch 526 batch loss 1.31229448 epoch total loss 1.19319153\n",
      "Trained batch 527 batch loss 1.32001543 epoch total loss 1.19343221\n",
      "Trained batch 528 batch loss 1.26150608 epoch total loss 1.1935612\n",
      "Trained batch 529 batch loss 1.18888032 epoch total loss 1.19355237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 530 batch loss 1.19359767 epoch total loss 1.19355249\n",
      "Trained batch 531 batch loss 1.07559717 epoch total loss 1.19333041\n",
      "Trained batch 532 batch loss 1.32337666 epoch total loss 1.19357479\n",
      "Trained batch 533 batch loss 1.35270166 epoch total loss 1.19387341\n",
      "Trained batch 534 batch loss 1.44725382 epoch total loss 1.19434798\n",
      "Trained batch 535 batch loss 1.33495498 epoch total loss 1.19461071\n",
      "Trained batch 536 batch loss 1.24892521 epoch total loss 1.19471204\n",
      "Trained batch 537 batch loss 1.28774667 epoch total loss 1.19488525\n",
      "Trained batch 538 batch loss 1.24560618 epoch total loss 1.19497955\n",
      "Trained batch 539 batch loss 1.33515787 epoch total loss 1.19523954\n",
      "Trained batch 540 batch loss 1.2117517 epoch total loss 1.19527006\n",
      "Trained batch 541 batch loss 1.13502812 epoch total loss 1.19515872\n",
      "Trained batch 542 batch loss 1.08059907 epoch total loss 1.19494736\n",
      "Trained batch 543 batch loss 1.17603314 epoch total loss 1.19491255\n",
      "Trained batch 544 batch loss 1.16838956 epoch total loss 1.1948638\n",
      "Trained batch 545 batch loss 1.20249295 epoch total loss 1.19487786\n",
      "Trained batch 546 batch loss 1.17948854 epoch total loss 1.19484973\n",
      "Trained batch 547 batch loss 1.03888273 epoch total loss 1.19456458\n",
      "Trained batch 548 batch loss 1.2010901 epoch total loss 1.1945765\n",
      "Trained batch 549 batch loss 1.18007374 epoch total loss 1.19455\n",
      "Trained batch 550 batch loss 1.28734767 epoch total loss 1.19471884\n",
      "Trained batch 551 batch loss 1.20196545 epoch total loss 1.19473195\n",
      "Trained batch 552 batch loss 1.15491128 epoch total loss 1.19465983\n",
      "Trained batch 553 batch loss 1.06602836 epoch total loss 1.19442725\n",
      "Trained batch 554 batch loss 1.29593349 epoch total loss 1.19461048\n",
      "Trained batch 555 batch loss 1.24343789 epoch total loss 1.19469845\n",
      "Trained batch 556 batch loss 1.21244407 epoch total loss 1.1947304\n",
      "Trained batch 557 batch loss 1.22739601 epoch total loss 1.19478905\n",
      "Trained batch 558 batch loss 1.17214811 epoch total loss 1.1947484\n",
      "Trained batch 559 batch loss 1.1101712 epoch total loss 1.19459713\n",
      "Trained batch 560 batch loss 1.03315616 epoch total loss 1.19430876\n",
      "Trained batch 561 batch loss 1.17586875 epoch total loss 1.19427586\n",
      "Trained batch 562 batch loss 1.21277857 epoch total loss 1.19430876\n",
      "Trained batch 563 batch loss 1.15791702 epoch total loss 1.19424415\n",
      "Trained batch 564 batch loss 1.26483524 epoch total loss 1.19436932\n",
      "Trained batch 565 batch loss 1.24171507 epoch total loss 1.194453\n",
      "Trained batch 566 batch loss 1.16197479 epoch total loss 1.19439566\n",
      "Trained batch 567 batch loss 1.21746838 epoch total loss 1.19443631\n",
      "Trained batch 568 batch loss 1.1920712 epoch total loss 1.19443226\n",
      "Trained batch 569 batch loss 1.20359564 epoch total loss 1.19444835\n",
      "Trained batch 570 batch loss 1.18408668 epoch total loss 1.19443011\n",
      "Trained batch 571 batch loss 1.09888804 epoch total loss 1.19426286\n",
      "Trained batch 572 batch loss 1.13334084 epoch total loss 1.19415641\n",
      "Trained batch 573 batch loss 1.12900329 epoch total loss 1.19404268\n",
      "Trained batch 574 batch loss 1.0924542 epoch total loss 1.19386578\n",
      "Trained batch 575 batch loss 1.24156773 epoch total loss 1.19394875\n",
      "Trained batch 576 batch loss 1.27308941 epoch total loss 1.19408607\n",
      "Trained batch 577 batch loss 1.19262433 epoch total loss 1.19408357\n",
      "Trained batch 578 batch loss 1.17074335 epoch total loss 1.19404316\n",
      "Trained batch 579 batch loss 1.24328756 epoch total loss 1.19412816\n",
      "Trained batch 580 batch loss 1.20945334 epoch total loss 1.19415462\n",
      "Trained batch 581 batch loss 1.15631533 epoch total loss 1.19408953\n",
      "Trained batch 582 batch loss 1.05501175 epoch total loss 1.19385052\n",
      "Trained batch 583 batch loss 1.19668341 epoch total loss 1.19385529\n",
      "Trained batch 584 batch loss 1.06776321 epoch total loss 1.1936394\n",
      "Trained batch 585 batch loss 1.16612744 epoch total loss 1.19359231\n",
      "Trained batch 586 batch loss 0.914082527 epoch total loss 1.19311535\n",
      "Trained batch 587 batch loss 0.935469508 epoch total loss 1.19267642\n",
      "Trained batch 588 batch loss 1.05408692 epoch total loss 1.19244075\n",
      "Trained batch 589 batch loss 1.20849395 epoch total loss 1.19246805\n",
      "Trained batch 590 batch loss 1.40592492 epoch total loss 1.19282985\n",
      "Trained batch 591 batch loss 1.50534403 epoch total loss 1.19335866\n",
      "Trained batch 592 batch loss 1.33885217 epoch total loss 1.19360447\n",
      "Trained batch 593 batch loss 1.1219573 epoch total loss 1.19348359\n",
      "Trained batch 594 batch loss 1.13698459 epoch total loss 1.19338846\n",
      "Trained batch 595 batch loss 1.21963382 epoch total loss 1.19343257\n",
      "Trained batch 596 batch loss 1.22749901 epoch total loss 1.19348967\n",
      "Trained batch 597 batch loss 1.23680234 epoch total loss 1.19356227\n",
      "Trained batch 598 batch loss 1.19871 epoch total loss 1.19357085\n",
      "Trained batch 599 batch loss 1.26104939 epoch total loss 1.19368351\n",
      "Trained batch 600 batch loss 1.19537926 epoch total loss 1.19368637\n",
      "Trained batch 601 batch loss 1.19143987 epoch total loss 1.19368267\n",
      "Trained batch 602 batch loss 1.06849885 epoch total loss 1.19347465\n",
      "Trained batch 603 batch loss 1.13221657 epoch total loss 1.19337308\n",
      "Trained batch 604 batch loss 1.16662729 epoch total loss 1.19332874\n",
      "Trained batch 605 batch loss 1.15456438 epoch total loss 1.1932646\n",
      "Trained batch 606 batch loss 1.18067801 epoch total loss 1.19324386\n",
      "Trained batch 607 batch loss 1.06230569 epoch total loss 1.19302821\n",
      "Trained batch 608 batch loss 1.18660426 epoch total loss 1.1930176\n",
      "Trained batch 609 batch loss 1.31084025 epoch total loss 1.19321108\n",
      "Trained batch 610 batch loss 1.13769591 epoch total loss 1.19312\n",
      "Trained batch 611 batch loss 1.0388633 epoch total loss 1.19286764\n",
      "Trained batch 612 batch loss 1.08174682 epoch total loss 1.19268596\n",
      "Trained batch 613 batch loss 1.0653466 epoch total loss 1.1924783\n",
      "Trained batch 614 batch loss 1.14212787 epoch total loss 1.19239628\n",
      "Trained batch 615 batch loss 1.10374868 epoch total loss 1.19225216\n",
      "Trained batch 616 batch loss 1.27663708 epoch total loss 1.19238913\n",
      "Trained batch 617 batch loss 1.16141737 epoch total loss 1.19233894\n",
      "Trained batch 618 batch loss 1.22101593 epoch total loss 1.19238544\n",
      "Trained batch 619 batch loss 1.18089652 epoch total loss 1.19236684\n",
      "Trained batch 620 batch loss 1.2436887 epoch total loss 1.19244969\n",
      "Trained batch 621 batch loss 1.39313877 epoch total loss 1.19277287\n",
      "Trained batch 622 batch loss 1.38222384 epoch total loss 1.19307733\n",
      "Trained batch 623 batch loss 1.4633081 epoch total loss 1.19351113\n",
      "Trained batch 624 batch loss 1.1599257 epoch total loss 1.19345725\n",
      "Trained batch 625 batch loss 1.03420234 epoch total loss 1.1932025\n",
      "Trained batch 626 batch loss 1.33669484 epoch total loss 1.19343162\n",
      "Trained batch 627 batch loss 1.3320328 epoch total loss 1.19365263\n",
      "Trained batch 628 batch loss 1.34334111 epoch total loss 1.19389105\n",
      "Trained batch 629 batch loss 1.29827142 epoch total loss 1.19405699\n",
      "Trained batch 630 batch loss 1.19357157 epoch total loss 1.19405615\n",
      "Trained batch 631 batch loss 1.19536638 epoch total loss 1.19405818\n",
      "Trained batch 632 batch loss 1.20203972 epoch total loss 1.19407082\n",
      "Trained batch 633 batch loss 1.26408792 epoch total loss 1.19418144\n",
      "Trained batch 634 batch loss 1.35672355 epoch total loss 1.19443786\n",
      "Trained batch 635 batch loss 1.18746328 epoch total loss 1.19442689\n",
      "Trained batch 636 batch loss 1.22355127 epoch total loss 1.19447267\n",
      "Trained batch 637 batch loss 1.21963465 epoch total loss 1.19451213\n",
      "Trained batch 638 batch loss 1.16744399 epoch total loss 1.19446969\n",
      "Trained batch 639 batch loss 1.12573874 epoch total loss 1.19436216\n",
      "Trained batch 640 batch loss 1.04594374 epoch total loss 1.19413018\n",
      "Trained batch 641 batch loss 1.16420889 epoch total loss 1.19408357\n",
      "Trained batch 642 batch loss 1.32300174 epoch total loss 1.19428432\n",
      "Trained batch 643 batch loss 1.35166478 epoch total loss 1.19452906\n",
      "Trained batch 644 batch loss 1.26154053 epoch total loss 1.19463313\n",
      "Trained batch 645 batch loss 1.32586122 epoch total loss 1.19483662\n",
      "Trained batch 646 batch loss 1.26411736 epoch total loss 1.19494379\n",
      "Trained batch 647 batch loss 1.09052753 epoch total loss 1.19478238\n",
      "Trained batch 648 batch loss 1.02884 epoch total loss 1.19452643\n",
      "Trained batch 649 batch loss 0.941049218 epoch total loss 1.19413579\n",
      "Trained batch 650 batch loss 0.964948475 epoch total loss 1.19378328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 651 batch loss 1.14989829 epoch total loss 1.19371581\n",
      "Trained batch 652 batch loss 0.999867201 epoch total loss 1.1934185\n",
      "Trained batch 653 batch loss 1.38012981 epoch total loss 1.19370449\n",
      "Trained batch 654 batch loss 1.31733716 epoch total loss 1.19389343\n",
      "Trained batch 655 batch loss 1.26955593 epoch total loss 1.19400895\n",
      "Trained batch 656 batch loss 1.40490437 epoch total loss 1.19433045\n",
      "Trained batch 657 batch loss 1.32996559 epoch total loss 1.19453692\n",
      "Trained batch 658 batch loss 1.12904215 epoch total loss 1.19443727\n",
      "Trained batch 659 batch loss 1.21459734 epoch total loss 1.1944679\n",
      "Trained batch 660 batch loss 1.2577157 epoch total loss 1.19456375\n",
      "Trained batch 661 batch loss 1.20866501 epoch total loss 1.19458508\n",
      "Trained batch 662 batch loss 1.21165872 epoch total loss 1.19461083\n",
      "Trained batch 663 batch loss 1.24553037 epoch total loss 1.19468772\n",
      "Trained batch 664 batch loss 1.37049091 epoch total loss 1.19495249\n",
      "Trained batch 665 batch loss 1.34186482 epoch total loss 1.19517338\n",
      "Trained batch 666 batch loss 1.31982303 epoch total loss 1.19536054\n",
      "Trained batch 667 batch loss 1.39530396 epoch total loss 1.19566035\n",
      "Trained batch 668 batch loss 1.38161564 epoch total loss 1.19593871\n",
      "Trained batch 669 batch loss 1.34190691 epoch total loss 1.19615686\n",
      "Trained batch 670 batch loss 1.26639414 epoch total loss 1.19626176\n",
      "Trained batch 671 batch loss 1.30440617 epoch total loss 1.19642282\n",
      "Trained batch 672 batch loss 1.26789784 epoch total loss 1.19652915\n",
      "Trained batch 673 batch loss 1.23433852 epoch total loss 1.1965853\n",
      "Trained batch 674 batch loss 1.12987685 epoch total loss 1.19648635\n",
      "Trained batch 675 batch loss 1.29329014 epoch total loss 1.19662976\n",
      "Trained batch 676 batch loss 1.38709426 epoch total loss 1.19691145\n",
      "Trained batch 677 batch loss 1.18264759 epoch total loss 1.19689035\n",
      "Trained batch 678 batch loss 1.16079116 epoch total loss 1.19683707\n",
      "Trained batch 679 batch loss 1.14171064 epoch total loss 1.19675601\n",
      "Trained batch 680 batch loss 0.968812585 epoch total loss 1.19642079\n",
      "Trained batch 681 batch loss 1.17464793 epoch total loss 1.19638872\n",
      "Trained batch 682 batch loss 1.24792171 epoch total loss 1.1964643\n",
      "Trained batch 683 batch loss 1.14000487 epoch total loss 1.19638169\n",
      "Trained batch 684 batch loss 1.12795043 epoch total loss 1.19628155\n",
      "Trained batch 685 batch loss 1.06801748 epoch total loss 1.19609427\n",
      "Trained batch 686 batch loss 1.11177206 epoch total loss 1.19597137\n",
      "Trained batch 687 batch loss 1.2751894 epoch total loss 1.19608665\n",
      "Trained batch 688 batch loss 1.39765131 epoch total loss 1.19637966\n",
      "Trained batch 689 batch loss 1.19479036 epoch total loss 1.19637728\n",
      "Trained batch 690 batch loss 1.16310072 epoch total loss 1.196329\n",
      "Trained batch 691 batch loss 0.905550241 epoch total loss 1.19590831\n",
      "Trained batch 692 batch loss 0.851506829 epoch total loss 1.19541061\n",
      "Trained batch 693 batch loss 0.910497248 epoch total loss 1.19499946\n",
      "Trained batch 694 batch loss 1.06480134 epoch total loss 1.19481194\n",
      "Trained batch 695 batch loss 1.40580547 epoch total loss 1.19511557\n",
      "Trained batch 696 batch loss 1.39962196 epoch total loss 1.1954093\n",
      "Trained batch 697 batch loss 1.34580493 epoch total loss 1.19562507\n",
      "Trained batch 698 batch loss 1.11720073 epoch total loss 1.19551277\n",
      "Trained batch 699 batch loss 1.20272374 epoch total loss 1.19552302\n",
      "Trained batch 700 batch loss 1.14608955 epoch total loss 1.19545245\n",
      "Trained batch 701 batch loss 1.22629154 epoch total loss 1.19549644\n",
      "Trained batch 702 batch loss 1.2090503 epoch total loss 1.19551575\n",
      "Trained batch 703 batch loss 1.20995009 epoch total loss 1.19553638\n",
      "Trained batch 704 batch loss 1.2919482 epoch total loss 1.19567323\n",
      "Trained batch 705 batch loss 1.35432076 epoch total loss 1.19589829\n",
      "Trained batch 706 batch loss 1.35600471 epoch total loss 1.19612503\n",
      "Trained batch 707 batch loss 1.2051878 epoch total loss 1.19613791\n",
      "Trained batch 708 batch loss 1.1901226 epoch total loss 1.19612944\n",
      "Trained batch 709 batch loss 1.13801551 epoch total loss 1.19604743\n",
      "Trained batch 710 batch loss 1.29471421 epoch total loss 1.19618642\n",
      "Trained batch 711 batch loss 1.22450602 epoch total loss 1.19622624\n",
      "Trained batch 712 batch loss 1.21450365 epoch total loss 1.19625187\n",
      "Trained batch 713 batch loss 1.16765237 epoch total loss 1.19621181\n",
      "Trained batch 714 batch loss 1.1462934 epoch total loss 1.19614184\n",
      "Trained batch 715 batch loss 1.20047259 epoch total loss 1.19614792\n",
      "Trained batch 716 batch loss 1.2000761 epoch total loss 1.1961534\n",
      "Trained batch 717 batch loss 1.19254375 epoch total loss 1.1961484\n",
      "Trained batch 718 batch loss 1.23113394 epoch total loss 1.19619715\n",
      "Trained batch 719 batch loss 1.26947415 epoch total loss 1.19629908\n",
      "Trained batch 720 batch loss 1.30643797 epoch total loss 1.19645202\n",
      "Trained batch 721 batch loss 1.24112439 epoch total loss 1.19651413\n",
      "Trained batch 722 batch loss 1.29653907 epoch total loss 1.19665253\n",
      "Trained batch 723 batch loss 1.27513742 epoch total loss 1.19676113\n",
      "Trained batch 724 batch loss 1.26256585 epoch total loss 1.19685209\n",
      "Trained batch 725 batch loss 1.26529825 epoch total loss 1.1969465\n",
      "Trained batch 726 batch loss 1.19835794 epoch total loss 1.19694841\n",
      "Trained batch 727 batch loss 1.2160337 epoch total loss 1.19697464\n",
      "Trained batch 728 batch loss 1.32711887 epoch total loss 1.19715345\n",
      "Trained batch 729 batch loss 1.29957783 epoch total loss 1.19729388\n",
      "Trained batch 730 batch loss 1.26276779 epoch total loss 1.19738364\n",
      "Trained batch 731 batch loss 1.15271604 epoch total loss 1.19732249\n",
      "Trained batch 732 batch loss 1.10223174 epoch total loss 1.19719255\n",
      "Trained batch 733 batch loss 1.04224277 epoch total loss 1.19698119\n",
      "Trained batch 734 batch loss 1.17651606 epoch total loss 1.1969533\n",
      "Trained batch 735 batch loss 1.13184714 epoch total loss 1.19686472\n",
      "Trained batch 736 batch loss 1.25482893 epoch total loss 1.1969434\n",
      "Trained batch 737 batch loss 1.23237729 epoch total loss 1.19699156\n",
      "Trained batch 738 batch loss 1.21092963 epoch total loss 1.1970104\n",
      "Trained batch 739 batch loss 1.13233376 epoch total loss 1.1969229\n",
      "Trained batch 740 batch loss 1.16965806 epoch total loss 1.19688606\n",
      "Trained batch 741 batch loss 1.16868544 epoch total loss 1.19684803\n",
      "Trained batch 742 batch loss 1.12689471 epoch total loss 1.19675374\n",
      "Trained batch 743 batch loss 1.12463105 epoch total loss 1.1966567\n",
      "Trained batch 744 batch loss 1.22974527 epoch total loss 1.19670117\n",
      "Trained batch 745 batch loss 1.06872344 epoch total loss 1.19652939\n",
      "Trained batch 746 batch loss 1.17810297 epoch total loss 1.19650459\n",
      "Trained batch 747 batch loss 0.89491868 epoch total loss 1.19610095\n",
      "Trained batch 748 batch loss 1.04754591 epoch total loss 1.19590235\n",
      "Trained batch 749 batch loss 1.20559406 epoch total loss 1.19591522\n",
      "Trained batch 750 batch loss 1.17933917 epoch total loss 1.19589305\n",
      "Trained batch 751 batch loss 1.14990759 epoch total loss 1.19583178\n",
      "Trained batch 752 batch loss 1.16627479 epoch total loss 1.19579256\n",
      "Trained batch 753 batch loss 1.14146399 epoch total loss 1.19572031\n",
      "Trained batch 754 batch loss 1.08798754 epoch total loss 1.1955775\n",
      "Trained batch 755 batch loss 1.09390152 epoch total loss 1.1954428\n",
      "Trained batch 756 batch loss 1.15957046 epoch total loss 1.19539535\n",
      "Trained batch 757 batch loss 1.23191023 epoch total loss 1.19544363\n",
      "Trained batch 758 batch loss 1.10941541 epoch total loss 1.19533014\n",
      "Trained batch 759 batch loss 1.10664797 epoch total loss 1.19521332\n",
      "Trained batch 760 batch loss 1.12454343 epoch total loss 1.19512033\n",
      "Trained batch 761 batch loss 1.17392826 epoch total loss 1.19509256\n",
      "Trained batch 762 batch loss 1.16112673 epoch total loss 1.19504797\n",
      "Trained batch 763 batch loss 1.23487473 epoch total loss 1.19510007\n",
      "Trained batch 764 batch loss 1.15286207 epoch total loss 1.19504476\n",
      "Trained batch 765 batch loss 1.17304373 epoch total loss 1.19501603\n",
      "Trained batch 766 batch loss 1.13390553 epoch total loss 1.19493628\n",
      "Trained batch 767 batch loss 1.18740869 epoch total loss 1.1949265\n",
      "Trained batch 768 batch loss 1.15125048 epoch total loss 1.19486964\n",
      "Trained batch 769 batch loss 1.20614433 epoch total loss 1.19488418\n",
      "Trained batch 770 batch loss 1.15480411 epoch total loss 1.19483209\n",
      "Trained batch 771 batch loss 1.04914725 epoch total loss 1.19464314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 772 batch loss 1.10376406 epoch total loss 1.19452548\n",
      "Trained batch 773 batch loss 1.16119063 epoch total loss 1.19448233\n",
      "Trained batch 774 batch loss 1.22449851 epoch total loss 1.19452107\n",
      "Trained batch 775 batch loss 1.27545285 epoch total loss 1.1946255\n",
      "Trained batch 776 batch loss 1.07683313 epoch total loss 1.19447374\n",
      "Trained batch 777 batch loss 0.938095033 epoch total loss 1.19414377\n",
      "Trained batch 778 batch loss 0.927333355 epoch total loss 1.19380081\n",
      "Trained batch 779 batch loss 0.943640172 epoch total loss 1.19347978\n",
      "Trained batch 780 batch loss 1.05557847 epoch total loss 1.19330299\n",
      "Trained batch 781 batch loss 1.03732562 epoch total loss 1.19310331\n",
      "Trained batch 782 batch loss 1.05456424 epoch total loss 1.19292617\n",
      "Trained batch 783 batch loss 1.17996156 epoch total loss 1.19290948\n",
      "Trained batch 784 batch loss 1.17203546 epoch total loss 1.1928829\n",
      "Trained batch 785 batch loss 1.08119631 epoch total loss 1.19274068\n",
      "Trained batch 786 batch loss 1.09887064 epoch total loss 1.19262123\n",
      "Trained batch 787 batch loss 1.06445551 epoch total loss 1.19245839\n",
      "Trained batch 788 batch loss 1.13686168 epoch total loss 1.19238782\n",
      "Trained batch 789 batch loss 1.14158642 epoch total loss 1.19232345\n",
      "Trained batch 790 batch loss 1.07192469 epoch total loss 1.19217098\n",
      "Trained batch 791 batch loss 1.17883587 epoch total loss 1.19215405\n",
      "Trained batch 792 batch loss 1.28391266 epoch total loss 1.19227\n",
      "Trained batch 793 batch loss 1.21043301 epoch total loss 1.19229293\n",
      "Trained batch 794 batch loss 1.17716575 epoch total loss 1.19227386\n",
      "Trained batch 795 batch loss 1.13626647 epoch total loss 1.19220352\n",
      "Trained batch 796 batch loss 1.16698384 epoch total loss 1.19217181\n",
      "Trained batch 797 batch loss 1.33463073 epoch total loss 1.19235063\n",
      "Trained batch 798 batch loss 1.12282932 epoch total loss 1.19226336\n",
      "Trained batch 799 batch loss 1.30970609 epoch total loss 1.19241035\n",
      "Trained batch 800 batch loss 1.09727561 epoch total loss 1.1922915\n",
      "Trained batch 801 batch loss 1.15737212 epoch total loss 1.19224787\n",
      "Trained batch 802 batch loss 1.10762525 epoch total loss 1.19214237\n",
      "Trained batch 803 batch loss 1.20463586 epoch total loss 1.19215786\n",
      "Trained batch 804 batch loss 1.16177154 epoch total loss 1.19212008\n",
      "Trained batch 805 batch loss 1.2474643 epoch total loss 1.19218874\n",
      "Trained batch 806 batch loss 1.16874826 epoch total loss 1.19215977\n",
      "Trained batch 807 batch loss 1.30764174 epoch total loss 1.19230282\n",
      "Trained batch 808 batch loss 1.42469871 epoch total loss 1.19259036\n",
      "Trained batch 809 batch loss 1.32072854 epoch total loss 1.19274879\n",
      "Trained batch 810 batch loss 1.23629439 epoch total loss 1.19280255\n",
      "Trained batch 811 batch loss 1.11365461 epoch total loss 1.19270492\n",
      "Trained batch 812 batch loss 0.90272969 epoch total loss 1.19234776\n",
      "Trained batch 813 batch loss 1.01853406 epoch total loss 1.19213402\n",
      "Trained batch 814 batch loss 1.2135787 epoch total loss 1.19216037\n",
      "Trained batch 815 batch loss 0.973567486 epoch total loss 1.19189215\n",
      "Trained batch 816 batch loss 0.945524 epoch total loss 1.19159019\n",
      "Trained batch 817 batch loss 0.888415217 epoch total loss 1.19121909\n",
      "Trained batch 818 batch loss 1.01656401 epoch total loss 1.19100559\n",
      "Trained batch 819 batch loss 1.11287344 epoch total loss 1.1909101\n",
      "Trained batch 820 batch loss 1.03562665 epoch total loss 1.1907208\n",
      "Trained batch 821 batch loss 1.19889534 epoch total loss 1.19073081\n",
      "Trained batch 822 batch loss 1.15988839 epoch total loss 1.19069326\n",
      "Trained batch 823 batch loss 1.23217595 epoch total loss 1.19074368\n",
      "Trained batch 824 batch loss 1.29881883 epoch total loss 1.19087481\n",
      "Trained batch 825 batch loss 1.0981338 epoch total loss 1.1907624\n",
      "Trained batch 826 batch loss 1.10282063 epoch total loss 1.19065607\n",
      "Trained batch 827 batch loss 0.982720494 epoch total loss 1.19040465\n",
      "Trained batch 828 batch loss 1.19289112 epoch total loss 1.19040751\n",
      "Trained batch 829 batch loss 1.23454201 epoch total loss 1.1904608\n",
      "Trained batch 830 batch loss 1.28616691 epoch total loss 1.1905762\n",
      "Trained batch 831 batch loss 1.24060059 epoch total loss 1.1906364\n",
      "Trained batch 832 batch loss 1.23502576 epoch total loss 1.1906898\n",
      "Trained batch 833 batch loss 1.20200419 epoch total loss 1.19070339\n",
      "Trained batch 834 batch loss 1.31065154 epoch total loss 1.19084716\n",
      "Trained batch 835 batch loss 1.19298935 epoch total loss 1.19084978\n",
      "Trained batch 836 batch loss 1.18608546 epoch total loss 1.19084406\n",
      "Trained batch 837 batch loss 1.1362859 epoch total loss 1.19077885\n",
      "Trained batch 838 batch loss 1.15947127 epoch total loss 1.19074154\n",
      "Trained batch 839 batch loss 1.05347586 epoch total loss 1.19057798\n",
      "Trained batch 840 batch loss 1.07674348 epoch total loss 1.19044244\n",
      "Trained batch 841 batch loss 1.18292594 epoch total loss 1.1904335\n",
      "Trained batch 842 batch loss 1.09368122 epoch total loss 1.19031858\n",
      "Trained batch 843 batch loss 1.08363533 epoch total loss 1.19019198\n",
      "Trained batch 844 batch loss 1.08540165 epoch total loss 1.19006777\n",
      "Trained batch 845 batch loss 1.09490407 epoch total loss 1.18995523\n",
      "Trained batch 846 batch loss 1.15902221 epoch total loss 1.18991864\n",
      "Trained batch 847 batch loss 1.2143302 epoch total loss 1.18994749\n",
      "Trained batch 848 batch loss 1.23873425 epoch total loss 1.19000494\n",
      "Trained batch 849 batch loss 1.23254824 epoch total loss 1.19005501\n",
      "Trained batch 850 batch loss 1.15108061 epoch total loss 1.19000924\n",
      "Trained batch 851 batch loss 1.17228973 epoch total loss 1.18998837\n",
      "Trained batch 852 batch loss 1.12589216 epoch total loss 1.18991315\n",
      "Trained batch 853 batch loss 1.05717516 epoch total loss 1.18975759\n",
      "Trained batch 854 batch loss 1.18550801 epoch total loss 1.18975258\n",
      "Trained batch 855 batch loss 1.12114036 epoch total loss 1.18967235\n",
      "Trained batch 856 batch loss 1.10941744 epoch total loss 1.18957865\n",
      "Trained batch 857 batch loss 1.14692354 epoch total loss 1.18952882\n",
      "Trained batch 858 batch loss 1.12817645 epoch total loss 1.1894573\n",
      "Trained batch 859 batch loss 1.05250752 epoch total loss 1.18929791\n",
      "Trained batch 860 batch loss 1.1978507 epoch total loss 1.18930781\n",
      "Trained batch 861 batch loss 1.15842605 epoch total loss 1.18927205\n",
      "Trained batch 862 batch loss 1.06862211 epoch total loss 1.18913209\n",
      "Trained batch 863 batch loss 1.12099338 epoch total loss 1.18905306\n",
      "Trained batch 864 batch loss 1.13796878 epoch total loss 1.18899393\n",
      "Trained batch 865 batch loss 1.11561203 epoch total loss 1.18890905\n",
      "Trained batch 866 batch loss 1.24950588 epoch total loss 1.18897903\n",
      "Trained batch 867 batch loss 1.21959066 epoch total loss 1.18901443\n",
      "Trained batch 868 batch loss 1.1378777 epoch total loss 1.18895543\n",
      "Trained batch 869 batch loss 1.14583981 epoch total loss 1.18890584\n",
      "Trained batch 870 batch loss 1.08135116 epoch total loss 1.18878222\n",
      "Trained batch 871 batch loss 1.22037804 epoch total loss 1.18881834\n",
      "Trained batch 872 batch loss 1.22392154 epoch total loss 1.18885863\n",
      "Trained batch 873 batch loss 1.10056782 epoch total loss 1.18875742\n",
      "Trained batch 874 batch loss 1.143695 epoch total loss 1.18870592\n",
      "Trained batch 875 batch loss 1.06436837 epoch total loss 1.1885637\n",
      "Trained batch 876 batch loss 1.14663589 epoch total loss 1.1885159\n",
      "Trained batch 877 batch loss 1.17708206 epoch total loss 1.18850291\n",
      "Trained batch 878 batch loss 1.29529488 epoch total loss 1.1886245\n",
      "Trained batch 879 batch loss 1.0049268 epoch total loss 1.18841541\n",
      "Trained batch 880 batch loss 1.07868183 epoch total loss 1.18829083\n",
      "Trained batch 881 batch loss 1.17291188 epoch total loss 1.18827331\n",
      "Trained batch 882 batch loss 1.16198456 epoch total loss 1.18824351\n",
      "Trained batch 883 batch loss 1.14841104 epoch total loss 1.18819845\n",
      "Trained batch 884 batch loss 1.18469214 epoch total loss 1.18819439\n",
      "Trained batch 885 batch loss 1.13334656 epoch total loss 1.18813241\n",
      "Trained batch 886 batch loss 1.06004667 epoch total loss 1.18798792\n",
      "Trained batch 887 batch loss 1.06604075 epoch total loss 1.18785036\n",
      "Trained batch 888 batch loss 1.09409285 epoch total loss 1.18774486\n",
      "Trained batch 889 batch loss 1.07550144 epoch total loss 1.18761861\n",
      "Trained batch 890 batch loss 1.08632803 epoch total loss 1.18750477\n",
      "Trained batch 891 batch loss 1.0528363 epoch total loss 1.18735373\n",
      "Trained batch 892 batch loss 1.01554239 epoch total loss 1.18716097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 893 batch loss 1.13089108 epoch total loss 1.18709803\n",
      "Trained batch 894 batch loss 1.06069422 epoch total loss 1.18695652\n",
      "Trained batch 895 batch loss 1.1227473 epoch total loss 1.18688488\n",
      "Trained batch 896 batch loss 1.33117831 epoch total loss 1.18704593\n",
      "Trained batch 897 batch loss 1.19750357 epoch total loss 1.18705761\n",
      "Trained batch 898 batch loss 1.33192956 epoch total loss 1.1872189\n",
      "Trained batch 899 batch loss 1.21735632 epoch total loss 1.18725252\n",
      "Trained batch 900 batch loss 1.14868379 epoch total loss 1.18720961\n",
      "Trained batch 901 batch loss 1.06115448 epoch total loss 1.18706965\n",
      "Trained batch 902 batch loss 1.0595969 epoch total loss 1.18692839\n",
      "Trained batch 903 batch loss 1.15838397 epoch total loss 1.18689668\n",
      "Trained batch 904 batch loss 1.23925877 epoch total loss 1.18695462\n",
      "Trained batch 905 batch loss 1.31722224 epoch total loss 1.18709862\n",
      "Trained batch 906 batch loss 1.19200623 epoch total loss 1.18710399\n",
      "Trained batch 907 batch loss 1.18804812 epoch total loss 1.18710494\n",
      "Trained batch 908 batch loss 1.16897702 epoch total loss 1.18708503\n",
      "Trained batch 909 batch loss 1.1767472 epoch total loss 1.18707359\n",
      "Trained batch 910 batch loss 1.09954262 epoch total loss 1.18697739\n",
      "Trained batch 911 batch loss 1.12904489 epoch total loss 1.18691373\n",
      "Trained batch 912 batch loss 1.14143479 epoch total loss 1.1868639\n",
      "Trained batch 913 batch loss 1.13868964 epoch total loss 1.18681121\n",
      "Trained batch 914 batch loss 1.21682191 epoch total loss 1.18684399\n",
      "Trained batch 915 batch loss 1.29246986 epoch total loss 1.18695939\n",
      "Trained batch 916 batch loss 1.11979103 epoch total loss 1.18688607\n",
      "Trained batch 917 batch loss 1.15914178 epoch total loss 1.18685579\n",
      "Trained batch 918 batch loss 1.16303241 epoch total loss 1.18682992\n",
      "Trained batch 919 batch loss 1.20965314 epoch total loss 1.18685472\n",
      "Trained batch 920 batch loss 1.18606091 epoch total loss 1.18685377\n",
      "Trained batch 921 batch loss 1.3402493 epoch total loss 1.1870203\n",
      "Trained batch 922 batch loss 1.12059116 epoch total loss 1.1869483\n",
      "Trained batch 923 batch loss 1.15737581 epoch total loss 1.18691623\n",
      "Trained batch 924 batch loss 1.11294484 epoch total loss 1.18683612\n",
      "Trained batch 925 batch loss 1.2115953 epoch total loss 1.18686283\n",
      "Trained batch 926 batch loss 1.24933 epoch total loss 1.18693042\n",
      "Trained batch 927 batch loss 1.20514679 epoch total loss 1.18695009\n",
      "Trained batch 928 batch loss 1.35906744 epoch total loss 1.18713546\n",
      "Trained batch 929 batch loss 1.33062983 epoch total loss 1.18729007\n",
      "Trained batch 930 batch loss 1.21395087 epoch total loss 1.18731868\n",
      "Trained batch 931 batch loss 1.28804684 epoch total loss 1.18742692\n",
      "Trained batch 932 batch loss 1.15990233 epoch total loss 1.18739748\n",
      "Trained batch 933 batch loss 1.05816031 epoch total loss 1.18725884\n",
      "Trained batch 934 batch loss 1.17981434 epoch total loss 1.18725085\n",
      "Trained batch 935 batch loss 1.19918764 epoch total loss 1.18726373\n",
      "Trained batch 936 batch loss 1.22781944 epoch total loss 1.187307\n",
      "Trained batch 937 batch loss 1.34938025 epoch total loss 1.18748\n",
      "Trained batch 938 batch loss 1.24585962 epoch total loss 1.1875422\n",
      "Trained batch 939 batch loss 1.47093296 epoch total loss 1.18784404\n",
      "Trained batch 940 batch loss 1.33143044 epoch total loss 1.18799675\n",
      "Trained batch 941 batch loss 1.04819107 epoch total loss 1.18784821\n",
      "Trained batch 942 batch loss 1.19041228 epoch total loss 1.18785095\n",
      "Trained batch 943 batch loss 1.20884728 epoch total loss 1.18787324\n",
      "Trained batch 944 batch loss 1.08992982 epoch total loss 1.18776953\n",
      "Trained batch 945 batch loss 0.99771595 epoch total loss 1.18756831\n",
      "Trained batch 946 batch loss 0.992308855 epoch total loss 1.18736196\n",
      "Trained batch 947 batch loss 1.07848704 epoch total loss 1.18724692\n",
      "Trained batch 948 batch loss 1.02347 epoch total loss 1.18707418\n",
      "Trained batch 949 batch loss 0.969176531 epoch total loss 1.18684447\n",
      "Trained batch 950 batch loss 1.03667951 epoch total loss 1.1866864\n",
      "Trained batch 951 batch loss 1.18925178 epoch total loss 1.18668902\n",
      "Trained batch 952 batch loss 1.0934875 epoch total loss 1.18659115\n",
      "Trained batch 953 batch loss 0.988319278 epoch total loss 1.18638301\n",
      "Trained batch 954 batch loss 1.16911674 epoch total loss 1.18636489\n",
      "Trained batch 955 batch loss 1.19157636 epoch total loss 1.18637025\n",
      "Trained batch 956 batch loss 1.12845135 epoch total loss 1.1863097\n",
      "Trained batch 957 batch loss 1.24644578 epoch total loss 1.18637252\n",
      "Trained batch 958 batch loss 1.29518485 epoch total loss 1.18648612\n",
      "Trained batch 959 batch loss 1.15798235 epoch total loss 1.18645632\n",
      "Trained batch 960 batch loss 1.06353867 epoch total loss 1.18632841\n",
      "Trained batch 961 batch loss 1.21596563 epoch total loss 1.18635917\n",
      "Trained batch 962 batch loss 1.18378723 epoch total loss 1.18635654\n",
      "Trained batch 963 batch loss 1.23043978 epoch total loss 1.18640244\n",
      "Trained batch 964 batch loss 1.15926719 epoch total loss 1.18637431\n",
      "Trained batch 965 batch loss 1.18554413 epoch total loss 1.18637335\n",
      "Trained batch 966 batch loss 1.30702138 epoch total loss 1.18649828\n",
      "Trained batch 967 batch loss 1.1067059 epoch total loss 1.18641579\n",
      "Trained batch 968 batch loss 1.05515409 epoch total loss 1.18628013\n",
      "Trained batch 969 batch loss 1.24810708 epoch total loss 1.18634391\n",
      "Trained batch 970 batch loss 1.18498206 epoch total loss 1.18634248\n",
      "Trained batch 971 batch loss 1.18310118 epoch total loss 1.18633914\n",
      "Trained batch 972 batch loss 1.19318867 epoch total loss 1.18634629\n",
      "Trained batch 973 batch loss 1.15842187 epoch total loss 1.18631756\n",
      "Trained batch 974 batch loss 1.11101007 epoch total loss 1.1862402\n",
      "Trained batch 975 batch loss 1.18625963 epoch total loss 1.1862402\n",
      "Trained batch 976 batch loss 1.30035877 epoch total loss 1.18635726\n",
      "Trained batch 977 batch loss 1.10285985 epoch total loss 1.18627179\n",
      "Trained batch 978 batch loss 1.36038888 epoch total loss 1.18644977\n",
      "Trained batch 979 batch loss 1.36335683 epoch total loss 1.18663049\n",
      "Trained batch 980 batch loss 1.31814289 epoch total loss 1.18676472\n",
      "Trained batch 981 batch loss 1.19964075 epoch total loss 1.18677783\n",
      "Trained batch 982 batch loss 1.30854 epoch total loss 1.18690181\n",
      "Trained batch 983 batch loss 1.18403184 epoch total loss 1.18689895\n",
      "Trained batch 984 batch loss 1.08175528 epoch total loss 1.18679214\n",
      "Trained batch 985 batch loss 1.21083534 epoch total loss 1.18681657\n",
      "Trained batch 986 batch loss 1.24938655 epoch total loss 1.18688\n",
      "Trained batch 987 batch loss 1.08004189 epoch total loss 1.18677175\n",
      "Trained batch 988 batch loss 1.11750102 epoch total loss 1.18670177\n",
      "Trained batch 989 batch loss 1.01789165 epoch total loss 1.18653107\n",
      "Trained batch 990 batch loss 0.97714293 epoch total loss 1.18631959\n",
      "Trained batch 991 batch loss 1.08480453 epoch total loss 1.18621719\n",
      "Trained batch 992 batch loss 1.40989339 epoch total loss 1.18644273\n",
      "Trained batch 993 batch loss 1.43243408 epoch total loss 1.18669033\n",
      "Trained batch 994 batch loss 1.47099781 epoch total loss 1.18697631\n",
      "Trained batch 995 batch loss 1.20692623 epoch total loss 1.18699634\n",
      "Trained batch 996 batch loss 1.32691753 epoch total loss 1.18713689\n",
      "Trained batch 997 batch loss 1.39240694 epoch total loss 1.18734276\n",
      "Trained batch 998 batch loss 1.33325553 epoch total loss 1.18748903\n",
      "Trained batch 999 batch loss 1.06952834 epoch total loss 1.18737102\n",
      "Trained batch 1000 batch loss 1.31022155 epoch total loss 1.1874938\n",
      "Trained batch 1001 batch loss 1.15494478 epoch total loss 1.18746126\n",
      "Trained batch 1002 batch loss 1.18660641 epoch total loss 1.18746042\n",
      "Trained batch 1003 batch loss 1.03736532 epoch total loss 1.1873107\n",
      "Trained batch 1004 batch loss 1.18283308 epoch total loss 1.18730628\n",
      "Trained batch 1005 batch loss 1.05848646 epoch total loss 1.18717813\n",
      "Trained batch 1006 batch loss 1.14282775 epoch total loss 1.18713403\n",
      "Trained batch 1007 batch loss 1.18323803 epoch total loss 1.18713009\n",
      "Trained batch 1008 batch loss 1.07237303 epoch total loss 1.18701637\n",
      "Trained batch 1009 batch loss 1.15809417 epoch total loss 1.18698764\n",
      "Trained batch 1010 batch loss 0.992833912 epoch total loss 1.18679535\n",
      "Trained batch 1011 batch loss 1.15610278 epoch total loss 1.18676507\n",
      "Trained batch 1012 batch loss 1.20347655 epoch total loss 1.18678153\n",
      "Trained batch 1013 batch loss 1.18993008 epoch total loss 1.18678474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1014 batch loss 1.16269112 epoch total loss 1.1867609\n",
      "Trained batch 1015 batch loss 1.16051769 epoch total loss 1.18673515\n",
      "Trained batch 1016 batch loss 1.22215319 epoch total loss 1.18677\n",
      "Trained batch 1017 batch loss 1.35421956 epoch total loss 1.18693471\n",
      "Trained batch 1018 batch loss 1.24657989 epoch total loss 1.18699324\n",
      "Trained batch 1019 batch loss 1.15769649 epoch total loss 1.18696451\n",
      "Trained batch 1020 batch loss 1.29461586 epoch total loss 1.18707\n",
      "Trained batch 1021 batch loss 1.27783787 epoch total loss 1.18715894\n",
      "Trained batch 1022 batch loss 1.2593646 epoch total loss 1.18722963\n",
      "Trained batch 1023 batch loss 1.15309203 epoch total loss 1.18719625\n",
      "Trained batch 1024 batch loss 1.17135978 epoch total loss 1.18718076\n",
      "Trained batch 1025 batch loss 1.11258614 epoch total loss 1.18710792\n",
      "Trained batch 1026 batch loss 1.11915088 epoch total loss 1.18704176\n",
      "Trained batch 1027 batch loss 1.18703675 epoch total loss 1.18704164\n",
      "Trained batch 1028 batch loss 1.19032204 epoch total loss 1.18704486\n",
      "Trained batch 1029 batch loss 1.17801642 epoch total loss 1.18703604\n",
      "Trained batch 1030 batch loss 1.21575284 epoch total loss 1.18706381\n",
      "Trained batch 1031 batch loss 1.14963877 epoch total loss 1.18702757\n",
      "Trained batch 1032 batch loss 1.10098171 epoch total loss 1.18694413\n",
      "Trained batch 1033 batch loss 1.21357083 epoch total loss 1.18697\n",
      "Trained batch 1034 batch loss 1.10894322 epoch total loss 1.18689454\n",
      "Trained batch 1035 batch loss 1.12947249 epoch total loss 1.1868391\n",
      "Trained batch 1036 batch loss 1.18335772 epoch total loss 1.18683565\n",
      "Trained batch 1037 batch loss 1.10680568 epoch total loss 1.18675852\n",
      "Trained batch 1038 batch loss 1.1950475 epoch total loss 1.18676651\n",
      "Trained batch 1039 batch loss 1.13242543 epoch total loss 1.18671429\n",
      "Trained batch 1040 batch loss 1.07314456 epoch total loss 1.18660498\n",
      "Trained batch 1041 batch loss 1.0723294 epoch total loss 1.1864953\n",
      "Trained batch 1042 batch loss 1.06510186 epoch total loss 1.18637872\n",
      "Trained batch 1043 batch loss 1.04568505 epoch total loss 1.18624389\n",
      "Trained batch 1044 batch loss 1.14408135 epoch total loss 1.18620336\n",
      "Trained batch 1045 batch loss 1.10891664 epoch total loss 1.18612945\n",
      "Trained batch 1046 batch loss 1.21080351 epoch total loss 1.18615305\n",
      "Trained batch 1047 batch loss 1.22645664 epoch total loss 1.18619156\n",
      "Trained batch 1048 batch loss 1.19994855 epoch total loss 1.18620467\n",
      "Trained batch 1049 batch loss 1.29387069 epoch total loss 1.18630719\n",
      "Trained batch 1050 batch loss 1.2583822 epoch total loss 1.18637586\n",
      "Trained batch 1051 batch loss 1.10996509 epoch total loss 1.18630326\n",
      "Trained batch 1052 batch loss 1.26408529 epoch total loss 1.18637717\n",
      "Trained batch 1053 batch loss 1.2006321 epoch total loss 1.18639076\n",
      "Trained batch 1054 batch loss 1.14906228 epoch total loss 1.18635523\n",
      "Trained batch 1055 batch loss 1.08659983 epoch total loss 1.1862607\n",
      "Trained batch 1056 batch loss 1.27291131 epoch total loss 1.18634272\n",
      "Trained batch 1057 batch loss 1.26785851 epoch total loss 1.18641984\n",
      "Trained batch 1058 batch loss 1.29341972 epoch total loss 1.18652105\n",
      "Trained batch 1059 batch loss 1.25616288 epoch total loss 1.18658674\n",
      "Trained batch 1060 batch loss 1.32869267 epoch total loss 1.18672085\n",
      "Trained batch 1061 batch loss 1.00082946 epoch total loss 1.18654561\n",
      "Trained batch 1062 batch loss 1.22260308 epoch total loss 1.1865797\n",
      "Trained batch 1063 batch loss 1.14423585 epoch total loss 1.18653989\n",
      "Trained batch 1064 batch loss 1.0844897 epoch total loss 1.18644392\n",
      "Trained batch 1065 batch loss 1.11111283 epoch total loss 1.18637311\n",
      "Trained batch 1066 batch loss 1.11581755 epoch total loss 1.18630695\n",
      "Trained batch 1067 batch loss 1.21676767 epoch total loss 1.18633556\n",
      "Trained batch 1068 batch loss 1.17152262 epoch total loss 1.18632174\n",
      "Trained batch 1069 batch loss 1.25400245 epoch total loss 1.18638504\n",
      "Trained batch 1070 batch loss 1.30028486 epoch total loss 1.18649149\n",
      "Trained batch 1071 batch loss 1.32725239 epoch total loss 1.18662298\n",
      "Trained batch 1072 batch loss 1.35103667 epoch total loss 1.1867764\n",
      "Trained batch 1073 batch loss 1.29284632 epoch total loss 1.18687522\n",
      "Trained batch 1074 batch loss 1.14698279 epoch total loss 1.18683803\n",
      "Trained batch 1075 batch loss 1.25361371 epoch total loss 1.18690026\n",
      "Trained batch 1076 batch loss 1.33064401 epoch total loss 1.18703389\n",
      "Trained batch 1077 batch loss 1.22074521 epoch total loss 1.18706512\n",
      "Trained batch 1078 batch loss 1.14839518 epoch total loss 1.18702924\n",
      "Trained batch 1079 batch loss 1.13630426 epoch total loss 1.18698227\n",
      "Trained batch 1080 batch loss 1.08161259 epoch total loss 1.18688476\n",
      "Trained batch 1081 batch loss 1.12532544 epoch total loss 1.1868279\n",
      "Trained batch 1082 batch loss 1.06261158 epoch total loss 1.1867131\n",
      "Trained batch 1083 batch loss 1.09189868 epoch total loss 1.1866256\n",
      "Trained batch 1084 batch loss 1.11286306 epoch total loss 1.18655753\n",
      "Trained batch 1085 batch loss 1.13329673 epoch total loss 1.18650842\n",
      "Trained batch 1086 batch loss 1.34002852 epoch total loss 1.18664992\n",
      "Trained batch 1087 batch loss 1.36959553 epoch total loss 1.18681824\n",
      "Trained batch 1088 batch loss 1.31407344 epoch total loss 1.18693519\n",
      "Trained batch 1089 batch loss 1.38782358 epoch total loss 1.18711972\n",
      "Trained batch 1090 batch loss 1.33564425 epoch total loss 1.18725598\n",
      "Trained batch 1091 batch loss 1.21680355 epoch total loss 1.18728304\n",
      "Trained batch 1092 batch loss 1.1933471 epoch total loss 1.18728864\n",
      "Trained batch 1093 batch loss 1.25729203 epoch total loss 1.18735266\n",
      "Trained batch 1094 batch loss 1.2627244 epoch total loss 1.18742156\n",
      "Trained batch 1095 batch loss 1.18008125 epoch total loss 1.18741488\n",
      "Trained batch 1096 batch loss 1.14115262 epoch total loss 1.18737257\n",
      "Trained batch 1097 batch loss 1.20453346 epoch total loss 1.1873883\n",
      "Trained batch 1098 batch loss 1.1122942 epoch total loss 1.18731987\n",
      "Trained batch 1099 batch loss 1.11695552 epoch total loss 1.18725586\n",
      "Trained batch 1100 batch loss 0.959269 epoch total loss 1.18704855\n",
      "Trained batch 1101 batch loss 1.045892 epoch total loss 1.1869204\n",
      "Trained batch 1102 batch loss 1.09214878 epoch total loss 1.18683434\n",
      "Trained batch 1103 batch loss 1.22917271 epoch total loss 1.18687272\n",
      "Trained batch 1104 batch loss 1.3166219 epoch total loss 1.18699026\n",
      "Trained batch 1105 batch loss 1.2987268 epoch total loss 1.18709135\n",
      "Trained batch 1106 batch loss 1.2574085 epoch total loss 1.18715501\n",
      "Trained batch 1107 batch loss 1.29105949 epoch total loss 1.18724883\n",
      "Trained batch 1108 batch loss 1.07088351 epoch total loss 1.1871438\n",
      "Trained batch 1109 batch loss 1.20941 epoch total loss 1.18716383\n",
      "Trained batch 1110 batch loss 1.1091373 epoch total loss 1.1870935\n",
      "Trained batch 1111 batch loss 1.2255969 epoch total loss 1.18712819\n",
      "Trained batch 1112 batch loss 1.33126569 epoch total loss 1.18725789\n",
      "Trained batch 1113 batch loss 1.41628337 epoch total loss 1.18746364\n",
      "Trained batch 1114 batch loss 1.19346213 epoch total loss 1.18746901\n",
      "Trained batch 1115 batch loss 1.29265702 epoch total loss 1.1875633\n",
      "Trained batch 1116 batch loss 1.24646497 epoch total loss 1.18761611\n",
      "Trained batch 1117 batch loss 1.29874778 epoch total loss 1.18771553\n",
      "Trained batch 1118 batch loss 1.17473078 epoch total loss 1.18770385\n",
      "Trained batch 1119 batch loss 1.19792402 epoch total loss 1.18771291\n",
      "Trained batch 1120 batch loss 1.16149819 epoch total loss 1.18768954\n",
      "Trained batch 1121 batch loss 1.19425869 epoch total loss 1.18769538\n",
      "Trained batch 1122 batch loss 1.24378419 epoch total loss 1.18774533\n",
      "Trained batch 1123 batch loss 1.20309973 epoch total loss 1.18775904\n",
      "Trained batch 1124 batch loss 1.17894065 epoch total loss 1.18775117\n",
      "Trained batch 1125 batch loss 1.10233593 epoch total loss 1.18767524\n",
      "Trained batch 1126 batch loss 1.10291982 epoch total loss 1.1875999\n",
      "Trained batch 1127 batch loss 1.04581642 epoch total loss 1.18747413\n",
      "Trained batch 1128 batch loss 1.16900349 epoch total loss 1.18745768\n",
      "Trained batch 1129 batch loss 1.22390199 epoch total loss 1.18749\n",
      "Trained batch 1130 batch loss 1.09715354 epoch total loss 1.18741\n",
      "Trained batch 1131 batch loss 1.26196432 epoch total loss 1.18747592\n",
      "Trained batch 1132 batch loss 1.29059422 epoch total loss 1.18756711\n",
      "Trained batch 1133 batch loss 1.23385012 epoch total loss 1.187608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1134 batch loss 1.14446688 epoch total loss 1.18756986\n",
      "Trained batch 1135 batch loss 1.14713061 epoch total loss 1.18753421\n",
      "Trained batch 1136 batch loss 1.10575342 epoch total loss 1.18746221\n",
      "Trained batch 1137 batch loss 1.01672256 epoch total loss 1.18731201\n",
      "Trained batch 1138 batch loss 1.26407242 epoch total loss 1.18737948\n",
      "Trained batch 1139 batch loss 1.2350868 epoch total loss 1.18742132\n",
      "Trained batch 1140 batch loss 1.29369545 epoch total loss 1.18751454\n",
      "Trained batch 1141 batch loss 1.18657279 epoch total loss 1.18751371\n",
      "Trained batch 1142 batch loss 1.19358158 epoch total loss 1.18751907\n",
      "Trained batch 1143 batch loss 1.20245254 epoch total loss 1.18753207\n",
      "Trained batch 1144 batch loss 1.12345183 epoch total loss 1.18747604\n",
      "Trained batch 1145 batch loss 1.04396427 epoch total loss 1.18735063\n",
      "Trained batch 1146 batch loss 1.10201812 epoch total loss 1.18727624\n",
      "Trained batch 1147 batch loss 1.23639 epoch total loss 1.18731904\n",
      "Trained batch 1148 batch loss 1.20646465 epoch total loss 1.18733573\n",
      "Trained batch 1149 batch loss 1.21932852 epoch total loss 1.18736362\n",
      "Trained batch 1150 batch loss 1.20606172 epoch total loss 1.18737984\n",
      "Trained batch 1151 batch loss 1.25523794 epoch total loss 1.18743885\n",
      "Trained batch 1152 batch loss 1.17501235 epoch total loss 1.187428\n",
      "Trained batch 1153 batch loss 1.22566211 epoch total loss 1.18746126\n",
      "Trained batch 1154 batch loss 1.26238227 epoch total loss 1.18752611\n",
      "Trained batch 1155 batch loss 1.21082163 epoch total loss 1.18754625\n",
      "Trained batch 1156 batch loss 1.17299032 epoch total loss 1.18753374\n",
      "Trained batch 1157 batch loss 1.19781435 epoch total loss 1.18754256\n",
      "Trained batch 1158 batch loss 1.22460115 epoch total loss 1.18757451\n",
      "Trained batch 1159 batch loss 1.12269533 epoch total loss 1.18751848\n",
      "Trained batch 1160 batch loss 1.11055171 epoch total loss 1.1874522\n",
      "Trained batch 1161 batch loss 1.15967846 epoch total loss 1.18742824\n",
      "Trained batch 1162 batch loss 1.15525973 epoch total loss 1.18740058\n",
      "Trained batch 1163 batch loss 1.06317759 epoch total loss 1.18729389\n",
      "Trained batch 1164 batch loss 1.03768826 epoch total loss 1.18716538\n",
      "Trained batch 1165 batch loss 1.01930499 epoch total loss 1.18702126\n",
      "Trained batch 1166 batch loss 1.03963459 epoch total loss 1.18689489\n",
      "Trained batch 1167 batch loss 1.17860389 epoch total loss 1.18688774\n",
      "Trained batch 1168 batch loss 1.22927606 epoch total loss 1.18692398\n",
      "Trained batch 1169 batch loss 1.11590028 epoch total loss 1.18686318\n",
      "Trained batch 1170 batch loss 1.28965819 epoch total loss 1.18695116\n",
      "Trained batch 1171 batch loss 1.19303322 epoch total loss 1.18695629\n",
      "Trained batch 1172 batch loss 1.14143014 epoch total loss 1.18691742\n",
      "Trained batch 1173 batch loss 1.1859293 epoch total loss 1.18691659\n",
      "Trained batch 1174 batch loss 1.21912372 epoch total loss 1.18694401\n",
      "Trained batch 1175 batch loss 1.26516008 epoch total loss 1.18701053\n",
      "Trained batch 1176 batch loss 1.24810743 epoch total loss 1.1870625\n",
      "Trained batch 1177 batch loss 1.20091057 epoch total loss 1.1870743\n",
      "Trained batch 1178 batch loss 1.20866895 epoch total loss 1.18709254\n",
      "Trained batch 1179 batch loss 1.34670711 epoch total loss 1.18722785\n",
      "Trained batch 1180 batch loss 1.21895063 epoch total loss 1.18725479\n",
      "Trained batch 1181 batch loss 1.29688644 epoch total loss 1.18734765\n",
      "Trained batch 1182 batch loss 1.26185107 epoch total loss 1.18741071\n",
      "Trained batch 1183 batch loss 1.20674467 epoch total loss 1.18742704\n",
      "Trained batch 1184 batch loss 1.25790823 epoch total loss 1.18748665\n",
      "Trained batch 1185 batch loss 1.31083536 epoch total loss 1.1875906\n",
      "Trained batch 1186 batch loss 1.19419384 epoch total loss 1.1875962\n",
      "Trained batch 1187 batch loss 1.22449601 epoch total loss 1.18762732\n",
      "Trained batch 1188 batch loss 1.17422795 epoch total loss 1.18761599\n",
      "Trained batch 1189 batch loss 1.19570625 epoch total loss 1.18762279\n",
      "Trained batch 1190 batch loss 1.16048658 epoch total loss 1.1876\n",
      "Trained batch 1191 batch loss 1.21827102 epoch total loss 1.18762577\n",
      "Trained batch 1192 batch loss 1.14185405 epoch total loss 1.18758738\n",
      "Trained batch 1193 batch loss 1.11700392 epoch total loss 1.18752813\n",
      "Trained batch 1194 batch loss 1.199862 epoch total loss 1.18753839\n",
      "Trained batch 1195 batch loss 1.09339094 epoch total loss 1.18745971\n",
      "Trained batch 1196 batch loss 1.28207803 epoch total loss 1.18753874\n",
      "Trained batch 1197 batch loss 1.13322663 epoch total loss 1.18749332\n",
      "Trained batch 1198 batch loss 1.08079958 epoch total loss 1.18740427\n",
      "Trained batch 1199 batch loss 1.16196322 epoch total loss 1.18738317\n",
      "Trained batch 1200 batch loss 1.14539266 epoch total loss 1.18734813\n",
      "Trained batch 1201 batch loss 1.04308128 epoch total loss 1.18722796\n",
      "Trained batch 1202 batch loss 1.05480504 epoch total loss 1.18711782\n",
      "Trained batch 1203 batch loss 0.96093905 epoch total loss 1.18692982\n",
      "Trained batch 1204 batch loss 0.977386594 epoch total loss 1.18675578\n",
      "Trained batch 1205 batch loss 1.13443124 epoch total loss 1.18671238\n",
      "Trained batch 1206 batch loss 0.972554 epoch total loss 1.18653476\n",
      "Trained batch 1207 batch loss 1.0233705 epoch total loss 1.18639958\n",
      "Trained batch 1208 batch loss 1.10373354 epoch total loss 1.18633115\n",
      "Trained batch 1209 batch loss 1.08142436 epoch total loss 1.18624437\n",
      "Trained batch 1210 batch loss 1.25953281 epoch total loss 1.18630493\n",
      "Trained batch 1211 batch loss 1.1464684 epoch total loss 1.18627203\n",
      "Trained batch 1212 batch loss 1.25063896 epoch total loss 1.18632507\n",
      "Trained batch 1213 batch loss 1.33308327 epoch total loss 1.18644619\n",
      "Trained batch 1214 batch loss 1.19456697 epoch total loss 1.18645287\n",
      "Trained batch 1215 batch loss 1.09874356 epoch total loss 1.18638062\n",
      "Trained batch 1216 batch loss 1.11977339 epoch total loss 1.18632591\n",
      "Trained batch 1217 batch loss 1.0851419 epoch total loss 1.1862427\n",
      "Trained batch 1218 batch loss 1.02122045 epoch total loss 1.18610716\n",
      "Trained batch 1219 batch loss 1.01055896 epoch total loss 1.18596315\n",
      "Trained batch 1220 batch loss 0.988747478 epoch total loss 1.18580151\n",
      "Trained batch 1221 batch loss 1.05846453 epoch total loss 1.1856972\n",
      "Trained batch 1222 batch loss 1.0076648 epoch total loss 1.18555152\n",
      "Trained batch 1223 batch loss 1.05935 epoch total loss 1.18544841\n",
      "Trained batch 1224 batch loss 1.13304651 epoch total loss 1.18540561\n",
      "Trained batch 1225 batch loss 1.0647527 epoch total loss 1.18530703\n",
      "Trained batch 1226 batch loss 1.1072818 epoch total loss 1.18524337\n",
      "Trained batch 1227 batch loss 1.18820035 epoch total loss 1.18524587\n",
      "Trained batch 1228 batch loss 1.24369192 epoch total loss 1.18529344\n",
      "Trained batch 1229 batch loss 1.39349604 epoch total loss 1.18546283\n",
      "Trained batch 1230 batch loss 1.11222506 epoch total loss 1.18540323\n",
      "Trained batch 1231 batch loss 1.16926622 epoch total loss 1.18539023\n",
      "Trained batch 1232 batch loss 1.28305411 epoch total loss 1.18546951\n",
      "Trained batch 1233 batch loss 1.26693165 epoch total loss 1.18553555\n",
      "Trained batch 1234 batch loss 1.22902763 epoch total loss 1.18557084\n",
      "Trained batch 1235 batch loss 1.11800528 epoch total loss 1.18551612\n",
      "Trained batch 1236 batch loss 1.09455562 epoch total loss 1.18544257\n",
      "Trained batch 1237 batch loss 1.22431827 epoch total loss 1.18547404\n",
      "Trained batch 1238 batch loss 1.17205262 epoch total loss 1.18546319\n",
      "Trained batch 1239 batch loss 1.18790257 epoch total loss 1.1854651\n",
      "Trained batch 1240 batch loss 1.22881961 epoch total loss 1.1855\n",
      "Trained batch 1241 batch loss 1.10434413 epoch total loss 1.1854347\n",
      "Trained batch 1242 batch loss 1.02030492 epoch total loss 1.18530166\n",
      "Trained batch 1243 batch loss 1.03000546 epoch total loss 1.18517673\n",
      "Trained batch 1244 batch loss 1.20426679 epoch total loss 1.18519199\n",
      "Trained batch 1245 batch loss 1.2235024 epoch total loss 1.18522286\n",
      "Trained batch 1246 batch loss 1.36390913 epoch total loss 1.18536627\n",
      "Trained batch 1247 batch loss 1.26632822 epoch total loss 1.18543112\n",
      "Trained batch 1248 batch loss 1.25142682 epoch total loss 1.18548405\n",
      "Trained batch 1249 batch loss 1.26187801 epoch total loss 1.18554521\n",
      "Trained batch 1250 batch loss 1.24607527 epoch total loss 1.18559361\n",
      "Trained batch 1251 batch loss 1.21615267 epoch total loss 1.18561816\n",
      "Trained batch 1252 batch loss 1.20208907 epoch total loss 1.18563128\n",
      "Trained batch 1253 batch loss 1.3670764 epoch total loss 1.18577611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1254 batch loss 1.17680895 epoch total loss 1.18576896\n",
      "Trained batch 1255 batch loss 1.3344506 epoch total loss 1.18588746\n",
      "Trained batch 1256 batch loss 1.24931419 epoch total loss 1.18593788\n",
      "Trained batch 1257 batch loss 1.19660985 epoch total loss 1.18594635\n",
      "Trained batch 1258 batch loss 1.14102912 epoch total loss 1.1859107\n",
      "Trained batch 1259 batch loss 1.07502317 epoch total loss 1.18582261\n",
      "Trained batch 1260 batch loss 1.04602981 epoch total loss 1.18571162\n",
      "Trained batch 1261 batch loss 1.10998344 epoch total loss 1.18565166\n",
      "Trained batch 1262 batch loss 1.18123472 epoch total loss 1.1856482\n",
      "Trained batch 1263 batch loss 1.25166416 epoch total loss 1.18570042\n",
      "Trained batch 1264 batch loss 1.24166656 epoch total loss 1.18574476\n",
      "Trained batch 1265 batch loss 1.16009927 epoch total loss 1.1857245\n",
      "Trained batch 1266 batch loss 1.05296922 epoch total loss 1.18561971\n",
      "Trained batch 1267 batch loss 1.05225205 epoch total loss 1.18551445\n",
      "Trained batch 1268 batch loss 1.0316056 epoch total loss 1.1853931\n",
      "Trained batch 1269 batch loss 1.17832935 epoch total loss 1.18538749\n",
      "Trained batch 1270 batch loss 1.18697739 epoch total loss 1.1853888\n",
      "Trained batch 1271 batch loss 1.31784558 epoch total loss 1.18549299\n",
      "Trained batch 1272 batch loss 1.16177571 epoch total loss 1.18547428\n",
      "Trained batch 1273 batch loss 1.15657878 epoch total loss 1.18545163\n",
      "Trained batch 1274 batch loss 1.11314869 epoch total loss 1.18539488\n",
      "Trained batch 1275 batch loss 1.17734742 epoch total loss 1.18538857\n",
      "Trained batch 1276 batch loss 1.18551397 epoch total loss 1.18538868\n",
      "Trained batch 1277 batch loss 1.1307137 epoch total loss 1.18534589\n",
      "Trained batch 1278 batch loss 1.24519038 epoch total loss 1.18539286\n",
      "Trained batch 1279 batch loss 1.22365046 epoch total loss 1.18542266\n",
      "Trained batch 1280 batch loss 1.2504065 epoch total loss 1.18547344\n",
      "Trained batch 1281 batch loss 1.11492097 epoch total loss 1.18541837\n",
      "Trained batch 1282 batch loss 1.15968919 epoch total loss 1.18539822\n",
      "Trained batch 1283 batch loss 1.00843751 epoch total loss 1.1852603\n",
      "Trained batch 1284 batch loss 0.979366839 epoch total loss 1.1851\n",
      "Trained batch 1285 batch loss 1.08342254 epoch total loss 1.1850208\n",
      "Trained batch 1286 batch loss 1.30264604 epoch total loss 1.18511224\n",
      "Trained batch 1287 batch loss 1.34272838 epoch total loss 1.18523479\n",
      "Trained batch 1288 batch loss 1.14826596 epoch total loss 1.18520606\n",
      "Trained batch 1289 batch loss 1.04540086 epoch total loss 1.18509758\n",
      "Trained batch 1290 batch loss 1.15050101 epoch total loss 1.18507075\n",
      "Trained batch 1291 batch loss 1.12369025 epoch total loss 1.18502319\n",
      "Trained batch 1292 batch loss 1.07861447 epoch total loss 1.18494081\n",
      "Trained batch 1293 batch loss 1.12767637 epoch total loss 1.18489659\n",
      "Trained batch 1294 batch loss 1.13152051 epoch total loss 1.18485534\n",
      "Trained batch 1295 batch loss 1.17075753 epoch total loss 1.18484437\n",
      "Trained batch 1296 batch loss 1.18318617 epoch total loss 1.18484318\n",
      "Trained batch 1297 batch loss 1.25532246 epoch total loss 1.18489754\n",
      "Trained batch 1298 batch loss 1.18998027 epoch total loss 1.18490148\n",
      "Trained batch 1299 batch loss 1.09128547 epoch total loss 1.18482935\n",
      "Trained batch 1300 batch loss 1.10998178 epoch total loss 1.18477178\n",
      "Trained batch 1301 batch loss 1.0936749 epoch total loss 1.1847018\n",
      "Trained batch 1302 batch loss 0.992862821 epoch total loss 1.18455446\n",
      "Trained batch 1303 batch loss 1.02926517 epoch total loss 1.18443537\n",
      "Trained batch 1304 batch loss 1.08372808 epoch total loss 1.18435812\n",
      "Trained batch 1305 batch loss 1.0397675 epoch total loss 1.18424737\n",
      "Trained batch 1306 batch loss 1.18731129 epoch total loss 1.18424964\n",
      "Trained batch 1307 batch loss 1.06350172 epoch total loss 1.18415725\n",
      "Trained batch 1308 batch loss 1.08653986 epoch total loss 1.18408263\n",
      "Trained batch 1309 batch loss 1.17898631 epoch total loss 1.18407869\n",
      "Trained batch 1310 batch loss 1.10514069 epoch total loss 1.18401837\n",
      "Trained batch 1311 batch loss 1.145051 epoch total loss 1.18398869\n",
      "Trained batch 1312 batch loss 1.08428633 epoch total loss 1.18391263\n",
      "Trained batch 1313 batch loss 0.984361768 epoch total loss 1.18376064\n",
      "Trained batch 1314 batch loss 1.06174016 epoch total loss 1.18366778\n",
      "Trained batch 1315 batch loss 1.0558697 epoch total loss 1.18357062\n",
      "Trained batch 1316 batch loss 1.1427356 epoch total loss 1.18353963\n",
      "Trained batch 1317 batch loss 1.25504327 epoch total loss 1.18359387\n",
      "Trained batch 1318 batch loss 1.40443277 epoch total loss 1.18376136\n",
      "Trained batch 1319 batch loss 1.44655728 epoch total loss 1.18396056\n",
      "Trained batch 1320 batch loss 1.16046882 epoch total loss 1.18394279\n",
      "Trained batch 1321 batch loss 1.01004887 epoch total loss 1.18381119\n",
      "Trained batch 1322 batch loss 1.12998486 epoch total loss 1.18377054\n",
      "Trained batch 1323 batch loss 1.29958177 epoch total loss 1.18385804\n",
      "Trained batch 1324 batch loss 1.31134546 epoch total loss 1.18395436\n",
      "Trained batch 1325 batch loss 1.32063222 epoch total loss 1.18405747\n",
      "Trained batch 1326 batch loss 1.29956174 epoch total loss 1.18414462\n",
      "Trained batch 1327 batch loss 1.24027681 epoch total loss 1.18418694\n",
      "Trained batch 1328 batch loss 1.20673048 epoch total loss 1.18420386\n",
      "Trained batch 1329 batch loss 1.18319976 epoch total loss 1.18420315\n",
      "Trained batch 1330 batch loss 1.18511701 epoch total loss 1.18420386\n",
      "Trained batch 1331 batch loss 1.17371976 epoch total loss 1.184196\n",
      "Trained batch 1332 batch loss 1.21748412 epoch total loss 1.18422091\n",
      "Trained batch 1333 batch loss 1.24150383 epoch total loss 1.18426394\n",
      "Trained batch 1334 batch loss 1.19866371 epoch total loss 1.18427467\n",
      "Trained batch 1335 batch loss 1.12753534 epoch total loss 1.18423212\n",
      "Trained batch 1336 batch loss 1.20526719 epoch total loss 1.18424797\n",
      "Trained batch 1337 batch loss 1.0907464 epoch total loss 1.18417799\n",
      "Trained batch 1338 batch loss 1.12184 epoch total loss 1.18413138\n",
      "Trained batch 1339 batch loss 1.24472451 epoch total loss 1.18417668\n",
      "Trained batch 1340 batch loss 1.07447934 epoch total loss 1.18409479\n",
      "Trained batch 1341 batch loss 1.03363609 epoch total loss 1.18398261\n",
      "Trained batch 1342 batch loss 1.06184697 epoch total loss 1.18389165\n",
      "Trained batch 1343 batch loss 1.13027048 epoch total loss 1.18385172\n",
      "Trained batch 1344 batch loss 1.1556679 epoch total loss 1.18383074\n",
      "Trained batch 1345 batch loss 1.22178924 epoch total loss 1.18385899\n",
      "Trained batch 1346 batch loss 1.24233198 epoch total loss 1.18390238\n",
      "Trained batch 1347 batch loss 1.14911401 epoch total loss 1.18387663\n",
      "Trained batch 1348 batch loss 1.14249456 epoch total loss 1.18384588\n",
      "Trained batch 1349 batch loss 1.25377667 epoch total loss 1.18389773\n",
      "Trained batch 1350 batch loss 1.10564137 epoch total loss 1.18383968\n",
      "Trained batch 1351 batch loss 1.19045794 epoch total loss 1.18384457\n",
      "Trained batch 1352 batch loss 1.32199156 epoch total loss 1.18394673\n",
      "Trained batch 1353 batch loss 1.2932626 epoch total loss 1.18402755\n",
      "Trained batch 1354 batch loss 1.12757564 epoch total loss 1.18398583\n",
      "Trained batch 1355 batch loss 1.04248238 epoch total loss 1.1838814\n",
      "Trained batch 1356 batch loss 1.05857253 epoch total loss 1.18378901\n",
      "Trained batch 1357 batch loss 0.959270716 epoch total loss 1.18362355\n",
      "Trained batch 1358 batch loss 0.945092499 epoch total loss 1.18344784\n",
      "Trained batch 1359 batch loss 1.03420734 epoch total loss 1.18333805\n",
      "Trained batch 1360 batch loss 1.06596553 epoch total loss 1.18325174\n",
      "Trained batch 1361 batch loss 1.10123599 epoch total loss 1.18319142\n",
      "Trained batch 1362 batch loss 1.09626067 epoch total loss 1.18312764\n",
      "Trained batch 1363 batch loss 1.15068579 epoch total loss 1.1831038\n",
      "Trained batch 1364 batch loss 1.19186616 epoch total loss 1.18311024\n",
      "Trained batch 1365 batch loss 1.17358279 epoch total loss 1.1831032\n",
      "Trained batch 1366 batch loss 1.17979765 epoch total loss 1.18310082\n",
      "Trained batch 1367 batch loss 1.20366454 epoch total loss 1.18311584\n",
      "Trained batch 1368 batch loss 1.27412152 epoch total loss 1.18318236\n",
      "Trained batch 1369 batch loss 1.10577703 epoch total loss 1.18312585\n",
      "Trained batch 1370 batch loss 1.16478825 epoch total loss 1.1831125\n",
      "Trained batch 1371 batch loss 1.12435246 epoch total loss 1.18306971\n",
      "Trained batch 1372 batch loss 1.30265832 epoch total loss 1.18315685\n",
      "Trained batch 1373 batch loss 1.26046729 epoch total loss 1.18321311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1374 batch loss 1.20855117 epoch total loss 1.18323159\n",
      "Trained batch 1375 batch loss 1.23441148 epoch total loss 1.18326879\n",
      "Trained batch 1376 batch loss 1.12865829 epoch total loss 1.18322909\n",
      "Trained batch 1377 batch loss 1.13107586 epoch total loss 1.18319118\n",
      "Trained batch 1378 batch loss 1.22642624 epoch total loss 1.18322253\n",
      "Trained batch 1379 batch loss 1.21274269 epoch total loss 1.18324399\n",
      "Trained batch 1380 batch loss 1.14451587 epoch total loss 1.18321598\n",
      "Trained batch 1381 batch loss 1.21764 epoch total loss 1.18324089\n",
      "Trained batch 1382 batch loss 1.23305535 epoch total loss 1.18327689\n",
      "Trained batch 1383 batch loss 1.25210285 epoch total loss 1.1833266\n",
      "Trained batch 1384 batch loss 1.31166625 epoch total loss 1.18341935\n",
      "Trained batch 1385 batch loss 1.30585361 epoch total loss 1.1835078\n",
      "Trained batch 1386 batch loss 1.16214871 epoch total loss 1.18349242\n",
      "Trained batch 1387 batch loss 1.26095593 epoch total loss 1.18354821\n",
      "Trained batch 1388 batch loss 1.20009065 epoch total loss 1.18356013\n",
      "Epoch 5 train loss 1.1835601329803467\n",
      "Validated batch 1 batch loss 1.12885118\n",
      "Validated batch 2 batch loss 1.14568603\n",
      "Validated batch 3 batch loss 1.12597\n",
      "Validated batch 4 batch loss 1.12809992\n",
      "Validated batch 5 batch loss 1.14716494\n",
      "Validated batch 6 batch loss 1.24333835\n",
      "Validated batch 7 batch loss 1.17269039\n",
      "Validated batch 8 batch loss 0.94753617\n",
      "Validated batch 9 batch loss 1.09573746\n",
      "Validated batch 10 batch loss 1.13033319\n",
      "Validated batch 11 batch loss 1.08592594\n",
      "Validated batch 12 batch loss 1.1583569\n",
      "Validated batch 13 batch loss 1.15375829\n",
      "Validated batch 14 batch loss 1.11482823\n",
      "Validated batch 15 batch loss 1.24118185\n",
      "Validated batch 16 batch loss 1.20012164\n",
      "Validated batch 17 batch loss 1.15549147\n",
      "Validated batch 18 batch loss 1.2443471\n",
      "Validated batch 19 batch loss 0.945081592\n",
      "Validated batch 20 batch loss 1.10897171\n",
      "Validated batch 21 batch loss 1.02955008\n",
      "Validated batch 22 batch loss 1.21690202\n",
      "Validated batch 23 batch loss 1.32127762\n",
      "Validated batch 24 batch loss 1.15860868\n",
      "Validated batch 25 batch loss 1.16965961\n",
      "Validated batch 26 batch loss 1.11193299\n",
      "Validated batch 27 batch loss 1.1453979\n",
      "Validated batch 28 batch loss 1.25436783\n",
      "Validated batch 29 batch loss 1.29098821\n",
      "Validated batch 30 batch loss 1.08309484\n",
      "Validated batch 31 batch loss 1.20325923\n",
      "Validated batch 32 batch loss 1.16024315\n",
      "Validated batch 33 batch loss 1.18264818\n",
      "Validated batch 34 batch loss 1.21799707\n",
      "Validated batch 35 batch loss 1.08372879\n",
      "Validated batch 36 batch loss 1.34004259\n",
      "Validated batch 37 batch loss 1.10044348\n",
      "Validated batch 38 batch loss 1.24292314\n",
      "Validated batch 39 batch loss 1.20349371\n",
      "Validated batch 40 batch loss 1.27246737\n",
      "Validated batch 41 batch loss 1.06013429\n",
      "Validated batch 42 batch loss 1.23308\n",
      "Validated batch 43 batch loss 1.10011518\n",
      "Validated batch 44 batch loss 1.22075379\n",
      "Validated batch 45 batch loss 1.16709507\n",
      "Validated batch 46 batch loss 1.16876411\n",
      "Validated batch 47 batch loss 1.15740919\n",
      "Validated batch 48 batch loss 1.08691096\n",
      "Validated batch 49 batch loss 1.13986528\n",
      "Validated batch 50 batch loss 1.10673881\n",
      "Validated batch 51 batch loss 1.17241585\n",
      "Validated batch 52 batch loss 1.17841387\n",
      "Validated batch 53 batch loss 1.22470534\n",
      "Validated batch 54 batch loss 1.12007499\n",
      "Validated batch 55 batch loss 1.19975233\n",
      "Validated batch 56 batch loss 1.13216376\n",
      "Validated batch 57 batch loss 1.17456973\n",
      "Validated batch 58 batch loss 1.24623024\n",
      "Validated batch 59 batch loss 1.12410045\n",
      "Validated batch 60 batch loss 1.1396178\n",
      "Validated batch 61 batch loss 1.18655884\n",
      "Validated batch 62 batch loss 1.12101912\n",
      "Validated batch 63 batch loss 1.33045447\n",
      "Validated batch 64 batch loss 1.23237896\n",
      "Validated batch 65 batch loss 1.03471804\n",
      "Validated batch 66 batch loss 1.26846778\n",
      "Validated batch 67 batch loss 1.16167712\n",
      "Validated batch 68 batch loss 1.07577109\n",
      "Validated batch 69 batch loss 1.18298769\n",
      "Validated batch 70 batch loss 1.17063344\n",
      "Validated batch 71 batch loss 1.22257888\n",
      "Validated batch 72 batch loss 1.13387275\n",
      "Validated batch 73 batch loss 1.13223958\n",
      "Validated batch 74 batch loss 1.18716574\n",
      "Validated batch 75 batch loss 1.23790014\n",
      "Validated batch 76 batch loss 1.24660969\n",
      "Validated batch 77 batch loss 1.28336215\n",
      "Validated batch 78 batch loss 1.1861434\n",
      "Validated batch 79 batch loss 1.14037526\n",
      "Validated batch 80 batch loss 1.20438135\n",
      "Validated batch 81 batch loss 1.14984977\n",
      "Validated batch 82 batch loss 1.21285748\n",
      "Validated batch 83 batch loss 1.3229\n",
      "Validated batch 84 batch loss 1.25284672\n",
      "Validated batch 85 batch loss 1.20661318\n",
      "Validated batch 86 batch loss 1.34400308\n",
      "Validated batch 87 batch loss 1.08876419\n",
      "Validated batch 88 batch loss 1.20820963\n",
      "Validated batch 89 batch loss 1.01645958\n",
      "Validated batch 90 batch loss 1.09950662\n",
      "Validated batch 91 batch loss 1.27957559\n",
      "Validated batch 92 batch loss 1.09172511\n",
      "Validated batch 93 batch loss 1.06515503\n",
      "Validated batch 94 batch loss 1.13901913\n",
      "Validated batch 95 batch loss 1.23896265\n",
      "Validated batch 96 batch loss 1.09878397\n",
      "Validated batch 97 batch loss 1.26808846\n",
      "Validated batch 98 batch loss 1.34914446\n",
      "Validated batch 99 batch loss 1.04103708\n",
      "Validated batch 100 batch loss 1.13696241\n",
      "Validated batch 101 batch loss 1.17715776\n",
      "Validated batch 102 batch loss 1.24235725\n",
      "Validated batch 103 batch loss 1.23461342\n",
      "Validated batch 104 batch loss 1.09244585\n",
      "Validated batch 105 batch loss 1.00848949\n",
      "Validated batch 106 batch loss 1.13140547\n",
      "Validated batch 107 batch loss 1.08315563\n",
      "Validated batch 108 batch loss 1.16647792\n",
      "Validated batch 109 batch loss 1.21895778\n",
      "Validated batch 110 batch loss 1.04457486\n",
      "Validated batch 111 batch loss 1.1860671\n",
      "Validated batch 112 batch loss 1.24883926\n",
      "Validated batch 113 batch loss 1.2076503\n",
      "Validated batch 114 batch loss 1.18710244\n",
      "Validated batch 115 batch loss 1.03586805\n",
      "Validated batch 116 batch loss 1.25936389\n",
      "Validated batch 117 batch loss 1.15355182\n",
      "Validated batch 118 batch loss 1.12333059\n",
      "Validated batch 119 batch loss 1.16841197\n",
      "Validated batch 120 batch loss 1.07209301\n",
      "Validated batch 121 batch loss 1.15669477\n",
      "Validated batch 122 batch loss 1.20875692\n",
      "Validated batch 123 batch loss 1.14628863\n",
      "Validated batch 124 batch loss 1.15519667\n",
      "Validated batch 125 batch loss 1.18976223\n",
      "Validated batch 126 batch loss 1.23087537\n",
      "Validated batch 127 batch loss 1.24371028\n",
      "Validated batch 128 batch loss 1.20930767\n",
      "Validated batch 129 batch loss 1.09468472\n",
      "Validated batch 130 batch loss 1.1333729\n",
      "Validated batch 131 batch loss 1.19911897\n",
      "Validated batch 132 batch loss 1.14311016\n",
      "Validated batch 133 batch loss 1.20709634\n",
      "Validated batch 134 batch loss 1.22046685\n",
      "Validated batch 135 batch loss 1.36790824\n",
      "Validated batch 136 batch loss 1.30668068\n",
      "Validated batch 137 batch loss 1.17574859\n",
      "Validated batch 138 batch loss 1.091488\n",
      "Validated batch 139 batch loss 1.07740414\n",
      "Validated batch 140 batch loss 1.14929712\n",
      "Validated batch 141 batch loss 1.12046957\n",
      "Validated batch 142 batch loss 1.13566506\n",
      "Validated batch 143 batch loss 1.11966515\n",
      "Validated batch 144 batch loss 1.22932112\n",
      "Validated batch 145 batch loss 1.17247629\n",
      "Validated batch 146 batch loss 1.26143599\n",
      "Validated batch 147 batch loss 1.23042274\n",
      "Validated batch 148 batch loss 1.22018063\n",
      "Validated batch 149 batch loss 1.2018044\n",
      "Validated batch 150 batch loss 1.33444428\n",
      "Validated batch 151 batch loss 1.24191689\n",
      "Validated batch 152 batch loss 1.27975607\n",
      "Validated batch 153 batch loss 1.26407027\n",
      "Validated batch 154 batch loss 1.34488881\n",
      "Validated batch 155 batch loss 1.26177883\n",
      "Validated batch 156 batch loss 1.12984276\n",
      "Validated batch 157 batch loss 1.15502381\n",
      "Validated batch 158 batch loss 1.26045215\n",
      "Validated batch 159 batch loss 1.26423383\n",
      "Validated batch 160 batch loss 1.1401931\n",
      "Validated batch 161 batch loss 1.19291496\n",
      "Validated batch 162 batch loss 1.17037272\n",
      "Validated batch 163 batch loss 1.20653677\n",
      "Validated batch 164 batch loss 1.10285378\n",
      "Validated batch 165 batch loss 1.15615904\n",
      "Validated batch 166 batch loss 1.19991565\n",
      "Validated batch 167 batch loss 1.1608007\n",
      "Validated batch 168 batch loss 1.21274018\n",
      "Validated batch 169 batch loss 1.16123\n",
      "Validated batch 170 batch loss 1.10713458\n",
      "Validated batch 171 batch loss 1.32057166\n",
      "Validated batch 172 batch loss 1.13767338\n",
      "Validated batch 173 batch loss 1.01081693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 174 batch loss 1.12775767\n",
      "Validated batch 175 batch loss 1.25934196\n",
      "Validated batch 176 batch loss 1.28943515\n",
      "Validated batch 177 batch loss 1.31264234\n",
      "Validated batch 178 batch loss 1.17590213\n",
      "Validated batch 179 batch loss 1.32264483\n",
      "Validated batch 180 batch loss 1.17188668\n",
      "Validated batch 181 batch loss 1.25854027\n",
      "Validated batch 182 batch loss 1.21494186\n",
      "Validated batch 183 batch loss 1.00116217\n",
      "Validated batch 184 batch loss 1.068187\n",
      "Validated batch 185 batch loss 1.28827\n",
      "Epoch 5 val loss 1.1768245697021484\n",
      "Model .//model-epoch-5-loss-1.1768.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "\n",
    "best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6317a6f",
   "metadata": {},
   "source": [
    "# 추론 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "475397f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 불러오기\n",
    "model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1)\n",
    "model.load_weights('model-epoch-5-loss-1.1768.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c48121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용했던 keypoint 들을 사용해야 하기 때문에 필요한 변수를 지정\n",
    "R_ANKLE = 0\n",
    "R_KNEE = 1\n",
    "R_HIP = 2\n",
    "L_HIP = 3\n",
    "L_KNEE = 4\n",
    "L_ANKLE = 5\n",
    "PELVIS = 6\n",
    "THORAX = 7\n",
    "UPPER_NECK = 8\n",
    "HEAD_TOP = 9\n",
    "R_WRIST = 10\n",
    "R_ELBOW = 11\n",
    "R_SHOULDER = 12\n",
    "L_SHOULDER = 13\n",
    "L_ELBOW = 14\n",
    "L_WRIST = 15\n",
    "\n",
    "MPII_BONES = [\n",
    "    [R_ANKLE, R_KNEE],\n",
    "    [R_KNEE, R_HIP],\n",
    "    [R_HIP, PELVIS],\n",
    "    [L_HIP, PELVIS],\n",
    "    [L_HIP, L_KNEE],\n",
    "    [L_KNEE, L_ANKLE],\n",
    "    [PELVIS, THORAX],\n",
    "    [THORAX, UPPER_NECK],\n",
    "    [UPPER_NECK, HEAD_TOP],\n",
    "    [R_WRIST, R_ELBOW],\n",
    "    [R_ELBOW, R_SHOULDER],\n",
    "    [THORAX, R_SHOULDER],\n",
    "    [THORAX, L_SHOULDER],\n",
    "    [L_SHOULDER, L_ELBOW],\n",
    "    [L_ELBOW, L_WRIST]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "189b4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 학습할 때 라벨이 되는 좌표를 heatmap으로 바꿨기 때문에 모델이 추론해 내놓은 결과도 heatmap이다.\n",
    "# 그래서 이 heatmap으로부터 좌표를 추출해야 한다.\n",
    "# heatmap중에 최대값을 갖는 지점을 찾아내면 된다.\n",
    "# 아래는 heatmap에서 최대값을 찾는 함수\n",
    "def find_max_coordinates(heatmaps):\n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (-1, 16))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    y = tf.cast(indices / 64, dtype=tf.int64)\n",
    "    x = indices - 64 * y\n",
    "    return tf.stack([x, y], axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "433ce3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 함수만으로는 256x256 이미지에 64x64 heatmap max 값을 표현할 때 quantization 오차가 발생한다.\n",
    "# 때문에 실제 계산에서는 3x3 필터를 이용해서 근사치를 구한다.\n",
    "def extract_keypoints_from_heatmap(heatmaps):\n",
    "    max_keypoints = find_max_coordinates(heatmaps)\n",
    "\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        \n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        patch[1][1] = 0\n",
    "        \n",
    "        index = np.argmax(patch)\n",
    "        \n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        \n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "        \n",
    "    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n",
    "    normalized_keypoints = adjusted_keypoints / 64\n",
    "    return normalized_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77b41dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델과 이미지 경로를 입력하면 이미지와 keypoint를 출력하는 함수\n",
    "def predict(model, image_path):\n",
    "    encoded = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(encoded)\n",
    "    inputs = tf.image.resize(image, (256, 256))\n",
    "    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    outputs = model(inputs, training=False)\n",
    "    if type(outputs) != list:\n",
    "        outputs = [outputs]\n",
    "    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n",
    "    kp = extract_keypoints_from_heatmap(heatmap)\n",
    "    return image, kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46aa37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화, 그림그리기\n",
    "# keypoint들과 뼈대 그리기. keypoint들은 관절 역할을 하고 keypoint들을 연결시킨 것이 뼈대.\n",
    "def draw_keypoints_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n",
    "    plt.show()\n",
    "\n",
    "def draw_skeleton_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        joints.append((joint_x, joint_y))\n",
    "    \n",
    "    for bone in MPII_BONES:\n",
    "        joint_1 = joints[bone[0]]\n",
    "        joint_2 = joints[bone[1]]\n",
    "        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3d17ae8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 스트 이미지를 이용해 모델의 성능 확인\n",
    "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')\n",
    "\n",
    "image, keypoints = predict(model, test_image)\n",
    "draw_keypoints_on_image(image, keypoints)\n",
    "draw_skeleton_on_image(image, keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643b04b",
   "metadata": {},
   "source": [
    "# simplebaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1651748c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 0s 0us/step\n",
      "94781440/94765736 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5bd70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "upconv1 = tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same')\n",
    "bn1 = tf.keras.layers.BatchNormalization()\n",
    "relu1 = tf.keras.layers.ReLU()\n",
    "upconv2 = tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same')\n",
    "bn2 = tf.keras.layers.BatchNormalization()\n",
    "relu2 = tf.keras.layers.ReLU()\n",
    "upconv3 = tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same')\n",
    "bn3 = tf.keras.layers.BatchNormalization()\n",
    "relu3 = tf.keras.layers.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74cc76fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = tf.keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same'))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization())\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model\n",
    "\n",
    "upconv = _make_deconv_layer(3)\n",
    "final_layer = tf.keras.layers.Conv2D(16, kernel_size=(1,1), padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a5acaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Functional)        (None, None, None, 2048)  23587712  \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 64, 64, 256)       10489600  \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 64, 16)        4112      \n",
      "=================================================================\n",
      "Total params: 34,081,424\n",
      "Trainable params: 34,026,768\n",
      "Non-trainable params: 54,656\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "inputs = keras.Input(shape=(256, 256, 3))\n",
    "x = resnet(inputs)\n",
    "x = upconv(x)\n",
    "out = final_layer(x)\n",
    "model = keras.Model(inputs, out)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d23c6e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape\n",
      "(1, 256, 256, 3)\n",
      "\n",
      "\n",
      "output shape\n",
      "(1, 64, 64, 16)\n",
      "tf.Tensor(\n",
      "[[[[ 8.1029320e-03 -1.5578655e-03  1.2331853e-04 ... -3.1136628e-03\n",
      "     2.3518853e-04  3.6388226e-03]\n",
      "   [-1.0283253e-02  3.2568260e-03  3.3530023e-02 ... -5.9978603e-03\n",
      "    -5.2200919e-03  7.1002110e-03]\n",
      "   [-7.4236179e-03 -5.4519679e-03 -9.3216067e-03 ... -3.3168239e-03\n",
      "    -5.9267681e-05  7.3093032e-03]\n",
      "   ...\n",
      "   [ 1.9996697e-03  2.4007423e-02  1.8313577e-02 ...  1.3646987e-02\n",
      "    -2.3890808e-02 -8.4457612e-03]\n",
      "   [ 6.5291822e-03 -1.3944256e-03  9.8870713e-03 ...  1.8842977e-03\n",
      "    -1.7062815e-02  8.3060628e-03]\n",
      "   [ 4.8704008e-03  1.0261943e-02  1.2551290e-02 ...  3.5774589e-03\n",
      "    -1.2594842e-02 -3.7560386e-03]]\n",
      "\n",
      "  [[ 6.1431713e-03  7.5562960e-03  1.3027805e-03 ...  9.4922073e-03\n",
      "    -9.9871671e-03 -6.4474284e-03]\n",
      "   [ 5.5780420e-03  2.2642156e-02  1.8777031e-02 ...  3.7814695e-03\n",
      "    -1.3954410e-02 -5.3976462e-03]\n",
      "   [ 2.5570642e-02  2.3934662e-02 -8.5755903e-03 ... -1.4907127e-03\n",
      "    -3.6819506e-02 -4.4831883e-02]\n",
      "   ...\n",
      "   [-2.0538315e-02  1.8463820e-02  3.1536944e-02 ... -3.2208643e-03\n",
      "    -2.0730912e-03  2.0678577e-03]\n",
      "   [-3.0774444e-03  7.3593948e-03  1.4207517e-02 ...  1.4015024e-02\n",
      "    -2.2262050e-02 -4.9291857e-02]\n",
      "   [ 1.1605882e-02  1.4737940e-02  1.2725576e-02 ...  4.9801013e-03\n",
      "    -7.0489277e-03 -5.3584403e-03]]\n",
      "\n",
      "  [[-1.9369359e-03  1.7217482e-02 -1.3184127e-02 ... -6.7678480e-03\n",
      "    -1.4862275e-02  3.7415605e-03]\n",
      "   [-2.1128922e-03  2.7421165e-02  2.6277239e-02 ...  6.3542421e-03\n",
      "    -2.4780719e-02 -1.0945878e-03]\n",
      "   [ 1.1972461e-02 -1.1399922e-02  1.3158177e-02 ...  2.8921453e-02\n",
      "    -1.9552004e-02  1.3243323e-03]\n",
      "   ...\n",
      "   [ 9.7311139e-03  4.6607897e-02  2.9546905e-02 ...  1.7401731e-02\n",
      "    -3.2861970e-02  1.2062460e-02]\n",
      "   [ 7.2381655e-03  2.4682792e-02  1.6189184e-02 ...  2.5123678e-02\n",
      "     1.8015394e-02  7.9417285e-03]\n",
      "   [ 9.3642818e-03  8.5551655e-03  1.0202584e-02 ...  1.2170547e-02\n",
      "    -2.6652588e-02  8.5361740e-03]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 3.1792673e-03  7.5959861e-03 -5.0425675e-04 ...  3.5427811e-03\n",
      "    -9.2053702e-03 -1.5705023e-02]\n",
      "   [ 6.2269238e-03  7.1076672e-03  3.0543984e-04 ... -1.5479277e-02\n",
      "    -1.4195364e-02  4.5122020e-03]\n",
      "   [-1.7658692e-02  8.4905336e-03  1.4936819e-02 ... -1.7106833e-02\n",
      "    -2.3907453e-02 -3.4465533e-02]\n",
      "   ...\n",
      "   [ 2.9551836e-03 -4.3363692e-03  1.4147186e-02 ...  1.3683357e-02\n",
      "    -2.0047881e-02  2.9282200e-03]\n",
      "   [ 1.4575476e-02  1.8941684e-02  1.3492685e-02 ... -2.1633517e-02\n",
      "    -1.1894857e-02 -5.4940805e-02]\n",
      "   [ 5.4690419e-03  6.3983221e-03  6.0813492e-03 ... -3.5860150e-03\n",
      "    -4.0706005e-03 -8.0845514e-03]]\n",
      "\n",
      "  [[-1.4501056e-02  1.0793590e-02 -3.3989896e-03 ...  3.1324818e-03\n",
      "     9.9480869e-03  3.6282183e-04]\n",
      "   [ 1.3191810e-02 -6.6203335e-03  1.5048899e-02 ... -7.2160922e-03\n",
      "    -3.1250164e-02 -3.5928991e-02]\n",
      "   [-1.6132765e-02  1.1938247e-02  3.8493397e-03 ...  2.2294369e-02\n",
      "    -2.0613279e-02  1.0743273e-03]\n",
      "   ...\n",
      "   [-6.0695194e-04  3.8814332e-02  3.3357427e-02 ...  3.5904303e-02\n",
      "    -1.7783138e-03  4.0719416e-03]\n",
      "   [ 1.6595541e-02  4.8333548e-02 -1.2754500e-02 ...  2.9494051e-02\n",
      "    -2.7644698e-02  2.2366274e-02]\n",
      "   [-2.5017564e-03  1.7315680e-02  9.4386991e-03 ...  1.0049164e-03\n",
      "    -1.5936749e-02 -9.0556145e-03]]\n",
      "\n",
      "  [[ 1.0887095e-03  8.2445983e-03  2.0716626e-03 ...  4.1766907e-03\n",
      "    -7.1726018e-03  1.3040297e-04]\n",
      "   [ 5.2914987e-03  1.0158971e-02  7.0445552e-03 ...  1.1961223e-02\n",
      "    -7.1598329e-03  8.0890460e-03]\n",
      "   [ 1.0250627e-02  5.8940277e-03  7.1506179e-03 ...  6.7577870e-03\n",
      "    -5.6203809e-03 -2.1047624e-02]\n",
      "   ...\n",
      "   [ 1.2533249e-02 -1.1818165e-02  1.4198859e-02 ... -3.0702734e-03\n",
      "    -2.2593520e-07 -1.9622338e-03]\n",
      "   [ 1.1220970e-02  1.2663147e-02  1.1295630e-02 ...  8.3740056e-03\n",
      "    -1.9769149e-03 -2.2839837e-02]\n",
      "   [ 5.9259259e-03  2.0459695e-03  2.7196803e-03 ...  5.1282994e-03\n",
      "    -2.1676701e-03 -8.6151477e-04]]]], shape=(1, 64, 64, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np_input = np.zeros((1,256,256,3), dtype=np.float32)\n",
    "tf_input = tf.convert_to_tensor(np_input, dtype=tf.float32)\n",
    "print('input shape')\n",
    "print (tf_input.shape)\n",
    "print('\\n')\n",
    "\n",
    "tf_output = model(tf_input)\n",
    "print('output shape')\n",
    "print (tf_output.shape)\n",
    "print (tf_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ca1af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = keras.Model(inputs, out)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc12b1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: in user code:\n",
      "\n",
      "    /tmp/ipykernel_832/3247077786.py:53 train_step  *\n",
      "        loss = self.compute_loss(labels, outputs)\n",
      "    /tmp/ipykernel_832/3247077786.py:44 compute_loss  *\n",
      "        loss += tf.math.reduce_mean(\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1383 binary_op_wrapper\n",
      "        raise e\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1367 binary_op_wrapper\n",
      "        return func(x, y, name=name)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n",
      "        return target(*args, **kwargs)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1700 _add_dispatch\n",
      "        return gen_math_ops.add_v2(x, y, name=name)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:464 add_v2\n",
      "        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:555 _apply_op_helper\n",
      "        raise TypeError(\n",
      "\n",
      "    TypeError: Input 'y' of 'AddV2' Op has type float32 that does not match type int32 of argument 'x'.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 346, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 695, in wrapper\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "TypeError: in user code:\n",
      "\n",
      "    /tmp/ipykernel_832/3247077786.py:53 train_step  *\n",
      "        loss = self.compute_loss(labels, outputs)\n",
      "    /tmp/ipykernel_832/3247077786.py:44 compute_loss  *\n",
      "        loss += tf.math.reduce_mean(\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1383 binary_op_wrapper\n",
      "        raise e\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1367 binary_op_wrapper\n",
      "        return func(x, y, name=name)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n",
      "        return target(*args, **kwargs)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1700 _add_dispatch\n",
      "        return gen_math_ops.add_v2(x, y, name=name)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:464 add_v2\n",
      "        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:555 _apply_op_helper\n",
      "        raise TypeError(\n",
      "\n",
      "    TypeError: Input 'y' of 'AddV2' Op has type float32 that does not match type int32 of argument 'x'.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    /tmp/ipykernel_832/3247077786.py:75 distributed_train_epoch  *\n        per_replica_loss = self.strategy.run(\n    /tmp/ipykernel_832/3247077786.py:53 train_step  *\n        loss = self.compute_loss(labels, outputs)\n    /tmp/ipykernel_832/3247077786.py:44 compute_loss  *\n        loss += tf.math.reduce_mean(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1383 binary_op_wrapper\n        raise e\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1367 binary_op_wrapper\n        return func(x, y, name=name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1700 _add_dispatch\n        return gen_math_ops.add_v2(x, y, name=name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:464 add_v2\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:555 _apply_op_helper\n        raise TypeError(\n\n    TypeError: Input 'y' of 'AddV2' Op has type float32 that does not match type int32 of argument 'x'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_832/1927619030.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0007\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mbest_model_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tfrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_tfrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_832/4078933612.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dist_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dist_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_832/3247077786.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, train_dist_dataset, val_dist_dataset)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 epoch, self.current_learning_rate))\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             train_total_loss, num_train_batches = distributed_train_epoch(\n\u001b[0m\u001b[1;32m    111\u001b[0m                 train_dist_dataset)\n\u001b[1;32m    112\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_total_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_train_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 759\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    760\u001b[0m             *args, **kwds))\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    /tmp/ipykernel_832/3247077786.py:75 distributed_train_epoch  *\n        per_replica_loss = self.strategy.run(\n    /tmp/ipykernel_832/3247077786.py:53 train_step  *\n        loss = self.compute_loss(labels, outputs)\n    /tmp/ipykernel_832/3247077786.py:44 compute_loss  *\n        loss += tf.math.reduce_mean(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1383 binary_op_wrapper\n        raise e\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1367 binary_op_wrapper\n        return func(x, y, name=name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1700 _add_dispatch\n        return gen_math_ops.add_v2(x, y, name=name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:464 add_v2\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:555 _apply_op_helper\n        raise TypeError(\n\n    TypeError: Input 'y' of 'AddV2' Op has type float32 that does not match type int32 of argument 'x'.\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "\n",
    "best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "225e4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype 에러가 발생한다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd8cc6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0.0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MODEL_PATH + '/simplebaseline_model-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f20df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4779a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = keras.Model(inputs, out)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d6ff176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n",
      "INFO:tensorflow:Error reported to Coordinator: in user code:\n",
      "\n",
      "    /tmp/ipykernel_2204/3096613398.py:57 train_step  *\n",
      "        self.optimizer.apply_gradients(\n",
      "    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:628 apply_gradients  **\n",
      "        self._create_all_weights(var_list)\n",
      "    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:815 _create_all_weights\n",
      "        self._create_slots(var_list)\n",
      "    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:117 _create_slots\n",
      "        self.add_slot(var, 'm')\n",
      "    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:892 add_slot\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x79c268046790>), which is different from the scope used for the original variable (<tf.Variable 'conv1_conv/kernel:0' shape=(7, 7, 3, 64) dtype=float32, numpy=\n",
      "    array([[[[ 2.82526277e-02, -1.18737184e-02,  1.51488732e-03, ...,\n",
      "              -1.07003953e-02, -5.27982824e-02, -1.36667420e-03],\n",
      "             [ 5.86827798e-03,  5.04415408e-02,  3.46324709e-03, ...,\n",
      "               1.01423981e-02,  1.39493728e-02,  1.67549420e-02],\n",
      "             [-2.44090753e-03, -4.86173332e-02,  2.69966386e-03, ...,\n",
      "              -3.44439060e-04,  3.48098315e-02,  6.28910400e-03]],\n",
      "    \n",
      "            [[ 1.81872323e-02, -7.20698107e-03,  4.80302610e-03, ...,\n",
      "              -7.43396254e-03, -8.56800564e-03,  1.16849300e-02],\n",
      "             [ 1.87554304e-02,  5.12730293e-02,  4.50406177e-03, ...,\n",
      "               1.39413681e-02,  1.26296384e-02, -1.73004344e-02],\n",
      "             [ 1.90453827e-02, -3.87909152e-02,  4.25842637e-03, ...,\n",
      "               2.75742816e-04, -1.27962548e-02, -8.35626759e-03]],\n",
      "    \n",
      "            [[ 1.58849321e-02, -1.06073255e-02,  1.30999666e-02, ...,\n",
      "              -2.26797583e-03, -3.98984266e-04,  3.39989027e-04],\n",
      "             [ 3.61421369e-02,  5.02430499e-02,  1.22699486e-02, ...,\n",
      "               1.19910473e-02,  2.02837810e-02, -1.96981970e-02],\n",
      "             [ 2.17959806e-02, -3.86004597e-02,  1.12379901e-02, ...,\n",
      "              -2.07756506e-03, -3.40645364e-03, -3.78638096e-02]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[-5.30153252e-02, -8.60502943e-03,  6.38643000e-03, ...,\n",
      "              -4.49256925e-03,  3.48024699e-03, -1.40979560e-02],\n",
      "             [-9.35578942e-02,  4.61557060e-02,  1.53722311e-03, ...,\n",
      "               1.21013075e-02,  5.05337631e-03,  3.30474339e-02],\n",
      "             [-7.69589692e-02, -3.51354294e-02,  2.22769519e-03, ...,\n",
      "               9.18304977e-06, -1.15465783e-02,  2.29630154e-02]],\n",
      "    \n",
      "            [[-4.73558307e-02, -4.07940615e-03,  4.76515992e-03, ...,\n",
      "              -9.73805040e-03, -1.03890402e-02,  1.62366014e-02],\n",
      "             [-1.24100089e-01,  4.78516519e-02, -9.90210217e-04, ...,\n",
      "               1.10340826e-02, -6.77202828e-03,  5.49102016e-02],\n",
      "             [-7.13113099e-02, -2.86470409e-02,  6.20829698e-04, ...,\n",
      "              -2.17762636e-03, -1.58942658e-02,  3.44766974e-02]],\n",
      "    \n",
      "            [[ 1.85429510e-02, -1.12518407e-02,  1.12506151e-02, ...,\n",
      "              -1.51338596e-02, -5.66656142e-03, -1.30050071e-02],\n",
      "             [-2.68079005e-02,  3.64737920e-02,  4.55197273e-03, ...,\n",
      "               5.53486776e-03,  1.12653999e-02,  2.46754289e-03],\n",
      "             [ 1.43940765e-02, -3.56382579e-02,  5.08728763e-03, ...,\n",
      "              -7.46753719e-03,  1.61169283e-02,  1.12382937e-02]]],\n",
      "    \n",
      "    \n",
      "           [[[ 7.99009297e-03, -9.49061289e-03, -4.21846565e-03, ...,\n",
      "              -1.23715792e-02, -3.82804796e-02, -5.90979494e-03],\n",
      "             [-7.68794632e-03,  5.46954982e-02, -1.03303632e-02, ...,\n",
      "               1.40626412e-02,  1.99436247e-02,  2.51518637e-02],\n",
      "             [ 3.70471564e-04, -3.70203964e-02, -9.80611611e-03, ...,\n",
      "              -4.95379185e-03,  2.27415562e-02,  1.38941938e-02]],\n",
      "    \n",
      "            [[ 2.48856675e-02, -9.57963988e-03, -2.37837038e-03, ...,\n",
      "              -1.08526833e-02,  2.24138368e-02, -2.40965877e-02],\n",
      "             [ 2.42966190e-02,  4.93442900e-02, -1.32921906e-02, ...,\n",
      "               1.47738317e-02,  2.67323572e-02,  1.14357602e-02],\n",
      "             [ 2.91274227e-02, -3.05654686e-02, -1.42364930e-02, ...,\n",
      "              -8.36174563e-03, -3.00847553e-02, -2.51545687e-03]],\n",
      "    \n",
      "            [[ 7.67260045e-02, -1.19650066e-02, -2.10191216e-03, ...,\n",
      "               1.79589365e-03,  2.02653632e-02, -1.33340694e-02],\n",
      "             [ 1.49444759e-01,  5.00719361e-02, -1.52172269e-02, ...,\n",
      "               1.83409695e-02,  1.56401172e-02,  8.53796005e-02],\n",
      "             [ 1.17180273e-01, -2.56576538e-02, -1.85890812e-02, ...,\n",
      "              -2.50462536e-03, -5.22738546e-02,  1.17943510e-02]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[-1.89151186e-02, -1.06457584e-02, -1.19606184e-03, ...,\n",
      "              -7.13960640e-03,  7.56816342e-02,  8.62411484e-02],\n",
      "             [ 1.33888470e-02,  4.24321182e-02, -1.93305630e-02, ...,\n",
      "               8.93499516e-03,  3.26688178e-02,  1.71118364e-01],\n",
      "             [-9.38678440e-03, -2.88689751e-02, -1.87061988e-02, ...,\n",
      "              -1.06920488e-02, -4.56195511e-02,  1.51734307e-01]],\n",
      "    \n",
      "            [[-7.93561861e-02, -8.69292021e-03,  1.06180850e-02, ...,\n",
      "              -8.22936464e-03,  5.34521677e-02,  2.43676770e-02],\n",
      "             [-1.76872283e-01,  4.03351039e-02, -6.91946782e-03, ...,\n",
      "               1.14902109e-02,  2.45164465e-02,  1.30252065e-02],\n",
      "             [-1.30214587e-01, -2.94868350e-02, -1.32359739e-03, ...,\n",
      "              -8.08166154e-03, -3.32693383e-02,  1.78283844e-02]],\n",
      "    \n",
      "            [[-1.53617216e-02, -1.02823023e-02,  1.44553250e-02, ...,\n",
      "              -1.23689836e-02,  2.81683691e-02, -1.52645903e-02],\n",
      "             [-1.22947149e-01,  3.72432098e-02, -2.82740779e-03, ...,\n",
      "               1.07275983e-02,  1.61965452e-02, -4.08420824e-02],\n",
      "             [-7.92325959e-02, -3.09139602e-02,  1.91061670e-04, ...,\n",
      "              -1.06926244e-02, -1.36199640e-02, -2.90216487e-02]]],\n",
      "    \n",
      "    \n",
      "           [[[-2.74732877e-02, -1.59629062e-02,  5.87167032e-03, ...,\n",
      "              -1.18064405e-02, -5.19699305e-02, -1.52737210e-02],\n",
      "             [-7.46604949e-02,  5.22083789e-02, -1.98963331e-03, ...,\n",
      "               1.27452025e-02,  7.53643783e-03, -1.96208209e-02],\n",
      "             [-3.34048420e-02, -3.39833461e-02, -1.99538236e-03, ...,\n",
      "              -9.30251833e-03,  3.30174603e-02, -1.65446047e-02]],\n",
      "    \n",
      "            [[-6.57535121e-02, -1.23513499e-02, -4.16519074e-03, ...,\n",
      "              -1.22041989e-03,  2.09396798e-02,  3.62350084e-02],\n",
      "             [-1.52494013e-01,  4.94739972e-02, -1.83443855e-02, ...,\n",
      "               2.37025358e-02,  2.67230812e-02,  8.47681686e-02],\n",
      "             [-8.80744159e-02, -2.57136654e-02, -2.17252262e-02, ...,\n",
      "              -3.12197860e-03, -2.06513535e-02,  6.63726628e-02]],\n",
      "    \n",
      "            [[ 1.99921392e-02, -1.76080931e-02,  1.81755237e-03, ...,\n",
      "               3.69562432e-02,  3.51557694e-02,  1.03931516e-01],\n",
      "             [ 6.10242449e-02,  4.46803048e-02, -1.41719123e-02, ...,\n",
      "               5.15808910e-02,  2.07974892e-02,  1.46060020e-01],\n",
      "             [ 8.05315524e-02, -2.88072433e-02, -1.85981095e-02, ...,\n",
      "               2.20173039e-02, -5.11762947e-02,  1.40093669e-01]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[ 1.15528561e-01, -1.67486407e-02,  8.49904679e-03, ...,\n",
      "               4.99674492e-03,  7.98972845e-02, -1.11083500e-01],\n",
      "             [ 3.32334489e-01,  4.24566194e-02, -9.70878359e-03, ...,\n",
      "               1.92873720e-02,  1.25060824e-03, -3.40990961e-01],\n",
      "             [ 2.16480315e-01, -2.68480480e-02, -8.96557700e-03, ...,\n",
      "              -6.44540135e-03, -7.85448179e-02, -2.04899684e-01]],\n",
      "    \n",
      "            [[-8.99803787e-02, -8.51823762e-03,  2.25046948e-02, ...,\n",
      "              -8.74274992e-04,  6.35959804e-02, -9.58404392e-02],\n",
      "             [-8.15074593e-02,  4.37885672e-02,  3.69152403e-03, ...,\n",
      "               1.71142723e-02,  6.33937493e-03, -2.73919165e-01],\n",
      "             [-9.73245725e-02, -2.61962153e-02,  8.95403326e-03, ...,\n",
      "              -7.23934872e-03, -5.64266555e-02, -1.84837982e-01]],\n",
      "    \n",
      "            [[-9.46454927e-02, -1.17739988e-02,  2.49665454e-02, ...,\n",
      "              -7.38179125e-03,  3.05740479e-02, -1.17530329e-02],\n",
      "             [-2.11111471e-01,  3.85808311e-02,  5.31885307e-03, ...,\n",
      "               1.61544569e-02,  3.10361455e-03, -8.36645439e-02],\n",
      "             [-1.75075874e-01, -3.21811885e-02,  9.45197884e-03, ...,\n",
      "              -1.05473688e-02, -2.80730613e-02, -6.67640790e-02]]],\n",
      "    \n",
      "    \n",
      "           ...,\n",
      "    \n",
      "    \n",
      "           [[[ 2.31804699e-02, -1.62718501e-02,  1.22078890e-02, ...,\n",
      "              -1.22131845e-02, -2.02786643e-02, -2.14508991e-03],\n",
      "             [ 2.30488200e-02,  4.41800952e-02,  3.59291583e-03, ...,\n",
      "               1.27932075e-02,  6.47032401e-03, -5.39429188e-02],\n",
      "             [ 2.03978457e-02, -2.67958529e-02,  5.69844292e-03, ...,\n",
      "              -8.20858125e-03,  2.51460597e-02, -3.12512405e-02]],\n",
      "    \n",
      "            [[-4.64516319e-02, -1.34653188e-02,  1.61393601e-02, ...,\n",
      "              -2.20572166e-02,  5.05596139e-02,  1.47165358e-03],\n",
      "             [-1.77852944e-01,  4.04180661e-02,  4.32515051e-03, ...,\n",
      "               7.27979047e-03,  1.37663782e-02, -5.00506982e-02],\n",
      "             [-1.09063022e-01, -2.11244933e-02,  6.98045455e-03, ...,\n",
      "              -2.00869981e-02, -6.30094185e-02, -4.20499854e-02]],\n",
      "    \n",
      "            [[-1.83006614e-01, -1.79655701e-02,  1.82811301e-02, ...,\n",
      "               1.56401389e-03,  9.29453745e-02,  4.12672907e-02],\n",
      "             [-4.11783189e-01,  3.40776965e-02,  8.74394365e-03, ...,\n",
      "               2.33494844e-02,  1.98237225e-02,  8.06325078e-02],\n",
      "             [-2.76736170e-01, -2.83147153e-02,  1.31541817e-02, ...,\n",
      "              -5.05925808e-03, -8.54580775e-02,  4.26753834e-02]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[ 5.36167026e-02, -1.07590063e-02,  2.19804980e-02, ...,\n",
      "              -8.83348845e-03,  1.40453711e-01,  3.20528477e-01],\n",
      "             [ 1.85792699e-01,  3.76442447e-02,  1.02089429e-02, ...,\n",
      "               1.29263047e-02, -3.70457745e-03,  6.66479290e-01],\n",
      "             [ 1.32038444e-01, -2.75047179e-02,  2.28339490e-02, ...,\n",
      "              -1.19996015e-02, -1.22367747e-01,  4.83815670e-01]],\n",
      "    \n",
      "            [[ 8.34956467e-02, -9.09057911e-03,  2.50242520e-02, ...,\n",
      "              -1.67011786e-02,  1.20522320e-01,  1.36462688e-01],\n",
      "             [ 2.50555605e-01,  4.07686047e-02,  1.08884834e-02, ...,\n",
      "               7.53540406e-03, -7.55708572e-03,  3.96415204e-01],\n",
      "             [ 1.49690762e-01, -3.11034787e-02,  2.43526250e-02, ...,\n",
      "              -1.65321939e-02, -1.09688722e-01,  2.64446586e-01]],\n",
      "    \n",
      "            [[ 3.69576029e-02, -1.27014471e-02,  3.19833457e-02, ...,\n",
      "              -1.48784053e-02,  9.22970548e-02,  6.54868260e-02],\n",
      "             [ 9.63706747e-02,  4.39107306e-02,  1.59802549e-02, ...,\n",
      "               1.22494521e-02,  8.10312852e-03,  1.78935930e-01],\n",
      "             [ 2.95156911e-02, -2.96487771e-02,  2.69996542e-02, ...,\n",
      "              -1.38547905e-02, -7.72434175e-02,  1.32773802e-01]]],\n",
      "    \n",
      "    \n",
      "           [[[ 4.22548056e-02, -8.30464344e-03,  5.34065207e-03, ...,\n",
      "              -8.06468353e-03, -4.70053628e-02,  4.45614867e-02],\n",
      "             [ 9.77012664e-02,  3.83502319e-02, -5.37837343e-03, ...,\n",
      "               1.17106764e-02, -4.59602941e-03,  6.98771998e-02],\n",
      "             [ 6.38262108e-02, -2.08319575e-02, -1.72756368e-03, ...,\n",
      "              -8.19445588e-03,  4.25621867e-02,  4.83920909e-02]],\n",
      "    \n",
      "            [[ 4.59470600e-02, -4.77699284e-03,  7.04339007e-03, ...,\n",
      "              -1.82104297e-02,  3.14848162e-02,  4.64068204e-02],\n",
      "             [ 3.89483608e-02,  3.78783308e-02, -6.85291924e-03, ...,\n",
      "               7.33014196e-03,  3.90656322e-04,  1.52848229e-01],\n",
      "             [ 4.57218140e-02, -1.34090437e-02, -8.30697361e-04, ...,\n",
      "              -1.85202472e-02, -3.45353335e-02,  9.25581828e-02]],\n",
      "    \n",
      "            [[-4.66161780e-02, -1.22223441e-02,  9.35023464e-03, ...,\n",
      "              -1.31351836e-02,  6.08736612e-02,  9.18865502e-02],\n",
      "             [-1.92336142e-01,  3.18407975e-02, -1.01881009e-03, ...,\n",
      "               7.55425170e-03, -8.62357323e-04,  2.88297594e-01],\n",
      "             [-1.15666650e-01, -2.35320851e-02,  6.74636895e-03, ...,\n",
      "              -1.94703583e-02, -5.66169359e-02,  1.95824102e-01]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[-2.10239179e-02, -9.81471874e-03,  9.81596112e-03, ...,\n",
      "              -1.36731779e-02,  1.20193027e-01, -1.26708716e-01],\n",
      "             [-3.72992679e-02,  3.05935629e-02, -3.00194928e-03, ...,\n",
      "               8.85152724e-03, -5.07611316e-03, -6.25461042e-02],\n",
      "             [ 7.84674310e-04, -2.91344281e-02,  1.12569630e-02, ...,\n",
      "              -1.38232643e-02, -9.49400812e-02, -8.74437019e-02]],\n",
      "    \n",
      "            [[ 3.32221799e-02, -4.22911346e-03,  1.13633750e-02, ...,\n",
      "              -1.41841583e-02,  9.59840789e-02, -1.23203963e-01],\n",
      "             [ 9.95653942e-02,  4.03233357e-02, -4.36036801e-03, ...,\n",
      "               8.42505507e-03, -1.50266392e-02, -1.58158958e-01],\n",
      "             [ 6.55353814e-02, -2.76978761e-02,  1.06595978e-02, ...,\n",
      "              -1.31017175e-02, -9.93799716e-02, -1.52014121e-01]],\n",
      "    \n",
      "            [[ 2.50522885e-02, -1.08845932e-02,  1.29567981e-02, ...,\n",
      "              -1.67823900e-02,  6.55406937e-02, -3.34061496e-02],\n",
      "             [ 1.00219429e-01,  4.24924381e-02, -4.06364352e-03, ...,\n",
      "               8.98410939e-03, -1.98677508e-03, -9.19047296e-02],\n",
      "             [ 6.97101504e-02, -3.41515057e-02,  8.97936709e-03, ...,\n",
      "              -1.51484888e-02, -8.06454644e-02, -8.53376985e-02]]],\n",
      "    \n",
      "    \n",
      "           [[[ 1.46303158e-02, -9.15218703e-03,  5.24803856e-03, ...,\n",
      "              -3.63799883e-03, -5.51798902e-02, -7.19531113e-03],\n",
      "             [ 6.12211153e-02,  2.67034862e-02, -4.38000960e-03, ...,\n",
      "               1.38858845e-02,  1.62421225e-03,  6.91889692e-03],\n",
      "             [ 1.86353922e-02, -2.39325576e-02,  5.56383107e-04, ...,\n",
      "              -6.68733614e-03,  7.36468807e-02,  3.71867418e-02]],\n",
      "    \n",
      "            [[ 3.52302976e-02, -3.27857491e-03,  7.14091491e-03, ...,\n",
      "              -9.93822515e-03,  2.38756705e-02, -2.10771449e-02],\n",
      "             [ 6.34438619e-02,  3.12160589e-02, -7.72275496e-03, ...,\n",
      "               1.49217555e-02,  3.86624038e-03, -1.16395289e-02],\n",
      "             [ 3.35849188e-02, -1.63664240e-02, -1.32562651e-03, ...,\n",
      "              -1.30512416e-02, -7.29435496e-03, -1.24825155e-02]],\n",
      "    \n",
      "            [[ 4.10873676e-03, -4.66612726e-03,  1.21031692e-02, ...,\n",
      "              -7.87103828e-03,  5.80726229e-02, -4.19587009e-02],\n",
      "             [-2.23153979e-02,  2.99241953e-02,  8.01213668e-04, ...,\n",
      "               1.82199273e-02,  9.57238674e-03, -8.57376456e-02],\n",
      "             [-2.01183017e-02, -1.96383689e-02,  7.32050464e-03, ...,\n",
      "              -1.07293837e-02, -2.17854325e-02, -7.95444921e-02]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[-1.71692297e-02, -3.16392444e-03,  2.40169745e-03, ...,\n",
      "              -9.67177004e-03,  9.26117748e-02, -1.16062798e-02],\n",
      "             [-8.63026828e-02,  3.55335064e-02, -1.06153013e-02, ...,\n",
      "               1.85809545e-02, -2.19932254e-02, -1.47949710e-01],\n",
      "             [-6.07556999e-02, -2.66596545e-02,  1.74473948e-03, ...,\n",
      "              -4.85855900e-03, -8.82942155e-02, -8.43590796e-02]],\n",
      "    \n",
      "            [[ 1.15142548e-02,  2.20947526e-03,  5.08834422e-03, ...,\n",
      "              -1.04352133e-02,  6.78158402e-02,  4.14623357e-02],\n",
      "             [ 7.41827395e-03,  4.52373996e-02, -1.10873608e-02, ...,\n",
      "               1.56368576e-02, -2.37460397e-02, -3.25448737e-02],\n",
      "             [ 7.84576032e-03, -2.45320965e-02,  5.84031455e-04, ...,\n",
      "              -8.31448287e-03, -8.92601907e-02, -3.36888898e-03]],\n",
      "    \n",
      "            [[ 4.79146978e-03, -4.22942545e-03,  1.15078716e-02, ...,\n",
      "              -2.12721284e-02,  4.96782959e-02,  2.05268860e-02],\n",
      "             [ 2.75192987e-02,  4.36737053e-02, -5.71439136e-03, ...,\n",
      "               9.46100149e-03, -8.58635467e-04, -1.79863740e-02],\n",
      "             [ 2.71184333e-02, -3.31169143e-02,  3.97488568e-03, ...,\n",
      "              -1.41424611e-02, -6.35233149e-02,  1.29984575e-03]]]],\n",
      "          dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 346, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 695, in wrapper\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "ValueError: in user code:\n",
      "\n",
      "    /tmp/ipykernel_2204/3096613398.py:57 train_step  *\n",
      "        self.optimizer.apply_gradients(\n",
      "    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:628 apply_gradients  **\n",
      "        self._create_all_weights(var_list)\n",
      "    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:815 _create_all_weights\n",
      "        self._create_slots(var_list)\n",
      "    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:117 _create_slots\n",
      "        self.add_slot(var, 'm')\n",
      "    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:892 add_slot\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x79c268046790>), which is different from the scope used for the original variable (<tf.Variable 'conv1_conv/kernel:0' shape=(7, 7, 3, 64) dtype=float32, numpy=\n",
      "    array([[[[ 2.82526277e-02, -1.18737184e-02,  1.51488732e-03, ...,\n",
      "              -1.07003953e-02, -5.27982824e-02, -1.36667420e-03],\n",
      "             [ 5.86827798e-03,  5.04415408e-02,  3.46324709e-03, ...,\n",
      "               1.01423981e-02,  1.39493728e-02,  1.67549420e-02],\n",
      "             [-2.44090753e-03, -4.86173332e-02,  2.69966386e-03, ...,\n",
      "              -3.44439060e-04,  3.48098315e-02,  6.28910400e-03]],\n",
      "    \n",
      "            [[ 1.81872323e-02, -7.20698107e-03,  4.80302610e-03, ...,\n",
      "              -7.43396254e-03, -8.56800564e-03,  1.16849300e-02],\n",
      "             [ 1.87554304e-02,  5.12730293e-02,  4.50406177e-03, ...,\n",
      "               1.39413681e-02,  1.26296384e-02, -1.73004344e-02],\n",
      "             [ 1.90453827e-02, -3.87909152e-02,  4.25842637e-03, ...,\n",
      "               2.75742816e-04, -1.27962548e-02, -8.35626759e-03]],\n",
      "    \n",
      "            [[ 1.58849321e-02, -1.06073255e-02,  1.30999666e-02, ...,\n",
      "              -2.26797583e-03, -3.98984266e-04,  3.39989027e-04],\n",
      "             [ 3.61421369e-02,  5.02430499e-02,  1.22699486e-02, ...,\n",
      "               1.19910473e-02,  2.02837810e-02, -1.96981970e-02],\n",
      "             [ 2.17959806e-02, -3.86004597e-02,  1.12379901e-02, ...,\n",
      "              -2.07756506e-03, -3.40645364e-03, -3.78638096e-02]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[-5.30153252e-02, -8.60502943e-03,  6.38643000e-03, ...,\n",
      "              -4.49256925e-03,  3.48024699e-03, -1.40979560e-02],\n",
      "             [-9.35578942e-02,  4.61557060e-02,  1.53722311e-03, ...,\n",
      "               1.21013075e-02,  5.05337631e-03,  3.30474339e-02],\n",
      "             [-7.69589692e-02, -3.51354294e-02,  2.22769519e-03, ...,\n",
      "               9.18304977e-06, -1.15465783e-02,  2.29630154e-02]],\n",
      "    \n",
      "            [[-4.73558307e-02, -4.07940615e-03,  4.76515992e-03, ...,\n",
      "              -9.73805040e-03, -1.03890402e-02,  1.62366014e-02],\n",
      "             [-1.24100089e-01,  4.78516519e-02, -9.90210217e-04, ...,\n",
      "               1.10340826e-02, -6.77202828e-03,  5.49102016e-02],\n",
      "             [-7.13113099e-02, -2.86470409e-02,  6.20829698e-04, ...,\n",
      "              -2.17762636e-03, -1.58942658e-02,  3.44766974e-02]],\n",
      "    \n",
      "            [[ 1.85429510e-02, -1.12518407e-02,  1.12506151e-02, ...,\n",
      "              -1.51338596e-02, -5.66656142e-03, -1.30050071e-02],\n",
      "             [-2.68079005e-02,  3.64737920e-02,  4.55197273e-03, ...,\n",
      "               5.53486776e-03,  1.12653999e-02,  2.46754289e-03],\n",
      "             [ 1.43940765e-02, -3.56382579e-02,  5.08728763e-03, ...,\n",
      "              -7.46753719e-03,  1.61169283e-02,  1.12382937e-02]]],\n",
      "    \n",
      "    \n",
      "           [[[ 7.99009297e-03, -9.49061289e-03, -4.21846565e-03, ...,\n",
      "              -1.23715792e-02, -3.82804796e-02, -5.90979494e-03],\n",
      "             [-7.68794632e-03,  5.46954982e-02, -1.03303632e-02, ...,\n",
      "               1.40626412e-02,  1.99436247e-02,  2.51518637e-02],\n",
      "             [ 3.70471564e-04, -3.70203964e-02, -9.80611611e-03, ...,\n",
      "              -4.95379185e-03,  2.27415562e-02,  1.38941938e-02]],\n",
      "    \n",
      "            [[ 2.48856675e-02, -9.57963988e-03, -2.37837038e-03, ...,\n",
      "              -1.08526833e-02,  2.24138368e-02, -2.40965877e-02],\n",
      "             [ 2.42966190e-02,  4.93442900e-02, -1.32921906e-02, ...,\n",
      "               1.47738317e-02,  2.67323572e-02,  1.14357602e-02],\n",
      "             [ 2.91274227e-02, -3.05654686e-02, -1.42364930e-02, ...,\n",
      "              -8.36174563e-03, -3.00847553e-02, -2.51545687e-03]],\n",
      "    \n",
      "            [[ 7.67260045e-02, -1.19650066e-02, -2.10191216e-03, ...,\n",
      "               1.79589365e-03,  2.02653632e-02, -1.33340694e-02],\n",
      "             [ 1.49444759e-01,  5.00719361e-02, -1.52172269e-02, ...,\n",
      "               1.83409695e-02,  1.56401172e-02,  8.53796005e-02],\n",
      "             [ 1.17180273e-01, -2.56576538e-02, -1.85890812e-02, ...,\n",
      "              -2.50462536e-03, -5.22738546e-02,  1.17943510e-02]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[-1.89151186e-02, -1.06457584e-02, -1.19606184e-03, ...,\n",
      "              -7.13960640e-03,  7.56816342e-02,  8.62411484e-02],\n",
      "             [ 1.33888470e-02,  4.24321182e-02, -1.93305630e-02, ...,\n",
      "               8.93499516e-03,  3.26688178e-02,  1.71118364e-01],\n",
      "             [-9.38678440e-03, -2.88689751e-02, -1.87061988e-02, ...,\n",
      "              -1.06920488e-02, -4.56195511e-02,  1.51734307e-01]],\n",
      "    \n",
      "            [[-7.93561861e-02, -8.69292021e-03,  1.06180850e-02, ...,\n",
      "              -8.22936464e-03,  5.34521677e-02,  2.43676770e-02],\n",
      "             [-1.76872283e-01,  4.03351039e-02, -6.91946782e-03, ...,\n",
      "               1.14902109e-02,  2.45164465e-02,  1.30252065e-02],\n",
      "             [-1.30214587e-01, -2.94868350e-02, -1.32359739e-03, ...,\n",
      "              -8.08166154e-03, -3.32693383e-02,  1.78283844e-02]],\n",
      "    \n",
      "            [[-1.53617216e-02, -1.02823023e-02,  1.44553250e-02, ...,\n",
      "              -1.23689836e-02,  2.81683691e-02, -1.52645903e-02],\n",
      "             [-1.22947149e-01,  3.72432098e-02, -2.82740779e-03, ...,\n",
      "               1.07275983e-02,  1.61965452e-02, -4.08420824e-02],\n",
      "             [-7.92325959e-02, -3.09139602e-02,  1.91061670e-04, ...,\n",
      "              -1.06926244e-02, -1.36199640e-02, -2.90216487e-02]]],\n",
      "    \n",
      "    \n",
      "           [[[-2.74732877e-02, -1.59629062e-02,  5.87167032e-03, ...,\n",
      "              -1.18064405e-02, -5.19699305e-02, -1.52737210e-02],\n",
      "             [-7.46604949e-02,  5.22083789e-02, -1.98963331e-03, ...,\n",
      "               1.27452025e-02,  7.53643783e-03, -1.96208209e-02],\n",
      "             [-3.34048420e-02, -3.39833461e-02, -1.99538236e-03, ...,\n",
      "              -9.30251833e-03,  3.30174603e-02, -1.65446047e-02]],\n",
      "    \n",
      "            [[-6.57535121e-02, -1.23513499e-02, -4.16519074e-03, ...,\n",
      "              -1.22041989e-03,  2.09396798e-02,  3.62350084e-02],\n",
      "             [-1.52494013e-01,  4.94739972e-02, -1.83443855e-02, ...,\n",
      "               2.37025358e-02,  2.67230812e-02,  8.47681686e-02],\n",
      "             [-8.80744159e-02, -2.57136654e-02, -2.17252262e-02, ...,\n",
      "              -3.12197860e-03, -2.06513535e-02,  6.63726628e-02]],\n",
      "    \n",
      "            [[ 1.99921392e-02, -1.76080931e-02,  1.81755237e-03, ...,\n",
      "               3.69562432e-02,  3.51557694e-02,  1.03931516e-01],\n",
      "             [ 6.10242449e-02,  4.46803048e-02, -1.41719123e-02, ...,\n",
      "               5.15808910e-02,  2.07974892e-02,  1.46060020e-01],\n",
      "             [ 8.05315524e-02, -2.88072433e-02, -1.85981095e-02, ...,\n",
      "               2.20173039e-02, -5.11762947e-02,  1.40093669e-01]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[ 1.15528561e-01, -1.67486407e-02,  8.49904679e-03, ...,\n",
      "               4.99674492e-03,  7.98972845e-02, -1.11083500e-01],\n",
      "             [ 3.32334489e-01,  4.24566194e-02, -9.70878359e-03, ...,\n",
      "               1.92873720e-02,  1.25060824e-03, -3.40990961e-01],\n",
      "             [ 2.16480315e-01, -2.68480480e-02, -8.96557700e-03, ...,\n",
      "              -6.44540135e-03, -7.85448179e-02, -2.04899684e-01]],\n",
      "    \n",
      "            [[-8.99803787e-02, -8.51823762e-03,  2.25046948e-02, ...,\n",
      "              -8.74274992e-04,  6.35959804e-02, -9.58404392e-02],\n",
      "             [-8.15074593e-02,  4.37885672e-02,  3.69152403e-03, ...,\n",
      "               1.71142723e-02,  6.33937493e-03, -2.73919165e-01],\n",
      "             [-9.73245725e-02, -2.61962153e-02,  8.95403326e-03, ...,\n",
      "              -7.23934872e-03, -5.64266555e-02, -1.84837982e-01]],\n",
      "    \n",
      "            [[-9.46454927e-02, -1.17739988e-02,  2.49665454e-02, ...,\n",
      "              -7.38179125e-03,  3.05740479e-02, -1.17530329e-02],\n",
      "             [-2.11111471e-01,  3.85808311e-02,  5.31885307e-03, ...,\n",
      "               1.61544569e-02,  3.10361455e-03, -8.36645439e-02],\n",
      "             [-1.75075874e-01, -3.21811885e-02,  9.45197884e-03, ...,\n",
      "              -1.05473688e-02, -2.80730613e-02, -6.67640790e-02]]],\n",
      "    \n",
      "    \n",
      "           ...,\n",
      "    \n",
      "    \n",
      "           [[[ 2.31804699e-02, -1.62718501e-02,  1.22078890e-02, ...,\n",
      "              -1.22131845e-02, -2.02786643e-02, -2.14508991e-03],\n",
      "             [ 2.30488200e-02,  4.41800952e-02,  3.59291583e-03, ...,\n",
      "               1.27932075e-02,  6.47032401e-03, -5.39429188e-02],\n",
      "             [ 2.03978457e-02, -2.67958529e-02,  5.69844292e-03, ...,\n",
      "              -8.20858125e-03,  2.51460597e-02, -3.12512405e-02]],\n",
      "    \n",
      "            [[-4.64516319e-02, -1.34653188e-02,  1.61393601e-02, ...,\n",
      "              -2.20572166e-02,  5.05596139e-02,  1.47165358e-03],\n",
      "             [-1.77852944e-01,  4.04180661e-02,  4.32515051e-03, ...,\n",
      "               7.27979047e-03,  1.37663782e-02, -5.00506982e-02],\n",
      "             [-1.09063022e-01, -2.11244933e-02,  6.98045455e-03, ...,\n",
      "              -2.00869981e-02, -6.30094185e-02, -4.20499854e-02]],\n",
      "    \n",
      "            [[-1.83006614e-01, -1.79655701e-02,  1.82811301e-02, ...,\n",
      "               1.56401389e-03,  9.29453745e-02,  4.12672907e-02],\n",
      "             [-4.11783189e-01,  3.40776965e-02,  8.74394365e-03, ...,\n",
      "               2.33494844e-02,  1.98237225e-02,  8.06325078e-02],\n",
      "             [-2.76736170e-01, -2.83147153e-02,  1.31541817e-02, ...,\n",
      "              -5.05925808e-03, -8.54580775e-02,  4.26753834e-02]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[ 5.36167026e-02, -1.07590063e-02,  2.19804980e-02, ...,\n",
      "              -8.83348845e-03,  1.40453711e-01,  3.20528477e-01],\n",
      "             [ 1.85792699e-01,  3.76442447e-02,  1.02089429e-02, ...,\n",
      "               1.29263047e-02, -3.70457745e-03,  6.66479290e-01],\n",
      "             [ 1.32038444e-01, -2.75047179e-02,  2.28339490e-02, ...,\n",
      "              -1.19996015e-02, -1.22367747e-01,  4.83815670e-01]],\n",
      "    \n",
      "            [[ 8.34956467e-02, -9.09057911e-03,  2.50242520e-02, ...,\n",
      "              -1.67011786e-02,  1.20522320e-01,  1.36462688e-01],\n",
      "             [ 2.50555605e-01,  4.07686047e-02,  1.08884834e-02, ...,\n",
      "               7.53540406e-03, -7.55708572e-03,  3.96415204e-01],\n",
      "             [ 1.49690762e-01, -3.11034787e-02,  2.43526250e-02, ...,\n",
      "              -1.65321939e-02, -1.09688722e-01,  2.64446586e-01]],\n",
      "    \n",
      "            [[ 3.69576029e-02, -1.27014471e-02,  3.19833457e-02, ...,\n",
      "              -1.48784053e-02,  9.22970548e-02,  6.54868260e-02],\n",
      "             [ 9.63706747e-02,  4.39107306e-02,  1.59802549e-02, ...,\n",
      "               1.22494521e-02,  8.10312852e-03,  1.78935930e-01],\n",
      "             [ 2.95156911e-02, -2.96487771e-02,  2.69996542e-02, ...,\n",
      "              -1.38547905e-02, -7.72434175e-02,  1.32773802e-01]]],\n",
      "    \n",
      "    \n",
      "           [[[ 4.22548056e-02, -8.30464344e-03,  5.34065207e-03, ...,\n",
      "              -8.06468353e-03, -4.70053628e-02,  4.45614867e-02],\n",
      "             [ 9.77012664e-02,  3.83502319e-02, -5.37837343e-03, ...,\n",
      "               1.17106764e-02, -4.59602941e-03,  6.98771998e-02],\n",
      "             [ 6.38262108e-02, -2.08319575e-02, -1.72756368e-03, ...,\n",
      "              -8.19445588e-03,  4.25621867e-02,  4.83920909e-02]],\n",
      "    \n",
      "            [[ 4.59470600e-02, -4.77699284e-03,  7.04339007e-03, ...,\n",
      "              -1.82104297e-02,  3.14848162e-02,  4.64068204e-02],\n",
      "             [ 3.89483608e-02,  3.78783308e-02, -6.85291924e-03, ...,\n",
      "               7.33014196e-03,  3.90656322e-04,  1.52848229e-01],\n",
      "             [ 4.57218140e-02, -1.34090437e-02, -8.30697361e-04, ...,\n",
      "              -1.85202472e-02, -3.45353335e-02,  9.25581828e-02]],\n",
      "    \n",
      "            [[-4.66161780e-02, -1.22223441e-02,  9.35023464e-03, ...,\n",
      "              -1.31351836e-02,  6.08736612e-02,  9.18865502e-02],\n",
      "             [-1.92336142e-01,  3.18407975e-02, -1.01881009e-03, ...,\n",
      "               7.55425170e-03, -8.62357323e-04,  2.88297594e-01],\n",
      "             [-1.15666650e-01, -2.35320851e-02,  6.74636895e-03, ...,\n",
      "              -1.94703583e-02, -5.66169359e-02,  1.95824102e-01]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[-2.10239179e-02, -9.81471874e-03,  9.81596112e-03, ...,\n",
      "              -1.36731779e-02,  1.20193027e-01, -1.26708716e-01],\n",
      "             [-3.72992679e-02,  3.05935629e-02, -3.00194928e-03, ...,\n",
      "               8.85152724e-03, -5.07611316e-03, -6.25461042e-02],\n",
      "             [ 7.84674310e-04, -2.91344281e-02,  1.12569630e-02, ...,\n",
      "              -1.38232643e-02, -9.49400812e-02, -8.74437019e-02]],\n",
      "    \n",
      "            [[ 3.32221799e-02, -4.22911346e-03,  1.13633750e-02, ...,\n",
      "              -1.41841583e-02,  9.59840789e-02, -1.23203963e-01],\n",
      "             [ 9.95653942e-02,  4.03233357e-02, -4.36036801e-03, ...,\n",
      "               8.42505507e-03, -1.50266392e-02, -1.58158958e-01],\n",
      "             [ 6.55353814e-02, -2.76978761e-02,  1.06595978e-02, ...,\n",
      "              -1.31017175e-02, -9.93799716e-02, -1.52014121e-01]],\n",
      "    \n",
      "            [[ 2.50522885e-02, -1.08845932e-02,  1.29567981e-02, ...,\n",
      "              -1.67823900e-02,  6.55406937e-02, -3.34061496e-02],\n",
      "             [ 1.00219429e-01,  4.24924381e-02, -4.06364352e-03, ...,\n",
      "               8.98410939e-03, -1.98677508e-03, -9.19047296e-02],\n",
      "             [ 6.97101504e-02, -3.41515057e-02,  8.97936709e-03, ...,\n",
      "              -1.51484888e-02, -8.06454644e-02, -8.53376985e-02]]],\n",
      "    \n",
      "    \n",
      "           [[[ 1.46303158e-02, -9.15218703e-03,  5.24803856e-03, ...,\n",
      "              -3.63799883e-03, -5.51798902e-02, -7.19531113e-03],\n",
      "             [ 6.12211153e-02,  2.67034862e-02, -4.38000960e-03, ...,\n",
      "               1.38858845e-02,  1.62421225e-03,  6.91889692e-03],\n",
      "             [ 1.86353922e-02, -2.39325576e-02,  5.56383107e-04, ...,\n",
      "              -6.68733614e-03,  7.36468807e-02,  3.71867418e-02]],\n",
      "    \n",
      "            [[ 3.52302976e-02, -3.27857491e-03,  7.14091491e-03, ...,\n",
      "              -9.93822515e-03,  2.38756705e-02, -2.10771449e-02],\n",
      "             [ 6.34438619e-02,  3.12160589e-02, -7.72275496e-03, ...,\n",
      "               1.49217555e-02,  3.86624038e-03, -1.16395289e-02],\n",
      "             [ 3.35849188e-02, -1.63664240e-02, -1.32562651e-03, ...,\n",
      "              -1.30512416e-02, -7.29435496e-03, -1.24825155e-02]],\n",
      "    \n",
      "            [[ 4.10873676e-03, -4.66612726e-03,  1.21031692e-02, ...,\n",
      "              -7.87103828e-03,  5.80726229e-02, -4.19587009e-02],\n",
      "             [-2.23153979e-02,  2.99241953e-02,  8.01213668e-04, ...,\n",
      "               1.82199273e-02,  9.57238674e-03, -8.57376456e-02],\n",
      "             [-2.01183017e-02, -1.96383689e-02,  7.32050464e-03, ...,\n",
      "              -1.07293837e-02, -2.17854325e-02, -7.95444921e-02]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[-1.71692297e-02, -3.16392444e-03,  2.40169745e-03, ...,\n",
      "              -9.67177004e-03,  9.26117748e-02, -1.16062798e-02],\n",
      "             [-8.63026828e-02,  3.55335064e-02, -1.06153013e-02, ...,\n",
      "               1.85809545e-02, -2.19932254e-02, -1.47949710e-01],\n",
      "             [-6.07556999e-02, -2.66596545e-02,  1.74473948e-03, ...,\n",
      "              -4.85855900e-03, -8.82942155e-02, -8.43590796e-02]],\n",
      "    \n",
      "            [[ 1.15142548e-02,  2.20947526e-03,  5.08834422e-03, ...,\n",
      "              -1.04352133e-02,  6.78158402e-02,  4.14623357e-02],\n",
      "             [ 7.41827395e-03,  4.52373996e-02, -1.10873608e-02, ...,\n",
      "               1.56368576e-02, -2.37460397e-02, -3.25448737e-02],\n",
      "             [ 7.84576032e-03, -2.45320965e-02,  5.84031455e-04, ...,\n",
      "              -8.31448287e-03, -8.92601907e-02, -3.36888898e-03]],\n",
      "    \n",
      "            [[ 4.79146978e-03, -4.22942545e-03,  1.15078716e-02, ...,\n",
      "              -2.12721284e-02,  4.96782959e-02,  2.05268860e-02],\n",
      "             [ 2.75192987e-02,  4.36737053e-02, -5.71439136e-03, ...,\n",
      "               9.46100149e-03, -8.58635467e-04, -1.79863740e-02],\n",
      "             [ 2.71184333e-02, -3.31169143e-02,  3.97488568e-03, ...,\n",
      "              -1.41424611e-02, -6.35233149e-02,  1.29984575e-03]]]],\n",
      "          dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /tmp/ipykernel_2204/3096613398.py:75 distributed_train_epoch  *\n        per_replica_loss = self.strategy.run(\n    /tmp/ipykernel_2204/3096613398.py:57 train_step  *\n        self.optimizer.apply_gradients(\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:628 apply_gradients  **\n        self._create_all_weights(var_list)\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:815 _create_all_weights\n        self._create_slots(var_list)\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:117 _create_slots\n        self.add_slot(var, 'm')\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:892 add_slot\n        raise ValueError(\n\n    ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x79c268046790>), which is different from the scope used for the original variable (<tf.Variable 'conv1_conv/kernel:0' shape=(7, 7, 3, 64) dtype=float32, numpy=\n    array([[[[ 2.82526277e-02, -1.18737184e-02,  1.51488732e-03, ...,\n              -1.07003953e-02, -5.27982824e-02, -1.36667420e-03],\n             [ 5.86827798e-03,  5.04415408e-02,  3.46324709e-03, ...,\n               1.01423981e-02,  1.39493728e-02,  1.67549420e-02],\n             [-2.44090753e-03, -4.86173332e-02,  2.69966386e-03, ...,\n              -3.44439060e-04,  3.48098315e-02,  6.28910400e-03]],\n    \n            [[ 1.81872323e-02, -7.20698107e-03,  4.80302610e-03, ...,\n              -7.43396254e-03, -8.56800564e-03,  1.16849300e-02],\n             [ 1.87554304e-02,  5.12730293e-02,  4.50406177e-03, ...,\n               1.39413681e-02,  1.26296384e-02, -1.73004344e-02],\n             [ 1.90453827e-02, -3.87909152e-02,  4.25842637e-03, ...,\n               2.75742816e-04, -1.27962548e-02, -8.35626759e-03]],\n    \n            [[ 1.58849321e-02, -1.06073255e-02,  1.30999666e-02, ...,\n              -2.26797583e-03, -3.98984266e-04,  3.39989027e-04],\n             [ 3.61421369e-02,  5.02430499e-02,  1.22699486e-02, ...,\n               1.19910473e-02,  2.02837810e-02, -1.96981970e-02],\n             [ 2.17959806e-02, -3.86004597e-02,  1.12379901e-02, ...,\n              -2.07756506e-03, -3.40645364e-03, -3.78638096e-02]],\n    \n            ...,\n    \n            [[-5.30153252e-02, -8.60502943e-03,  6.38643000e-03, ...,\n              -4.49256925e-03,  3.48024699e-03, -1.40979560e-02],\n             [-9.35578942e-02,  4.61557060e-02,  1.53722311e-03, ...,\n               1.21013075e-02,  5.05337631e-03,  3.30474339e-02],\n             [-7.69589692e-02, -3.51354294e-02,  2.22769519e-03, ...,\n               9.18304977e-06, -1.15465783e-02,  2.29630154e-02]],\n    \n            [[-4.73558307e-02, -4.07940615e-03,  4.76515992e-03, ...,\n              -9.73805040e-03, -1.03890402e-02,  1.62366014e-02],\n             [-1.24100089e-01,  4.78516519e-02, -9.90210217e-04, ...,\n               1.10340826e-02, -6.77202828e-03,  5.49102016e-02],\n             [-7.13113099e-02, -2.86470409e-02,  6.20829698e-04, ...,\n              -2.17762636e-03, -1.58942658e-02,  3.44766974e-02]],\n    \n            [[ 1.85429510e-02, -1.12518407e-02,  1.12506151e-02, ...,\n              -1.51338596e-02, -5.66656142e-03, -1.30050071e-02],\n             [-2.68079005e-02,  3.64737920e-02,  4.55197273e-03, ...,\n               5.53486776e-03,  1.12653999e-02,  2.46754289e-03],\n             [ 1.43940765e-02, -3.56382579e-02,  5.08728763e-03, ...,\n              -7.46753719e-03,  1.61169283e-02,  1.12382937e-02]]],\n    \n    \n           [[[ 7.99009297e-03, -9.49061289e-03, -4.21846565e-03, ...,\n              -1.23715792e-02, -3.82804796e-02, -5.90979494e-03],\n             [-7.68794632e-03,  5.46954982e-02, -1.03303632e-02, ...,\n               1.40626412e-02,  1.99436247e-02,  2.51518637e-02],\n             [ 3.70471564e-04, -3.70203964e-02, -9.80611611e-03, ...,\n              -4.95379185e-03,  2.27415562e-02,  1.38941938e-02]],\n    \n            [[ 2.48856675e-02, -9.57963988e-03, -2.37837038e-03, ...,\n              -1.08526833e-02,  2.24138368e-02, -2.40965877e-02],\n             [ 2.42966190e-02,  4.93442900e-02, -1.32921906e-02, ...,\n               1.47738317e-02,  2.67323572e-02,  1.14357602e-02],\n             [ 2.91274227e-02, -3.05654686e-02, -1.42364930e-02, ...,\n              -8.36174563e-03, -3.00847553e-02, -2.51545687e-03]],\n    \n            [[ 7.67260045e-02, -1.19650066e-02, -2.10191216e-03, ...,\n               1.79589365e-03,  2.02653632e-02, -1.33340694e-02],\n             [ 1.49444759e-01,  5.00719361e-02, -1.52172269e-02, ...,\n               1.83409695e-02,  1.56401172e-02,  8.53796005e-02],\n             [ 1.17180273e-01, -2.56576538e-02, -1.85890812e-02, ...,\n              -2.50462536e-03, -5.22738546e-02,  1.17943510e-02]],\n    \n            ...,\n    \n            [[-1.89151186e-02, -1.06457584e-02, -1.19606184e-03, ...,\n              -7.13960640e-03,  7.56816342e-02,  8.62411484e-02],\n             [ 1.33888470e-02,  4.24321182e-02, -1.93305630e-02, ...,\n               8.93499516e-03,  3.26688178e-02,  1.71118364e-01],\n             [-9.38678440e-03, -2.88689751e-02, -1.87061988e-02, ...,\n              -1.06920488e-02, -4.56195511e-02,  1.51734307e-01]],\n    \n            [[-7.93561861e-02, -8.69292021e-03,  1.06180850e-02, ...,\n              -8.22936464e-03,  5.34521677e-02,  2.43676770e-02],\n             [-1.76872283e-01,  4.03351039e-02, -6.91946782e-03, ...,\n               1.14902109e-02,  2.45164465e-02,  1.30252065e-02],\n             [-1.30214587e-01, -2.94868350e-02, -1.32359739e-03, ...,\n              -8.08166154e-03, -3.32693383e-02,  1.78283844e-02]],\n    \n            [[-1.53617216e-02, -1.02823023e-02,  1.44553250e-02, ...,\n              -1.23689836e-02,  2.81683691e-02, -1.52645903e-02],\n             [-1.22947149e-01,  3.72432098e-02, -2.82740779e-03, ...,\n               1.07275983e-02,  1.61965452e-02, -4.08420824e-02],\n             [-7.92325959e-02, -3.09139602e-02,  1.91061670e-04, ...,\n              -1.06926244e-02, -1.36199640e-02, -2.90216487e-02]]],\n    \n    \n           [[[-2.74732877e-02, -1.59629062e-02,  5.87167032e-03, ...,\n              -1.18064405e-02, -5.19699305e-02, -1.52737210e-02],\n             [-7.46604949e-02,  5.22083789e-02, -1.98963331e-03, ...,\n               1.27452025e-02,  7.53643783e-03, -1.96208209e-02],\n             [-3.34048420e-02, -3.39833461e-02, -1.99538236e-03, ...,\n              -9.30251833e-03,  3.30174603e-02, -1.65446047e-02]],\n    \n            [[-6.57535121e-02, -1.23513499e-02, -4.16519074e-03, ...,\n              -1.22041989e-03,  2.09396798e-02,  3.62350084e-02],\n             [-1.52494013e-01,  4.94739972e-02, -1.83443855e-02, ...,\n               2.37025358e-02,  2.67230812e-02,  8.47681686e-02],\n             [-8.80744159e-02, -2.57136654e-02, -2.17252262e-02, ...,\n              -3.12197860e-03, -2.06513535e-02,  6.63726628e-02]],\n    \n            [[ 1.99921392e-02, -1.76080931e-02,  1.81755237e-03, ...,\n               3.69562432e-02,  3.51557694e-02,  1.03931516e-01],\n             [ 6.10242449e-02,  4.46803048e-02, -1.41719123e-02, ...,\n               5.15808910e-02,  2.07974892e-02,  1.46060020e-01],\n             [ 8.05315524e-02, -2.88072433e-02, -1.85981095e-02, ...,\n               2.20173039e-02, -5.11762947e-02,  1.40093669e-01]],\n    \n            ...,\n    \n            [[ 1.15528561e-01, -1.67486407e-02,  8.49904679e-03, ...,\n               4.99674492e-03,  7.98972845e-02, -1.11083500e-01],\n             [ 3.32334489e-01,  4.24566194e-02, -9.70878359e-03, ...,\n               1.92873720e-02,  1.25060824e-03, -3.40990961e-01],\n             [ 2.16480315e-01, -2.68480480e-02, -8.96557700e-03, ...,\n              -6.44540135e-03, -7.85448179e-02, -2.04899684e-01]],\n    \n            [[-8.99803787e-02, -8.51823762e-03,  2.25046948e-02, ...,\n              -8.74274992e-04,  6.35959804e-02, -9.58404392e-02],\n             [-8.15074593e-02,  4.37885672e-02,  3.69152403e-03, ...,\n               1.71142723e-02,  6.33937493e-03, -2.73919165e-01],\n             [-9.73245725e-02, -2.61962153e-02,  8.95403326e-03, ...,\n              -7.23934872e-03, -5.64266555e-02, -1.84837982e-01]],\n    \n            [[-9.46454927e-02, -1.17739988e-02,  2.49665454e-02, ...,\n              -7.38179125e-03,  3.05740479e-02, -1.17530329e-02],\n             [-2.11111471e-01,  3.85808311e-02,  5.31885307e-03, ...,\n               1.61544569e-02,  3.10361455e-03, -8.36645439e-02],\n             [-1.75075874e-01, -3.21811885e-02,  9.45197884e-03, ...,\n              -1.05473688e-02, -2.80730613e-02, -6.67640790e-02]]],\n    \n    \n           ...,\n    \n    \n           [[[ 2.31804699e-02, -1.62718501e-02,  1.22078890e-02, ...,\n              -1.22131845e-02, -2.02786643e-02, -2.14508991e-03],\n             [ 2.30488200e-02,  4.41800952e-02,  3.59291583e-03, ...,\n               1.27932075e-02,  6.47032401e-03, -5.39429188e-02],\n             [ 2.03978457e-02, -2.67958529e-02,  5.69844292e-03, ...,\n              -8.20858125e-03,  2.51460597e-02, -3.12512405e-02]],\n    \n            [[-4.64516319e-02, -1.34653188e-02,  1.61393601e-02, ...,\n              -2.20572166e-02,  5.05596139e-02,  1.47165358e-03],\n             [-1.77852944e-01,  4.04180661e-02,  4.32515051e-03, ...,\n               7.27979047e-03,  1.37663782e-02, -5.00506982e-02],\n             [-1.09063022e-01, -2.11244933e-02,  6.98045455e-03, ...,\n              -2.00869981e-02, -6.30094185e-02, -4.20499854e-02]],\n    \n            [[-1.83006614e-01, -1.79655701e-02,  1.82811301e-02, ...,\n               1.56401389e-03,  9.29453745e-02,  4.12672907e-02],\n             [-4.11783189e-01,  3.40776965e-02,  8.74394365e-03, ...,\n               2.33494844e-02,  1.98237225e-02,  8.06325078e-02],\n             [-2.76736170e-01, -2.83147153e-02,  1.31541817e-02, ...,\n              -5.05925808e-03, -8.54580775e-02,  4.26753834e-02]],\n    \n            ...,\n    \n            [[ 5.36167026e-02, -1.07590063e-02,  2.19804980e-02, ...,\n              -8.83348845e-03,  1.40453711e-01,  3.20528477e-01],\n             [ 1.85792699e-01,  3.76442447e-02,  1.02089429e-02, ...,\n               1.29263047e-02, -3.70457745e-03,  6.66479290e-01],\n             [ 1.32038444e-01, -2.75047179e-02,  2.28339490e-02, ...,\n              -1.19996015e-02, -1.22367747e-01,  4.83815670e-01]],\n    \n            [[ 8.34956467e-02, -9.09057911e-03,  2.50242520e-02, ...,\n              -1.67011786e-02,  1.20522320e-01,  1.36462688e-01],\n             [ 2.50555605e-01,  4.07686047e-02,  1.08884834e-02, ...,\n               7.53540406e-03, -7.55708572e-03,  3.96415204e-01],\n             [ 1.49690762e-01, -3.11034787e-02,  2.43526250e-02, ...,\n              -1.65321939e-02, -1.09688722e-01,  2.64446586e-01]],\n    \n            [[ 3.69576029e-02, -1.27014471e-02,  3.19833457e-02, ...,\n              -1.48784053e-02,  9.22970548e-02,  6.54868260e-02],\n             [ 9.63706747e-02,  4.39107306e-02,  1.59802549e-02, ...,\n               1.22494521e-02,  8.10312852e-03,  1.78935930e-01],\n             [ 2.95156911e-02, -2.96487771e-02,  2.69996542e-02, ...,\n              -1.38547905e-02, -7.72434175e-02,  1.32773802e-01]]],\n    \n    \n           [[[ 4.22548056e-02, -8.30464344e-03,  5.34065207e-03, ...,\n              -8.06468353e-03, -4.70053628e-02,  4.45614867e-02],\n             [ 9.77012664e-02,  3.83502319e-02, -5.37837343e-03, ...,\n               1.17106764e-02, -4.59602941e-03,  6.98771998e-02],\n             [ 6.38262108e-02, -2.08319575e-02, -1.72756368e-03, ...,\n              -8.19445588e-03,  4.25621867e-02,  4.83920909e-02]],\n    \n            [[ 4.59470600e-02, -4.77699284e-03,  7.04339007e-03, ...,\n              -1.82104297e-02,  3.14848162e-02,  4.64068204e-02],\n             [ 3.89483608e-02,  3.78783308e-02, -6.85291924e-03, ...,\n               7.33014196e-03,  3.90656322e-04,  1.52848229e-01],\n             [ 4.57218140e-02, -1.34090437e-02, -8.30697361e-04, ...,\n              -1.85202472e-02, -3.45353335e-02,  9.25581828e-02]],\n    \n            [[-4.66161780e-02, -1.22223441e-02,  9.35023464e-03, ...,\n              -1.31351836e-02,  6.08736612e-02,  9.18865502e-02],\n             [-1.92336142e-01,  3.18407975e-02, -1.01881009e-03, ...,\n               7.55425170e-03, -8.62357323e-04,  2.88297594e-01],\n             [-1.15666650e-01, -2.35320851e-02,  6.74636895e-03, ...,\n              -1.94703583e-02, -5.66169359e-02,  1.95824102e-01]],\n    \n            ...,\n    \n            [[-2.10239179e-02, -9.81471874e-03,  9.81596112e-03, ...,\n              -1.36731779e-02,  1.20193027e-01, -1.26708716e-01],\n             [-3.72992679e-02,  3.05935629e-02, -3.00194928e-03, ...,\n               8.85152724e-03, -5.07611316e-03, -6.25461042e-02],\n             [ 7.84674310e-04, -2.91344281e-02,  1.12569630e-02, ...,\n              -1.38232643e-02, -9.49400812e-02, -8.74437019e-02]],\n    \n            [[ 3.32221799e-02, -4.22911346e-03,  1.13633750e-02, ...,\n              -1.41841583e-02,  9.59840789e-02, -1.23203963e-01],\n             [ 9.95653942e-02,  4.03233357e-02, -4.36036801e-03, ...,\n               8.42505507e-03, -1.50266392e-02, -1.58158958e-01],\n             [ 6.55353814e-02, -2.76978761e-02,  1.06595978e-02, ...,\n              -1.31017175e-02, -9.93799716e-02, -1.52014121e-01]],\n    \n            [[ 2.50522885e-02, -1.08845932e-02,  1.29567981e-02, ...,\n              -1.67823900e-02,  6.55406937e-02, -3.34061496e-02],\n             [ 1.00219429e-01,  4.24924381e-02, -4.06364352e-03, ...,\n               8.98410939e-03, -1.98677508e-03, -9.19047296e-02],\n             [ 6.97101504e-02, -3.41515057e-02,  8.97936709e-03, ...,\n              -1.51484888e-02, -8.06454644e-02, -8.53376985e-02]]],\n    \n    \n           [[[ 1.46303158e-02, -9.15218703e-03,  5.24803856e-03, ...,\n              -3.63799883e-03, -5.51798902e-02, -7.19531113e-03],\n             [ 6.12211153e-02,  2.67034862e-02, -4.38000960e-03, ...,\n               1.38858845e-02,  1.62421225e-03,  6.91889692e-03],\n             [ 1.86353922e-02, -2.39325576e-02,  5.56383107e-04, ...,\n              -6.68733614e-03,  7.36468807e-02,  3.71867418e-02]],\n    \n            [[ 3.52302976e-02, -3.27857491e-03,  7.14091491e-03, ...,\n              -9.93822515e-03,  2.38756705e-02, -2.10771449e-02],\n             [ 6.34438619e-02,  3.12160589e-02, -7.72275496e-03, ...,\n               1.49217555e-02,  3.86624038e-03, -1.16395289e-02],\n             [ 3.35849188e-02, -1.63664240e-02, -1.32562651e-03, ...,\n              -1.30512416e-02, -7.29435496e-03, -1.24825155e-02]],\n    \n            [[ 4.10873676e-03, -4.66612726e-03,  1.21031692e-02, ...,\n              -7.87103828e-03,  5.80726229e-02, -4.19587009e-02],\n             [-2.23153979e-02,  2.99241953e-02,  8.01213668e-04, ...,\n               1.82199273e-02,  9.57238674e-03, -8.57376456e-02],\n             [-2.01183017e-02, -1.96383689e-02,  7.32050464e-03, ...,\n              -1.07293837e-02, -2.17854325e-02, -7.95444921e-02]],\n    \n            ...,\n    \n            [[-1.71692297e-02, -3.16392444e-03,  2.40169745e-03, ...,\n              -9.67177004e-03,  9.26117748e-02, -1.16062798e-02],\n             [-8.63026828e-02,  3.55335064e-02, -1.06153013e-02, ...,\n               1.85809545e-02, -2.19932254e-02, -1.47949710e-01],\n             [-6.07556999e-02, -2.66596545e-02,  1.74473948e-03, ...,\n              -4.85855900e-03, -8.82942155e-02, -8.43590796e-02]],\n    \n            [[ 1.15142548e-02,  2.20947526e-03,  5.08834422e-03, ...,\n              -1.04352133e-02,  6.78158402e-02,  4.14623357e-02],\n             [ 7.41827395e-03,  4.52373996e-02, -1.10873608e-02, ...,\n               1.56368576e-02, -2.37460397e-02, -3.25448737e-02],\n             [ 7.84576032e-03, -2.45320965e-02,  5.84031455e-04, ...,\n              -8.31448287e-03, -8.92601907e-02, -3.36888898e-03]],\n    \n            [[ 4.79146978e-03, -4.22942545e-03,  1.15078716e-02, ...,\n              -2.12721284e-02,  4.96782959e-02,  2.05268860e-02],\n             [ 2.75192987e-02,  4.36737053e-02, -5.71439136e-03, ...,\n               9.46100149e-03, -8.58635467e-04, -1.79863740e-02],\n             [ 2.71184333e-02, -3.31169143e-02,  3.97488568e-03, ...,\n              -1.41424611e-02, -6.35233149e-02,  1.29984575e-03]]]],\n          dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2204/2290517386.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mbest_model_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tfrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_tfrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2204/4078933612.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dist_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dist_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2204/773057136.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, train_dist_dataset, val_dist_dataset)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 epoch, self.current_learning_rate))\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             train_total_loss, num_train_batches = distributed_train_epoch(\n\u001b[0m\u001b[1;32m    111\u001b[0m                 train_dist_dataset)\n\u001b[1;32m    112\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_total_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_train_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 759\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    760\u001b[0m             *args, **kwds))\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /tmp/ipykernel_2204/3096613398.py:75 distributed_train_epoch  *\n        per_replica_loss = self.strategy.run(\n    /tmp/ipykernel_2204/3096613398.py:57 train_step  *\n        self.optimizer.apply_gradients(\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:628 apply_gradients  **\n        self._create_all_weights(var_list)\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:815 _create_all_weights\n        self._create_slots(var_list)\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:117 _create_slots\n        self.add_slot(var, 'm')\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:892 add_slot\n        raise ValueError(\n\n    ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x79c268046790>), which is different from the scope used for the original variable (<tf.Variable 'conv1_conv/kernel:0' shape=(7, 7, 3, 64) dtype=float32, numpy=\n    array([[[[ 2.82526277e-02, -1.18737184e-02,  1.51488732e-03, ...,\n              -1.07003953e-02, -5.27982824e-02, -1.36667420e-03],\n             [ 5.86827798e-03,  5.04415408e-02,  3.46324709e-03, ...,\n               1.01423981e-02,  1.39493728e-02,  1.67549420e-02],\n             [-2.44090753e-03, -4.86173332e-02,  2.69966386e-03, ...,\n              -3.44439060e-04,  3.48098315e-02,  6.28910400e-03]],\n    \n            [[ 1.81872323e-02, -7.20698107e-03,  4.80302610e-03, ...,\n              -7.43396254e-03, -8.56800564e-03,  1.16849300e-02],\n             [ 1.87554304e-02,  5.12730293e-02,  4.50406177e-03, ...,\n               1.39413681e-02,  1.26296384e-02, -1.73004344e-02],\n             [ 1.90453827e-02, -3.87909152e-02,  4.25842637e-03, ...,\n               2.75742816e-04, -1.27962548e-02, -8.35626759e-03]],\n    \n            [[ 1.58849321e-02, -1.06073255e-02,  1.30999666e-02, ...,\n              -2.26797583e-03, -3.98984266e-04,  3.39989027e-04],\n             [ 3.61421369e-02,  5.02430499e-02,  1.22699486e-02, ...,\n               1.19910473e-02,  2.02837810e-02, -1.96981970e-02],\n             [ 2.17959806e-02, -3.86004597e-02,  1.12379901e-02, ...,\n              -2.07756506e-03, -3.40645364e-03, -3.78638096e-02]],\n    \n            ...,\n    \n            [[-5.30153252e-02, -8.60502943e-03,  6.38643000e-03, ...,\n              -4.49256925e-03,  3.48024699e-03, -1.40979560e-02],\n             [-9.35578942e-02,  4.61557060e-02,  1.53722311e-03, ...,\n               1.21013075e-02,  5.05337631e-03,  3.30474339e-02],\n             [-7.69589692e-02, -3.51354294e-02,  2.22769519e-03, ...,\n               9.18304977e-06, -1.15465783e-02,  2.29630154e-02]],\n    \n            [[-4.73558307e-02, -4.07940615e-03,  4.76515992e-03, ...,\n              -9.73805040e-03, -1.03890402e-02,  1.62366014e-02],\n             [-1.24100089e-01,  4.78516519e-02, -9.90210217e-04, ...,\n               1.10340826e-02, -6.77202828e-03,  5.49102016e-02],\n             [-7.13113099e-02, -2.86470409e-02,  6.20829698e-04, ...,\n              -2.17762636e-03, -1.58942658e-02,  3.44766974e-02]],\n    \n            [[ 1.85429510e-02, -1.12518407e-02,  1.12506151e-02, ...,\n              -1.51338596e-02, -5.66656142e-03, -1.30050071e-02],\n             [-2.68079005e-02,  3.64737920e-02,  4.55197273e-03, ...,\n               5.53486776e-03,  1.12653999e-02,  2.46754289e-03],\n             [ 1.43940765e-02, -3.56382579e-02,  5.08728763e-03, ...,\n              -7.46753719e-03,  1.61169283e-02,  1.12382937e-02]]],\n    \n    \n           [[[ 7.99009297e-03, -9.49061289e-03, -4.21846565e-03, ...,\n              -1.23715792e-02, -3.82804796e-02, -5.90979494e-03],\n             [-7.68794632e-03,  5.46954982e-02, -1.03303632e-02, ...,\n               1.40626412e-02,  1.99436247e-02,  2.51518637e-02],\n             [ 3.70471564e-04, -3.70203964e-02, -9.80611611e-03, ...,\n              -4.95379185e-03,  2.27415562e-02,  1.38941938e-02]],\n    \n            [[ 2.48856675e-02, -9.57963988e-03, -2.37837038e-03, ...,\n              -1.08526833e-02,  2.24138368e-02, -2.40965877e-02],\n             [ 2.42966190e-02,  4.93442900e-02, -1.32921906e-02, ...,\n               1.47738317e-02,  2.67323572e-02,  1.14357602e-02],\n             [ 2.91274227e-02, -3.05654686e-02, -1.42364930e-02, ...,\n              -8.36174563e-03, -3.00847553e-02, -2.51545687e-03]],\n    \n            [[ 7.67260045e-02, -1.19650066e-02, -2.10191216e-03, ...,\n               1.79589365e-03,  2.02653632e-02, -1.33340694e-02],\n             [ 1.49444759e-01,  5.00719361e-02, -1.52172269e-02, ...,\n               1.83409695e-02,  1.56401172e-02,  8.53796005e-02],\n             [ 1.17180273e-01, -2.56576538e-02, -1.85890812e-02, ...,\n              -2.50462536e-03, -5.22738546e-02,  1.17943510e-02]],\n    \n            ...,\n    \n            [[-1.89151186e-02, -1.06457584e-02, -1.19606184e-03, ...,\n              -7.13960640e-03,  7.56816342e-02,  8.62411484e-02],\n             [ 1.33888470e-02,  4.24321182e-02, -1.93305630e-02, ...,\n               8.93499516e-03,  3.26688178e-02,  1.71118364e-01],\n             [-9.38678440e-03, -2.88689751e-02, -1.87061988e-02, ...,\n              -1.06920488e-02, -4.56195511e-02,  1.51734307e-01]],\n    \n            [[-7.93561861e-02, -8.69292021e-03,  1.06180850e-02, ...,\n              -8.22936464e-03,  5.34521677e-02,  2.43676770e-02],\n             [-1.76872283e-01,  4.03351039e-02, -6.91946782e-03, ...,\n               1.14902109e-02,  2.45164465e-02,  1.30252065e-02],\n             [-1.30214587e-01, -2.94868350e-02, -1.32359739e-03, ...,\n              -8.08166154e-03, -3.32693383e-02,  1.78283844e-02]],\n    \n            [[-1.53617216e-02, -1.02823023e-02,  1.44553250e-02, ...,\n              -1.23689836e-02,  2.81683691e-02, -1.52645903e-02],\n             [-1.22947149e-01,  3.72432098e-02, -2.82740779e-03, ...,\n               1.07275983e-02,  1.61965452e-02, -4.08420824e-02],\n             [-7.92325959e-02, -3.09139602e-02,  1.91061670e-04, ...,\n              -1.06926244e-02, -1.36199640e-02, -2.90216487e-02]]],\n    \n    \n           [[[-2.74732877e-02, -1.59629062e-02,  5.87167032e-03, ...,\n              -1.18064405e-02, -5.19699305e-02, -1.52737210e-02],\n             [-7.46604949e-02,  5.22083789e-02, -1.98963331e-03, ...,\n               1.27452025e-02,  7.53643783e-03, -1.96208209e-02],\n             [-3.34048420e-02, -3.39833461e-02, -1.99538236e-03, ...,\n              -9.30251833e-03,  3.30174603e-02, -1.65446047e-02]],\n    \n            [[-6.57535121e-02, -1.23513499e-02, -4.16519074e-03, ...,\n              -1.22041989e-03,  2.09396798e-02,  3.62350084e-02],\n             [-1.52494013e-01,  4.94739972e-02, -1.83443855e-02, ...,\n               2.37025358e-02,  2.67230812e-02,  8.47681686e-02],\n             [-8.80744159e-02, -2.57136654e-02, -2.17252262e-02, ...,\n              -3.12197860e-03, -2.06513535e-02,  6.63726628e-02]],\n    \n            [[ 1.99921392e-02, -1.76080931e-02,  1.81755237e-03, ...,\n               3.69562432e-02,  3.51557694e-02,  1.03931516e-01],\n             [ 6.10242449e-02,  4.46803048e-02, -1.41719123e-02, ...,\n               5.15808910e-02,  2.07974892e-02,  1.46060020e-01],\n             [ 8.05315524e-02, -2.88072433e-02, -1.85981095e-02, ...,\n               2.20173039e-02, -5.11762947e-02,  1.40093669e-01]],\n    \n            ...,\n    \n            [[ 1.15528561e-01, -1.67486407e-02,  8.49904679e-03, ...,\n               4.99674492e-03,  7.98972845e-02, -1.11083500e-01],\n             [ 3.32334489e-01,  4.24566194e-02, -9.70878359e-03, ...,\n               1.92873720e-02,  1.25060824e-03, -3.40990961e-01],\n             [ 2.16480315e-01, -2.68480480e-02, -8.96557700e-03, ...,\n              -6.44540135e-03, -7.85448179e-02, -2.04899684e-01]],\n    \n            [[-8.99803787e-02, -8.51823762e-03,  2.25046948e-02, ...,\n              -8.74274992e-04,  6.35959804e-02, -9.58404392e-02],\n             [-8.15074593e-02,  4.37885672e-02,  3.69152403e-03, ...,\n               1.71142723e-02,  6.33937493e-03, -2.73919165e-01],\n             [-9.73245725e-02, -2.61962153e-02,  8.95403326e-03, ...,\n              -7.23934872e-03, -5.64266555e-02, -1.84837982e-01]],\n    \n            [[-9.46454927e-02, -1.17739988e-02,  2.49665454e-02, ...,\n              -7.38179125e-03,  3.05740479e-02, -1.17530329e-02],\n             [-2.11111471e-01,  3.85808311e-02,  5.31885307e-03, ...,\n               1.61544569e-02,  3.10361455e-03, -8.36645439e-02],\n             [-1.75075874e-01, -3.21811885e-02,  9.45197884e-03, ...,\n              -1.05473688e-02, -2.80730613e-02, -6.67640790e-02]]],\n    \n    \n           ...,\n    \n    \n           [[[ 2.31804699e-02, -1.62718501e-02,  1.22078890e-02, ...,\n              -1.22131845e-02, -2.02786643e-02, -2.14508991e-03],\n             [ 2.30488200e-02,  4.41800952e-02,  3.59291583e-03, ...,\n               1.27932075e-02,  6.47032401e-03, -5.39429188e-02],\n             [ 2.03978457e-02, -2.67958529e-02,  5.69844292e-03, ...,\n              -8.20858125e-03,  2.51460597e-02, -3.12512405e-02]],\n    \n            [[-4.64516319e-02, -1.34653188e-02,  1.61393601e-02, ...,\n              -2.20572166e-02,  5.05596139e-02,  1.47165358e-03],\n             [-1.77852944e-01,  4.04180661e-02,  4.32515051e-03, ...,\n               7.27979047e-03,  1.37663782e-02, -5.00506982e-02],\n             [-1.09063022e-01, -2.11244933e-02,  6.98045455e-03, ...,\n              -2.00869981e-02, -6.30094185e-02, -4.20499854e-02]],\n    \n            [[-1.83006614e-01, -1.79655701e-02,  1.82811301e-02, ...,\n               1.56401389e-03,  9.29453745e-02,  4.12672907e-02],\n             [-4.11783189e-01,  3.40776965e-02,  8.74394365e-03, ...,\n               2.33494844e-02,  1.98237225e-02,  8.06325078e-02],\n             [-2.76736170e-01, -2.83147153e-02,  1.31541817e-02, ...,\n              -5.05925808e-03, -8.54580775e-02,  4.26753834e-02]],\n    \n            ...,\n    \n            [[ 5.36167026e-02, -1.07590063e-02,  2.19804980e-02, ...,\n              -8.83348845e-03,  1.40453711e-01,  3.20528477e-01],\n             [ 1.85792699e-01,  3.76442447e-02,  1.02089429e-02, ...,\n               1.29263047e-02, -3.70457745e-03,  6.66479290e-01],\n             [ 1.32038444e-01, -2.75047179e-02,  2.28339490e-02, ...,\n              -1.19996015e-02, -1.22367747e-01,  4.83815670e-01]],\n    \n            [[ 8.34956467e-02, -9.09057911e-03,  2.50242520e-02, ...,\n              -1.67011786e-02,  1.20522320e-01,  1.36462688e-01],\n             [ 2.50555605e-01,  4.07686047e-02,  1.08884834e-02, ...,\n               7.53540406e-03, -7.55708572e-03,  3.96415204e-01],\n             [ 1.49690762e-01, -3.11034787e-02,  2.43526250e-02, ...,\n              -1.65321939e-02, -1.09688722e-01,  2.64446586e-01]],\n    \n            [[ 3.69576029e-02, -1.27014471e-02,  3.19833457e-02, ...,\n              -1.48784053e-02,  9.22970548e-02,  6.54868260e-02],\n             [ 9.63706747e-02,  4.39107306e-02,  1.59802549e-02, ...,\n               1.22494521e-02,  8.10312852e-03,  1.78935930e-01],\n             [ 2.95156911e-02, -2.96487771e-02,  2.69996542e-02, ...,\n              -1.38547905e-02, -7.72434175e-02,  1.32773802e-01]]],\n    \n    \n           [[[ 4.22548056e-02, -8.30464344e-03,  5.34065207e-03, ...,\n              -8.06468353e-03, -4.70053628e-02,  4.45614867e-02],\n             [ 9.77012664e-02,  3.83502319e-02, -5.37837343e-03, ...,\n               1.17106764e-02, -4.59602941e-03,  6.98771998e-02],\n             [ 6.38262108e-02, -2.08319575e-02, -1.72756368e-03, ...,\n              -8.19445588e-03,  4.25621867e-02,  4.83920909e-02]],\n    \n            [[ 4.59470600e-02, -4.77699284e-03,  7.04339007e-03, ...,\n              -1.82104297e-02,  3.14848162e-02,  4.64068204e-02],\n             [ 3.89483608e-02,  3.78783308e-02, -6.85291924e-03, ...,\n               7.33014196e-03,  3.90656322e-04,  1.52848229e-01],\n             [ 4.57218140e-02, -1.34090437e-02, -8.30697361e-04, ...,\n              -1.85202472e-02, -3.45353335e-02,  9.25581828e-02]],\n    \n            [[-4.66161780e-02, -1.22223441e-02,  9.35023464e-03, ...,\n              -1.31351836e-02,  6.08736612e-02,  9.18865502e-02],\n             [-1.92336142e-01,  3.18407975e-02, -1.01881009e-03, ...,\n               7.55425170e-03, -8.62357323e-04,  2.88297594e-01],\n             [-1.15666650e-01, -2.35320851e-02,  6.74636895e-03, ...,\n              -1.94703583e-02, -5.66169359e-02,  1.95824102e-01]],\n    \n            ...,\n    \n            [[-2.10239179e-02, -9.81471874e-03,  9.81596112e-03, ...,\n              -1.36731779e-02,  1.20193027e-01, -1.26708716e-01],\n             [-3.72992679e-02,  3.05935629e-02, -3.00194928e-03, ...,\n               8.85152724e-03, -5.07611316e-03, -6.25461042e-02],\n             [ 7.84674310e-04, -2.91344281e-02,  1.12569630e-02, ...,\n              -1.38232643e-02, -9.49400812e-02, -8.74437019e-02]],\n    \n            [[ 3.32221799e-02, -4.22911346e-03,  1.13633750e-02, ...,\n              -1.41841583e-02,  9.59840789e-02, -1.23203963e-01],\n             [ 9.95653942e-02,  4.03233357e-02, -4.36036801e-03, ...,\n               8.42505507e-03, -1.50266392e-02, -1.58158958e-01],\n             [ 6.55353814e-02, -2.76978761e-02,  1.06595978e-02, ...,\n              -1.31017175e-02, -9.93799716e-02, -1.52014121e-01]],\n    \n            [[ 2.50522885e-02, -1.08845932e-02,  1.29567981e-02, ...,\n              -1.67823900e-02,  6.55406937e-02, -3.34061496e-02],\n             [ 1.00219429e-01,  4.24924381e-02, -4.06364352e-03, ...,\n               8.98410939e-03, -1.98677508e-03, -9.19047296e-02],\n             [ 6.97101504e-02, -3.41515057e-02,  8.97936709e-03, ...,\n              -1.51484888e-02, -8.06454644e-02, -8.53376985e-02]]],\n    \n    \n           [[[ 1.46303158e-02, -9.15218703e-03,  5.24803856e-03, ...,\n              -3.63799883e-03, -5.51798902e-02, -7.19531113e-03],\n             [ 6.12211153e-02,  2.67034862e-02, -4.38000960e-03, ...,\n               1.38858845e-02,  1.62421225e-03,  6.91889692e-03],\n             [ 1.86353922e-02, -2.39325576e-02,  5.56383107e-04, ...,\n              -6.68733614e-03,  7.36468807e-02,  3.71867418e-02]],\n    \n            [[ 3.52302976e-02, -3.27857491e-03,  7.14091491e-03, ...,\n              -9.93822515e-03,  2.38756705e-02, -2.10771449e-02],\n             [ 6.34438619e-02,  3.12160589e-02, -7.72275496e-03, ...,\n               1.49217555e-02,  3.86624038e-03, -1.16395289e-02],\n             [ 3.35849188e-02, -1.63664240e-02, -1.32562651e-03, ...,\n              -1.30512416e-02, -7.29435496e-03, -1.24825155e-02]],\n    \n            [[ 4.10873676e-03, -4.66612726e-03,  1.21031692e-02, ...,\n              -7.87103828e-03,  5.80726229e-02, -4.19587009e-02],\n             [-2.23153979e-02,  2.99241953e-02,  8.01213668e-04, ...,\n               1.82199273e-02,  9.57238674e-03, -8.57376456e-02],\n             [-2.01183017e-02, -1.96383689e-02,  7.32050464e-03, ...,\n              -1.07293837e-02, -2.17854325e-02, -7.95444921e-02]],\n    \n            ...,\n    \n            [[-1.71692297e-02, -3.16392444e-03,  2.40169745e-03, ...,\n              -9.67177004e-03,  9.26117748e-02, -1.16062798e-02],\n             [-8.63026828e-02,  3.55335064e-02, -1.06153013e-02, ...,\n               1.85809545e-02, -2.19932254e-02, -1.47949710e-01],\n             [-6.07556999e-02, -2.66596545e-02,  1.74473948e-03, ...,\n              -4.85855900e-03, -8.82942155e-02, -8.43590796e-02]],\n    \n            [[ 1.15142548e-02,  2.20947526e-03,  5.08834422e-03, ...,\n              -1.04352133e-02,  6.78158402e-02,  4.14623357e-02],\n             [ 7.41827395e-03,  4.52373996e-02, -1.10873608e-02, ...,\n               1.56368576e-02, -2.37460397e-02, -3.25448737e-02],\n             [ 7.84576032e-03, -2.45320965e-02,  5.84031455e-04, ...,\n              -8.31448287e-03, -8.92601907e-02, -3.36888898e-03]],\n    \n            [[ 4.79146978e-03, -4.22942545e-03,  1.15078716e-02, ...,\n              -2.12721284e-02,  4.96782959e-02,  2.05268860e-02],\n             [ 2.75192987e-02,  4.36737053e-02, -5.71439136e-03, ...,\n               9.46100149e-03, -8.58635467e-04, -1.79863740e-02],\n             [ 2.71184333e-02, -3.31169143e-02,  3.97488568e-03, ...,\n              -1.41424611e-02, -6.35233149e-02,  1.29984575e-03]]]],\n          dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "model = keras.Model(inputs, out)\n",
    "\n",
    "best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f3158dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델이 밖에서 선언되었다는 메시지다.\n",
    "# def으로 모델 생성 함수를 만들어 내부에서 호출하도록 해야할 것 같다.\n",
    "# 이생각 하는데 3시간이 걸렸다는..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9025a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0.0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MODEL_PATH + '/simplebaseline_model-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c366503",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3146b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = tf.keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same'))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization())\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model\n",
    "\n",
    "def simplebaseline_model(input_shape=(256, 256, 3),\n",
    "                         num_stack=3, \n",
    "                         num_heatmap=16):\n",
    "    resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')\n",
    "    \n",
    "    upconv = _make_deconv_layer(num_stack)\n",
    "    final_layer = tf.keras.layers.Conv2D(num_heatmap, kernel_size=(1,1), padding='same')\n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = resnet(inputs)\n",
    "    x = upconv(x)\n",
    "    out = final_layer(x)\n",
    "    \n",
    "    return keras.Model(inputs, out)  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f785049d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Functional)        (None, None, None, 2048)  23587712  \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 64, 64, 256)       10489600  \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 16)        4112      \n",
      "=================================================================\n",
      "Total params: 34,081,424\n",
      "Trainable params: 34,026,768\n",
      "Non-trainable params: 54,656\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simplebaseline_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b7e7928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = simplebaseline_model()\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ff8e7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 10.7244587 epoch total loss 10.7244587\n",
      "Trained batch 2 batch loss 11.3415661 epoch total loss 11.0330124\n",
      "Trained batch 3 batch loss 9.46085835 epoch total loss 10.5089607\n",
      "Trained batch 4 batch loss 8.64404869 epoch total loss 10.0427322\n",
      "Trained batch 5 batch loss 7.75864363 epoch total loss 9.58591461\n",
      "Trained batch 6 batch loss 7.60032368 epoch total loss 9.25498295\n",
      "Trained batch 7 batch loss 7.15584564 epoch total loss 8.95510578\n",
      "Trained batch 8 batch loss 6.97844648 epoch total loss 8.70802307\n",
      "Trained batch 9 batch loss 6.90503788 epoch total loss 8.50769138\n",
      "Trained batch 10 batch loss 7.13323641 epoch total loss 8.37024593\n",
      "Trained batch 11 batch loss 6.72354364 epoch total loss 8.22054577\n",
      "Trained batch 12 batch loss 6.76198769 epoch total loss 8.09899902\n",
      "Trained batch 13 batch loss 6.72018194 epoch total loss 7.99293613\n",
      "Trained batch 14 batch loss 6.31946564 epoch total loss 7.8734026\n",
      "Trained batch 15 batch loss 6.7305913 epoch total loss 7.79721546\n",
      "Trained batch 16 batch loss 6.67613363 epoch total loss 7.72714758\n",
      "Trained batch 17 batch loss 6.730165 epoch total loss 7.66850185\n",
      "Trained batch 18 batch loss 6.41730547 epoch total loss 7.59899139\n",
      "Trained batch 19 batch loss 6.30548477 epoch total loss 7.53091192\n",
      "Trained batch 20 batch loss 6.17946291 epoch total loss 7.46333933\n",
      "Trained batch 21 batch loss 6.17419386 epoch total loss 7.40195131\n",
      "Trained batch 22 batch loss 6.79942274 epoch total loss 7.37456369\n",
      "Trained batch 23 batch loss 7.00971746 epoch total loss 7.35870075\n",
      "Trained batch 24 batch loss 6.74136829 epoch total loss 7.33297873\n",
      "Trained batch 25 batch loss 6.65771961 epoch total loss 7.30596781\n",
      "Trained batch 26 batch loss 6.92272186 epoch total loss 7.29122829\n",
      "Trained batch 27 batch loss 7.21992159 epoch total loss 7.28858709\n",
      "Trained batch 28 batch loss 6.56707239 epoch total loss 7.26281881\n",
      "Trained batch 29 batch loss 6.23021555 epoch total loss 7.22721195\n",
      "Trained batch 30 batch loss 6.35482693 epoch total loss 7.19813251\n",
      "Trained batch 31 batch loss 5.93211126 epoch total loss 7.15729284\n",
      "Trained batch 32 batch loss 5.59414482 epoch total loss 7.10844469\n",
      "Trained batch 33 batch loss 6.15923643 epoch total loss 7.07968092\n",
      "Trained batch 34 batch loss 5.81722975 epoch total loss 7.04255\n",
      "Trained batch 35 batch loss 5.32858181 epoch total loss 6.99357939\n",
      "Trained batch 36 batch loss 5.2856164 epoch total loss 6.946136\n",
      "Trained batch 37 batch loss 5.81841087 epoch total loss 6.91565704\n",
      "Trained batch 38 batch loss 5.98537493 epoch total loss 6.89117575\n",
      "Trained batch 39 batch loss 6.33211851 epoch total loss 6.87684107\n",
      "Trained batch 40 batch loss 6.74513626 epoch total loss 6.87354898\n",
      "Trained batch 41 batch loss 7.03530455 epoch total loss 6.87749434\n",
      "Trained batch 42 batch loss 7.21794939 epoch total loss 6.88560057\n",
      "Trained batch 43 batch loss 6.71939087 epoch total loss 6.88173532\n",
      "Trained batch 44 batch loss 6.63246202 epoch total loss 6.87607\n",
      "Trained batch 45 batch loss 6.50795746 epoch total loss 6.86789\n",
      "Trained batch 46 batch loss 6.73787117 epoch total loss 6.86506367\n",
      "Trained batch 47 batch loss 6.81106424 epoch total loss 6.86391497\n",
      "Trained batch 48 batch loss 6.6550703 epoch total loss 6.85956383\n",
      "Trained batch 49 batch loss 6.55047703 epoch total loss 6.85325575\n",
      "Trained batch 50 batch loss 6.70161152 epoch total loss 6.85022259\n",
      "Trained batch 51 batch loss 6.40903759 epoch total loss 6.84157181\n",
      "Trained batch 52 batch loss 6.54288101 epoch total loss 6.83582783\n",
      "Trained batch 53 batch loss 6.92953634 epoch total loss 6.83759594\n",
      "Trained batch 54 batch loss 6.52061796 epoch total loss 6.83172607\n",
      "Trained batch 55 batch loss 6.12218428 epoch total loss 6.81882524\n",
      "Trained batch 56 batch loss 6.07686281 epoch total loss 6.80557632\n",
      "Trained batch 57 batch loss 5.59505129 epoch total loss 6.78433943\n",
      "Trained batch 58 batch loss 5.93898821 epoch total loss 6.76976442\n",
      "Trained batch 59 batch loss 6.13093233 epoch total loss 6.75893641\n",
      "Trained batch 60 batch loss 6.04562569 epoch total loss 6.7470479\n",
      "Trained batch 61 batch loss 5.66993093 epoch total loss 6.72939\n",
      "Trained batch 62 batch loss 6.12597799 epoch total loss 6.71965742\n",
      "Trained batch 63 batch loss 6.51988935 epoch total loss 6.71648693\n",
      "Trained batch 64 batch loss 5.7090106 epoch total loss 6.70074511\n",
      "Trained batch 65 batch loss 6.28080463 epoch total loss 6.69428444\n",
      "Trained batch 66 batch loss 6.95728159 epoch total loss 6.69826889\n",
      "Trained batch 67 batch loss 6.17812443 epoch total loss 6.69050598\n",
      "Trained batch 68 batch loss 6.01888895 epoch total loss 6.68062925\n",
      "Trained batch 69 batch loss 5.57107401 epoch total loss 6.6645484\n",
      "Trained batch 70 batch loss 6.05806446 epoch total loss 6.65588474\n",
      "Trained batch 71 batch loss 6.48648167 epoch total loss 6.65349865\n",
      "Trained batch 72 batch loss 6.59017 epoch total loss 6.65261936\n",
      "Trained batch 73 batch loss 6.34088802 epoch total loss 6.64834881\n",
      "Trained batch 74 batch loss 6.53613901 epoch total loss 6.64683247\n",
      "Trained batch 75 batch loss 6.6376543 epoch total loss 6.6467104\n",
      "Trained batch 76 batch loss 6.46668625 epoch total loss 6.64434147\n",
      "Trained batch 77 batch loss 6.52138186 epoch total loss 6.64274454\n",
      "Trained batch 78 batch loss 5.96857357 epoch total loss 6.63410139\n",
      "Trained batch 79 batch loss 6.42623043 epoch total loss 6.63146973\n",
      "Trained batch 80 batch loss 6.38877821 epoch total loss 6.62843609\n",
      "Trained batch 81 batch loss 6.2046361 epoch total loss 6.62320423\n",
      "Trained batch 82 batch loss 6.6333313 epoch total loss 6.62332821\n",
      "Trained batch 83 batch loss 6.50189 epoch total loss 6.62186527\n",
      "Trained batch 84 batch loss 6.31814289 epoch total loss 6.61824894\n",
      "Trained batch 85 batch loss 6.5061388 epoch total loss 6.61693048\n",
      "Trained batch 86 batch loss 6.56520748 epoch total loss 6.61632872\n",
      "Trained batch 87 batch loss 6.66600609 epoch total loss 6.6169\n",
      "Trained batch 88 batch loss 6.44549847 epoch total loss 6.61495209\n",
      "Trained batch 89 batch loss 6.41849518 epoch total loss 6.61274481\n",
      "Trained batch 90 batch loss 5.77105474 epoch total loss 6.60339308\n",
      "Trained batch 91 batch loss 6.12768459 epoch total loss 6.59816551\n",
      "Trained batch 92 batch loss 6.25410748 epoch total loss 6.5944252\n",
      "Trained batch 93 batch loss 6.20085096 epoch total loss 6.59019375\n",
      "Trained batch 94 batch loss 6.41528654 epoch total loss 6.58833265\n",
      "Trained batch 95 batch loss 6.62540245 epoch total loss 6.58872318\n",
      "Trained batch 96 batch loss 6.47762 epoch total loss 6.5875659\n",
      "Trained batch 97 batch loss 6.51149797 epoch total loss 6.5867815\n",
      "Trained batch 98 batch loss 6.91243649 epoch total loss 6.5901041\n",
      "Trained batch 99 batch loss 7.06253195 epoch total loss 6.59487629\n",
      "Trained batch 100 batch loss 6.68508101 epoch total loss 6.59577799\n",
      "Trained batch 101 batch loss 6.28964567 epoch total loss 6.59274769\n",
      "Trained batch 102 batch loss 6.35505962 epoch total loss 6.59041691\n",
      "Trained batch 103 batch loss 6.170187 epoch total loss 6.58633709\n",
      "Trained batch 104 batch loss 6.26668262 epoch total loss 6.58326292\n",
      "Trained batch 105 batch loss 6.3301177 epoch total loss 6.58085251\n",
      "Trained batch 106 batch loss 6.43965721 epoch total loss 6.57952\n",
      "Trained batch 107 batch loss 6.10570431 epoch total loss 6.57509184\n",
      "Trained batch 108 batch loss 5.99565458 epoch total loss 6.56972694\n",
      "Trained batch 109 batch loss 5.83583593 epoch total loss 6.562994\n",
      "Trained batch 110 batch loss 5.35568285 epoch total loss 6.55201864\n",
      "Trained batch 111 batch loss 5.27125311 epoch total loss 6.54048\n",
      "Trained batch 112 batch loss 6.24298334 epoch total loss 6.53782368\n",
      "Trained batch 113 batch loss 6.92919588 epoch total loss 6.54128742\n",
      "Trained batch 114 batch loss 7.2735219 epoch total loss 6.54771042\n",
      "Trained batch 115 batch loss 6.0105052 epoch total loss 6.54303885\n",
      "Trained batch 116 batch loss 6.43823195 epoch total loss 6.54213524\n",
      "Trained batch 117 batch loss 6.15089512 epoch total loss 6.53879118\n",
      "Trained batch 118 batch loss 6.59934902 epoch total loss 6.53930473\n",
      "Trained batch 119 batch loss 6.87737942 epoch total loss 6.54214573\n",
      "Trained batch 120 batch loss 6.72059059 epoch total loss 6.54363251\n",
      "Trained batch 121 batch loss 6.63794804 epoch total loss 6.54441214\n",
      "Trained batch 122 batch loss 6.7685504 epoch total loss 6.54624891\n",
      "Trained batch 123 batch loss 6.51128912 epoch total loss 6.54596472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 124 batch loss 6.70883179 epoch total loss 6.5472784\n",
      "Trained batch 125 batch loss 6.4540658 epoch total loss 6.54653263\n",
      "Trained batch 126 batch loss 6.40398073 epoch total loss 6.54540157\n",
      "Trained batch 127 batch loss 6.92113638 epoch total loss 6.54836\n",
      "Trained batch 128 batch loss 6.7236414 epoch total loss 6.54972935\n",
      "Trained batch 129 batch loss 6.05535936 epoch total loss 6.54589701\n",
      "Trained batch 130 batch loss 6.26304 epoch total loss 6.5437212\n",
      "Trained batch 131 batch loss 6.67791939 epoch total loss 6.54474592\n",
      "Trained batch 132 batch loss 6.57366657 epoch total loss 6.54496479\n",
      "Trained batch 133 batch loss 6.5288353 epoch total loss 6.5448432\n",
      "Trained batch 134 batch loss 6.54491282 epoch total loss 6.54484415\n",
      "Trained batch 135 batch loss 6.44368172 epoch total loss 6.54409456\n",
      "Trained batch 136 batch loss 6.51075506 epoch total loss 6.54384947\n",
      "Trained batch 137 batch loss 6.7775054 epoch total loss 6.54555511\n",
      "Trained batch 138 batch loss 6.53580618 epoch total loss 6.54548454\n",
      "Trained batch 139 batch loss 6.65836954 epoch total loss 6.5462966\n",
      "Trained batch 140 batch loss 6.79035664 epoch total loss 6.54804\n",
      "Trained batch 141 batch loss 6.76213264 epoch total loss 6.54955816\n",
      "Trained batch 142 batch loss 6.40937185 epoch total loss 6.54857111\n",
      "Trained batch 143 batch loss 6.71478939 epoch total loss 6.54973364\n",
      "Trained batch 144 batch loss 6.70428133 epoch total loss 6.55080652\n",
      "Trained batch 145 batch loss 6.46331835 epoch total loss 6.55020332\n",
      "Trained batch 146 batch loss 6.65125608 epoch total loss 6.55089521\n",
      "Trained batch 147 batch loss 6.57782555 epoch total loss 6.55107832\n",
      "Trained batch 148 batch loss 6.43368435 epoch total loss 6.55028534\n",
      "Trained batch 149 batch loss 6.78663588 epoch total loss 6.5518713\n",
      "Trained batch 150 batch loss 6.59763145 epoch total loss 6.55217648\n",
      "Trained batch 151 batch loss 6.8749752 epoch total loss 6.55431461\n",
      "Trained batch 152 batch loss 6.41882133 epoch total loss 6.55342293\n",
      "Trained batch 153 batch loss 6.7140317 epoch total loss 6.55447292\n",
      "Trained batch 154 batch loss 6.5019908 epoch total loss 6.55413246\n",
      "Trained batch 155 batch loss 6.51587868 epoch total loss 6.55388546\n",
      "Trained batch 156 batch loss 6.56563282 epoch total loss 6.55396032\n",
      "Trained batch 157 batch loss 6.750072 epoch total loss 6.55521\n",
      "Trained batch 158 batch loss 6.48207903 epoch total loss 6.5547471\n",
      "Trained batch 159 batch loss 6.23678303 epoch total loss 6.55274725\n",
      "Trained batch 160 batch loss 6.44050074 epoch total loss 6.5520463\n",
      "Trained batch 161 batch loss 6.58376122 epoch total loss 6.55224323\n",
      "Trained batch 162 batch loss 6.59646225 epoch total loss 6.55251598\n",
      "Trained batch 163 batch loss 6.29269743 epoch total loss 6.55092192\n",
      "Trained batch 164 batch loss 6.31406641 epoch total loss 6.54947805\n",
      "Trained batch 165 batch loss 6.15762281 epoch total loss 6.54710293\n",
      "Trained batch 166 batch loss 6.69369888 epoch total loss 6.54798603\n",
      "Trained batch 167 batch loss 6.53973627 epoch total loss 6.54793692\n",
      "Trained batch 168 batch loss 6.47211123 epoch total loss 6.54748631\n",
      "Trained batch 169 batch loss 6.43627214 epoch total loss 6.54682827\n",
      "Trained batch 170 batch loss 6.19535542 epoch total loss 6.54476\n",
      "Trained batch 171 batch loss 6.48672819 epoch total loss 6.54442072\n",
      "Trained batch 172 batch loss 6.30619431 epoch total loss 6.54303551\n",
      "Trained batch 173 batch loss 6.29846859 epoch total loss 6.54162169\n",
      "Trained batch 174 batch loss 6.389884 epoch total loss 6.54074955\n",
      "Trained batch 175 batch loss 6.09256 epoch total loss 6.53818846\n",
      "Trained batch 176 batch loss 6.41927338 epoch total loss 6.53751326\n",
      "Trained batch 177 batch loss 6.07433033 epoch total loss 6.53489637\n",
      "Trained batch 178 batch loss 6.35445499 epoch total loss 6.53388262\n",
      "Trained batch 179 batch loss 6.4897747 epoch total loss 6.53363609\n",
      "Trained batch 180 batch loss 7.17010212 epoch total loss 6.53717184\n",
      "Trained batch 181 batch loss 7.15608597 epoch total loss 6.54059124\n",
      "Trained batch 182 batch loss 6.81744623 epoch total loss 6.54211283\n",
      "Trained batch 183 batch loss 6.67728 epoch total loss 6.54285145\n",
      "Trained batch 184 batch loss 6.41329527 epoch total loss 6.54214764\n",
      "Trained batch 185 batch loss 6.67865944 epoch total loss 6.54288578\n",
      "Trained batch 186 batch loss 6.347651 epoch total loss 6.54183578\n",
      "Trained batch 187 batch loss 6.72760344 epoch total loss 6.54282951\n",
      "Trained batch 188 batch loss 6.37364626 epoch total loss 6.54192972\n",
      "Trained batch 189 batch loss 6.39863586 epoch total loss 6.54117203\n",
      "Trained batch 190 batch loss 7.08615828 epoch total loss 6.54404\n",
      "Trained batch 191 batch loss 6.93271255 epoch total loss 6.54607534\n",
      "Trained batch 192 batch loss 6.44191647 epoch total loss 6.5455327\n",
      "Trained batch 193 batch loss 6.16114283 epoch total loss 6.54354095\n",
      "Trained batch 194 batch loss 6.44012976 epoch total loss 6.54300833\n",
      "Trained batch 195 batch loss 6.20039892 epoch total loss 6.54125166\n",
      "Trained batch 196 batch loss 6.24838066 epoch total loss 6.53975773\n",
      "Trained batch 197 batch loss 6.37846518 epoch total loss 6.53893852\n",
      "Trained batch 198 batch loss 6.74859953 epoch total loss 6.53999758\n",
      "Trained batch 199 batch loss 6.23589706 epoch total loss 6.53846931\n",
      "Trained batch 200 batch loss 6.02085209 epoch total loss 6.53588152\n",
      "Trained batch 201 batch loss 5.67676353 epoch total loss 6.53160715\n",
      "Trained batch 202 batch loss 6.02819633 epoch total loss 6.5291152\n",
      "Trained batch 203 batch loss 6.82679653 epoch total loss 6.53058147\n",
      "Trained batch 204 batch loss 6.87420273 epoch total loss 6.53226566\n",
      "Trained batch 205 batch loss 6.77476215 epoch total loss 6.53344822\n",
      "Trained batch 206 batch loss 6.77685833 epoch total loss 6.53463\n",
      "Trained batch 207 batch loss 7.00484228 epoch total loss 6.53690195\n",
      "Trained batch 208 batch loss 6.66865253 epoch total loss 6.53753567\n",
      "Trained batch 209 batch loss 6.61694098 epoch total loss 6.53791523\n",
      "Trained batch 210 batch loss 6.41821575 epoch total loss 6.53734541\n",
      "Trained batch 211 batch loss 6.82241 epoch total loss 6.53869629\n",
      "Trained batch 212 batch loss 6.49726582 epoch total loss 6.53850126\n",
      "Trained batch 213 batch loss 6.56097078 epoch total loss 6.53860617\n",
      "Trained batch 214 batch loss 6.66324663 epoch total loss 6.53918839\n",
      "Trained batch 215 batch loss 6.51738596 epoch total loss 6.53908682\n",
      "Trained batch 216 batch loss 6.42967415 epoch total loss 6.53858042\n",
      "Trained batch 217 batch loss 6.35509872 epoch total loss 6.53773499\n",
      "Trained batch 218 batch loss 6.43060398 epoch total loss 6.53724384\n",
      "Trained batch 219 batch loss 6.19130611 epoch total loss 6.53566408\n",
      "Trained batch 220 batch loss 5.71805286 epoch total loss 6.53194761\n",
      "Trained batch 221 batch loss 5.76452971 epoch total loss 6.52847481\n",
      "Trained batch 222 batch loss 5.9915185 epoch total loss 6.52605629\n",
      "Trained batch 223 batch loss 6.48144817 epoch total loss 6.52585649\n",
      "Trained batch 224 batch loss 6.25819683 epoch total loss 6.52466154\n",
      "Trained batch 225 batch loss 5.83249187 epoch total loss 6.52158546\n",
      "Trained batch 226 batch loss 6.13509798 epoch total loss 6.51987553\n",
      "Trained batch 227 batch loss 6.58890104 epoch total loss 6.52017927\n",
      "Trained batch 228 batch loss 6.73389864 epoch total loss 6.52111673\n",
      "Trained batch 229 batch loss 6.43236303 epoch total loss 6.52072906\n",
      "Trained batch 230 batch loss 6.54729939 epoch total loss 6.52084446\n",
      "Trained batch 231 batch loss 6.61108 epoch total loss 6.52123499\n",
      "Trained batch 232 batch loss 5.85001087 epoch total loss 6.51834154\n",
      "Trained batch 233 batch loss 4.69386578 epoch total loss 6.51051092\n",
      "Trained batch 234 batch loss 5.39881945 epoch total loss 6.50576\n",
      "Trained batch 235 batch loss 6.68209362 epoch total loss 6.50651073\n",
      "Trained batch 236 batch loss 7.30523109 epoch total loss 6.50989485\n",
      "Trained batch 237 batch loss 6.94861841 epoch total loss 6.51174593\n",
      "Trained batch 238 batch loss 6.96427345 epoch total loss 6.51364708\n",
      "Trained batch 239 batch loss 7.0308795 epoch total loss 6.51581144\n",
      "Trained batch 240 batch loss 7.00810909 epoch total loss 6.51786232\n",
      "Trained batch 241 batch loss 6.55495596 epoch total loss 6.51801634\n",
      "Trained batch 242 batch loss 6.80682468 epoch total loss 6.51921\n",
      "Trained batch 243 batch loss 6.51954794 epoch total loss 6.51921129\n",
      "Trained batch 244 batch loss 6.77527285 epoch total loss 6.52026081\n",
      "Trained batch 245 batch loss 6.20713234 epoch total loss 6.51898289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 246 batch loss 6.40397167 epoch total loss 6.51851511\n",
      "Trained batch 247 batch loss 6.32648754 epoch total loss 6.51773787\n",
      "Trained batch 248 batch loss 6.65907478 epoch total loss 6.51830769\n",
      "Trained batch 249 batch loss 6.53419256 epoch total loss 6.51837111\n",
      "Trained batch 250 batch loss 6.48020506 epoch total loss 6.51821852\n",
      "Trained batch 251 batch loss 6.58515263 epoch total loss 6.51848555\n",
      "Trained batch 252 batch loss 6.64919281 epoch total loss 6.51900434\n",
      "Trained batch 253 batch loss 6.70398092 epoch total loss 6.51973534\n",
      "Trained batch 254 batch loss 6.71149921 epoch total loss 6.52049065\n",
      "Trained batch 255 batch loss 6.45088482 epoch total loss 6.5202179\n",
      "Trained batch 256 batch loss 6.42233658 epoch total loss 6.51983547\n",
      "Trained batch 257 batch loss 5.94086409 epoch total loss 6.51758289\n",
      "Trained batch 258 batch loss 6.41949844 epoch total loss 6.51720285\n",
      "Trained batch 259 batch loss 6.36343765 epoch total loss 6.51660919\n",
      "Trained batch 260 batch loss 5.93710709 epoch total loss 6.51438046\n",
      "Trained batch 261 batch loss 6.57961416 epoch total loss 6.51463032\n",
      "Trained batch 262 batch loss 6.49900389 epoch total loss 6.51457071\n",
      "Trained batch 263 batch loss 6.61011839 epoch total loss 6.51493406\n",
      "Trained batch 264 batch loss 6.73623753 epoch total loss 6.51577187\n",
      "Trained batch 265 batch loss 6.71107388 epoch total loss 6.51650906\n",
      "Trained batch 266 batch loss 6.45301104 epoch total loss 6.51627\n",
      "Trained batch 267 batch loss 6.42259169 epoch total loss 6.51591921\n",
      "Trained batch 268 batch loss 6.54695225 epoch total loss 6.51603556\n",
      "Trained batch 269 batch loss 6.61014271 epoch total loss 6.51638508\n",
      "Trained batch 270 batch loss 6.80639 epoch total loss 6.51745939\n",
      "Trained batch 271 batch loss 6.63124037 epoch total loss 6.51787901\n",
      "Trained batch 272 batch loss 5.96525383 epoch total loss 6.51584721\n",
      "Trained batch 273 batch loss 6.27651739 epoch total loss 6.5149703\n",
      "Trained batch 274 batch loss 6.40906763 epoch total loss 6.51458406\n",
      "Trained batch 275 batch loss 6.80139732 epoch total loss 6.51562691\n",
      "Trained batch 276 batch loss 6.28032732 epoch total loss 6.51477385\n",
      "Trained batch 277 batch loss 6.33029842 epoch total loss 6.51410818\n",
      "Trained batch 278 batch loss 6.83625317 epoch total loss 6.5152669\n",
      "Trained batch 279 batch loss 6.66894388 epoch total loss 6.51581812\n",
      "Trained batch 280 batch loss 6.55558205 epoch total loss 6.51595974\n",
      "Trained batch 281 batch loss 6.50762415 epoch total loss 6.51593\n",
      "Trained batch 282 batch loss 6.39993477 epoch total loss 6.51551867\n",
      "Trained batch 283 batch loss 6.51859426 epoch total loss 6.51552916\n",
      "Trained batch 284 batch loss 6.20221758 epoch total loss 6.51442623\n",
      "Trained batch 285 batch loss 6.41404533 epoch total loss 6.51407385\n",
      "Trained batch 286 batch loss 6.27773714 epoch total loss 6.51324749\n",
      "Trained batch 287 batch loss 6.59772825 epoch total loss 6.51354218\n",
      "Trained batch 288 batch loss 6.4030323 epoch total loss 6.5131588\n",
      "Trained batch 289 batch loss 6.62010908 epoch total loss 6.51352882\n",
      "Trained batch 290 batch loss 6.21324492 epoch total loss 6.51249313\n",
      "Trained batch 291 batch loss 6.16290092 epoch total loss 6.5112915\n",
      "Trained batch 292 batch loss 6.19060659 epoch total loss 6.51019335\n",
      "Trained batch 293 batch loss 6.4960022 epoch total loss 6.51014471\n",
      "Trained batch 294 batch loss 6.58244705 epoch total loss 6.51039028\n",
      "Trained batch 295 batch loss 6.78329372 epoch total loss 6.51131582\n",
      "Trained batch 296 batch loss 6.90417194 epoch total loss 6.51264286\n",
      "Trained batch 297 batch loss 7.23631907 epoch total loss 6.5150795\n",
      "Trained batch 298 batch loss 6.82062 epoch total loss 6.51610518\n",
      "Trained batch 299 batch loss 6.54760933 epoch total loss 6.51621056\n",
      "Trained batch 300 batch loss 5.85385513 epoch total loss 6.5140028\n",
      "Trained batch 301 batch loss 5.52848673 epoch total loss 6.51072836\n",
      "Trained batch 302 batch loss 5.59926891 epoch total loss 6.50771046\n",
      "Trained batch 303 batch loss 5.38310766 epoch total loss 6.50399828\n",
      "Trained batch 304 batch loss 6.08267212 epoch total loss 6.50261259\n",
      "Trained batch 305 batch loss 6.13325644 epoch total loss 6.50140142\n",
      "Trained batch 306 batch loss 6.41698 epoch total loss 6.50112581\n",
      "Trained batch 307 batch loss 6.83596373 epoch total loss 6.50221634\n",
      "Trained batch 308 batch loss 6.39660406 epoch total loss 6.50187349\n",
      "Trained batch 309 batch loss 6.36444807 epoch total loss 6.50142908\n",
      "Trained batch 310 batch loss 6.65446711 epoch total loss 6.50192261\n",
      "Trained batch 311 batch loss 6.4081111 epoch total loss 6.50162077\n",
      "Trained batch 312 batch loss 6.068326 epoch total loss 6.50023222\n",
      "Trained batch 313 batch loss 6.11653757 epoch total loss 6.49900627\n",
      "Trained batch 314 batch loss 6.43413591 epoch total loss 6.49879932\n",
      "Trained batch 315 batch loss 6.53323 epoch total loss 6.49890852\n",
      "Trained batch 316 batch loss 6.78649712 epoch total loss 6.49981833\n",
      "Trained batch 317 batch loss 6.40791607 epoch total loss 6.49952888\n",
      "Trained batch 318 batch loss 6.68241358 epoch total loss 6.50010347\n",
      "Trained batch 319 batch loss 6.11965656 epoch total loss 6.4989109\n",
      "Trained batch 320 batch loss 6.41399717 epoch total loss 6.49864578\n",
      "Trained batch 321 batch loss 6.42726326 epoch total loss 6.49842358\n",
      "Trained batch 322 batch loss 6.30663872 epoch total loss 6.49782753\n",
      "Trained batch 323 batch loss 6.26860714 epoch total loss 6.497118\n",
      "Trained batch 324 batch loss 6.2190485 epoch total loss 6.49625969\n",
      "Trained batch 325 batch loss 6.50941181 epoch total loss 6.4963\n",
      "Trained batch 326 batch loss 6.51329374 epoch total loss 6.4963522\n",
      "Trained batch 327 batch loss 6.22329378 epoch total loss 6.49551725\n",
      "Trained batch 328 batch loss 6.91089344 epoch total loss 6.49678373\n",
      "Trained batch 329 batch loss 6.58570957 epoch total loss 6.4970541\n",
      "Trained batch 330 batch loss 6.65710545 epoch total loss 6.49753952\n",
      "Trained batch 331 batch loss 6.91058588 epoch total loss 6.4987874\n",
      "Trained batch 332 batch loss 6.78108215 epoch total loss 6.4996376\n",
      "Trained batch 333 batch loss 6.55476618 epoch total loss 6.49980259\n",
      "Trained batch 334 batch loss 6.57161188 epoch total loss 6.50001764\n",
      "Trained batch 335 batch loss 6.61494637 epoch total loss 6.50036097\n",
      "Trained batch 336 batch loss 6.87738419 epoch total loss 6.50148296\n",
      "Trained batch 337 batch loss 6.61081553 epoch total loss 6.50180769\n",
      "Trained batch 338 batch loss 6.70814371 epoch total loss 6.50241852\n",
      "Trained batch 339 batch loss 6.70556498 epoch total loss 6.50301743\n",
      "Trained batch 340 batch loss 6.64932203 epoch total loss 6.50344801\n",
      "Trained batch 341 batch loss 6.29331112 epoch total loss 6.50283146\n",
      "Trained batch 342 batch loss 6.67999029 epoch total loss 6.5033493\n",
      "Trained batch 343 batch loss 6.70488501 epoch total loss 6.50393677\n",
      "Trained batch 344 batch loss 6.66473722 epoch total loss 6.50440454\n",
      "Trained batch 345 batch loss 6.3692956 epoch total loss 6.50401306\n",
      "Trained batch 346 batch loss 5.48571062 epoch total loss 6.50106955\n",
      "Trained batch 347 batch loss 5.30537415 epoch total loss 6.49762392\n",
      "Trained batch 348 batch loss 6.07783413 epoch total loss 6.496418\n",
      "Trained batch 349 batch loss 6.52769852 epoch total loss 6.49650717\n",
      "Trained batch 350 batch loss 6.53800583 epoch total loss 6.4966259\n",
      "Trained batch 351 batch loss 6.81245518 epoch total loss 6.49752569\n",
      "Trained batch 352 batch loss 6.76200104 epoch total loss 6.49827719\n",
      "Trained batch 353 batch loss 6.70506573 epoch total loss 6.49886322\n",
      "Trained batch 354 batch loss 6.58796787 epoch total loss 6.49911451\n",
      "Trained batch 355 batch loss 6.3804841 epoch total loss 6.49878\n",
      "Trained batch 356 batch loss 6.34913254 epoch total loss 6.49835968\n",
      "Trained batch 357 batch loss 6.46801233 epoch total loss 6.4982748\n",
      "Trained batch 358 batch loss 6.64364672 epoch total loss 6.49868059\n",
      "Trained batch 359 batch loss 6.70130491 epoch total loss 6.49924517\n",
      "Trained batch 360 batch loss 6.39217281 epoch total loss 6.49894762\n",
      "Trained batch 361 batch loss 6.37139177 epoch total loss 6.49859381\n",
      "Trained batch 362 batch loss 5.87239552 epoch total loss 6.49686384\n",
      "Trained batch 363 batch loss 5.95296192 epoch total loss 6.49536514\n",
      "Trained batch 364 batch loss 6.47279835 epoch total loss 6.49530363\n",
      "Trained batch 365 batch loss 6.53072786 epoch total loss 6.49540091\n",
      "Trained batch 366 batch loss 6.56739902 epoch total loss 6.49559736\n",
      "Trained batch 367 batch loss 6.33915424 epoch total loss 6.49517107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 368 batch loss 6.07728243 epoch total loss 6.49403572\n",
      "Trained batch 369 batch loss 5.99033356 epoch total loss 6.49267054\n",
      "Trained batch 370 batch loss 6.70027733 epoch total loss 6.4932313\n",
      "Trained batch 371 batch loss 6.32056522 epoch total loss 6.4927659\n",
      "Trained batch 372 batch loss 6.05866098 epoch total loss 6.49159861\n",
      "Trained batch 373 batch loss 5.82338238 epoch total loss 6.48980761\n",
      "Trained batch 374 batch loss 5.80539417 epoch total loss 6.4879775\n",
      "Trained batch 375 batch loss 5.81084871 epoch total loss 6.48617172\n",
      "Trained batch 376 batch loss 6.08776331 epoch total loss 6.48511219\n",
      "Trained batch 377 batch loss 6.21330452 epoch total loss 6.48439121\n",
      "Trained batch 378 batch loss 6.77974129 epoch total loss 6.48517275\n",
      "Trained batch 379 batch loss 6.70380497 epoch total loss 6.48574972\n",
      "Trained batch 380 batch loss 6.52264404 epoch total loss 6.485847\n",
      "Trained batch 381 batch loss 6.51249361 epoch total loss 6.48591661\n",
      "Trained batch 382 batch loss 6.54842281 epoch total loss 6.48608\n",
      "Trained batch 383 batch loss 5.96536875 epoch total loss 6.48472071\n",
      "Trained batch 384 batch loss 5.79581 epoch total loss 6.48292685\n",
      "Trained batch 385 batch loss 6.12814045 epoch total loss 6.48200512\n",
      "Trained batch 386 batch loss 6.59405136 epoch total loss 6.48229551\n",
      "Trained batch 387 batch loss 6.51509047 epoch total loss 6.48238039\n",
      "Trained batch 388 batch loss 6.45997381 epoch total loss 6.48232269\n",
      "Trained batch 389 batch loss 6.60741663 epoch total loss 6.48264408\n",
      "Trained batch 390 batch loss 6.84148216 epoch total loss 6.48356438\n",
      "Trained batch 391 batch loss 7.33755922 epoch total loss 6.48574877\n",
      "Trained batch 392 batch loss 7.2004323 epoch total loss 6.48757172\n",
      "Trained batch 393 batch loss 6.16480494 epoch total loss 6.4867506\n",
      "Trained batch 394 batch loss 6.59553528 epoch total loss 6.48702669\n",
      "Trained batch 395 batch loss 6.50528622 epoch total loss 6.48707294\n",
      "Trained batch 396 batch loss 6.37813044 epoch total loss 6.48679781\n",
      "Trained batch 397 batch loss 6.24770164 epoch total loss 6.48619604\n",
      "Trained batch 398 batch loss 6.09220505 epoch total loss 6.48520613\n",
      "Trained batch 399 batch loss 6.15741587 epoch total loss 6.48438501\n",
      "Trained batch 400 batch loss 6.61321545 epoch total loss 6.48470688\n",
      "Trained batch 401 batch loss 6.677701 epoch total loss 6.48518848\n",
      "Trained batch 402 batch loss 6.71098423 epoch total loss 6.48575\n",
      "Trained batch 403 batch loss 6.43278122 epoch total loss 6.48561859\n",
      "Trained batch 404 batch loss 6.37670851 epoch total loss 6.48534918\n",
      "Trained batch 405 batch loss 6.35445309 epoch total loss 6.48502588\n",
      "Trained batch 406 batch loss 6.4295907 epoch total loss 6.48488951\n",
      "Trained batch 407 batch loss 6.59351683 epoch total loss 6.48515654\n",
      "Trained batch 408 batch loss 6.53469467 epoch total loss 6.48527813\n",
      "Trained batch 409 batch loss 6.38185358 epoch total loss 6.48502493\n",
      "Trained batch 410 batch loss 6.08431149 epoch total loss 6.48404741\n",
      "Trained batch 411 batch loss 6.1414032 epoch total loss 6.4832139\n",
      "Trained batch 412 batch loss 5.81930828 epoch total loss 6.48160219\n",
      "Trained batch 413 batch loss 6.05670404 epoch total loss 6.48057318\n",
      "Trained batch 414 batch loss 6.61278963 epoch total loss 6.48089266\n",
      "Trained batch 415 batch loss 6.4946537 epoch total loss 6.48092604\n",
      "Trained batch 416 batch loss 6.65819025 epoch total loss 6.48135185\n",
      "Trained batch 417 batch loss 6.58595276 epoch total loss 6.48160267\n",
      "Trained batch 418 batch loss 6.72243834 epoch total loss 6.48217869\n",
      "Trained batch 419 batch loss 6.56340265 epoch total loss 6.48237276\n",
      "Trained batch 420 batch loss 6.31925821 epoch total loss 6.48198462\n",
      "Trained batch 421 batch loss 6.70534658 epoch total loss 6.48251534\n",
      "Trained batch 422 batch loss 6.40089703 epoch total loss 6.48232174\n",
      "Trained batch 423 batch loss 5.83171654 epoch total loss 6.48078394\n",
      "Trained batch 424 batch loss 6.2260828 epoch total loss 6.48018312\n",
      "Trained batch 425 batch loss 6.59380388 epoch total loss 6.48045\n",
      "Trained batch 426 batch loss 6.71243238 epoch total loss 6.4809947\n",
      "Trained batch 427 batch loss 6.64752674 epoch total loss 6.48138475\n",
      "Trained batch 428 batch loss 6.60870123 epoch total loss 6.48168182\n",
      "Trained batch 429 batch loss 6.35616875 epoch total loss 6.48138952\n",
      "Trained batch 430 batch loss 6.56843853 epoch total loss 6.4815917\n",
      "Trained batch 431 batch loss 6.53105545 epoch total loss 6.48170662\n",
      "Trained batch 432 batch loss 6.2998333 epoch total loss 6.48128557\n",
      "Trained batch 433 batch loss 6.55788231 epoch total loss 6.481462\n",
      "Trained batch 434 batch loss 6.18666601 epoch total loss 6.48078299\n",
      "Trained batch 435 batch loss 6.15168762 epoch total loss 6.48002672\n",
      "Trained batch 436 batch loss 5.86819363 epoch total loss 6.47862291\n",
      "Trained batch 437 batch loss 5.79148436 epoch total loss 6.47705078\n",
      "Trained batch 438 batch loss 5.43495893 epoch total loss 6.47467184\n",
      "Trained batch 439 batch loss 5.82504034 epoch total loss 6.47319174\n",
      "Trained batch 440 batch loss 5.83577633 epoch total loss 6.47174311\n",
      "Trained batch 441 batch loss 6.30756569 epoch total loss 6.4713707\n",
      "Trained batch 442 batch loss 7.06887627 epoch total loss 6.47272253\n",
      "Trained batch 443 batch loss 6.89381552 epoch total loss 6.47367287\n",
      "Trained batch 444 batch loss 6.72450829 epoch total loss 6.4742384\n",
      "Trained batch 445 batch loss 6.71269608 epoch total loss 6.47477388\n",
      "Trained batch 446 batch loss 7.07845783 epoch total loss 6.47612715\n",
      "Trained batch 447 batch loss 6.58840799 epoch total loss 6.47637844\n",
      "Trained batch 448 batch loss 6.69536448 epoch total loss 6.4768672\n",
      "Trained batch 449 batch loss 6.7383213 epoch total loss 6.47744942\n",
      "Trained batch 450 batch loss 6.86603975 epoch total loss 6.47831249\n",
      "Trained batch 451 batch loss 6.53971958 epoch total loss 6.47844887\n",
      "Trained batch 452 batch loss 6.68540716 epoch total loss 6.47890663\n",
      "Trained batch 453 batch loss 6.29691315 epoch total loss 6.47850466\n",
      "Trained batch 454 batch loss 5.91651106 epoch total loss 6.47726679\n",
      "Trained batch 455 batch loss 6.00695181 epoch total loss 6.47623301\n",
      "Trained batch 456 batch loss 6.14212513 epoch total loss 6.4755\n",
      "Trained batch 457 batch loss 6.34527206 epoch total loss 6.47521496\n",
      "Trained batch 458 batch loss 6.22480392 epoch total loss 6.4746685\n",
      "Trained batch 459 batch loss 6.29669571 epoch total loss 6.47428083\n",
      "Trained batch 460 batch loss 6.52956867 epoch total loss 6.474401\n",
      "Trained batch 461 batch loss 5.7502284 epoch total loss 6.47283\n",
      "Trained batch 462 batch loss 6.24342155 epoch total loss 6.47233343\n",
      "Trained batch 463 batch loss 6.2805 epoch total loss 6.47191906\n",
      "Trained batch 464 batch loss 6.39083958 epoch total loss 6.47174454\n",
      "Trained batch 465 batch loss 6.84759045 epoch total loss 6.47255278\n",
      "Trained batch 466 batch loss 6.47612953 epoch total loss 6.47256041\n",
      "Trained batch 467 batch loss 6.45277691 epoch total loss 6.47251844\n",
      "Trained batch 468 batch loss 6.37440157 epoch total loss 6.47230864\n",
      "Trained batch 469 batch loss 6.37695599 epoch total loss 6.4721055\n",
      "Trained batch 470 batch loss 6.13906622 epoch total loss 6.47139692\n",
      "Trained batch 471 batch loss 6.14527225 epoch total loss 6.47070456\n",
      "Trained batch 472 batch loss 6.23943758 epoch total loss 6.47021484\n",
      "Trained batch 473 batch loss 6.22105265 epoch total loss 6.46968794\n",
      "Trained batch 474 batch loss 6.41381121 epoch total loss 6.46957\n",
      "Trained batch 475 batch loss 6.4090004 epoch total loss 6.46944237\n",
      "Trained batch 476 batch loss 6.29714966 epoch total loss 6.46908045\n",
      "Trained batch 477 batch loss 6.21313 epoch total loss 6.46854353\n",
      "Trained batch 478 batch loss 6.33013868 epoch total loss 6.46825409\n",
      "Trained batch 479 batch loss 6.11672401 epoch total loss 6.46752\n",
      "Trained batch 480 batch loss 6.2276 epoch total loss 6.46702\n",
      "Trained batch 481 batch loss 6.10958385 epoch total loss 6.46627712\n",
      "Trained batch 482 batch loss 6.11413 epoch total loss 6.46554613\n",
      "Trained batch 483 batch loss 5.70083284 epoch total loss 6.46396303\n",
      "Trained batch 484 batch loss 5.91711378 epoch total loss 6.46283293\n",
      "Trained batch 485 batch loss 5.57616615 epoch total loss 6.46100473\n",
      "Trained batch 486 batch loss 6.22443485 epoch total loss 6.46051788\n",
      "Trained batch 487 batch loss 6.58377743 epoch total loss 6.46077108\n",
      "Trained batch 488 batch loss 6.62106895 epoch total loss 6.46109962\n",
      "Trained batch 489 batch loss 6.44115257 epoch total loss 6.46105862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 490 batch loss 6.10022259 epoch total loss 6.46032286\n",
      "Trained batch 491 batch loss 6.02439737 epoch total loss 6.45943499\n",
      "Trained batch 492 batch loss 6.33848715 epoch total loss 6.45918894\n",
      "Trained batch 493 batch loss 6.5986867 epoch total loss 6.4594717\n",
      "Trained batch 494 batch loss 6.43330288 epoch total loss 6.45941877\n",
      "Trained batch 495 batch loss 6.18292093 epoch total loss 6.45886\n",
      "Trained batch 496 batch loss 6.16198492 epoch total loss 6.45826149\n",
      "Trained batch 497 batch loss 6.3616128 epoch total loss 6.45806694\n",
      "Trained batch 498 batch loss 6.34563255 epoch total loss 6.45784092\n",
      "Trained batch 499 batch loss 6.29885101 epoch total loss 6.45752239\n",
      "Trained batch 500 batch loss 6.52973938 epoch total loss 6.45766687\n",
      "Trained batch 501 batch loss 6.62280798 epoch total loss 6.45799637\n",
      "Trained batch 502 batch loss 6.58091974 epoch total loss 6.45824146\n",
      "Trained batch 503 batch loss 6.48429775 epoch total loss 6.45829344\n",
      "Trained batch 504 batch loss 6.51104546 epoch total loss 6.45839787\n",
      "Trained batch 505 batch loss 6.51047134 epoch total loss 6.45850086\n",
      "Trained batch 506 batch loss 6.32119799 epoch total loss 6.45822954\n",
      "Trained batch 507 batch loss 6.36806 epoch total loss 6.45805216\n",
      "Trained batch 508 batch loss 6.38538265 epoch total loss 6.45790911\n",
      "Trained batch 509 batch loss 6.03655958 epoch total loss 6.45708179\n",
      "Trained batch 510 batch loss 6.57979155 epoch total loss 6.45732212\n",
      "Trained batch 511 batch loss 6.23047781 epoch total loss 6.45687819\n",
      "Trained batch 512 batch loss 6.18737268 epoch total loss 6.45635176\n",
      "Trained batch 513 batch loss 6.29709911 epoch total loss 6.45604134\n",
      "Trained batch 514 batch loss 5.91478348 epoch total loss 6.45498848\n",
      "Trained batch 515 batch loss 6.27757072 epoch total loss 6.45464373\n",
      "Trained batch 516 batch loss 6.24363613 epoch total loss 6.45423508\n",
      "Trained batch 517 batch loss 6.17729568 epoch total loss 6.45369911\n",
      "Trained batch 518 batch loss 6.11450291 epoch total loss 6.45304441\n",
      "Trained batch 519 batch loss 6.42208672 epoch total loss 6.45298481\n",
      "Trained batch 520 batch loss 6.38099098 epoch total loss 6.45284653\n",
      "Trained batch 521 batch loss 5.91033697 epoch total loss 6.45180559\n",
      "Trained batch 522 batch loss 5.35008383 epoch total loss 6.44969463\n",
      "Trained batch 523 batch loss 6.06898403 epoch total loss 6.44896698\n",
      "Trained batch 524 batch loss 6.31910276 epoch total loss 6.4487195\n",
      "Trained batch 525 batch loss 6.53885651 epoch total loss 6.44889069\n",
      "Trained batch 526 batch loss 6.55223036 epoch total loss 6.44908762\n",
      "Trained batch 527 batch loss 6.62985277 epoch total loss 6.44943047\n",
      "Trained batch 528 batch loss 6.28343964 epoch total loss 6.44911623\n",
      "Trained batch 529 batch loss 6.80122137 epoch total loss 6.44978189\n",
      "Trained batch 530 batch loss 6.53822136 epoch total loss 6.44994879\n",
      "Trained batch 531 batch loss 6.49949837 epoch total loss 6.45004225\n",
      "Trained batch 532 batch loss 6.48048258 epoch total loss 6.45009947\n",
      "Trained batch 533 batch loss 6.46153831 epoch total loss 6.45012045\n",
      "Trained batch 534 batch loss 6.88553858 epoch total loss 6.45093584\n",
      "Trained batch 535 batch loss 6.39826298 epoch total loss 6.45083761\n",
      "Trained batch 536 batch loss 6.52832127 epoch total loss 6.45098209\n",
      "Trained batch 537 batch loss 6.3769455 epoch total loss 6.45084429\n",
      "Trained batch 538 batch loss 6.37266922 epoch total loss 6.45069838\n",
      "Trained batch 539 batch loss 6.45096064 epoch total loss 6.45069885\n",
      "Trained batch 540 batch loss 6.45476198 epoch total loss 6.45070648\n",
      "Trained batch 541 batch loss 6.69246387 epoch total loss 6.45115328\n",
      "Trained batch 542 batch loss 6.5110383 epoch total loss 6.4512639\n",
      "Trained batch 543 batch loss 6.37730265 epoch total loss 6.45112753\n",
      "Trained batch 544 batch loss 6.40152884 epoch total loss 6.45103645\n",
      "Trained batch 545 batch loss 6.26458645 epoch total loss 6.45069456\n",
      "Trained batch 546 batch loss 6.37727356 epoch total loss 6.45055962\n",
      "Trained batch 547 batch loss 6.48478031 epoch total loss 6.45062256\n",
      "Trained batch 548 batch loss 6.86321831 epoch total loss 6.45137548\n",
      "Trained batch 549 batch loss 6.49417591 epoch total loss 6.45145321\n",
      "Trained batch 550 batch loss 6.25306654 epoch total loss 6.45109272\n",
      "Trained batch 551 batch loss 5.97391 epoch total loss 6.45022678\n",
      "Trained batch 552 batch loss 6.05142689 epoch total loss 6.44950438\n",
      "Trained batch 553 batch loss 6.32104063 epoch total loss 6.44927216\n",
      "Trained batch 554 batch loss 6.24274921 epoch total loss 6.44889927\n",
      "Trained batch 555 batch loss 6.68215322 epoch total loss 6.44931936\n",
      "Trained batch 556 batch loss 6.51028204 epoch total loss 6.44942904\n",
      "Trained batch 557 batch loss 6.42503405 epoch total loss 6.44938517\n",
      "Trained batch 558 batch loss 6.60971928 epoch total loss 6.4496727\n",
      "Trained batch 559 batch loss 6.22459221 epoch total loss 6.44927\n",
      "Trained batch 560 batch loss 6.37582397 epoch total loss 6.44913864\n",
      "Trained batch 561 batch loss 6.33058167 epoch total loss 6.4489274\n",
      "Trained batch 562 batch loss 5.96103811 epoch total loss 6.44805908\n",
      "Trained batch 563 batch loss 6.57982302 epoch total loss 6.44829273\n",
      "Trained batch 564 batch loss 7.01383686 epoch total loss 6.449296\n",
      "Trained batch 565 batch loss 6.79974079 epoch total loss 6.44991636\n",
      "Trained batch 566 batch loss 6.62598848 epoch total loss 6.45022726\n",
      "Trained batch 567 batch loss 6.5722 epoch total loss 6.45044231\n",
      "Trained batch 568 batch loss 6.35726833 epoch total loss 6.45027828\n",
      "Trained batch 569 batch loss 6.71583366 epoch total loss 6.45074511\n",
      "Trained batch 570 batch loss 6.53072357 epoch total loss 6.4508853\n",
      "Trained batch 571 batch loss 6.55457258 epoch total loss 6.45106697\n",
      "Trained batch 572 batch loss 6.06148291 epoch total loss 6.45038605\n",
      "Trained batch 573 batch loss 6.12956429 epoch total loss 6.44982624\n",
      "Trained batch 574 batch loss 5.80346346 epoch total loss 6.44870043\n",
      "Trained batch 575 batch loss 5.6167345 epoch total loss 6.44725323\n",
      "Trained batch 576 batch loss 5.83127785 epoch total loss 6.44618416\n",
      "Trained batch 577 batch loss 5.58736706 epoch total loss 6.44469547\n",
      "Trained batch 578 batch loss 5.50029421 epoch total loss 6.44306183\n",
      "Trained batch 579 batch loss 5.07567596 epoch total loss 6.4407\n",
      "Trained batch 580 batch loss 5.4870677 epoch total loss 6.43905592\n",
      "Trained batch 581 batch loss 6.70390463 epoch total loss 6.4395113\n",
      "Trained batch 582 batch loss 6.37727642 epoch total loss 6.43940449\n",
      "Trained batch 583 batch loss 6.35268641 epoch total loss 6.43925571\n",
      "Trained batch 584 batch loss 6.03803158 epoch total loss 6.43856907\n",
      "Trained batch 585 batch loss 6.46596861 epoch total loss 6.4386158\n",
      "Trained batch 586 batch loss 6.64437485 epoch total loss 6.43896675\n",
      "Trained batch 587 batch loss 6.14361 epoch total loss 6.43846369\n",
      "Trained batch 588 batch loss 5.99494648 epoch total loss 6.43770933\n",
      "Trained batch 589 batch loss 5.76540136 epoch total loss 6.43656778\n",
      "Trained batch 590 batch loss 5.70107698 epoch total loss 6.43532133\n",
      "Trained batch 591 batch loss 5.9134593 epoch total loss 6.43443871\n",
      "Trained batch 592 batch loss 7.36912918 epoch total loss 6.43601751\n",
      "Trained batch 593 batch loss 7.07549429 epoch total loss 6.43709564\n",
      "Trained batch 594 batch loss 7.04916239 epoch total loss 6.43812609\n",
      "Trained batch 595 batch loss 6.87208891 epoch total loss 6.43885517\n",
      "Trained batch 596 batch loss 6.7270174 epoch total loss 6.43933868\n",
      "Trained batch 597 batch loss 6.71792221 epoch total loss 6.43980551\n",
      "Trained batch 598 batch loss 6.54423285 epoch total loss 6.43998\n",
      "Trained batch 599 batch loss 6.55183887 epoch total loss 6.44016695\n",
      "Trained batch 600 batch loss 6.7187109 epoch total loss 6.44063091\n",
      "Trained batch 601 batch loss 6.32488203 epoch total loss 6.44043875\n",
      "Trained batch 602 batch loss 6.60221767 epoch total loss 6.44070768\n",
      "Trained batch 603 batch loss 6.53179 epoch total loss 6.44085836\n",
      "Trained batch 604 batch loss 6.84876776 epoch total loss 6.44153404\n",
      "Trained batch 605 batch loss 6.35092115 epoch total loss 6.44138384\n",
      "Trained batch 606 batch loss 6.63226748 epoch total loss 6.44169903\n",
      "Trained batch 607 batch loss 6.29460382 epoch total loss 6.44145679\n",
      "Trained batch 608 batch loss 5.43718481 epoch total loss 6.43980551\n",
      "Trained batch 609 batch loss 5.86846685 epoch total loss 6.43886709\n",
      "Trained batch 610 batch loss 6.2300477 epoch total loss 6.43852472\n",
      "Trained batch 611 batch loss 5.65376568 epoch total loss 6.43724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 612 batch loss 6.45347929 epoch total loss 6.43726683\n",
      "Trained batch 613 batch loss 6.33566952 epoch total loss 6.43710089\n",
      "Trained batch 614 batch loss 6.46396542 epoch total loss 6.43714476\n",
      "Trained batch 615 batch loss 6.31719255 epoch total loss 6.43694925\n",
      "Trained batch 616 batch loss 6.3629961 epoch total loss 6.43682957\n",
      "Trained batch 617 batch loss 6.56248474 epoch total loss 6.43703318\n",
      "Trained batch 618 batch loss 6.29229927 epoch total loss 6.43679857\n",
      "Trained batch 619 batch loss 6.18394041 epoch total loss 6.43639\n",
      "Trained batch 620 batch loss 5.98840666 epoch total loss 6.43566799\n",
      "Trained batch 621 batch loss 6.31979465 epoch total loss 6.43548107\n",
      "Trained batch 622 batch loss 6.39525795 epoch total loss 6.4354167\n",
      "Trained batch 623 batch loss 6.50870371 epoch total loss 6.43553448\n",
      "Trained batch 624 batch loss 6.87809372 epoch total loss 6.43624353\n",
      "Trained batch 625 batch loss 6.65472651 epoch total loss 6.43659353\n",
      "Trained batch 626 batch loss 6.36683226 epoch total loss 6.43648195\n",
      "Trained batch 627 batch loss 6.11640024 epoch total loss 6.43597174\n",
      "Trained batch 628 batch loss 5.86425209 epoch total loss 6.43506145\n",
      "Trained batch 629 batch loss 6.16334677 epoch total loss 6.43462944\n",
      "Trained batch 630 batch loss 5.78456 epoch total loss 6.43359756\n",
      "Trained batch 631 batch loss 6.63688755 epoch total loss 6.43392\n",
      "Trained batch 632 batch loss 7.22153664 epoch total loss 6.43516588\n",
      "Trained batch 633 batch loss 6.81067085 epoch total loss 6.43575954\n",
      "Trained batch 634 batch loss 6.98380041 epoch total loss 6.43662405\n",
      "Trained batch 635 batch loss 7.04100752 epoch total loss 6.43757582\n",
      "Trained batch 636 batch loss 7.00875473 epoch total loss 6.4384737\n",
      "Trained batch 637 batch loss 6.35639381 epoch total loss 6.43834496\n",
      "Trained batch 638 batch loss 6.5431509 epoch total loss 6.43850851\n",
      "Trained batch 639 batch loss 6.74503326 epoch total loss 6.43898869\n",
      "Trained batch 640 batch loss 6.25267601 epoch total loss 6.43869686\n",
      "Trained batch 641 batch loss 6.2075839 epoch total loss 6.43833637\n",
      "Trained batch 642 batch loss 6.79869223 epoch total loss 6.43889809\n",
      "Trained batch 643 batch loss 6.38354635 epoch total loss 6.43881226\n",
      "Trained batch 644 batch loss 6.22585964 epoch total loss 6.43848181\n",
      "Trained batch 645 batch loss 6.21577215 epoch total loss 6.43813658\n",
      "Trained batch 646 batch loss 6.64321852 epoch total loss 6.43845367\n",
      "Trained batch 647 batch loss 6.75683594 epoch total loss 6.43894577\n",
      "Trained batch 648 batch loss 6.88298798 epoch total loss 6.43963099\n",
      "Trained batch 649 batch loss 6.87325191 epoch total loss 6.44029856\n",
      "Trained batch 650 batch loss 6.84005499 epoch total loss 6.44091368\n",
      "Trained batch 651 batch loss 6.76000786 epoch total loss 6.44140339\n",
      "Trained batch 652 batch loss 6.78186417 epoch total loss 6.44192505\n",
      "Trained batch 653 batch loss 6.75457048 epoch total loss 6.44240379\n",
      "Trained batch 654 batch loss 6.74961615 epoch total loss 6.44287348\n",
      "Trained batch 655 batch loss 6.34552908 epoch total loss 6.44272518\n",
      "Trained batch 656 batch loss 6.49290419 epoch total loss 6.442801\n",
      "Trained batch 657 batch loss 6.73322153 epoch total loss 6.4432435\n",
      "Trained batch 658 batch loss 6.42055416 epoch total loss 6.44320869\n",
      "Trained batch 659 batch loss 6.83791637 epoch total loss 6.4438076\n",
      "Trained batch 660 batch loss 7.11002254 epoch total loss 6.44481659\n",
      "Trained batch 661 batch loss 6.99287891 epoch total loss 6.44564581\n",
      "Trained batch 662 batch loss 6.78751945 epoch total loss 6.44616222\n",
      "Trained batch 663 batch loss 6.51225758 epoch total loss 6.44626188\n",
      "Trained batch 664 batch loss 6.58923292 epoch total loss 6.44647741\n",
      "Trained batch 665 batch loss 6.52284288 epoch total loss 6.44659233\n",
      "Trained batch 666 batch loss 6.05519295 epoch total loss 6.44600439\n",
      "Trained batch 667 batch loss 6.73723459 epoch total loss 6.44644117\n",
      "Trained batch 668 batch loss 6.64952278 epoch total loss 6.4467454\n",
      "Trained batch 669 batch loss 6.59365845 epoch total loss 6.44696474\n",
      "Trained batch 670 batch loss 6.70274973 epoch total loss 6.44734669\n",
      "Trained batch 671 batch loss 6.39494133 epoch total loss 6.44726849\n",
      "Trained batch 672 batch loss 6.49967 epoch total loss 6.44734621\n",
      "Trained batch 673 batch loss 6.55427837 epoch total loss 6.447505\n",
      "Trained batch 674 batch loss 7.07296753 epoch total loss 6.44843292\n",
      "Trained batch 675 batch loss 6.79114 epoch total loss 6.44894028\n",
      "Trained batch 676 batch loss 6.51684856 epoch total loss 6.44904089\n",
      "Trained batch 677 batch loss 6.21545506 epoch total loss 6.44869566\n",
      "Trained batch 678 batch loss 6.21403217 epoch total loss 6.44834948\n",
      "Trained batch 679 batch loss 6.40072298 epoch total loss 6.44827938\n",
      "Trained batch 680 batch loss 6.5468955 epoch total loss 6.44842434\n",
      "Trained batch 681 batch loss 6.64066172 epoch total loss 6.44870663\n",
      "Trained batch 682 batch loss 6.70267773 epoch total loss 6.44907904\n",
      "Trained batch 683 batch loss 6.07001495 epoch total loss 6.448524\n",
      "Trained batch 684 batch loss 6.16624212 epoch total loss 6.44811106\n",
      "Trained batch 685 batch loss 6.71731567 epoch total loss 6.44850397\n",
      "Trained batch 686 batch loss 6.72351789 epoch total loss 6.44890499\n",
      "Trained batch 687 batch loss 6.51591682 epoch total loss 6.44900274\n",
      "Trained batch 688 batch loss 6.76301241 epoch total loss 6.44945955\n",
      "Trained batch 689 batch loss 6.4587245 epoch total loss 6.44947243\n",
      "Trained batch 690 batch loss 6.27478456 epoch total loss 6.44921923\n",
      "Trained batch 691 batch loss 6.47633934 epoch total loss 6.4492588\n",
      "Trained batch 692 batch loss 6.42140293 epoch total loss 6.44921875\n",
      "Trained batch 693 batch loss 6.56056404 epoch total loss 6.44937944\n",
      "Trained batch 694 batch loss 6.25946856 epoch total loss 6.44910526\n",
      "Trained batch 695 batch loss 5.59624243 epoch total loss 6.44787836\n",
      "Trained batch 696 batch loss 6.16020536 epoch total loss 6.44746494\n",
      "Trained batch 697 batch loss 6.39237261 epoch total loss 6.44738626\n",
      "Trained batch 698 batch loss 6.47843933 epoch total loss 6.44743061\n",
      "Trained batch 699 batch loss 6.56983662 epoch total loss 6.44760561\n",
      "Trained batch 700 batch loss 6.40155602 epoch total loss 6.44754\n",
      "Trained batch 701 batch loss 6.95228815 epoch total loss 6.44826\n",
      "Trained batch 702 batch loss 6.78468561 epoch total loss 6.44873905\n",
      "Trained batch 703 batch loss 6.63950062 epoch total loss 6.44901037\n",
      "Trained batch 704 batch loss 6.92182255 epoch total loss 6.44968224\n",
      "Trained batch 705 batch loss 6.80274677 epoch total loss 6.45018291\n",
      "Trained batch 706 batch loss 6.67569065 epoch total loss 6.4505024\n",
      "Trained batch 707 batch loss 6.37357712 epoch total loss 6.45039368\n",
      "Trained batch 708 batch loss 6.07647514 epoch total loss 6.44986582\n",
      "Trained batch 709 batch loss 6.00284481 epoch total loss 6.44923544\n",
      "Trained batch 710 batch loss 6.21734619 epoch total loss 6.44890881\n",
      "Trained batch 711 batch loss 6.24285269 epoch total loss 6.44861841\n",
      "Trained batch 712 batch loss 6.50419474 epoch total loss 6.44869709\n",
      "Trained batch 713 batch loss 6.27575493 epoch total loss 6.44845438\n",
      "Trained batch 714 batch loss 6.46815157 epoch total loss 6.44848204\n",
      "Trained batch 715 batch loss 6.27406025 epoch total loss 6.4482379\n",
      "Trained batch 716 batch loss 6.36398792 epoch total loss 6.44812\n",
      "Trained batch 717 batch loss 6.40577841 epoch total loss 6.44806099\n",
      "Trained batch 718 batch loss 6.02788067 epoch total loss 6.44747591\n",
      "Trained batch 719 batch loss 6.23082924 epoch total loss 6.44717455\n",
      "Trained batch 720 batch loss 6.13473892 epoch total loss 6.44674063\n",
      "Trained batch 721 batch loss 5.9797163 epoch total loss 6.44609261\n",
      "Trained batch 722 batch loss 6.24127531 epoch total loss 6.44580889\n",
      "Trained batch 723 batch loss 6.09664488 epoch total loss 6.44532585\n",
      "Trained batch 724 batch loss 6.42023182 epoch total loss 6.44529152\n",
      "Trained batch 725 batch loss 6.34234667 epoch total loss 6.44514942\n",
      "Trained batch 726 batch loss 6.64592171 epoch total loss 6.44542599\n",
      "Trained batch 727 batch loss 6.68543911 epoch total loss 6.44575644\n",
      "Trained batch 728 batch loss 6.3128314 epoch total loss 6.44557428\n",
      "Trained batch 729 batch loss 5.99702311 epoch total loss 6.44495869\n",
      "Trained batch 730 batch loss 6.4346509 epoch total loss 6.44494438\n",
      "Trained batch 731 batch loss 6.78343725 epoch total loss 6.44540739\n",
      "Trained batch 732 batch loss 6.27120972 epoch total loss 6.44516897\n",
      "Trained batch 733 batch loss 6.69141531 epoch total loss 6.44550514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 734 batch loss 6.66672325 epoch total loss 6.44580603\n",
      "Trained batch 735 batch loss 5.98295355 epoch total loss 6.44517612\n",
      "Trained batch 736 batch loss 6.18919134 epoch total loss 6.44482803\n",
      "Trained batch 737 batch loss 6.41000175 epoch total loss 6.4447813\n",
      "Trained batch 738 batch loss 6.53338909 epoch total loss 6.44490099\n",
      "Trained batch 739 batch loss 6.48326206 epoch total loss 6.44495296\n",
      "Trained batch 740 batch loss 6.48994255 epoch total loss 6.44501352\n",
      "Trained batch 741 batch loss 6.72002602 epoch total loss 6.44538498\n",
      "Trained batch 742 batch loss 6.56839752 epoch total loss 6.44555092\n",
      "Trained batch 743 batch loss 6.67914963 epoch total loss 6.44586515\n",
      "Trained batch 744 batch loss 6.64471436 epoch total loss 6.44613218\n",
      "Trained batch 745 batch loss 6.65896511 epoch total loss 6.44641829\n",
      "Trained batch 746 batch loss 6.50665188 epoch total loss 6.44649935\n",
      "Trained batch 747 batch loss 6.42841339 epoch total loss 6.44647455\n",
      "Trained batch 748 batch loss 6.61819363 epoch total loss 6.44670439\n",
      "Trained batch 749 batch loss 6.65626 epoch total loss 6.44698381\n",
      "Trained batch 750 batch loss 6.27274704 epoch total loss 6.44675207\n",
      "Trained batch 751 batch loss 6.42255831 epoch total loss 6.44671965\n",
      "Trained batch 752 batch loss 6.42255259 epoch total loss 6.44668722\n",
      "Trained batch 753 batch loss 6.40637445 epoch total loss 6.44663334\n",
      "Trained batch 754 batch loss 6.26869774 epoch total loss 6.4463973\n",
      "Trained batch 755 batch loss 6.28308678 epoch total loss 6.44618082\n",
      "Trained batch 756 batch loss 6.57399082 epoch total loss 6.44635057\n",
      "Trained batch 757 batch loss 6.32083559 epoch total loss 6.44618464\n",
      "Trained batch 758 batch loss 6.62589073 epoch total loss 6.44642162\n",
      "Trained batch 759 batch loss 6.47614861 epoch total loss 6.44646072\n",
      "Trained batch 760 batch loss 6.37407494 epoch total loss 6.44636536\n",
      "Trained batch 761 batch loss 6.08811331 epoch total loss 6.44589424\n",
      "Trained batch 762 batch loss 6.17412472 epoch total loss 6.44553804\n",
      "Trained batch 763 batch loss 6.07239389 epoch total loss 6.44504881\n",
      "Trained batch 764 batch loss 6.19489717 epoch total loss 6.44472122\n",
      "Trained batch 765 batch loss 6.25984859 epoch total loss 6.44447947\n",
      "Trained batch 766 batch loss 6.48910379 epoch total loss 6.44453812\n",
      "Trained batch 767 batch loss 6.41544962 epoch total loss 6.4445\n",
      "Trained batch 768 batch loss 6.2571888 epoch total loss 6.44425631\n",
      "Trained batch 769 batch loss 5.66528559 epoch total loss 6.44324398\n",
      "Trained batch 770 batch loss 5.53846359 epoch total loss 6.44206905\n",
      "Trained batch 771 batch loss 5.83225632 epoch total loss 6.4412775\n",
      "Trained batch 772 batch loss 6.07299519 epoch total loss 6.4408\n",
      "Trained batch 773 batch loss 5.99362326 epoch total loss 6.44022179\n",
      "Trained batch 774 batch loss 6.23059607 epoch total loss 6.43995094\n",
      "Trained batch 775 batch loss 6.41520309 epoch total loss 6.43991852\n",
      "Trained batch 776 batch loss 6.32875919 epoch total loss 6.43977547\n",
      "Trained batch 777 batch loss 6.30112314 epoch total loss 6.43959713\n",
      "Trained batch 778 batch loss 6.4794631 epoch total loss 6.43964815\n",
      "Trained batch 779 batch loss 7.01916409 epoch total loss 6.44039202\n",
      "Trained batch 780 batch loss 7.00661755 epoch total loss 6.44111824\n",
      "Trained batch 781 batch loss 6.06839371 epoch total loss 6.44064093\n",
      "Trained batch 782 batch loss 6.31335211 epoch total loss 6.44047832\n",
      "Trained batch 783 batch loss 6.45613861 epoch total loss 6.44049835\n",
      "Trained batch 784 batch loss 6.47181177 epoch total loss 6.44053793\n",
      "Trained batch 785 batch loss 6.33077145 epoch total loss 6.44039774\n",
      "Trained batch 786 batch loss 6.35789776 epoch total loss 6.44029284\n",
      "Trained batch 787 batch loss 6.32730055 epoch total loss 6.44014931\n",
      "Trained batch 788 batch loss 6.56067 epoch total loss 6.4403019\n",
      "Trained batch 789 batch loss 6.7783742 epoch total loss 6.44073057\n",
      "Trained batch 790 batch loss 6.66691113 epoch total loss 6.44101667\n",
      "Trained batch 791 batch loss 6.66494274 epoch total loss 6.4413\n",
      "Trained batch 792 batch loss 6.63088036 epoch total loss 6.44153929\n",
      "Trained batch 793 batch loss 6.5503993 epoch total loss 6.44167662\n",
      "Trained batch 794 batch loss 6.38545227 epoch total loss 6.44160557\n",
      "Trained batch 795 batch loss 6.04960489 epoch total loss 6.44111252\n",
      "Trained batch 796 batch loss 6.58115 epoch total loss 6.44128847\n",
      "Trained batch 797 batch loss 6.99400806 epoch total loss 6.44198227\n",
      "Trained batch 798 batch loss 6.5934124 epoch total loss 6.44217157\n",
      "Trained batch 799 batch loss 6.15846157 epoch total loss 6.44181681\n",
      "Trained batch 800 batch loss 6.23598623 epoch total loss 6.44155931\n",
      "Trained batch 801 batch loss 5.95793 epoch total loss 6.44095564\n",
      "Trained batch 802 batch loss 6.3164587 epoch total loss 6.44080067\n",
      "Trained batch 803 batch loss 6.52025461 epoch total loss 6.4408989\n",
      "Trained batch 804 batch loss 6.30490971 epoch total loss 6.44072962\n",
      "Trained batch 805 batch loss 6.66559839 epoch total loss 6.44100904\n",
      "Trained batch 806 batch loss 7.25465822 epoch total loss 6.44201851\n",
      "Trained batch 807 batch loss 7.35079575 epoch total loss 6.4431448\n",
      "Trained batch 808 batch loss 7.17584038 epoch total loss 6.44405127\n",
      "Trained batch 809 batch loss 7.08055067 epoch total loss 6.44483805\n",
      "Trained batch 810 batch loss 6.84057903 epoch total loss 6.44532681\n",
      "Trained batch 811 batch loss 6.5652914 epoch total loss 6.4454751\n",
      "Trained batch 812 batch loss 5.79904366 epoch total loss 6.44467878\n",
      "Trained batch 813 batch loss 6.3166213 epoch total loss 6.44452095\n",
      "Trained batch 814 batch loss 6.56021261 epoch total loss 6.44466305\n",
      "Trained batch 815 batch loss 6.64089775 epoch total loss 6.44490385\n",
      "Trained batch 816 batch loss 6.29825449 epoch total loss 6.44472408\n",
      "Trained batch 817 batch loss 6.48080969 epoch total loss 6.44476843\n",
      "Trained batch 818 batch loss 6.33013344 epoch total loss 6.44462824\n",
      "Trained batch 819 batch loss 6.21035576 epoch total loss 6.44434261\n",
      "Trained batch 820 batch loss 6.20742321 epoch total loss 6.44405365\n",
      "Trained batch 821 batch loss 6.58604574 epoch total loss 6.44422674\n",
      "Trained batch 822 batch loss 6.02409458 epoch total loss 6.4437151\n",
      "Trained batch 823 batch loss 6.30659533 epoch total loss 6.44354868\n",
      "Trained batch 824 batch loss 6.56384 epoch total loss 6.44369459\n",
      "Trained batch 825 batch loss 6.6622777 epoch total loss 6.44395971\n",
      "Trained batch 826 batch loss 6.8692317 epoch total loss 6.44447422\n",
      "Trained batch 827 batch loss 7.02118397 epoch total loss 6.44517136\n",
      "Trained batch 828 batch loss 7.03157568 epoch total loss 6.44588\n",
      "Trained batch 829 batch loss 7.00927591 epoch total loss 6.44655943\n",
      "Trained batch 830 batch loss 6.77983189 epoch total loss 6.44696093\n",
      "Trained batch 831 batch loss 6.40191603 epoch total loss 6.44690657\n",
      "Trained batch 832 batch loss 6.01915264 epoch total loss 6.44639254\n",
      "Trained batch 833 batch loss 6.77630281 epoch total loss 6.44678831\n",
      "Trained batch 834 batch loss 6.82718515 epoch total loss 6.44724464\n",
      "Trained batch 835 batch loss 6.7875762 epoch total loss 6.44765234\n",
      "Trained batch 836 batch loss 6.74372959 epoch total loss 6.44800615\n",
      "Trained batch 837 batch loss 6.34554768 epoch total loss 6.44788408\n",
      "Trained batch 838 batch loss 6.62444496 epoch total loss 6.44809484\n",
      "Trained batch 839 batch loss 6.63357162 epoch total loss 6.4483161\n",
      "Trained batch 840 batch loss 6.87585497 epoch total loss 6.44882536\n",
      "Trained batch 841 batch loss 6.90099716 epoch total loss 6.44936275\n",
      "Trained batch 842 batch loss 6.90070534 epoch total loss 6.4498992\n",
      "Trained batch 843 batch loss 6.50311327 epoch total loss 6.44996166\n",
      "Trained batch 844 batch loss 6.57026 epoch total loss 6.45010471\n",
      "Trained batch 845 batch loss 6.89895725 epoch total loss 6.45063543\n",
      "Trained batch 846 batch loss 6.73997307 epoch total loss 6.45097733\n",
      "Trained batch 847 batch loss 6.20545864 epoch total loss 6.45068741\n",
      "Trained batch 848 batch loss 6.54411793 epoch total loss 6.45079756\n",
      "Trained batch 849 batch loss 6.2409358 epoch total loss 6.45055\n",
      "Trained batch 850 batch loss 6.32626152 epoch total loss 6.45040369\n",
      "Trained batch 851 batch loss 6.75788879 epoch total loss 6.45076513\n",
      "Trained batch 852 batch loss 6.68635798 epoch total loss 6.4510417\n",
      "Trained batch 853 batch loss 6.51808071 epoch total loss 6.45112038\n",
      "Trained batch 854 batch loss 6.06128359 epoch total loss 6.45066404\n",
      "Trained batch 855 batch loss 6.21902895 epoch total loss 6.45039368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 856 batch loss 6.22869158 epoch total loss 6.45013428\n",
      "Trained batch 857 batch loss 6.43684959 epoch total loss 6.45011902\n",
      "Trained batch 858 batch loss 6.50146103 epoch total loss 6.45017862\n",
      "Trained batch 859 batch loss 6.65205956 epoch total loss 6.4504137\n",
      "Trained batch 860 batch loss 6.69370317 epoch total loss 6.45069647\n",
      "Trained batch 861 batch loss 6.79820728 epoch total loss 6.45110035\n",
      "Trained batch 862 batch loss 6.88587379 epoch total loss 6.45160484\n",
      "Trained batch 863 batch loss 6.70349 epoch total loss 6.45189667\n",
      "Trained batch 864 batch loss 6.50441551 epoch total loss 6.45195723\n",
      "Trained batch 865 batch loss 6.45339298 epoch total loss 6.45195913\n",
      "Trained batch 866 batch loss 6.45933676 epoch total loss 6.45196819\n",
      "Trained batch 867 batch loss 6.49085665 epoch total loss 6.45201254\n",
      "Trained batch 868 batch loss 6.04433966 epoch total loss 6.45154333\n",
      "Trained batch 869 batch loss 6.31609535 epoch total loss 6.45138693\n",
      "Trained batch 870 batch loss 6.66116714 epoch total loss 6.45162821\n",
      "Trained batch 871 batch loss 6.51948166 epoch total loss 6.45170593\n",
      "Trained batch 872 batch loss 6.67540073 epoch total loss 6.45196247\n",
      "Trained batch 873 batch loss 6.54223394 epoch total loss 6.45206547\n",
      "Trained batch 874 batch loss 6.55429411 epoch total loss 6.45218229\n",
      "Trained batch 875 batch loss 6.30552387 epoch total loss 6.45201492\n",
      "Trained batch 876 batch loss 6.21181726 epoch total loss 6.45174074\n",
      "Trained batch 877 batch loss 6.20774746 epoch total loss 6.45146227\n",
      "Trained batch 878 batch loss 6.45248222 epoch total loss 6.4514637\n",
      "Trained batch 879 batch loss 6.50020123 epoch total loss 6.45151901\n",
      "Trained batch 880 batch loss 6.56857395 epoch total loss 6.45165205\n",
      "Trained batch 881 batch loss 7.09551096 epoch total loss 6.45238304\n",
      "Trained batch 882 batch loss 7.13247919 epoch total loss 6.45315361\n",
      "Trained batch 883 batch loss 6.90969849 epoch total loss 6.45367098\n",
      "Trained batch 884 batch loss 6.83060312 epoch total loss 6.45409727\n",
      "Trained batch 885 batch loss 7.01707363 epoch total loss 6.45473337\n",
      "Trained batch 886 batch loss 7.0273037 epoch total loss 6.45537949\n",
      "Trained batch 887 batch loss 6.96901846 epoch total loss 6.45595884\n",
      "Trained batch 888 batch loss 6.45988464 epoch total loss 6.45596361\n",
      "Trained batch 889 batch loss 6.02925682 epoch total loss 6.45548344\n",
      "Trained batch 890 batch loss 5.92180777 epoch total loss 6.45488405\n",
      "Trained batch 891 batch loss 6.32939053 epoch total loss 6.45474339\n",
      "Trained batch 892 batch loss 6.63876677 epoch total loss 6.45494938\n",
      "Trained batch 893 batch loss 6.39809799 epoch total loss 6.45488548\n",
      "Trained batch 894 batch loss 6.64608145 epoch total loss 6.45509958\n",
      "Trained batch 895 batch loss 6.36044121 epoch total loss 6.45499372\n",
      "Trained batch 896 batch loss 6.47952461 epoch total loss 6.4550209\n",
      "Trained batch 897 batch loss 6.0936656 epoch total loss 6.45461798\n",
      "Trained batch 898 batch loss 6.69529438 epoch total loss 6.45488596\n",
      "Trained batch 899 batch loss 6.47456121 epoch total loss 6.45490789\n",
      "Trained batch 900 batch loss 6.50944233 epoch total loss 6.45496845\n",
      "Trained batch 901 batch loss 6.68626 epoch total loss 6.45522499\n",
      "Trained batch 902 batch loss 6.36697292 epoch total loss 6.45512724\n",
      "Trained batch 903 batch loss 5.8637557 epoch total loss 6.45447254\n",
      "Trained batch 904 batch loss 5.97996426 epoch total loss 6.45394754\n",
      "Trained batch 905 batch loss 5.55402422 epoch total loss 6.45295334\n",
      "Trained batch 906 batch loss 5.72500467 epoch total loss 6.45215\n",
      "Trained batch 907 batch loss 6.34286642 epoch total loss 6.45202923\n",
      "Trained batch 908 batch loss 6.39400673 epoch total loss 6.45196581\n",
      "Trained batch 909 batch loss 6.97480297 epoch total loss 6.4525404\n",
      "Trained batch 910 batch loss 6.77031708 epoch total loss 6.45289\n",
      "Trained batch 911 batch loss 6.51493168 epoch total loss 6.45295811\n",
      "Trained batch 912 batch loss 5.97010183 epoch total loss 6.45242882\n",
      "Trained batch 913 batch loss 6.31848907 epoch total loss 6.45228195\n",
      "Trained batch 914 batch loss 6.55845308 epoch total loss 6.4523983\n",
      "Trained batch 915 batch loss 6.42827 epoch total loss 6.45237207\n",
      "Trained batch 916 batch loss 6.4446311 epoch total loss 6.45236397\n",
      "Trained batch 917 batch loss 5.99219227 epoch total loss 6.45186186\n",
      "Trained batch 918 batch loss 6.08584833 epoch total loss 6.45146322\n",
      "Trained batch 919 batch loss 6.04653454 epoch total loss 6.45102262\n",
      "Trained batch 920 batch loss 6.15391874 epoch total loss 6.45069933\n",
      "Trained batch 921 batch loss 6.14033461 epoch total loss 6.45036221\n",
      "Trained batch 922 batch loss 6.27425241 epoch total loss 6.45017147\n",
      "Trained batch 923 batch loss 5.82729387 epoch total loss 6.44949627\n",
      "Trained batch 924 batch loss 6.01517773 epoch total loss 6.44902658\n",
      "Trained batch 925 batch loss 5.3999548 epoch total loss 6.44789219\n",
      "Trained batch 926 batch loss 6.04050303 epoch total loss 6.44745207\n",
      "Trained batch 927 batch loss 6.47071314 epoch total loss 6.44747734\n",
      "Trained batch 928 batch loss 6.36375332 epoch total loss 6.44738722\n",
      "Trained batch 929 batch loss 6.54694939 epoch total loss 6.44749403\n",
      "Trained batch 930 batch loss 6.58819771 epoch total loss 6.44764566\n",
      "Trained batch 931 batch loss 6.62794065 epoch total loss 6.44783926\n",
      "Trained batch 932 batch loss 6.34891319 epoch total loss 6.4477334\n",
      "Trained batch 933 batch loss 6.40146065 epoch total loss 6.44768381\n",
      "Trained batch 934 batch loss 6.36244774 epoch total loss 6.44759226\n",
      "Trained batch 935 batch loss 6.18306 epoch total loss 6.44730949\n",
      "Trained batch 936 batch loss 6.27356052 epoch total loss 6.44712353\n",
      "Trained batch 937 batch loss 6.45577097 epoch total loss 6.44713259\n",
      "Trained batch 938 batch loss 6.50214 epoch total loss 6.44719124\n",
      "Trained batch 939 batch loss 6.54514 epoch total loss 6.44729519\n",
      "Trained batch 940 batch loss 6.46345854 epoch total loss 6.44731236\n",
      "Trained batch 941 batch loss 5.96389914 epoch total loss 6.4467988\n",
      "Trained batch 942 batch loss 6.20670605 epoch total loss 6.44654369\n",
      "Trained batch 943 batch loss 6.04533625 epoch total loss 6.44611835\n",
      "Trained batch 944 batch loss 6.29462433 epoch total loss 6.44595766\n",
      "Trained batch 945 batch loss 6.3801465 epoch total loss 6.44588804\n",
      "Trained batch 946 batch loss 5.5025 epoch total loss 6.44489098\n",
      "Trained batch 947 batch loss 6.46633673 epoch total loss 6.44491339\n",
      "Trained batch 948 batch loss 6.133389 epoch total loss 6.44458485\n",
      "Trained batch 949 batch loss 6.33450127 epoch total loss 6.4444685\n",
      "Trained batch 950 batch loss 6.57669353 epoch total loss 6.44460773\n",
      "Trained batch 951 batch loss 6.82071447 epoch total loss 6.44500351\n",
      "Trained batch 952 batch loss 6.5436492 epoch total loss 6.44510698\n",
      "Trained batch 953 batch loss 6.21930361 epoch total loss 6.44487\n",
      "Trained batch 954 batch loss 5.91776752 epoch total loss 6.44431734\n",
      "Trained batch 955 batch loss 6.16685057 epoch total loss 6.44402695\n",
      "Trained batch 956 batch loss 6.27968168 epoch total loss 6.44385529\n",
      "Trained batch 957 batch loss 6.26436901 epoch total loss 6.44366741\n",
      "Trained batch 958 batch loss 6.3186512 epoch total loss 6.44353724\n",
      "Trained batch 959 batch loss 6.60001469 epoch total loss 6.44370031\n",
      "Trained batch 960 batch loss 6.65536118 epoch total loss 6.44392109\n",
      "Trained batch 961 batch loss 6.48179913 epoch total loss 6.44396067\n",
      "Trained batch 962 batch loss 6.4389286 epoch total loss 6.44395542\n",
      "Trained batch 963 batch loss 6.05488205 epoch total loss 6.44355106\n",
      "Trained batch 964 batch loss 5.98656607 epoch total loss 6.44307661\n",
      "Trained batch 965 batch loss 6.4505887 epoch total loss 6.44308472\n",
      "Trained batch 966 batch loss 6.3603282 epoch total loss 6.44299889\n",
      "Trained batch 967 batch loss 6.16333532 epoch total loss 6.44271\n",
      "Trained batch 968 batch loss 6.48684883 epoch total loss 6.4427557\n",
      "Trained batch 969 batch loss 6.51647 epoch total loss 6.44283199\n",
      "Trained batch 970 batch loss 6.63181257 epoch total loss 6.44302654\n",
      "Trained batch 971 batch loss 6.43464231 epoch total loss 6.44301796\n",
      "Trained batch 972 batch loss 5.62669754 epoch total loss 6.44217777\n",
      "Trained batch 973 batch loss 6.36608028 epoch total loss 6.44209957\n",
      "Trained batch 974 batch loss 6.48035145 epoch total loss 6.44213915\n",
      "Trained batch 975 batch loss 6.50429201 epoch total loss 6.44220304\n",
      "Trained batch 976 batch loss 6.65682745 epoch total loss 6.44242287\n",
      "Trained batch 977 batch loss 6.35147667 epoch total loss 6.44233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 978 batch loss 6.4730792 epoch total loss 6.44236135\n",
      "Trained batch 979 batch loss 6.26519728 epoch total loss 6.44218\n",
      "Trained batch 980 batch loss 6.3218832 epoch total loss 6.44205761\n",
      "Trained batch 981 batch loss 6.44820404 epoch total loss 6.44206381\n",
      "Trained batch 982 batch loss 6.04478 epoch total loss 6.44165945\n",
      "Trained batch 983 batch loss 6.26638746 epoch total loss 6.44148111\n",
      "Trained batch 984 batch loss 6.03773928 epoch total loss 6.44107103\n",
      "Trained batch 985 batch loss 6.17295408 epoch total loss 6.44079828\n",
      "Trained batch 986 batch loss 6.35797787 epoch total loss 6.44071436\n",
      "Trained batch 987 batch loss 6.55974579 epoch total loss 6.440835\n",
      "Trained batch 988 batch loss 6.32791948 epoch total loss 6.44072056\n",
      "Trained batch 989 batch loss 6.53827477 epoch total loss 6.44081926\n",
      "Trained batch 990 batch loss 6.58580589 epoch total loss 6.44096565\n",
      "Trained batch 991 batch loss 6.76269102 epoch total loss 6.44129038\n",
      "Trained batch 992 batch loss 6.11046 epoch total loss 6.44095707\n",
      "Trained batch 993 batch loss 6.35411024 epoch total loss 6.44086933\n",
      "Trained batch 994 batch loss 6.50508881 epoch total loss 6.4409337\n",
      "Trained batch 995 batch loss 6.40365 epoch total loss 6.44089651\n",
      "Trained batch 996 batch loss 6.65531206 epoch total loss 6.44111156\n",
      "Trained batch 997 batch loss 6.86752415 epoch total loss 6.44153929\n",
      "Trained batch 998 batch loss 6.99821568 epoch total loss 6.44209719\n",
      "Trained batch 999 batch loss 6.98422146 epoch total loss 6.44264\n",
      "Trained batch 1000 batch loss 6.58434105 epoch total loss 6.44278193\n",
      "Trained batch 1001 batch loss 6.88940144 epoch total loss 6.44322777\n",
      "Trained batch 1002 batch loss 6.32156277 epoch total loss 6.44310665\n",
      "Trained batch 1003 batch loss 6.12072945 epoch total loss 6.44278479\n",
      "Trained batch 1004 batch loss 6.59120274 epoch total loss 6.44293308\n",
      "Trained batch 1005 batch loss 6.69216251 epoch total loss 6.44318104\n",
      "Trained batch 1006 batch loss 6.4928565 epoch total loss 6.44323\n",
      "Trained batch 1007 batch loss 6.50399542 epoch total loss 6.44329071\n",
      "Trained batch 1008 batch loss 6.31101179 epoch total loss 6.4431591\n",
      "Trained batch 1009 batch loss 6.32814789 epoch total loss 6.44304514\n",
      "Trained batch 1010 batch loss 6.4368248 epoch total loss 6.44303942\n",
      "Trained batch 1011 batch loss 6.52102423 epoch total loss 6.44311666\n",
      "Trained batch 1012 batch loss 6.64944506 epoch total loss 6.44332027\n",
      "Trained batch 1013 batch loss 6.35674953 epoch total loss 6.44323492\n",
      "Trained batch 1014 batch loss 6.1416707 epoch total loss 6.44293737\n",
      "Trained batch 1015 batch loss 6.56452274 epoch total loss 6.44305706\n",
      "Trained batch 1016 batch loss 6.13984442 epoch total loss 6.44275856\n",
      "Trained batch 1017 batch loss 6.28049898 epoch total loss 6.44259882\n",
      "Trained batch 1018 batch loss 5.90944481 epoch total loss 6.44207525\n",
      "Trained batch 1019 batch loss 5.83813286 epoch total loss 6.44148254\n",
      "Trained batch 1020 batch loss 6.13441563 epoch total loss 6.44118118\n",
      "Trained batch 1021 batch loss 6.340168 epoch total loss 6.44108248\n",
      "Trained batch 1022 batch loss 6.29861736 epoch total loss 6.44094324\n",
      "Trained batch 1023 batch loss 6.50499201 epoch total loss 6.44100571\n",
      "Trained batch 1024 batch loss 6.58363247 epoch total loss 6.44114494\n",
      "Trained batch 1025 batch loss 6.42411184 epoch total loss 6.44112873\n",
      "Trained batch 1026 batch loss 6.40731239 epoch total loss 6.44109535\n",
      "Trained batch 1027 batch loss 6.50426674 epoch total loss 6.44115734\n",
      "Trained batch 1028 batch loss 6.59913588 epoch total loss 6.44131088\n",
      "Trained batch 1029 batch loss 6.47061682 epoch total loss 6.44133949\n",
      "Trained batch 1030 batch loss 6.74236298 epoch total loss 6.44163132\n",
      "Trained batch 1031 batch loss 6.87881088 epoch total loss 6.4420557\n",
      "Trained batch 1032 batch loss 6.83198404 epoch total loss 6.44243336\n",
      "Trained batch 1033 batch loss 6.80023432 epoch total loss 6.44278\n",
      "Trained batch 1034 batch loss 6.81190205 epoch total loss 6.44313717\n",
      "Trained batch 1035 batch loss 6.75379276 epoch total loss 6.4434371\n",
      "Trained batch 1036 batch loss 6.53244972 epoch total loss 6.44352293\n",
      "Trained batch 1037 batch loss 6.55196095 epoch total loss 6.44362736\n",
      "Trained batch 1038 batch loss 6.44206524 epoch total loss 6.44362545\n",
      "Trained batch 1039 batch loss 6.79588127 epoch total loss 6.44396448\n",
      "Trained batch 1040 batch loss 6.42249298 epoch total loss 6.44394398\n",
      "Trained batch 1041 batch loss 6.51548529 epoch total loss 6.44401264\n",
      "Trained batch 1042 batch loss 6.49458504 epoch total loss 6.44406128\n",
      "Trained batch 1043 batch loss 6.373528 epoch total loss 6.44399357\n",
      "Trained batch 1044 batch loss 6.25971413 epoch total loss 6.44381714\n",
      "Trained batch 1045 batch loss 6.51154613 epoch total loss 6.44388247\n",
      "Trained batch 1046 batch loss 6.29451036 epoch total loss 6.44373941\n",
      "Trained batch 1047 batch loss 6.62962961 epoch total loss 6.4439168\n",
      "Trained batch 1048 batch loss 6.29078674 epoch total loss 6.44377089\n",
      "Trained batch 1049 batch loss 6.43127918 epoch total loss 6.44375896\n",
      "Trained batch 1050 batch loss 6.33493948 epoch total loss 6.44365501\n",
      "Trained batch 1051 batch loss 6.68086767 epoch total loss 6.44388056\n",
      "Trained batch 1052 batch loss 6.47059679 epoch total loss 6.44390631\n",
      "Trained batch 1053 batch loss 6.29005527 epoch total loss 6.44376\n",
      "Trained batch 1054 batch loss 6.33955956 epoch total loss 6.44366074\n",
      "Trained batch 1055 batch loss 6.57210875 epoch total loss 6.44378281\n",
      "Trained batch 1056 batch loss 6.67849779 epoch total loss 6.44400549\n",
      "Trained batch 1057 batch loss 6.59995079 epoch total loss 6.44415283\n",
      "Trained batch 1058 batch loss 6.76867819 epoch total loss 6.44445944\n",
      "Trained batch 1059 batch loss 6.37683105 epoch total loss 6.44439602\n",
      "Trained batch 1060 batch loss 6.38097525 epoch total loss 6.44433594\n",
      "Trained batch 1061 batch loss 6.76385641 epoch total loss 6.44463682\n",
      "Trained batch 1062 batch loss 6.27646065 epoch total loss 6.44447851\n",
      "Trained batch 1063 batch loss 6.34706068 epoch total loss 6.44438696\n",
      "Trained batch 1064 batch loss 5.96042633 epoch total loss 6.44393206\n",
      "Trained batch 1065 batch loss 6.10341787 epoch total loss 6.44361258\n",
      "Trained batch 1066 batch loss 6.43548441 epoch total loss 6.44360495\n",
      "Trained batch 1067 batch loss 6.36977863 epoch total loss 6.44353533\n",
      "Trained batch 1068 batch loss 6.573493 epoch total loss 6.4436574\n",
      "Trained batch 1069 batch loss 6.03740406 epoch total loss 6.44327784\n",
      "Trained batch 1070 batch loss 5.75455856 epoch total loss 6.44263363\n",
      "Trained batch 1071 batch loss 6.0870223 epoch total loss 6.44230175\n",
      "Trained batch 1072 batch loss 6.50346375 epoch total loss 6.44235849\n",
      "Trained batch 1073 batch loss 6.8255167 epoch total loss 6.44271612\n",
      "Trained batch 1074 batch loss 6.56972408 epoch total loss 6.44283438\n",
      "Trained batch 1075 batch loss 6.20541906 epoch total loss 6.4426136\n",
      "Trained batch 1076 batch loss 6.14190435 epoch total loss 6.44233418\n",
      "Trained batch 1077 batch loss 6.28368807 epoch total loss 6.44218683\n",
      "Trained batch 1078 batch loss 7.06003571 epoch total loss 6.44276\n",
      "Trained batch 1079 batch loss 6.72829151 epoch total loss 6.44302511\n",
      "Trained batch 1080 batch loss 6.43554831 epoch total loss 6.44301796\n",
      "Trained batch 1081 batch loss 6.23302269 epoch total loss 6.44282389\n",
      "Trained batch 1082 batch loss 6.44811964 epoch total loss 6.44282866\n",
      "Trained batch 1083 batch loss 6.30363798 epoch total loss 6.44270039\n",
      "Trained batch 1084 batch loss 6.4196558 epoch total loss 6.44267893\n",
      "Trained batch 1085 batch loss 6.69433212 epoch total loss 6.44291067\n",
      "Trained batch 1086 batch loss 6.41516113 epoch total loss 6.44288492\n",
      "Trained batch 1087 batch loss 6.20956421 epoch total loss 6.44267035\n",
      "Trained batch 1088 batch loss 6.35853767 epoch total loss 6.44259262\n",
      "Trained batch 1089 batch loss 6.19805336 epoch total loss 6.44236851\n",
      "Trained batch 1090 batch loss 6.34671879 epoch total loss 6.44228077\n",
      "Trained batch 1091 batch loss 5.95740318 epoch total loss 6.44183636\n",
      "Trained batch 1092 batch loss 6.34553528 epoch total loss 6.44174814\n",
      "Trained batch 1093 batch loss 6.25280905 epoch total loss 6.44157553\n",
      "Trained batch 1094 batch loss 6.09431839 epoch total loss 6.44125795\n",
      "Trained batch 1095 batch loss 6.36118793 epoch total loss 6.441185\n",
      "Trained batch 1096 batch loss 6.78993368 epoch total loss 6.44150352\n",
      "Trained batch 1097 batch loss 6.5424571 epoch total loss 6.44159555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1098 batch loss 6.2080822 epoch total loss 6.44138288\n",
      "Trained batch 1099 batch loss 6.39117622 epoch total loss 6.44133711\n",
      "Trained batch 1100 batch loss 6.56128359 epoch total loss 6.4414463\n",
      "Trained batch 1101 batch loss 6.94714785 epoch total loss 6.4419055\n",
      "Trained batch 1102 batch loss 6.60864687 epoch total loss 6.44205713\n",
      "Trained batch 1103 batch loss 6.17754936 epoch total loss 6.44181728\n",
      "Trained batch 1104 batch loss 6.24183 epoch total loss 6.44163609\n",
      "Trained batch 1105 batch loss 5.96993876 epoch total loss 6.44120932\n",
      "Trained batch 1106 batch loss 6.2420764 epoch total loss 6.44102907\n",
      "Trained batch 1107 batch loss 6.24594879 epoch total loss 6.44085312\n",
      "Trained batch 1108 batch loss 6.31635523 epoch total loss 6.44074059\n",
      "Trained batch 1109 batch loss 6.23468781 epoch total loss 6.4405551\n",
      "Trained batch 1110 batch loss 6.60961437 epoch total loss 6.44070721\n",
      "Trained batch 1111 batch loss 6.74710083 epoch total loss 6.44098282\n",
      "Trained batch 1112 batch loss 6.89699316 epoch total loss 6.4413929\n",
      "Trained batch 1113 batch loss 6.82620096 epoch total loss 6.44173861\n",
      "Trained batch 1114 batch loss 5.88465786 epoch total loss 6.44123888\n",
      "Trained batch 1115 batch loss 6.21295547 epoch total loss 6.44103384\n",
      "Trained batch 1116 batch loss 6.40581417 epoch total loss 6.44100237\n",
      "Trained batch 1117 batch loss 6.35474062 epoch total loss 6.4409256\n",
      "Trained batch 1118 batch loss 6.21511126 epoch total loss 6.44072342\n",
      "Trained batch 1119 batch loss 6.51807451 epoch total loss 6.44079256\n",
      "Trained batch 1120 batch loss 6.42730141 epoch total loss 6.44078064\n",
      "Trained batch 1121 batch loss 6.44327545 epoch total loss 6.44078302\n",
      "Trained batch 1122 batch loss 6.44313288 epoch total loss 6.44078541\n",
      "Trained batch 1123 batch loss 6.30048323 epoch total loss 6.44066\n",
      "Trained batch 1124 batch loss 6.30949 epoch total loss 6.44054365\n",
      "Trained batch 1125 batch loss 6.50590897 epoch total loss 6.44060135\n",
      "Trained batch 1126 batch loss 6.34294415 epoch total loss 6.44051456\n",
      "Trained batch 1127 batch loss 6.36131191 epoch total loss 6.44044447\n",
      "Trained batch 1128 batch loss 6.42303324 epoch total loss 6.44042873\n",
      "Trained batch 1129 batch loss 6.3178215 epoch total loss 6.44032049\n",
      "Trained batch 1130 batch loss 6.43769073 epoch total loss 6.44031763\n",
      "Trained batch 1131 batch loss 6.5614 epoch total loss 6.44042492\n",
      "Trained batch 1132 batch loss 6.20560265 epoch total loss 6.44021749\n",
      "Trained batch 1133 batch loss 6.66622114 epoch total loss 6.44041681\n",
      "Trained batch 1134 batch loss 6.31500387 epoch total loss 6.44030619\n",
      "Trained batch 1135 batch loss 6.21443415 epoch total loss 6.44010687\n",
      "Trained batch 1136 batch loss 6.54012442 epoch total loss 6.44019508\n",
      "Trained batch 1137 batch loss 6.29682 epoch total loss 6.44006872\n",
      "Trained batch 1138 batch loss 6.47441626 epoch total loss 6.44009924\n",
      "Trained batch 1139 batch loss 6.47392368 epoch total loss 6.44012928\n",
      "Trained batch 1140 batch loss 6.5497 epoch total loss 6.4402256\n",
      "Trained batch 1141 batch loss 6.19321346 epoch total loss 6.44000912\n",
      "Trained batch 1142 batch loss 6.80036545 epoch total loss 6.44032431\n",
      "Trained batch 1143 batch loss 6.67760277 epoch total loss 6.44053221\n",
      "Trained batch 1144 batch loss 7.14750671 epoch total loss 6.44115\n",
      "Trained batch 1145 batch loss 7.39327621 epoch total loss 6.44198132\n",
      "Trained batch 1146 batch loss 7.49089909 epoch total loss 6.44289684\n",
      "Trained batch 1147 batch loss 7.30928898 epoch total loss 6.44365168\n",
      "Trained batch 1148 batch loss 7.20490503 epoch total loss 6.44431496\n",
      "Trained batch 1149 batch loss 6.27610826 epoch total loss 6.44416857\n",
      "Trained batch 1150 batch loss 6.31569433 epoch total loss 6.44405699\n",
      "Trained batch 1151 batch loss 6.44749165 epoch total loss 6.44406\n",
      "Trained batch 1152 batch loss 6.50814438 epoch total loss 6.44411564\n",
      "Trained batch 1153 batch loss 6.30528545 epoch total loss 6.443995\n",
      "Trained batch 1154 batch loss 6.36991024 epoch total loss 6.4439311\n",
      "Trained batch 1155 batch loss 6.449 epoch total loss 6.44393539\n",
      "Trained batch 1156 batch loss 6.7326231 epoch total loss 6.44418526\n",
      "Trained batch 1157 batch loss 5.90201712 epoch total loss 6.44371653\n",
      "Trained batch 1158 batch loss 5.61287 epoch total loss 6.44299889\n",
      "Trained batch 1159 batch loss 4.89053869 epoch total loss 6.44165945\n",
      "Trained batch 1160 batch loss 5.64751577 epoch total loss 6.44097471\n",
      "Trained batch 1161 batch loss 6.30355215 epoch total loss 6.44085646\n",
      "Trained batch 1162 batch loss 6.57064581 epoch total loss 6.44096851\n",
      "Trained batch 1163 batch loss 6.88831711 epoch total loss 6.44135284\n",
      "Trained batch 1164 batch loss 5.91337299 epoch total loss 6.44089937\n",
      "Trained batch 1165 batch loss 6.75400543 epoch total loss 6.44116831\n",
      "Trained batch 1166 batch loss 6.26740837 epoch total loss 6.44101954\n",
      "Trained batch 1167 batch loss 5.94429207 epoch total loss 6.44059372\n",
      "Trained batch 1168 batch loss 6.02195358 epoch total loss 6.44023514\n",
      "Trained batch 1169 batch loss 6.07772493 epoch total loss 6.43992519\n",
      "Trained batch 1170 batch loss 5.64805365 epoch total loss 6.43924809\n",
      "Trained batch 1171 batch loss 6.37595701 epoch total loss 6.4391942\n",
      "Trained batch 1172 batch loss 6.58285856 epoch total loss 6.43931675\n",
      "Trained batch 1173 batch loss 5.78592396 epoch total loss 6.43876\n",
      "Trained batch 1174 batch loss 6.61392117 epoch total loss 6.43890905\n",
      "Trained batch 1175 batch loss 5.70319843 epoch total loss 6.43828297\n",
      "Trained batch 1176 batch loss 6.09022617 epoch total loss 6.43798685\n",
      "Trained batch 1177 batch loss 6.337008 epoch total loss 6.43790102\n",
      "Trained batch 1178 batch loss 6.02509 epoch total loss 6.43755054\n",
      "Trained batch 1179 batch loss 5.31424856 epoch total loss 6.43659782\n",
      "Trained batch 1180 batch loss 5.40414858 epoch total loss 6.4357233\n",
      "Trained batch 1181 batch loss 5.99100161 epoch total loss 6.4353466\n",
      "Trained batch 1182 batch loss 5.78055286 epoch total loss 6.434793\n",
      "Trained batch 1183 batch loss 5.55495787 epoch total loss 6.43404961\n",
      "Trained batch 1184 batch loss 6.05470657 epoch total loss 6.43372917\n",
      "Trained batch 1185 batch loss 5.96072197 epoch total loss 6.43333\n",
      "Trained batch 1186 batch loss 5.90627146 epoch total loss 6.43288565\n",
      "Trained batch 1187 batch loss 6.09370518 epoch total loss 6.4326\n",
      "Trained batch 1188 batch loss 5.82956219 epoch total loss 6.43209219\n",
      "Trained batch 1189 batch loss 6.04687929 epoch total loss 6.43176842\n",
      "Trained batch 1190 batch loss 6.40457058 epoch total loss 6.43174553\n",
      "Trained batch 1191 batch loss 6.52510309 epoch total loss 6.43182373\n",
      "Trained batch 1192 batch loss 6.75178671 epoch total loss 6.43209267\n",
      "Trained batch 1193 batch loss 6.68423557 epoch total loss 6.43230391\n",
      "Trained batch 1194 batch loss 6.65515184 epoch total loss 6.43249035\n",
      "Trained batch 1195 batch loss 6.52963924 epoch total loss 6.43257189\n",
      "Trained batch 1196 batch loss 6.10873 epoch total loss 6.43230104\n",
      "Trained batch 1197 batch loss 6.2921195 epoch total loss 6.43218374\n",
      "Trained batch 1198 batch loss 6.57763052 epoch total loss 6.43230534\n",
      "Trained batch 1199 batch loss 6.27242851 epoch total loss 6.4321723\n",
      "Trained batch 1200 batch loss 6.23090029 epoch total loss 6.43200445\n",
      "Trained batch 1201 batch loss 6.32456636 epoch total loss 6.43191528\n",
      "Trained batch 1202 batch loss 6.28190708 epoch total loss 6.43179035\n",
      "Trained batch 1203 batch loss 6.87451124 epoch total loss 6.43215799\n",
      "Trained batch 1204 batch loss 6.78583479 epoch total loss 6.43245173\n",
      "Trained batch 1205 batch loss 6.59253931 epoch total loss 6.43258476\n",
      "Trained batch 1206 batch loss 6.66000652 epoch total loss 6.43277359\n",
      "Trained batch 1207 batch loss 6.75420046 epoch total loss 6.43304\n",
      "Trained batch 1208 batch loss 6.71978283 epoch total loss 6.43327713\n",
      "Trained batch 1209 batch loss 6.51568556 epoch total loss 6.43334532\n",
      "Trained batch 1210 batch loss 6.52311754 epoch total loss 6.43341923\n",
      "Trained batch 1211 batch loss 6.6162014 epoch total loss 6.43357038\n",
      "Trained batch 1212 batch loss 6.93656683 epoch total loss 6.43398523\n",
      "Trained batch 1213 batch loss 6.619 epoch total loss 6.43413782\n",
      "Trained batch 1214 batch loss 6.60278368 epoch total loss 6.43427706\n",
      "Trained batch 1215 batch loss 6.57640696 epoch total loss 6.43439388\n",
      "Trained batch 1216 batch loss 6.32399797 epoch total loss 6.43430328\n",
      "Trained batch 1217 batch loss 6.29563284 epoch total loss 6.43418932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1218 batch loss 6.26935434 epoch total loss 6.4340539\n",
      "Trained batch 1219 batch loss 6.16562414 epoch total loss 6.4338336\n",
      "Trained batch 1220 batch loss 5.94750357 epoch total loss 6.43343496\n",
      "Trained batch 1221 batch loss 6.19745064 epoch total loss 6.43324137\n",
      "Trained batch 1222 batch loss 6.1795 epoch total loss 6.43303394\n",
      "Trained batch 1223 batch loss 6.20523548 epoch total loss 6.4328475\n",
      "Trained batch 1224 batch loss 5.99611568 epoch total loss 6.43249083\n",
      "Trained batch 1225 batch loss 5.79685354 epoch total loss 6.43197203\n",
      "Trained batch 1226 batch loss 5.78448868 epoch total loss 6.43144369\n",
      "Trained batch 1227 batch loss 5.48105621 epoch total loss 6.43066931\n",
      "Trained batch 1228 batch loss 6.02321 epoch total loss 6.43033743\n",
      "Trained batch 1229 batch loss 6.38578367 epoch total loss 6.43030119\n",
      "Trained batch 1230 batch loss 6.15650225 epoch total loss 6.43007898\n",
      "Trained batch 1231 batch loss 6.31691694 epoch total loss 6.42998695\n",
      "Trained batch 1232 batch loss 6.52813053 epoch total loss 6.43006659\n",
      "Trained batch 1233 batch loss 6.69250679 epoch total loss 6.43027973\n",
      "Trained batch 1234 batch loss 6.76594114 epoch total loss 6.43055153\n",
      "Trained batch 1235 batch loss 6.54379225 epoch total loss 6.43064356\n",
      "Trained batch 1236 batch loss 6.61519146 epoch total loss 6.43079281\n",
      "Trained batch 1237 batch loss 6.1100564 epoch total loss 6.43053341\n",
      "Trained batch 1238 batch loss 6.10458565 epoch total loss 6.43027\n",
      "Trained batch 1239 batch loss 6.12952042 epoch total loss 6.43002701\n",
      "Trained batch 1240 batch loss 6.49939251 epoch total loss 6.43008327\n",
      "Trained batch 1241 batch loss 5.9039917 epoch total loss 6.42965937\n",
      "Trained batch 1242 batch loss 6.03203487 epoch total loss 6.42933941\n",
      "Trained batch 1243 batch loss 6.11245203 epoch total loss 6.4290843\n",
      "Trained batch 1244 batch loss 6.35336208 epoch total loss 6.42902327\n",
      "Trained batch 1245 batch loss 6.20232439 epoch total loss 6.42884111\n",
      "Trained batch 1246 batch loss 6.50379133 epoch total loss 6.4289012\n",
      "Trained batch 1247 batch loss 6.85613203 epoch total loss 6.42924404\n",
      "Trained batch 1248 batch loss 6.36872911 epoch total loss 6.4291954\n",
      "Trained batch 1249 batch loss 6.0201416 epoch total loss 6.42886782\n",
      "Trained batch 1250 batch loss 6.45791817 epoch total loss 6.42889118\n",
      "Trained batch 1251 batch loss 6.73437929 epoch total loss 6.42913532\n",
      "Trained batch 1252 batch loss 6.51548433 epoch total loss 6.42920446\n",
      "Trained batch 1253 batch loss 6.52018929 epoch total loss 6.42927694\n",
      "Trained batch 1254 batch loss 6.48315573 epoch total loss 6.42932\n",
      "Trained batch 1255 batch loss 6.12713575 epoch total loss 6.42907906\n",
      "Trained batch 1256 batch loss 6.26991463 epoch total loss 6.42895222\n",
      "Trained batch 1257 batch loss 5.96007395 epoch total loss 6.42857933\n",
      "Trained batch 1258 batch loss 5.77976894 epoch total loss 6.42806339\n",
      "Trained batch 1259 batch loss 6.30909443 epoch total loss 6.42796898\n",
      "Trained batch 1260 batch loss 6.23357534 epoch total loss 6.42781448\n",
      "Trained batch 1261 batch loss 6.28415966 epoch total loss 6.42770052\n",
      "Trained batch 1262 batch loss 6.43513727 epoch total loss 6.42770672\n",
      "Trained batch 1263 batch loss 6.05204868 epoch total loss 6.42740917\n",
      "Trained batch 1264 batch loss 6.24440765 epoch total loss 6.42726469\n",
      "Trained batch 1265 batch loss 6.45719957 epoch total loss 6.42728806\n",
      "Trained batch 1266 batch loss 6.29923 epoch total loss 6.42718697\n",
      "Trained batch 1267 batch loss 6.44965792 epoch total loss 6.42720509\n",
      "Trained batch 1268 batch loss 6.24266338 epoch total loss 6.42705917\n",
      "Trained batch 1269 batch loss 6.25272799 epoch total loss 6.42692232\n",
      "Trained batch 1270 batch loss 6.27377653 epoch total loss 6.42680168\n",
      "Trained batch 1271 batch loss 6.17087555 epoch total loss 6.42660046\n",
      "Trained batch 1272 batch loss 6.13980532 epoch total loss 6.42637491\n",
      "Trained batch 1273 batch loss 6.43010712 epoch total loss 6.42637777\n",
      "Trained batch 1274 batch loss 6.39907551 epoch total loss 6.42635632\n",
      "Trained batch 1275 batch loss 6.33755684 epoch total loss 6.4262867\n",
      "Trained batch 1276 batch loss 6.00540829 epoch total loss 6.4259572\n",
      "Trained batch 1277 batch loss 6.06030226 epoch total loss 6.4256711\n",
      "Trained batch 1278 batch loss 6.51591969 epoch total loss 6.42574167\n",
      "Trained batch 1279 batch loss 6.18216419 epoch total loss 6.42555141\n",
      "Trained batch 1280 batch loss 6.34990931 epoch total loss 6.42549229\n",
      "Trained batch 1281 batch loss 5.8377676 epoch total loss 6.42503357\n",
      "Trained batch 1282 batch loss 6.40854931 epoch total loss 6.42502\n",
      "Trained batch 1283 batch loss 6.27725 epoch total loss 6.4249053\n",
      "Trained batch 1284 batch loss 6.84500265 epoch total loss 6.42523193\n",
      "Trained batch 1285 batch loss 6.75187492 epoch total loss 6.42548656\n",
      "Trained batch 1286 batch loss 6.52178049 epoch total loss 6.42556095\n",
      "Trained batch 1287 batch loss 6.56826878 epoch total loss 6.42567205\n",
      "Trained batch 1288 batch loss 6.71363926 epoch total loss 6.42589569\n",
      "Trained batch 1289 batch loss 6.23383284 epoch total loss 6.42574644\n",
      "Trained batch 1290 batch loss 6.32122898 epoch total loss 6.42566538\n",
      "Trained batch 1291 batch loss 6.03000402 epoch total loss 6.42535925\n",
      "Trained batch 1292 batch loss 5.81482506 epoch total loss 6.42488623\n",
      "Trained batch 1293 batch loss 5.71721458 epoch total loss 6.42433882\n",
      "Trained batch 1294 batch loss 6.12078619 epoch total loss 6.42410421\n",
      "Trained batch 1295 batch loss 6.48999929 epoch total loss 6.42415524\n",
      "Trained batch 1296 batch loss 6.07226 epoch total loss 6.42388391\n",
      "Trained batch 1297 batch loss 6.53270245 epoch total loss 6.42396736\n",
      "Trained batch 1298 batch loss 6.55002 epoch total loss 6.42406416\n",
      "Trained batch 1299 batch loss 5.97787237 epoch total loss 6.42372084\n",
      "Trained batch 1300 batch loss 6.18850851 epoch total loss 6.42353964\n",
      "Trained batch 1301 batch loss 6.37741756 epoch total loss 6.42350388\n",
      "Trained batch 1302 batch loss 6.26276541 epoch total loss 6.42338037\n",
      "Trained batch 1303 batch loss 6.09489679 epoch total loss 6.42312813\n",
      "Trained batch 1304 batch loss 6.36646318 epoch total loss 6.42308426\n",
      "Trained batch 1305 batch loss 6.15125895 epoch total loss 6.42287636\n",
      "Trained batch 1306 batch loss 6.13389778 epoch total loss 6.42265511\n",
      "Trained batch 1307 batch loss 6.22473764 epoch total loss 6.42250347\n",
      "Trained batch 1308 batch loss 6.03696251 epoch total loss 6.42220879\n",
      "Trained batch 1309 batch loss 6.18923616 epoch total loss 6.42203093\n",
      "Trained batch 1310 batch loss 6.04612732 epoch total loss 6.42174387\n",
      "Trained batch 1311 batch loss 6.08386421 epoch total loss 6.42148638\n",
      "Trained batch 1312 batch loss 5.90382528 epoch total loss 6.42109203\n",
      "Trained batch 1313 batch loss 6.29825974 epoch total loss 6.4209981\n",
      "Trained batch 1314 batch loss 6.37383842 epoch total loss 6.42096233\n",
      "Trained batch 1315 batch loss 6.60807848 epoch total loss 6.42110491\n",
      "Trained batch 1316 batch loss 6.57381 epoch total loss 6.42122126\n",
      "Trained batch 1317 batch loss 6.11746216 epoch total loss 6.42099047\n",
      "Trained batch 1318 batch loss 6.14916563 epoch total loss 6.42078447\n",
      "Trained batch 1319 batch loss 6.37780809 epoch total loss 6.42075205\n",
      "Trained batch 1320 batch loss 5.9548192 epoch total loss 6.42039919\n",
      "Trained batch 1321 batch loss 6.16270924 epoch total loss 6.42020416\n",
      "Trained batch 1322 batch loss 6.44953394 epoch total loss 6.4202261\n",
      "Trained batch 1323 batch loss 6.66097975 epoch total loss 6.42040825\n",
      "Trained batch 1324 batch loss 6.1113472 epoch total loss 6.42017508\n",
      "Trained batch 1325 batch loss 6.59184933 epoch total loss 6.4203043\n",
      "Trained batch 1326 batch loss 6.40279627 epoch total loss 6.42029095\n",
      "Trained batch 1327 batch loss 6.16648436 epoch total loss 6.42009926\n",
      "Trained batch 1328 batch loss 5.65117121 epoch total loss 6.41952038\n",
      "Trained batch 1329 batch loss 6.55227089 epoch total loss 6.41962051\n",
      "Trained batch 1330 batch loss 6.31973791 epoch total loss 6.41954517\n",
      "Trained batch 1331 batch loss 6.60776472 epoch total loss 6.41968632\n",
      "Trained batch 1332 batch loss 6.39869308 epoch total loss 6.41967058\n",
      "Trained batch 1333 batch loss 6.37537098 epoch total loss 6.41963673\n",
      "Trained batch 1334 batch loss 6.12453461 epoch total loss 6.41941595\n",
      "Trained batch 1335 batch loss 6.34880066 epoch total loss 6.41936302\n",
      "Trained batch 1336 batch loss 6.12462282 epoch total loss 6.41914272\n",
      "Trained batch 1337 batch loss 6.32934284 epoch total loss 6.41907549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1338 batch loss 6.25042629 epoch total loss 6.41894913\n",
      "Trained batch 1339 batch loss 6.58372116 epoch total loss 6.41907215\n",
      "Trained batch 1340 batch loss 6.79872131 epoch total loss 6.41935539\n",
      "Trained batch 1341 batch loss 6.91333151 epoch total loss 6.41972399\n",
      "Trained batch 1342 batch loss 7.42100286 epoch total loss 6.42046976\n",
      "Trained batch 1343 batch loss 6.57964516 epoch total loss 6.42058849\n",
      "Trained batch 1344 batch loss 6.49106693 epoch total loss 6.42064142\n",
      "Trained batch 1345 batch loss 6.44833279 epoch total loss 6.42066193\n",
      "Trained batch 1346 batch loss 6.313344 epoch total loss 6.42058229\n",
      "Trained batch 1347 batch loss 6.37582111 epoch total loss 6.42054892\n",
      "Trained batch 1348 batch loss 6.20883751 epoch total loss 6.42039204\n",
      "Trained batch 1349 batch loss 6.19728184 epoch total loss 6.42022657\n",
      "Trained batch 1350 batch loss 6.42793846 epoch total loss 6.4202323\n",
      "Trained batch 1351 batch loss 6.45408821 epoch total loss 6.42025709\n",
      "Trained batch 1352 batch loss 6.0353446 epoch total loss 6.41997242\n",
      "Trained batch 1353 batch loss 6.44973135 epoch total loss 6.41999483\n",
      "Trained batch 1354 batch loss 6.44748306 epoch total loss 6.42001486\n",
      "Trained batch 1355 batch loss 6.02593613 epoch total loss 6.41972446\n",
      "Trained batch 1356 batch loss 6.72408724 epoch total loss 6.41994858\n",
      "Trained batch 1357 batch loss 6.60076618 epoch total loss 6.42008162\n",
      "Trained batch 1358 batch loss 6.89722872 epoch total loss 6.42043304\n",
      "Trained batch 1359 batch loss 6.81332302 epoch total loss 6.42072248\n",
      "Trained batch 1360 batch loss 6.92092752 epoch total loss 6.42109\n",
      "Trained batch 1361 batch loss 6.00942898 epoch total loss 6.42078781\n",
      "Trained batch 1362 batch loss 5.62588072 epoch total loss 6.42020416\n",
      "Trained batch 1363 batch loss 6.07671452 epoch total loss 6.41995287\n",
      "Trained batch 1364 batch loss 6.373 epoch total loss 6.41991806\n",
      "Trained batch 1365 batch loss 6.68702 epoch total loss 6.42011452\n",
      "Trained batch 1366 batch loss 6.44162703 epoch total loss 6.42013\n",
      "Trained batch 1367 batch loss 6.59872484 epoch total loss 6.42026043\n",
      "Trained batch 1368 batch loss 6.42074537 epoch total loss 6.42026091\n",
      "Trained batch 1369 batch loss 6.45903492 epoch total loss 6.42028904\n",
      "Trained batch 1370 batch loss 6.26064396 epoch total loss 6.42017269\n",
      "Trained batch 1371 batch loss 6.18344069 epoch total loss 6.42\n",
      "Trained batch 1372 batch loss 6.31127357 epoch total loss 6.4199214\n",
      "Trained batch 1373 batch loss 6.19023 epoch total loss 6.41975403\n",
      "Trained batch 1374 batch loss 6.31759357 epoch total loss 6.41967964\n",
      "Trained batch 1375 batch loss 6.78745461 epoch total loss 6.41994667\n",
      "Trained batch 1376 batch loss 6.4804 epoch total loss 6.41999054\n",
      "Trained batch 1377 batch loss 6.80061483 epoch total loss 6.42026711\n",
      "Trained batch 1378 batch loss 6.63642883 epoch total loss 6.42042446\n",
      "Trained batch 1379 batch loss 6.52057648 epoch total loss 6.42049694\n",
      "Trained batch 1380 batch loss 6.0152297 epoch total loss 6.42020369\n",
      "Trained batch 1381 batch loss 6.05995083 epoch total loss 6.41994238\n",
      "Trained batch 1382 batch loss 6.14463568 epoch total loss 6.41974306\n",
      "Trained batch 1383 batch loss 6.18076229 epoch total loss 6.41957045\n",
      "Trained batch 1384 batch loss 6.14883327 epoch total loss 6.41937447\n",
      "Trained batch 1385 batch loss 6.23886585 epoch total loss 6.41924429\n",
      "Trained batch 1386 batch loss 6.31653929 epoch total loss 6.41917\n",
      "Trained batch 1387 batch loss 6.3208456 epoch total loss 6.41909933\n",
      "Trained batch 1388 batch loss 6.44310284 epoch total loss 6.41911697\n",
      "Epoch 1 train loss 6.419116973876953\n",
      "Validated batch 1 batch loss 6.58436489\n",
      "Validated batch 2 batch loss 6.68122053\n",
      "Validated batch 3 batch loss 6.2313652\n",
      "Validated batch 4 batch loss 5.80147839\n",
      "Validated batch 5 batch loss 6.26973057\n",
      "Validated batch 6 batch loss 6.35874653\n",
      "Validated batch 7 batch loss 5.99719191\n",
      "Validated batch 8 batch loss 6.06271648\n",
      "Validated batch 9 batch loss 6.32352877\n",
      "Validated batch 10 batch loss 6.35237265\n",
      "Validated batch 11 batch loss 6.14039278\n",
      "Validated batch 12 batch loss 5.91566181\n",
      "Validated batch 13 batch loss 6.26461554\n",
      "Validated batch 14 batch loss 6.26285076\n",
      "Validated batch 15 batch loss 6.30673933\n",
      "Validated batch 16 batch loss 6.26296425\n",
      "Validated batch 17 batch loss 6.35200071\n",
      "Validated batch 18 batch loss 6.51910353\n",
      "Validated batch 19 batch loss 6.32792473\n",
      "Validated batch 20 batch loss 6.16319132\n",
      "Validated batch 21 batch loss 6.60406446\n",
      "Validated batch 22 batch loss 5.4829464\n",
      "Validated batch 23 batch loss 6.81136847\n",
      "Validated batch 24 batch loss 6.09300327\n",
      "Validated batch 25 batch loss 6.6364789\n",
      "Validated batch 26 batch loss 6.424088\n",
      "Validated batch 27 batch loss 6.52481556\n",
      "Validated batch 28 batch loss 6.48554373\n",
      "Validated batch 29 batch loss 6.73522234\n",
      "Validated batch 30 batch loss 6.30607557\n",
      "Validated batch 31 batch loss 6.66448164\n",
      "Validated batch 32 batch loss 6.15090847\n",
      "Validated batch 33 batch loss 6.47832346\n",
      "Validated batch 34 batch loss 6.40509033\n",
      "Validated batch 35 batch loss 5.81647\n",
      "Validated batch 36 batch loss 5.8388319\n",
      "Validated batch 37 batch loss 6.59517097\n",
      "Validated batch 38 batch loss 6.50496292\n",
      "Validated batch 39 batch loss 6.116539\n",
      "Validated batch 40 batch loss 6.52502441\n",
      "Validated batch 41 batch loss 6.54204798\n",
      "Validated batch 42 batch loss 6.53861\n",
      "Validated batch 43 batch loss 6.61847591\n",
      "Validated batch 44 batch loss 6.58323097\n",
      "Validated batch 45 batch loss 6.25433588\n",
      "Validated batch 46 batch loss 6.03830385\n",
      "Validated batch 47 batch loss 6.27535105\n",
      "Validated batch 48 batch loss 6.39645815\n",
      "Validated batch 49 batch loss 6.08014345\n",
      "Validated batch 50 batch loss 6.16366482\n",
      "Validated batch 51 batch loss 6.51407957\n",
      "Validated batch 52 batch loss 6.34596443\n",
      "Validated batch 53 batch loss 6.41817713\n",
      "Validated batch 54 batch loss 6.27282\n",
      "Validated batch 55 batch loss 6.25105429\n",
      "Validated batch 56 batch loss 6.6896019\n",
      "Validated batch 57 batch loss 6.7091074\n",
      "Validated batch 58 batch loss 6.47971344\n",
      "Validated batch 59 batch loss 6.44809484\n",
      "Validated batch 60 batch loss 6.29419899\n",
      "Validated batch 61 batch loss 6.4703207\n",
      "Validated batch 62 batch loss 6.3212328\n",
      "Validated batch 63 batch loss 6.54998255\n",
      "Validated batch 64 batch loss 6.35080957\n",
      "Validated batch 65 batch loss 6.12734795\n",
      "Validated batch 66 batch loss 6.49450302\n",
      "Validated batch 67 batch loss 6.13631535\n",
      "Validated batch 68 batch loss 6.62542915\n",
      "Validated batch 69 batch loss 6.62565327\n",
      "Validated batch 70 batch loss 6.29272556\n",
      "Validated batch 71 batch loss 6.45144701\n",
      "Validated batch 72 batch loss 6.08573151\n",
      "Validated batch 73 batch loss 5.76403332\n",
      "Validated batch 74 batch loss 6.11924362\n",
      "Validated batch 75 batch loss 6.67254877\n",
      "Validated batch 76 batch loss 6.15438509\n",
      "Validated batch 77 batch loss 6.37156391\n",
      "Validated batch 78 batch loss 6.48238707\n",
      "Validated batch 79 batch loss 6.59654331\n",
      "Validated batch 80 batch loss 6.50701427\n",
      "Validated batch 81 batch loss 6.32140827\n",
      "Validated batch 82 batch loss 6.38792372\n",
      "Validated batch 83 batch loss 6.34698677\n",
      "Validated batch 84 batch loss 6.59437323\n",
      "Validated batch 85 batch loss 6.70582914\n",
      "Validated batch 86 batch loss 6.22632074\n",
      "Validated batch 87 batch loss 6.71171379\n",
      "Validated batch 88 batch loss 6.4930687\n",
      "Validated batch 89 batch loss 6.11697817\n",
      "Validated batch 90 batch loss 6.31700516\n",
      "Validated batch 91 batch loss 6.47314405\n",
      "Validated batch 92 batch loss 6.71785355\n",
      "Validated batch 93 batch loss 6.35612202\n",
      "Validated batch 94 batch loss 6.4203043\n",
      "Validated batch 95 batch loss 6.20089912\n",
      "Validated batch 96 batch loss 6.79404116\n",
      "Validated batch 97 batch loss 6.72468662\n",
      "Validated batch 98 batch loss 6.51644659\n",
      "Validated batch 99 batch loss 6.50115\n",
      "Validated batch 100 batch loss 6.47782421\n",
      "Validated batch 101 batch loss 6.60306215\n",
      "Validated batch 102 batch loss 6.47689104\n",
      "Validated batch 103 batch loss 6.85965\n",
      "Validated batch 104 batch loss 6.79552507\n",
      "Validated batch 105 batch loss 6.0497818\n",
      "Validated batch 106 batch loss 6.60380125\n",
      "Validated batch 107 batch loss 6.27525\n",
      "Validated batch 108 batch loss 6.52835\n",
      "Validated batch 109 batch loss 6.61305714\n",
      "Validated batch 110 batch loss 5.73310709\n",
      "Validated batch 111 batch loss 6.21792889\n",
      "Validated batch 112 batch loss 6.50722885\n",
      "Validated batch 113 batch loss 6.25145435\n",
      "Validated batch 114 batch loss 6.89248\n",
      "Validated batch 115 batch loss 6.24494839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 116 batch loss 6.59942341\n",
      "Validated batch 117 batch loss 6.4599905\n",
      "Validated batch 118 batch loss 6.22137451\n",
      "Validated batch 119 batch loss 6.09299469\n",
      "Validated batch 120 batch loss 6.21237803\n",
      "Validated batch 121 batch loss 6.41581392\n",
      "Validated batch 122 batch loss 6.10210848\n",
      "Validated batch 123 batch loss 6.38992405\n",
      "Validated batch 124 batch loss 6.46985579\n",
      "Validated batch 125 batch loss 6.46022654\n",
      "Validated batch 126 batch loss 6.22030687\n",
      "Validated batch 127 batch loss 6.05318499\n",
      "Validated batch 128 batch loss 6.24820518\n",
      "Validated batch 129 batch loss 6.71689558\n",
      "Validated batch 130 batch loss 6.27535677\n",
      "Validated batch 131 batch loss 6.31961584\n",
      "Validated batch 132 batch loss 6.2674427\n",
      "Validated batch 133 batch loss 5.9588685\n",
      "Validated batch 134 batch loss 5.86595154\n",
      "Validated batch 135 batch loss 6.40772629\n",
      "Validated batch 136 batch loss 6.12510395\n",
      "Validated batch 137 batch loss 6.11883926\n",
      "Validated batch 138 batch loss 6.352355\n",
      "Validated batch 139 batch loss 6.30360937\n",
      "Validated batch 140 batch loss 6.16663074\n",
      "Validated batch 141 batch loss 6.34917212\n",
      "Validated batch 142 batch loss 6.3437233\n",
      "Validated batch 143 batch loss 6.00918484\n",
      "Validated batch 144 batch loss 6.68195915\n",
      "Validated batch 145 batch loss 6.45911741\n",
      "Validated batch 146 batch loss 6.32248259\n",
      "Validated batch 147 batch loss 6.50377893\n",
      "Validated batch 148 batch loss 6.58631372\n",
      "Validated batch 149 batch loss 6.33713341\n",
      "Validated batch 150 batch loss 6.30848789\n",
      "Validated batch 151 batch loss 6.29749107\n",
      "Validated batch 152 batch loss 6.51257658\n",
      "Validated batch 153 batch loss 6.65443\n",
      "Validated batch 154 batch loss 6.43605757\n",
      "Validated batch 155 batch loss 6.1909523\n",
      "Validated batch 156 batch loss 6.00057793\n",
      "Validated batch 157 batch loss 6.41812277\n",
      "Validated batch 158 batch loss 6.42872429\n",
      "Validated batch 159 batch loss 6.43004608\n",
      "Validated batch 160 batch loss 6.46121359\n",
      "Validated batch 161 batch loss 6.30158\n",
      "Validated batch 162 batch loss 6.7712307\n",
      "Validated batch 163 batch loss 6.46480227\n",
      "Validated batch 164 batch loss 6.54697657\n",
      "Validated batch 165 batch loss 6.3906436\n",
      "Validated batch 166 batch loss 6.38835812\n",
      "Validated batch 167 batch loss 6.71306944\n",
      "Validated batch 168 batch loss 6.43550348\n",
      "Validated batch 169 batch loss 6.10268593\n",
      "Validated batch 170 batch loss 6.08824253\n",
      "Validated batch 171 batch loss 6.38367462\n",
      "Validated batch 172 batch loss 6.39487219\n",
      "Validated batch 173 batch loss 6.36167479\n",
      "Validated batch 174 batch loss 6.14808321\n",
      "Validated batch 175 batch loss 6.08825874\n",
      "Validated batch 176 batch loss 6.29646921\n",
      "Validated batch 177 batch loss 6.2678318\n",
      "Validated batch 178 batch loss 6.56546068\n",
      "Validated batch 179 batch loss 6.5359745\n",
      "Validated batch 180 batch loss 6.79387712\n",
      "Validated batch 181 batch loss 7.23794222\n",
      "Validated batch 182 batch loss 7.0449338\n",
      "Validated batch 183 batch loss 6.37414265\n",
      "Validated batch 184 batch loss 5.94416809\n",
      "Validated batch 185 batch loss 3.13851213\n",
      "Epoch 1 val loss 6.356117248535156\n",
      "Model .//simplebaseline_model-epoch-1-loss-6.3561.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 6.42608261 epoch total loss 6.42608261\n",
      "Trained batch 2 batch loss 6.5417757 epoch total loss 6.48392916\n",
      "Trained batch 3 batch loss 6.47059631 epoch total loss 6.47948456\n",
      "Trained batch 4 batch loss 6.52642059 epoch total loss 6.49121857\n",
      "Trained batch 5 batch loss 6.14161539 epoch total loss 6.42129803\n",
      "Trained batch 6 batch loss 6.24918079 epoch total loss 6.39261198\n",
      "Trained batch 7 batch loss 6.27574444 epoch total loss 6.37591648\n",
      "Trained batch 8 batch loss 6.26669693 epoch total loss 6.36226416\n",
      "Trained batch 9 batch loss 6.16163778 epoch total loss 6.33997202\n",
      "Trained batch 10 batch loss 6.63456726 epoch total loss 6.3694315\n",
      "Trained batch 11 batch loss 6.93907261 epoch total loss 6.42121744\n",
      "Trained batch 12 batch loss 6.68875885 epoch total loss 6.44351244\n",
      "Trained batch 13 batch loss 6.64183855 epoch total loss 6.45876837\n",
      "Trained batch 14 batch loss 6.72263622 epoch total loss 6.47761583\n",
      "Trained batch 15 batch loss 6.71019506 epoch total loss 6.49312115\n",
      "Trained batch 16 batch loss 6.46787739 epoch total loss 6.49154377\n",
      "Trained batch 17 batch loss 6.39098406 epoch total loss 6.4856286\n",
      "Trained batch 18 batch loss 6.96279812 epoch total loss 6.51213789\n",
      "Trained batch 19 batch loss 6.84552717 epoch total loss 6.52968454\n",
      "Trained batch 20 batch loss 6.75167561 epoch total loss 6.54078436\n",
      "Trained batch 21 batch loss 6.43186808 epoch total loss 6.5355978\n",
      "Trained batch 22 batch loss 6.52999067 epoch total loss 6.53534269\n",
      "Trained batch 23 batch loss 6.25438261 epoch total loss 6.52312708\n",
      "Trained batch 24 batch loss 6.22515249 epoch total loss 6.51071167\n",
      "Trained batch 25 batch loss 6.50683689 epoch total loss 6.5105567\n",
      "Trained batch 26 batch loss 6.32613516 epoch total loss 6.50346375\n",
      "Trained batch 27 batch loss 6.39353704 epoch total loss 6.49939251\n",
      "Trained batch 28 batch loss 6.49888849 epoch total loss 6.49937439\n",
      "Trained batch 29 batch loss 6.30827618 epoch total loss 6.4927845\n",
      "Trained batch 30 batch loss 6.32045269 epoch total loss 6.48704\n",
      "Trained batch 31 batch loss 6.00224113 epoch total loss 6.47140169\n",
      "Trained batch 32 batch loss 6.3528204 epoch total loss 6.46769571\n",
      "Trained batch 33 batch loss 6.46141195 epoch total loss 6.46750546\n",
      "Trained batch 34 batch loss 6.45735 epoch total loss 6.46720648\n",
      "Trained batch 35 batch loss 6.60247755 epoch total loss 6.47107172\n",
      "Trained batch 36 batch loss 6.84615803 epoch total loss 6.48149061\n",
      "Trained batch 37 batch loss 5.94463873 epoch total loss 6.46698141\n",
      "Trained batch 38 batch loss 6.43960333 epoch total loss 6.46626091\n",
      "Trained batch 39 batch loss 6.46755409 epoch total loss 6.46629429\n",
      "Trained batch 40 batch loss 6.37787151 epoch total loss 6.46408367\n",
      "Trained batch 41 batch loss 6.08790827 epoch total loss 6.45490932\n",
      "Trained batch 42 batch loss 6.1586194 epoch total loss 6.447855\n",
      "Trained batch 43 batch loss 6.31186676 epoch total loss 6.44469213\n",
      "Trained batch 44 batch loss 6.74472427 epoch total loss 6.45151091\n",
      "Trained batch 45 batch loss 6.78025436 epoch total loss 6.45881605\n",
      "Trained batch 46 batch loss 6.7965045 epoch total loss 6.46615744\n",
      "Trained batch 47 batch loss 6.82805824 epoch total loss 6.4738574\n",
      "Trained batch 48 batch loss 6.77939749 epoch total loss 6.4802227\n",
      "Trained batch 49 batch loss 6.61191416 epoch total loss 6.48291\n",
      "Trained batch 50 batch loss 6.441535 epoch total loss 6.48208237\n",
      "Trained batch 51 batch loss 6.46280289 epoch total loss 6.48170424\n",
      "Trained batch 52 batch loss 6.11391544 epoch total loss 6.47463179\n",
      "Trained batch 53 batch loss 6.21614695 epoch total loss 6.4697547\n",
      "Trained batch 54 batch loss 6.33498859 epoch total loss 6.46725893\n",
      "Trained batch 55 batch loss 6.21208572 epoch total loss 6.46262\n",
      "Trained batch 56 batch loss 6.38014555 epoch total loss 6.46114731\n",
      "Trained batch 57 batch loss 6.0789876 epoch total loss 6.4544425\n",
      "Trained batch 58 batch loss 6.24306393 epoch total loss 6.45079851\n",
      "Trained batch 59 batch loss 6.14891863 epoch total loss 6.44568205\n",
      "Trained batch 60 batch loss 6.09560299 epoch total loss 6.43984747\n",
      "Trained batch 61 batch loss 6.35372829 epoch total loss 6.43843555\n",
      "Trained batch 62 batch loss 6.15441084 epoch total loss 6.43385458\n",
      "Trained batch 63 batch loss 5.88032913 epoch total loss 6.42506886\n",
      "Trained batch 64 batch loss 6.20478868 epoch total loss 6.42162704\n",
      "Trained batch 65 batch loss 5.53565693 epoch total loss 6.40799665\n",
      "Trained batch 66 batch loss 5.13188839 epoch total loss 6.38866186\n",
      "Trained batch 67 batch loss 5.43190527 epoch total loss 6.37438202\n",
      "Trained batch 68 batch loss 6.20472431 epoch total loss 6.37188673\n",
      "Trained batch 69 batch loss 6.04222 epoch total loss 6.36710882\n",
      "Trained batch 70 batch loss 6.42690182 epoch total loss 6.36796331\n",
      "Trained batch 71 batch loss 6.32384777 epoch total loss 6.367342\n",
      "Trained batch 72 batch loss 6.43736172 epoch total loss 6.36831427\n",
      "Trained batch 73 batch loss 6.38543129 epoch total loss 6.36854887\n",
      "Trained batch 74 batch loss 6.35440207 epoch total loss 6.36835766\n",
      "Trained batch 75 batch loss 6.47985172 epoch total loss 6.36984396\n",
      "Trained batch 76 batch loss 6.40065098 epoch total loss 6.37024975\n",
      "Trained batch 77 batch loss 5.87064 epoch total loss 6.36376143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 78 batch loss 6.11301565 epoch total loss 6.36054659\n",
      "Trained batch 79 batch loss 6.36627245 epoch total loss 6.36061907\n",
      "Trained batch 80 batch loss 6.34886789 epoch total loss 6.3604722\n",
      "Trained batch 81 batch loss 6.30618095 epoch total loss 6.35980129\n",
      "Trained batch 82 batch loss 6.96661615 epoch total loss 6.36720181\n",
      "Trained batch 83 batch loss 6.72139597 epoch total loss 6.37146902\n",
      "Trained batch 84 batch loss 6.42906237 epoch total loss 6.37215471\n",
      "Trained batch 85 batch loss 5.66032 epoch total loss 6.3637805\n",
      "Trained batch 86 batch loss 5.827106 epoch total loss 6.35753965\n",
      "Trained batch 87 batch loss 6.13754177 epoch total loss 6.35501051\n",
      "Trained batch 88 batch loss 6.50124884 epoch total loss 6.35667229\n",
      "Trained batch 89 batch loss 6.65870094 epoch total loss 6.36006546\n",
      "Trained batch 90 batch loss 6.97828627 epoch total loss 6.36693478\n",
      "Trained batch 91 batch loss 7.21166229 epoch total loss 6.37621737\n",
      "Trained batch 92 batch loss 7.05504227 epoch total loss 6.38359594\n",
      "Trained batch 93 batch loss 7.22095299 epoch total loss 6.39259958\n",
      "Trained batch 94 batch loss 6.58034039 epoch total loss 6.39459705\n",
      "Trained batch 95 batch loss 6.69570065 epoch total loss 6.39776611\n",
      "Trained batch 96 batch loss 5.94380903 epoch total loss 6.39303732\n",
      "Trained batch 97 batch loss 6.13290548 epoch total loss 6.39035559\n",
      "Trained batch 98 batch loss 6.64745569 epoch total loss 6.39297915\n",
      "Trained batch 99 batch loss 6.69476366 epoch total loss 6.39602757\n",
      "Trained batch 100 batch loss 6.24515867 epoch total loss 6.39451885\n",
      "Trained batch 101 batch loss 6.49829531 epoch total loss 6.39554644\n",
      "Trained batch 102 batch loss 6.1686945 epoch total loss 6.39332247\n",
      "Trained batch 103 batch loss 6.3649354 epoch total loss 6.39304686\n",
      "Trained batch 104 batch loss 6.06915808 epoch total loss 6.38993263\n",
      "Trained batch 105 batch loss 6.36008358 epoch total loss 6.38964844\n",
      "Trained batch 106 batch loss 6.17322111 epoch total loss 6.38760662\n",
      "Trained batch 107 batch loss 6.15150261 epoch total loss 6.3854\n",
      "Trained batch 108 batch loss 6.55883551 epoch total loss 6.38700581\n",
      "Trained batch 109 batch loss 6.16437817 epoch total loss 6.38496351\n",
      "Trained batch 110 batch loss 6.19636 epoch total loss 6.38324881\n",
      "Trained batch 111 batch loss 6.94241381 epoch total loss 6.38828659\n",
      "Trained batch 112 batch loss 6.34167051 epoch total loss 6.38787031\n",
      "Trained batch 113 batch loss 6.5262928 epoch total loss 6.38909531\n",
      "Trained batch 114 batch loss 6.47345829 epoch total loss 6.38983536\n",
      "Trained batch 115 batch loss 6.46508646 epoch total loss 6.39048958\n",
      "Trained batch 116 batch loss 6.55794 epoch total loss 6.39193296\n",
      "Trained batch 117 batch loss 6.36095524 epoch total loss 6.39166832\n",
      "Trained batch 118 batch loss 6.343925 epoch total loss 6.39126396\n",
      "Trained batch 119 batch loss 6.26972437 epoch total loss 6.39024258\n",
      "Trained batch 120 batch loss 5.57366562 epoch total loss 6.38343763\n",
      "Trained batch 121 batch loss 6.50121212 epoch total loss 6.38441086\n",
      "Trained batch 122 batch loss 6.43584347 epoch total loss 6.38483286\n",
      "Trained batch 123 batch loss 6.67207432 epoch total loss 6.38716793\n",
      "Trained batch 124 batch loss 6.58141518 epoch total loss 6.38873434\n",
      "Trained batch 125 batch loss 6.40752935 epoch total loss 6.38888454\n",
      "Trained batch 126 batch loss 6.35446262 epoch total loss 6.38861179\n",
      "Trained batch 127 batch loss 6.25204849 epoch total loss 6.38753653\n",
      "Trained batch 128 batch loss 6.87266445 epoch total loss 6.3913269\n",
      "Trained batch 129 batch loss 6.47857 epoch total loss 6.39200306\n",
      "Trained batch 130 batch loss 6.23672676 epoch total loss 6.39080906\n",
      "Trained batch 131 batch loss 6.04693556 epoch total loss 6.38818407\n",
      "Trained batch 132 batch loss 6.31752968 epoch total loss 6.38764858\n",
      "Trained batch 133 batch loss 6.29959393 epoch total loss 6.38698673\n",
      "Trained batch 134 batch loss 6.66355133 epoch total loss 6.38905096\n",
      "Trained batch 135 batch loss 6.50249577 epoch total loss 6.38989115\n",
      "Trained batch 136 batch loss 6.71892595 epoch total loss 6.39231062\n",
      "Trained batch 137 batch loss 6.43114424 epoch total loss 6.39259434\n",
      "Trained batch 138 batch loss 6.39177847 epoch total loss 6.39258814\n",
      "Trained batch 139 batch loss 6.30166531 epoch total loss 6.39193392\n",
      "Trained batch 140 batch loss 6.65931273 epoch total loss 6.39384365\n",
      "Trained batch 141 batch loss 6.6573782 epoch total loss 6.39571238\n",
      "Trained batch 142 batch loss 6.32368231 epoch total loss 6.39520502\n",
      "Trained batch 143 batch loss 6.49928522 epoch total loss 6.39593315\n",
      "Trained batch 144 batch loss 6.20890427 epoch total loss 6.39463425\n",
      "Trained batch 145 batch loss 5.96104 epoch total loss 6.391644\n",
      "Trained batch 146 batch loss 5.6948328 epoch total loss 6.38687134\n",
      "Trained batch 147 batch loss 6.45956326 epoch total loss 6.38736582\n",
      "Trained batch 148 batch loss 6.57623625 epoch total loss 6.38864183\n",
      "Trained batch 149 batch loss 6.61557245 epoch total loss 6.39016485\n",
      "Trained batch 150 batch loss 6.19537973 epoch total loss 6.38886642\n",
      "Trained batch 151 batch loss 6.20130348 epoch total loss 6.38762426\n",
      "Trained batch 152 batch loss 6.19546652 epoch total loss 6.38636\n",
      "Trained batch 153 batch loss 6.33856773 epoch total loss 6.38604784\n",
      "Trained batch 154 batch loss 5.90410709 epoch total loss 6.38291836\n",
      "Trained batch 155 batch loss 6.02709913 epoch total loss 6.38062286\n",
      "Trained batch 156 batch loss 5.73239279 epoch total loss 6.3764677\n",
      "Trained batch 157 batch loss 5.93262386 epoch total loss 6.37364054\n",
      "Trained batch 158 batch loss 7.07209349 epoch total loss 6.37806082\n",
      "Trained batch 159 batch loss 6.95027637 epoch total loss 6.38165951\n",
      "Trained batch 160 batch loss 6.95914602 epoch total loss 6.38526917\n",
      "Trained batch 161 batch loss 6.73129368 epoch total loss 6.38741875\n",
      "Trained batch 162 batch loss 6.86915636 epoch total loss 6.3903923\n",
      "Trained batch 163 batch loss 6.91472149 epoch total loss 6.39360857\n",
      "Trained batch 164 batch loss 6.44511032 epoch total loss 6.39392233\n",
      "Trained batch 165 batch loss 6.41182947 epoch total loss 6.39403105\n",
      "Trained batch 166 batch loss 6.63347912 epoch total loss 6.39547348\n",
      "Trained batch 167 batch loss 6.4916029 epoch total loss 6.39604855\n",
      "Trained batch 168 batch loss 6.52768803 epoch total loss 6.39683247\n",
      "Trained batch 169 batch loss 6.53638554 epoch total loss 6.39765835\n",
      "Trained batch 170 batch loss 6.57856607 epoch total loss 6.39872265\n",
      "Trained batch 171 batch loss 6.50078297 epoch total loss 6.39931917\n",
      "Trained batch 172 batch loss 6.52624941 epoch total loss 6.40005684\n",
      "Trained batch 173 batch loss 6.29223919 epoch total loss 6.39943361\n",
      "Trained batch 174 batch loss 5.50544453 epoch total loss 6.39429617\n",
      "Trained batch 175 batch loss 5.49622726 epoch total loss 6.38916445\n",
      "Trained batch 176 batch loss 5.58033943 epoch total loss 6.38456869\n",
      "Trained batch 177 batch loss 6.23608446 epoch total loss 6.38373\n",
      "Trained batch 178 batch loss 6.72880602 epoch total loss 6.38566828\n",
      "Trained batch 179 batch loss 7.37216759 epoch total loss 6.39117956\n",
      "Trained batch 180 batch loss 5.99604464 epoch total loss 6.38898468\n",
      "Trained batch 181 batch loss 6.03052425 epoch total loss 6.3870039\n",
      "Trained batch 182 batch loss 6.40897655 epoch total loss 6.38712454\n",
      "Trained batch 183 batch loss 6.42123079 epoch total loss 6.38731098\n",
      "Trained batch 184 batch loss 6.5606823 epoch total loss 6.38825321\n",
      "Trained batch 185 batch loss 6.48106194 epoch total loss 6.38875484\n",
      "Trained batch 186 batch loss 6.50589657 epoch total loss 6.38938475\n",
      "Trained batch 187 batch loss 6.51199579 epoch total loss 6.39004\n",
      "Trained batch 188 batch loss 6.52362394 epoch total loss 6.39075089\n",
      "Trained batch 189 batch loss 6.52280807 epoch total loss 6.39145\n",
      "Trained batch 190 batch loss 6.52463484 epoch total loss 6.39215088\n",
      "Trained batch 191 batch loss 6.23534584 epoch total loss 6.39133\n",
      "Trained batch 192 batch loss 6.64021826 epoch total loss 6.39262629\n",
      "Trained batch 193 batch loss 6.60577488 epoch total loss 6.39373112\n",
      "Trained batch 194 batch loss 6.47977304 epoch total loss 6.39417458\n",
      "Trained batch 195 batch loss 5.71653843 epoch total loss 6.39069939\n",
      "Trained batch 196 batch loss 6.06650305 epoch total loss 6.38904572\n",
      "Trained batch 197 batch loss 6.36086321 epoch total loss 6.38890219\n",
      "Trained batch 198 batch loss 5.94706774 epoch total loss 6.38667059\n",
      "Trained batch 199 batch loss 5.48606396 epoch total loss 6.38214493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 200 batch loss 5.75640249 epoch total loss 6.37901592\n",
      "Trained batch 201 batch loss 6.11513138 epoch total loss 6.37770319\n",
      "Trained batch 202 batch loss 6.05390549 epoch total loss 6.37610054\n",
      "Trained batch 203 batch loss 6.07348347 epoch total loss 6.37461\n",
      "Trained batch 204 batch loss 6.3416276 epoch total loss 6.3744483\n",
      "Trained batch 205 batch loss 6.17630863 epoch total loss 6.37348175\n",
      "Trained batch 206 batch loss 6.47761297 epoch total loss 6.3739872\n",
      "Trained batch 207 batch loss 6.15948677 epoch total loss 6.37295151\n",
      "Trained batch 208 batch loss 6.4013381 epoch total loss 6.37308788\n",
      "Trained batch 209 batch loss 6.81168509 epoch total loss 6.37518644\n",
      "Trained batch 210 batch loss 5.84373617 epoch total loss 6.37265587\n",
      "Trained batch 211 batch loss 4.76462126 epoch total loss 6.36503458\n",
      "Trained batch 212 batch loss 5.27993 epoch total loss 6.35991621\n",
      "Trained batch 213 batch loss 5.95301199 epoch total loss 6.358006\n",
      "Trained batch 214 batch loss 7.35754967 epoch total loss 6.36267662\n",
      "Trained batch 215 batch loss 7.0929594 epoch total loss 6.36607361\n",
      "Trained batch 216 batch loss 6.85310268 epoch total loss 6.36832857\n",
      "Trained batch 217 batch loss 6.68747568 epoch total loss 6.36979914\n",
      "Trained batch 218 batch loss 6.84685802 epoch total loss 6.37198734\n",
      "Trained batch 219 batch loss 6.54782915 epoch total loss 6.37279034\n",
      "Trained batch 220 batch loss 7.1966753 epoch total loss 6.37653542\n",
      "Trained batch 221 batch loss 7.15265 epoch total loss 6.38004732\n",
      "Trained batch 222 batch loss 6.93017292 epoch total loss 6.38252544\n",
      "Trained batch 223 batch loss 6.3832655 epoch total loss 6.38252878\n",
      "Trained batch 224 batch loss 6.20837164 epoch total loss 6.38175154\n",
      "Trained batch 225 batch loss 6.52204227 epoch total loss 6.38237524\n",
      "Trained batch 226 batch loss 6.82600927 epoch total loss 6.38433838\n",
      "Trained batch 227 batch loss 6.95849466 epoch total loss 6.38686752\n",
      "Trained batch 228 batch loss 6.71194267 epoch total loss 6.38829327\n",
      "Trained batch 229 batch loss 6.39696884 epoch total loss 6.38833141\n",
      "Trained batch 230 batch loss 6.60937929 epoch total loss 6.38929224\n",
      "Trained batch 231 batch loss 6.62742 epoch total loss 6.39032316\n",
      "Trained batch 232 batch loss 6.72450399 epoch total loss 6.39176369\n",
      "Trained batch 233 batch loss 6.91400576 epoch total loss 6.3940053\n",
      "Trained batch 234 batch loss 7.00089312 epoch total loss 6.39659882\n",
      "Trained batch 235 batch loss 6.7540226 epoch total loss 6.39811945\n",
      "Trained batch 236 batch loss 6.75197506 epoch total loss 6.3996191\n",
      "Trained batch 237 batch loss 6.59506893 epoch total loss 6.40044355\n",
      "Trained batch 238 batch loss 6.96809435 epoch total loss 6.40282917\n",
      "Trained batch 239 batch loss 6.29238605 epoch total loss 6.40236664\n",
      "Trained batch 240 batch loss 5.94663668 epoch total loss 6.40046787\n",
      "Trained batch 241 batch loss 6.46696615 epoch total loss 6.40074348\n",
      "Trained batch 242 batch loss 6.58127165 epoch total loss 6.40148973\n",
      "Trained batch 243 batch loss 6.42013836 epoch total loss 6.40156651\n",
      "Trained batch 244 batch loss 6.40732574 epoch total loss 6.40159035\n",
      "Trained batch 245 batch loss 5.9903183 epoch total loss 6.39991188\n",
      "Trained batch 246 batch loss 6.40493 epoch total loss 6.39993191\n",
      "Trained batch 247 batch loss 6.06545544 epoch total loss 6.39857769\n",
      "Trained batch 248 batch loss 6.03621197 epoch total loss 6.39711666\n",
      "Trained batch 249 batch loss 6.2401104 epoch total loss 6.39648628\n",
      "Trained batch 250 batch loss 6.01832199 epoch total loss 6.39497375\n",
      "Trained batch 251 batch loss 6.00505209 epoch total loss 6.39341974\n",
      "Trained batch 252 batch loss 6.01772833 epoch total loss 6.39192915\n",
      "Trained batch 253 batch loss 5.80054331 epoch total loss 6.38959169\n",
      "Trained batch 254 batch loss 6.17117214 epoch total loss 6.38873148\n",
      "Trained batch 255 batch loss 6.24256849 epoch total loss 6.38815832\n",
      "Trained batch 256 batch loss 6.21799183 epoch total loss 6.38749361\n",
      "Trained batch 257 batch loss 6.49599171 epoch total loss 6.38791561\n",
      "Trained batch 258 batch loss 6.79685307 epoch total loss 6.38950062\n",
      "Trained batch 259 batch loss 6.28378534 epoch total loss 6.38909292\n",
      "Trained batch 260 batch loss 6.17954254 epoch total loss 6.38828707\n",
      "Trained batch 261 batch loss 6.18387365 epoch total loss 6.38750362\n",
      "Trained batch 262 batch loss 6.00833035 epoch total loss 6.38605642\n",
      "Trained batch 263 batch loss 6.53630733 epoch total loss 6.3866272\n",
      "Trained batch 264 batch loss 6.48262548 epoch total loss 6.38699102\n",
      "Trained batch 265 batch loss 6.45491457 epoch total loss 6.38724756\n",
      "Trained batch 266 batch loss 6.52589035 epoch total loss 6.38776875\n",
      "Trained batch 267 batch loss 6.53278732 epoch total loss 6.38831186\n",
      "Trained batch 268 batch loss 6.3336072 epoch total loss 6.38810778\n",
      "Trained batch 269 batch loss 6.40239954 epoch total loss 6.38816071\n",
      "Trained batch 270 batch loss 6.72501373 epoch total loss 6.38940859\n",
      "Trained batch 271 batch loss 6.2223525 epoch total loss 6.38879204\n",
      "Trained batch 272 batch loss 6.43480349 epoch total loss 6.38896132\n",
      "Trained batch 273 batch loss 6.24149418 epoch total loss 6.38842106\n",
      "Trained batch 274 batch loss 6.29457569 epoch total loss 6.38807869\n",
      "Trained batch 275 batch loss 5.81406641 epoch total loss 6.3859911\n",
      "Trained batch 276 batch loss 5.66157103 epoch total loss 6.38336658\n",
      "Trained batch 277 batch loss 5.66605902 epoch total loss 6.38077688\n",
      "Trained batch 278 batch loss 6.24640083 epoch total loss 6.38029385\n",
      "Trained batch 279 batch loss 6.56243 epoch total loss 6.38094664\n",
      "Trained batch 280 batch loss 6.43012524 epoch total loss 6.38112211\n",
      "Trained batch 281 batch loss 7.1056776 epoch total loss 6.38370085\n",
      "Trained batch 282 batch loss 6.83686638 epoch total loss 6.38530779\n",
      "Trained batch 283 batch loss 6.48761654 epoch total loss 6.38566971\n",
      "Trained batch 284 batch loss 6.20716763 epoch total loss 6.38504124\n",
      "Trained batch 285 batch loss 6.28888321 epoch total loss 6.38470411\n",
      "Trained batch 286 batch loss 6.50481606 epoch total loss 6.38512373\n",
      "Trained batch 287 batch loss 6.76245356 epoch total loss 6.38643837\n",
      "Trained batch 288 batch loss 6.43930769 epoch total loss 6.38662195\n",
      "Trained batch 289 batch loss 6.45019245 epoch total loss 6.38684225\n",
      "Trained batch 290 batch loss 6.2825985 epoch total loss 6.38648272\n",
      "Trained batch 291 batch loss 6.30045366 epoch total loss 6.38618708\n",
      "Trained batch 292 batch loss 6.02890968 epoch total loss 6.38496351\n",
      "Trained batch 293 batch loss 6.43103361 epoch total loss 6.38512039\n",
      "Trained batch 294 batch loss 6.27974176 epoch total loss 6.38476229\n",
      "Trained batch 295 batch loss 6.43651438 epoch total loss 6.38493776\n",
      "Trained batch 296 batch loss 6.48249483 epoch total loss 6.38526773\n",
      "Trained batch 297 batch loss 6.41064692 epoch total loss 6.38535309\n",
      "Trained batch 298 batch loss 6.919487 epoch total loss 6.38714504\n",
      "Trained batch 299 batch loss 5.67819405 epoch total loss 6.38477421\n",
      "Trained batch 300 batch loss 5.99382496 epoch total loss 6.38347101\n",
      "Trained batch 301 batch loss 6.4727006 epoch total loss 6.38376713\n",
      "Trained batch 302 batch loss 6.54338312 epoch total loss 6.38429546\n",
      "Trained batch 303 batch loss 6.55780792 epoch total loss 6.38486814\n",
      "Trained batch 304 batch loss 6.57985878 epoch total loss 6.38550949\n",
      "Trained batch 305 batch loss 6.50406075 epoch total loss 6.38589811\n",
      "Trained batch 306 batch loss 6.38760138 epoch total loss 6.38590384\n",
      "Trained batch 307 batch loss 6.38290501 epoch total loss 6.3858943\n",
      "Trained batch 308 batch loss 6.50734043 epoch total loss 6.38628817\n",
      "Trained batch 309 batch loss 6.36616135 epoch total loss 6.38622332\n",
      "Trained batch 310 batch loss 6.80039406 epoch total loss 6.38755941\n",
      "Trained batch 311 batch loss 6.58367968 epoch total loss 6.38819027\n",
      "Trained batch 312 batch loss 6.56878853 epoch total loss 6.38876915\n",
      "Trained batch 313 batch loss 6.7325387 epoch total loss 6.38986778\n",
      "Trained batch 314 batch loss 6.73315287 epoch total loss 6.39096069\n",
      "Trained batch 315 batch loss 6.50899315 epoch total loss 6.39133549\n",
      "Trained batch 316 batch loss 6.3412714 epoch total loss 6.39117718\n",
      "Trained batch 317 batch loss 6.78568363 epoch total loss 6.39242172\n",
      "Trained batch 318 batch loss 6.28408 epoch total loss 6.39208078\n",
      "Trained batch 319 batch loss 6.38769817 epoch total loss 6.39206743\n",
      "Trained batch 320 batch loss 6.72474718 epoch total loss 6.39310694\n",
      "Trained batch 321 batch loss 6.65777874 epoch total loss 6.39393139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 322 batch loss 6.48302698 epoch total loss 6.39420748\n",
      "Trained batch 323 batch loss 6.53614759 epoch total loss 6.39464712\n",
      "Trained batch 324 batch loss 6.65704918 epoch total loss 6.39545679\n",
      "Trained batch 325 batch loss 6.62879181 epoch total loss 6.39617491\n",
      "Trained batch 326 batch loss 6.76320791 epoch total loss 6.39730072\n",
      "Trained batch 327 batch loss 6.65442181 epoch total loss 6.3980875\n",
      "Trained batch 328 batch loss 6.75189161 epoch total loss 6.39916611\n",
      "Trained batch 329 batch loss 6.54497576 epoch total loss 6.39960909\n",
      "Trained batch 330 batch loss 6.98766184 epoch total loss 6.40139103\n",
      "Trained batch 331 batch loss 6.77922106 epoch total loss 6.40253258\n",
      "Trained batch 332 batch loss 6.21194792 epoch total loss 6.40195847\n",
      "Trained batch 333 batch loss 6.29031181 epoch total loss 6.40162325\n",
      "Trained batch 334 batch loss 5.96011 epoch total loss 6.40030146\n",
      "Trained batch 335 batch loss 5.48934126 epoch total loss 6.39758205\n",
      "Trained batch 336 batch loss 5.97489262 epoch total loss 6.39632368\n",
      "Trained batch 337 batch loss 6.10080385 epoch total loss 6.39544678\n",
      "Trained batch 338 batch loss 5.36995792 epoch total loss 6.39241266\n",
      "Trained batch 339 batch loss 5.2239933 epoch total loss 6.38896561\n",
      "Trained batch 340 batch loss 5.28786135 epoch total loss 6.38572693\n",
      "Trained batch 341 batch loss 5.68480778 epoch total loss 6.38367176\n",
      "Trained batch 342 batch loss 5.63049889 epoch total loss 6.38146973\n",
      "Trained batch 343 batch loss 6.3872776 epoch total loss 6.38148642\n",
      "Trained batch 344 batch loss 6.57269859 epoch total loss 6.38204241\n",
      "Trained batch 345 batch loss 6.72580099 epoch total loss 6.383039\n",
      "Trained batch 346 batch loss 6.93947172 epoch total loss 6.38464689\n",
      "Trained batch 347 batch loss 6.80592203 epoch total loss 6.38586092\n",
      "Trained batch 348 batch loss 6.68510818 epoch total loss 6.38672066\n",
      "Trained batch 349 batch loss 6.27376223 epoch total loss 6.38639688\n",
      "Trained batch 350 batch loss 6.32623339 epoch total loss 6.38622475\n",
      "Trained batch 351 batch loss 6.51160383 epoch total loss 6.38658237\n",
      "Trained batch 352 batch loss 6.27021933 epoch total loss 6.38625193\n",
      "Trained batch 353 batch loss 6.38003445 epoch total loss 6.38623476\n",
      "Trained batch 354 batch loss 6.47538328 epoch total loss 6.38648653\n",
      "Trained batch 355 batch loss 6.48927593 epoch total loss 6.38677597\n",
      "Trained batch 356 batch loss 6.56662655 epoch total loss 6.38728094\n",
      "Trained batch 357 batch loss 6.51480389 epoch total loss 6.38763857\n",
      "Trained batch 358 batch loss 6.14437294 epoch total loss 6.3869586\n",
      "Trained batch 359 batch loss 6.34016895 epoch total loss 6.38682842\n",
      "Trained batch 360 batch loss 6.4107666 epoch total loss 6.38689518\n",
      "Trained batch 361 batch loss 6.62197161 epoch total loss 6.38754654\n",
      "Trained batch 362 batch loss 6.67182 epoch total loss 6.38833189\n",
      "Trained batch 363 batch loss 6.77062559 epoch total loss 6.38938475\n",
      "Trained batch 364 batch loss 6.5155983 epoch total loss 6.38973141\n",
      "Trained batch 365 batch loss 6.36728382 epoch total loss 6.38967\n",
      "Trained batch 366 batch loss 6.51608753 epoch total loss 6.39001513\n",
      "Trained batch 367 batch loss 6.79866409 epoch total loss 6.39112854\n",
      "Trained batch 368 batch loss 6.4687438 epoch total loss 6.3913393\n",
      "Trained batch 369 batch loss 6.19391537 epoch total loss 6.39080429\n",
      "Trained batch 370 batch loss 6.43046761 epoch total loss 6.39091158\n",
      "Trained batch 371 batch loss 5.88130665 epoch total loss 6.38953781\n",
      "Trained batch 372 batch loss 6.64131165 epoch total loss 6.39021492\n",
      "Trained batch 373 batch loss 6.61051 epoch total loss 6.39080572\n",
      "Trained batch 374 batch loss 5.88227749 epoch total loss 6.38944626\n",
      "Trained batch 375 batch loss 6.15340328 epoch total loss 6.38881636\n",
      "Trained batch 376 batch loss 6.23739338 epoch total loss 6.38841343\n",
      "Trained batch 377 batch loss 6.19895 epoch total loss 6.38791084\n",
      "Trained batch 378 batch loss 6.46929169 epoch total loss 6.3881259\n",
      "Trained batch 379 batch loss 6.40604591 epoch total loss 6.3881731\n",
      "Trained batch 380 batch loss 6.37930584 epoch total loss 6.38815\n",
      "Trained batch 381 batch loss 6.35731363 epoch total loss 6.38806963\n",
      "Trained batch 382 batch loss 6.28770781 epoch total loss 6.38780642\n",
      "Trained batch 383 batch loss 5.96394777 epoch total loss 6.38669968\n",
      "Trained batch 384 batch loss 6.33882856 epoch total loss 6.38657522\n",
      "Trained batch 385 batch loss 6.25836563 epoch total loss 6.38624191\n",
      "Trained batch 386 batch loss 6.22372103 epoch total loss 6.38582039\n",
      "Trained batch 387 batch loss 6.28614807 epoch total loss 6.3855629\n",
      "Trained batch 388 batch loss 6.55183506 epoch total loss 6.38599157\n",
      "Trained batch 389 batch loss 6.12770462 epoch total loss 6.38532734\n",
      "Trained batch 390 batch loss 6.19721317 epoch total loss 6.38484526\n",
      "Trained batch 391 batch loss 6.51603794 epoch total loss 6.38518095\n",
      "Trained batch 392 batch loss 6.42374182 epoch total loss 6.38527966\n",
      "Trained batch 393 batch loss 6.12269 epoch total loss 6.38461161\n",
      "Trained batch 394 batch loss 5.72710466 epoch total loss 6.38294268\n",
      "Trained batch 395 batch loss 5.49228 epoch total loss 6.38068771\n",
      "Trained batch 396 batch loss 5.95482779 epoch total loss 6.37961197\n",
      "Trained batch 397 batch loss 6.00237465 epoch total loss 6.37866211\n",
      "Trained batch 398 batch loss 6.42027617 epoch total loss 6.37876654\n",
      "Trained batch 399 batch loss 6.43677759 epoch total loss 6.37891197\n",
      "Trained batch 400 batch loss 6.39052248 epoch total loss 6.37894106\n",
      "Trained batch 401 batch loss 6.34980392 epoch total loss 6.37886858\n",
      "Trained batch 402 batch loss 6.82461882 epoch total loss 6.3799777\n",
      "Trained batch 403 batch loss 6.56489229 epoch total loss 6.38043642\n",
      "Trained batch 404 batch loss 6.51375818 epoch total loss 6.38076639\n",
      "Trained batch 405 batch loss 6.25318623 epoch total loss 6.3804512\n",
      "Trained batch 406 batch loss 6.29406071 epoch total loss 6.38023806\n",
      "Trained batch 407 batch loss 6.35027933 epoch total loss 6.38016462\n",
      "Trained batch 408 batch loss 5.91228533 epoch total loss 6.37901831\n",
      "Trained batch 409 batch loss 6.09374952 epoch total loss 6.37832069\n",
      "Trained batch 410 batch loss 6.16734695 epoch total loss 6.37780571\n",
      "Trained batch 411 batch loss 5.84967136 epoch total loss 6.37652063\n",
      "Trained batch 412 batch loss 6.27024 epoch total loss 6.37626266\n",
      "Trained batch 413 batch loss 6.34871101 epoch total loss 6.37619591\n",
      "Trained batch 414 batch loss 6.42859221 epoch total loss 6.37632275\n",
      "Trained batch 415 batch loss 6.38131332 epoch total loss 6.37633467\n",
      "Trained batch 416 batch loss 6.15850306 epoch total loss 6.3758111\n",
      "Trained batch 417 batch loss 6.09236383 epoch total loss 6.37513113\n",
      "Trained batch 418 batch loss 6.34001875 epoch total loss 6.37504721\n",
      "Trained batch 419 batch loss 6.2264986 epoch total loss 6.37469292\n",
      "Trained batch 420 batch loss 6.23951578 epoch total loss 6.37437105\n",
      "Trained batch 421 batch loss 6.42146397 epoch total loss 6.37448263\n",
      "Trained batch 422 batch loss 6.49122906 epoch total loss 6.3747592\n",
      "Trained batch 423 batch loss 6.26949406 epoch total loss 6.37451077\n",
      "Trained batch 424 batch loss 6.67328548 epoch total loss 6.37521553\n",
      "Trained batch 425 batch loss 6.72606182 epoch total loss 6.37604094\n",
      "Trained batch 426 batch loss 6.85002899 epoch total loss 6.37715387\n",
      "Trained batch 427 batch loss 6.70036554 epoch total loss 6.37791061\n",
      "Trained batch 428 batch loss 6.6547122 epoch total loss 6.37855768\n",
      "Trained batch 429 batch loss 5.61277676 epoch total loss 6.37677288\n",
      "Trained batch 430 batch loss 5.84513 epoch total loss 6.37553644\n",
      "Trained batch 431 batch loss 6.25345755 epoch total loss 6.3752532\n",
      "Trained batch 432 batch loss 6.5591383 epoch total loss 6.37567854\n",
      "Trained batch 433 batch loss 6.55469561 epoch total loss 6.37609196\n",
      "Trained batch 434 batch loss 6.60323191 epoch total loss 6.37661552\n",
      "Trained batch 435 batch loss 6.50669622 epoch total loss 6.3769145\n",
      "Trained batch 436 batch loss 6.41547346 epoch total loss 6.37700319\n",
      "Trained batch 437 batch loss 6.53496647 epoch total loss 6.37736416\n",
      "Trained batch 438 batch loss 6.36271524 epoch total loss 6.37733126\n",
      "Trained batch 439 batch loss 6.74602222 epoch total loss 6.37817097\n",
      "Trained batch 440 batch loss 6.7005682 epoch total loss 6.37890387\n",
      "Trained batch 441 batch loss 5.9580822 epoch total loss 6.37794971\n",
      "Trained batch 442 batch loss 6.45131826 epoch total loss 6.37811565\n",
      "Trained batch 443 batch loss 6.57658958 epoch total loss 6.37856388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 444 batch loss 6.50908566 epoch total loss 6.37885809\n",
      "Trained batch 445 batch loss 6.43181372 epoch total loss 6.37897682\n",
      "Trained batch 446 batch loss 5.92159653 epoch total loss 6.37795162\n",
      "Trained batch 447 batch loss 6.46072 epoch total loss 6.37813663\n",
      "Trained batch 448 batch loss 6.2350564 epoch total loss 6.37781763\n",
      "Trained batch 449 batch loss 6.30688429 epoch total loss 6.37765932\n",
      "Trained batch 450 batch loss 6.35196447 epoch total loss 6.37760258\n",
      "Trained batch 451 batch loss 6.25443459 epoch total loss 6.37732935\n",
      "Trained batch 452 batch loss 6.14968395 epoch total loss 6.37682581\n",
      "Trained batch 453 batch loss 6.48549795 epoch total loss 6.37706566\n",
      "Trained batch 454 batch loss 6.59768724 epoch total loss 6.37755156\n",
      "Trained batch 455 batch loss 5.95101738 epoch total loss 6.37661409\n",
      "Trained batch 456 batch loss 6.39429903 epoch total loss 6.37665272\n",
      "Trained batch 457 batch loss 5.95575047 epoch total loss 6.37573195\n",
      "Trained batch 458 batch loss 5.99288177 epoch total loss 6.37489605\n",
      "Trained batch 459 batch loss 6.22621107 epoch total loss 6.37457228\n",
      "Trained batch 460 batch loss 6.50031 epoch total loss 6.3748455\n",
      "Trained batch 461 batch loss 6.83125401 epoch total loss 6.3758359\n",
      "Trained batch 462 batch loss 6.49690342 epoch total loss 6.37609768\n",
      "Trained batch 463 batch loss 6.29970932 epoch total loss 6.37593269\n",
      "Trained batch 464 batch loss 6.27516413 epoch total loss 6.37571573\n",
      "Trained batch 465 batch loss 6.01298761 epoch total loss 6.37493563\n",
      "Trained batch 466 batch loss 6.32990646 epoch total loss 6.37483883\n",
      "Trained batch 467 batch loss 6.4839139 epoch total loss 6.375072\n",
      "Trained batch 468 batch loss 6.51364136 epoch total loss 6.37536812\n",
      "Trained batch 469 batch loss 6.33828735 epoch total loss 6.37528944\n",
      "Trained batch 470 batch loss 6.93769455 epoch total loss 6.3764863\n",
      "Trained batch 471 batch loss 6.81776 epoch total loss 6.37742329\n",
      "Trained batch 472 batch loss 6.85719967 epoch total loss 6.37844\n",
      "Trained batch 473 batch loss 6.5631876 epoch total loss 6.37883043\n",
      "Trained batch 474 batch loss 6.61703444 epoch total loss 6.37933254\n",
      "Trained batch 475 batch loss 6.62368393 epoch total loss 6.37984753\n",
      "Trained batch 476 batch loss 6.15152645 epoch total loss 6.37936783\n",
      "Trained batch 477 batch loss 6.5963006 epoch total loss 6.37982225\n",
      "Trained batch 478 batch loss 6.38322163 epoch total loss 6.37983\n",
      "Trained batch 479 batch loss 6.34309578 epoch total loss 6.37975264\n",
      "Trained batch 480 batch loss 6.62955618 epoch total loss 6.38027334\n",
      "Trained batch 481 batch loss 6.18610239 epoch total loss 6.37986946\n",
      "Trained batch 482 batch loss 6.5487566 epoch total loss 6.38022\n",
      "Trained batch 483 batch loss 6.32084656 epoch total loss 6.38009691\n",
      "Trained batch 484 batch loss 6.36955452 epoch total loss 6.38007545\n",
      "Trained batch 485 batch loss 6.53932762 epoch total loss 6.380404\n",
      "Trained batch 486 batch loss 6.28759 epoch total loss 6.38021278\n",
      "Trained batch 487 batch loss 6.53069639 epoch total loss 6.38052177\n",
      "Trained batch 488 batch loss 6.43780231 epoch total loss 6.38063908\n",
      "Trained batch 489 batch loss 6.5101409 epoch total loss 6.3809042\n",
      "Trained batch 490 batch loss 6.01534128 epoch total loss 6.38015842\n",
      "Trained batch 491 batch loss 6.93818092 epoch total loss 6.38129473\n",
      "Trained batch 492 batch loss 6.56474209 epoch total loss 6.38166761\n",
      "Trained batch 493 batch loss 6.94756651 epoch total loss 6.38281536\n",
      "Trained batch 494 batch loss 7.16956615 epoch total loss 6.38440847\n",
      "Trained batch 495 batch loss 7.28522444 epoch total loss 6.38622808\n",
      "Trained batch 496 batch loss 7.52036524 epoch total loss 6.38851452\n",
      "Trained batch 497 batch loss 7.03473186 epoch total loss 6.38981438\n",
      "Trained batch 498 batch loss 6.56515026 epoch total loss 6.39016676\n",
      "Trained batch 499 batch loss 6.59646749 epoch total loss 6.39058\n",
      "Trained batch 500 batch loss 6.39083385 epoch total loss 6.39058065\n",
      "Trained batch 501 batch loss 6.72265434 epoch total loss 6.39124346\n",
      "Trained batch 502 batch loss 6.76168394 epoch total loss 6.3919816\n",
      "Trained batch 503 batch loss 6.58168697 epoch total loss 6.39235878\n",
      "Trained batch 504 batch loss 6.43915844 epoch total loss 6.39245176\n",
      "Trained batch 505 batch loss 6.66172218 epoch total loss 6.39298487\n",
      "Trained batch 506 batch loss 6.40688038 epoch total loss 6.39301252\n",
      "Trained batch 507 batch loss 6.16366911 epoch total loss 6.39256\n",
      "Trained batch 508 batch loss 6.207757 epoch total loss 6.39219618\n",
      "Trained batch 509 batch loss 5.98628 epoch total loss 6.39139891\n",
      "Trained batch 510 batch loss 6.30226946 epoch total loss 6.39122391\n",
      "Trained batch 511 batch loss 6.53213835 epoch total loss 6.3915\n",
      "Trained batch 512 batch loss 6.23276234 epoch total loss 6.39118958\n",
      "Trained batch 513 batch loss 6.05463409 epoch total loss 6.39053345\n",
      "Trained batch 514 batch loss 6.19227886 epoch total loss 6.39014816\n",
      "Trained batch 515 batch loss 6.23867512 epoch total loss 6.38985395\n",
      "Trained batch 516 batch loss 5.95911121 epoch total loss 6.38901949\n",
      "Trained batch 517 batch loss 6.13191557 epoch total loss 6.38852215\n",
      "Trained batch 518 batch loss 6.58010197 epoch total loss 6.38889217\n",
      "Trained batch 519 batch loss 6.05823 epoch total loss 6.38825512\n",
      "Trained batch 520 batch loss 6.46303 epoch total loss 6.38839912\n",
      "Trained batch 521 batch loss 6.550879 epoch total loss 6.38871098\n",
      "Trained batch 522 batch loss 6.17000151 epoch total loss 6.38829184\n",
      "Trained batch 523 batch loss 6.54028654 epoch total loss 6.38858223\n",
      "Trained batch 524 batch loss 6.62015772 epoch total loss 6.38902426\n",
      "Trained batch 525 batch loss 6.49796963 epoch total loss 6.38923168\n",
      "Trained batch 526 batch loss 6.61527681 epoch total loss 6.38966131\n",
      "Trained batch 527 batch loss 6.52406263 epoch total loss 6.3899169\n",
      "Trained batch 528 batch loss 6.55651474 epoch total loss 6.39023209\n",
      "Trained batch 529 batch loss 6.41485357 epoch total loss 6.39027834\n",
      "Trained batch 530 batch loss 6.341887 epoch total loss 6.39018679\n",
      "Trained batch 531 batch loss 6.89292145 epoch total loss 6.39113331\n",
      "Trained batch 532 batch loss 6.81959391 epoch total loss 6.39193869\n",
      "Trained batch 533 batch loss 6.94595575 epoch total loss 6.39297867\n",
      "Trained batch 534 batch loss 6.66262627 epoch total loss 6.39348316\n",
      "Trained batch 535 batch loss 6.81241798 epoch total loss 6.39426661\n",
      "Trained batch 536 batch loss 6.80090714 epoch total loss 6.39502525\n",
      "Trained batch 537 batch loss 6.73709202 epoch total loss 6.39566231\n",
      "Trained batch 538 batch loss 6.4738636 epoch total loss 6.39580774\n",
      "Trained batch 539 batch loss 6.52125072 epoch total loss 6.39604044\n",
      "Trained batch 540 batch loss 6.70976448 epoch total loss 6.39662123\n",
      "Trained batch 541 batch loss 6.55403614 epoch total loss 6.3969121\n",
      "Trained batch 542 batch loss 6.3987689 epoch total loss 6.39691544\n",
      "Trained batch 543 batch loss 6.45290518 epoch total loss 6.39701843\n",
      "Trained batch 544 batch loss 6.53658772 epoch total loss 6.39727497\n",
      "Trained batch 545 batch loss 6.4544878 epoch total loss 6.39738035\n",
      "Trained batch 546 batch loss 6.27663851 epoch total loss 6.3971591\n",
      "Trained batch 547 batch loss 6.46839428 epoch total loss 6.39728975\n",
      "Trained batch 548 batch loss 6.35963821 epoch total loss 6.39722061\n",
      "Trained batch 549 batch loss 6.39000368 epoch total loss 6.39720726\n",
      "Trained batch 550 batch loss 6.0101943 epoch total loss 6.39650393\n",
      "Trained batch 551 batch loss 6.00501728 epoch total loss 6.39579344\n",
      "Trained batch 552 batch loss 5.76267624 epoch total loss 6.39464664\n",
      "Trained batch 553 batch loss 6.30675411 epoch total loss 6.39448738\n",
      "Trained batch 554 batch loss 6.41461086 epoch total loss 6.39452362\n",
      "Trained batch 555 batch loss 6.25171852 epoch total loss 6.39426661\n",
      "Trained batch 556 batch loss 6.46686268 epoch total loss 6.39439678\n",
      "Trained batch 557 batch loss 6.49737692 epoch total loss 6.39458179\n",
      "Trained batch 558 batch loss 6.47303057 epoch total loss 6.39472246\n",
      "Trained batch 559 batch loss 6.19782 epoch total loss 6.39437\n",
      "Trained batch 560 batch loss 6.3123455 epoch total loss 6.39422369\n",
      "Trained batch 561 batch loss 6.19158 epoch total loss 6.39386225\n",
      "Trained batch 562 batch loss 6.34050512 epoch total loss 6.39376736\n",
      "Trained batch 563 batch loss 6.50744438 epoch total loss 6.39396906\n",
      "Trained batch 564 batch loss 6.55681849 epoch total loss 6.39425802\n",
      "Trained batch 565 batch loss 6.41744709 epoch total loss 6.39429903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 566 batch loss 6.30791 epoch total loss 6.39414644\n",
      "Trained batch 567 batch loss 6.30162811 epoch total loss 6.39398289\n",
      "Trained batch 568 batch loss 5.84476185 epoch total loss 6.39301634\n",
      "Trained batch 569 batch loss 6.29217577 epoch total loss 6.39283895\n",
      "Trained batch 570 batch loss 7.25392485 epoch total loss 6.39434958\n",
      "Trained batch 571 batch loss 6.71924257 epoch total loss 6.39491844\n",
      "Trained batch 572 batch loss 6.62342167 epoch total loss 6.39531851\n",
      "Trained batch 573 batch loss 6.98503065 epoch total loss 6.39634752\n",
      "Trained batch 574 batch loss 6.47191763 epoch total loss 6.39647913\n",
      "Trained batch 575 batch loss 6.48739815 epoch total loss 6.39663744\n",
      "Trained batch 576 batch loss 6.43316317 epoch total loss 6.39670038\n",
      "Trained batch 577 batch loss 6.55977488 epoch total loss 6.39698315\n",
      "Trained batch 578 batch loss 6.4649992 epoch total loss 6.39710093\n",
      "Trained batch 579 batch loss 6.3141017 epoch total loss 6.39695787\n",
      "Trained batch 580 batch loss 6.70376 epoch total loss 6.39748716\n",
      "Trained batch 581 batch loss 7.06187963 epoch total loss 6.39863\n",
      "Trained batch 582 batch loss 6.41462469 epoch total loss 6.3986578\n",
      "Trained batch 583 batch loss 6.3471508 epoch total loss 6.39856958\n",
      "Trained batch 584 batch loss 6.14671803 epoch total loss 6.39813805\n",
      "Trained batch 585 batch loss 6.23061705 epoch total loss 6.39785194\n",
      "Trained batch 586 batch loss 6.03229332 epoch total loss 6.39722824\n",
      "Trained batch 587 batch loss 6.33573151 epoch total loss 6.39712334\n",
      "Trained batch 588 batch loss 6.45844316 epoch total loss 6.39722776\n",
      "Trained batch 589 batch loss 6.54226255 epoch total loss 6.39747381\n",
      "Trained batch 590 batch loss 6.38761663 epoch total loss 6.39745712\n",
      "Trained batch 591 batch loss 6.504951 epoch total loss 6.3976388\n",
      "Trained batch 592 batch loss 6.22633314 epoch total loss 6.39734936\n",
      "Trained batch 593 batch loss 6.4729166 epoch total loss 6.39747715\n",
      "Trained batch 594 batch loss 6.63105774 epoch total loss 6.39787054\n",
      "Trained batch 595 batch loss 6.60250139 epoch total loss 6.39821434\n",
      "Trained batch 596 batch loss 6.40897512 epoch total loss 6.39823246\n",
      "Trained batch 597 batch loss 6.29049778 epoch total loss 6.39805174\n",
      "Trained batch 598 batch loss 5.85270739 epoch total loss 6.39714\n",
      "Trained batch 599 batch loss 5.58430052 epoch total loss 6.39578295\n",
      "Trained batch 600 batch loss 5.54523134 epoch total loss 6.39436531\n",
      "Trained batch 601 batch loss 6.2032 epoch total loss 6.39404726\n",
      "Trained batch 602 batch loss 6.09938097 epoch total loss 6.39355755\n",
      "Trained batch 603 batch loss 6.05948782 epoch total loss 6.39300346\n",
      "Trained batch 604 batch loss 5.88501 epoch total loss 6.3921628\n",
      "Trained batch 605 batch loss 6.12195587 epoch total loss 6.391716\n",
      "Trained batch 606 batch loss 5.80575848 epoch total loss 6.39074898\n",
      "Trained batch 607 batch loss 6.30080462 epoch total loss 6.39060068\n",
      "Trained batch 608 batch loss 6.75005913 epoch total loss 6.39119196\n",
      "Trained batch 609 batch loss 6.22032881 epoch total loss 6.3909111\n",
      "Trained batch 610 batch loss 6.28833199 epoch total loss 6.39074326\n",
      "Trained batch 611 batch loss 6.64853954 epoch total loss 6.39116478\n",
      "Trained batch 612 batch loss 6.59464836 epoch total loss 6.39149761\n",
      "Trained batch 613 batch loss 6.55425549 epoch total loss 6.39176273\n",
      "Trained batch 614 batch loss 6.40046263 epoch total loss 6.39177704\n",
      "Trained batch 615 batch loss 6.38031 epoch total loss 6.39175844\n",
      "Trained batch 616 batch loss 5.96089745 epoch total loss 6.39105892\n",
      "Trained batch 617 batch loss 5.80853462 epoch total loss 6.39011478\n",
      "Trained batch 618 batch loss 5.5655694 epoch total loss 6.38878107\n",
      "Trained batch 619 batch loss 5.55361748 epoch total loss 6.38743162\n",
      "Trained batch 620 batch loss 5.58599 epoch total loss 6.38613892\n",
      "Trained batch 621 batch loss 5.31145763 epoch total loss 6.38440847\n",
      "Trained batch 622 batch loss 5.12839603 epoch total loss 6.38238955\n",
      "Trained batch 623 batch loss 5.15732956 epoch total loss 6.38042259\n",
      "Trained batch 624 batch loss 5.87919283 epoch total loss 6.3796196\n",
      "Trained batch 625 batch loss 6.46847868 epoch total loss 6.3797617\n",
      "Trained batch 626 batch loss 6.56746387 epoch total loss 6.38006163\n",
      "Trained batch 627 batch loss 6.09907866 epoch total loss 6.3796134\n",
      "Trained batch 628 batch loss 6.17253828 epoch total loss 6.37928391\n",
      "Trained batch 629 batch loss 6.81313753 epoch total loss 6.37997341\n",
      "Trained batch 630 batch loss 7.28940868 epoch total loss 6.3814168\n",
      "Trained batch 631 batch loss 6.66283417 epoch total loss 6.38186312\n",
      "Trained batch 632 batch loss 6.38281631 epoch total loss 6.38186455\n",
      "Trained batch 633 batch loss 6.72219896 epoch total loss 6.38240194\n",
      "Trained batch 634 batch loss 6.6479249 epoch total loss 6.38282108\n",
      "Trained batch 635 batch loss 6.76686192 epoch total loss 6.38342571\n",
      "Trained batch 636 batch loss 6.54127026 epoch total loss 6.38367367\n",
      "Trained batch 637 batch loss 6.52227545 epoch total loss 6.38389158\n",
      "Trained batch 638 batch loss 5.89669514 epoch total loss 6.38312769\n",
      "Trained batch 639 batch loss 6.75907707 epoch total loss 6.38371611\n",
      "Trained batch 640 batch loss 6.22727823 epoch total loss 6.38347149\n",
      "Trained batch 641 batch loss 6.54941273 epoch total loss 6.38373041\n",
      "Trained batch 642 batch loss 6.72957897 epoch total loss 6.38426876\n",
      "Trained batch 643 batch loss 6.42292881 epoch total loss 6.38432884\n",
      "Trained batch 644 batch loss 6.72390318 epoch total loss 6.3848567\n",
      "Trained batch 645 batch loss 6.62898111 epoch total loss 6.38523483\n",
      "Trained batch 646 batch loss 6.47275209 epoch total loss 6.38537025\n",
      "Trained batch 647 batch loss 6.04799652 epoch total loss 6.38484859\n",
      "Trained batch 648 batch loss 5.9638381 epoch total loss 6.38419914\n",
      "Trained batch 649 batch loss 6.35251141 epoch total loss 6.38415\n",
      "Trained batch 650 batch loss 6.35462713 epoch total loss 6.38410473\n",
      "Trained batch 651 batch loss 6.13616419 epoch total loss 6.38372374\n",
      "Trained batch 652 batch loss 6.35727501 epoch total loss 6.38368368\n",
      "Trained batch 653 batch loss 6.22108078 epoch total loss 6.38343477\n",
      "Trained batch 654 batch loss 6.49618292 epoch total loss 6.38360691\n",
      "Trained batch 655 batch loss 6.04450512 epoch total loss 6.38308907\n",
      "Trained batch 656 batch loss 5.95291328 epoch total loss 6.38243341\n",
      "Trained batch 657 batch loss 6.35288668 epoch total loss 6.38238907\n",
      "Trained batch 658 batch loss 6.04454756 epoch total loss 6.38187504\n",
      "Trained batch 659 batch loss 6.21439695 epoch total loss 6.38162088\n",
      "Trained batch 660 batch loss 6.00466919 epoch total loss 6.38105\n",
      "Trained batch 661 batch loss 6.05841064 epoch total loss 6.38056231\n",
      "Trained batch 662 batch loss 6.19607639 epoch total loss 6.38028383\n",
      "Trained batch 663 batch loss 6.71810293 epoch total loss 6.38079405\n",
      "Trained batch 664 batch loss 6.49892759 epoch total loss 6.38097191\n",
      "Trained batch 665 batch loss 6.14248896 epoch total loss 6.38061333\n",
      "Trained batch 666 batch loss 6.24727964 epoch total loss 6.38041306\n",
      "Trained batch 667 batch loss 6.11377621 epoch total loss 6.38001299\n",
      "Trained batch 668 batch loss 6.46869087 epoch total loss 6.38014603\n",
      "Trained batch 669 batch loss 6.43999386 epoch total loss 6.3802352\n",
      "Trained batch 670 batch loss 6.17749357 epoch total loss 6.37993288\n",
      "Trained batch 671 batch loss 5.91766644 epoch total loss 6.37924385\n",
      "Trained batch 672 batch loss 6.6417942 epoch total loss 6.37963438\n",
      "Trained batch 673 batch loss 6.47337341 epoch total loss 6.37977314\n",
      "Trained batch 674 batch loss 6.89848137 epoch total loss 6.38054276\n",
      "Trained batch 675 batch loss 6.90705252 epoch total loss 6.38132286\n",
      "Trained batch 676 batch loss 6.31991673 epoch total loss 6.38123226\n",
      "Trained batch 677 batch loss 6.49767637 epoch total loss 6.38140392\n",
      "Trained batch 678 batch loss 5.78206301 epoch total loss 6.38052034\n",
      "Trained batch 679 batch loss 6.5475812 epoch total loss 6.38076591\n",
      "Trained batch 680 batch loss 6.67650509 epoch total loss 6.38120031\n",
      "Trained batch 681 batch loss 6.34483528 epoch total loss 6.38114691\n",
      "Trained batch 682 batch loss 6.68718529 epoch total loss 6.38159561\n",
      "Trained batch 683 batch loss 6.18438387 epoch total loss 6.38130713\n",
      "Trained batch 684 batch loss 6.2835412 epoch total loss 6.38116407\n",
      "Trained batch 685 batch loss 6.51341391 epoch total loss 6.38135672\n",
      "Trained batch 686 batch loss 6.50023 epoch total loss 6.38153\n",
      "Trained batch 687 batch loss 6.57017326 epoch total loss 6.38180447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 688 batch loss 6.816782 epoch total loss 6.38243723\n",
      "Trained batch 689 batch loss 6.46341848 epoch total loss 6.38255453\n",
      "Trained batch 690 batch loss 6.31180859 epoch total loss 6.38245249\n",
      "Trained batch 691 batch loss 6.42572069 epoch total loss 6.38251495\n",
      "Trained batch 692 batch loss 6.31495667 epoch total loss 6.3824172\n",
      "Trained batch 693 batch loss 6.646698 epoch total loss 6.38279819\n",
      "Trained batch 694 batch loss 6.61372614 epoch total loss 6.38313103\n",
      "Trained batch 695 batch loss 6.78077221 epoch total loss 6.38370323\n",
      "Trained batch 696 batch loss 5.97571802 epoch total loss 6.3831172\n",
      "Trained batch 697 batch loss 6.2825942 epoch total loss 6.38297272\n",
      "Trained batch 698 batch loss 5.92121553 epoch total loss 6.38231182\n",
      "Trained batch 699 batch loss 6.03349781 epoch total loss 6.38181305\n",
      "Trained batch 700 batch loss 6.40492392 epoch total loss 6.38184547\n",
      "Trained batch 701 batch loss 5.74404621 epoch total loss 6.38093615\n",
      "Trained batch 702 batch loss 5.41142941 epoch total loss 6.37955523\n",
      "Trained batch 703 batch loss 5.45161104 epoch total loss 6.37823534\n",
      "Trained batch 704 batch loss 5.50310755 epoch total loss 6.37699175\n",
      "Trained batch 705 batch loss 5.87475443 epoch total loss 6.37627935\n",
      "Trained batch 706 batch loss 5.630445 epoch total loss 6.37522268\n",
      "Trained batch 707 batch loss 6.20969868 epoch total loss 6.37498808\n",
      "Trained batch 708 batch loss 5.94330215 epoch total loss 6.37437868\n",
      "Trained batch 709 batch loss 5.94779587 epoch total loss 6.37377691\n",
      "Trained batch 710 batch loss 5.92332077 epoch total loss 6.37314224\n",
      "Trained batch 711 batch loss 5.97125626 epoch total loss 6.37257719\n",
      "Trained batch 712 batch loss 5.8259058 epoch total loss 6.37180901\n",
      "Trained batch 713 batch loss 6.52995539 epoch total loss 6.37203074\n",
      "Trained batch 714 batch loss 6.4626894 epoch total loss 6.37215805\n",
      "Trained batch 715 batch loss 6.94618654 epoch total loss 6.37296104\n",
      "Trained batch 716 batch loss 6.9620719 epoch total loss 6.37378359\n",
      "Trained batch 717 batch loss 7.20460033 epoch total loss 6.3749423\n",
      "Trained batch 718 batch loss 6.68342352 epoch total loss 6.37537193\n",
      "Trained batch 719 batch loss 6.92336464 epoch total loss 6.37613392\n",
      "Trained batch 720 batch loss 6.28324366 epoch total loss 6.37600517\n",
      "Trained batch 721 batch loss 6.49708891 epoch total loss 6.37617302\n",
      "Trained batch 722 batch loss 6.35654783 epoch total loss 6.37614584\n",
      "Trained batch 723 batch loss 6.41324949 epoch total loss 6.37619686\n",
      "Trained batch 724 batch loss 6.3767333 epoch total loss 6.37619781\n",
      "Trained batch 725 batch loss 6.48251867 epoch total loss 6.3763442\n",
      "Trained batch 726 batch loss 6.33517122 epoch total loss 6.37628746\n",
      "Trained batch 727 batch loss 6.44781065 epoch total loss 6.37638569\n",
      "Trained batch 728 batch loss 6.34115171 epoch total loss 6.37633753\n",
      "Trained batch 729 batch loss 6.43533325 epoch total loss 6.37641859\n",
      "Trained batch 730 batch loss 6.49020338 epoch total loss 6.37657452\n",
      "Trained batch 731 batch loss 6.46287441 epoch total loss 6.37669277\n",
      "Trained batch 732 batch loss 6.41313887 epoch total loss 6.37674236\n",
      "Trained batch 733 batch loss 6.60938835 epoch total loss 6.37706\n",
      "Trained batch 734 batch loss 6.21225643 epoch total loss 6.37683535\n",
      "Trained batch 735 batch loss 6.28769684 epoch total loss 6.37671375\n",
      "Trained batch 736 batch loss 5.64801693 epoch total loss 6.37572384\n",
      "Trained batch 737 batch loss 5.49808073 epoch total loss 6.3745327\n",
      "Trained batch 738 batch loss 5.7219038 epoch total loss 6.37364817\n",
      "Trained batch 739 batch loss 5.73314953 epoch total loss 6.37278128\n",
      "Trained batch 740 batch loss 6.05133963 epoch total loss 6.37234688\n",
      "Trained batch 741 batch loss 6.31977415 epoch total loss 6.37227583\n",
      "Trained batch 742 batch loss 6.15112829 epoch total loss 6.37197828\n",
      "Trained batch 743 batch loss 6.60132074 epoch total loss 6.37228727\n",
      "Trained batch 744 batch loss 6.61607218 epoch total loss 6.37261486\n",
      "Trained batch 745 batch loss 6.3423 epoch total loss 6.37257433\n",
      "Trained batch 746 batch loss 6.58425331 epoch total loss 6.37285852\n",
      "Trained batch 747 batch loss 6.61328888 epoch total loss 6.37318039\n",
      "Trained batch 748 batch loss 6.38054752 epoch total loss 6.37319\n",
      "Trained batch 749 batch loss 6.56001329 epoch total loss 6.37343931\n",
      "Trained batch 750 batch loss 6.45476627 epoch total loss 6.37354755\n",
      "Trained batch 751 batch loss 6.7054038 epoch total loss 6.37398958\n",
      "Trained batch 752 batch loss 6.59799814 epoch total loss 6.37428761\n",
      "Trained batch 753 batch loss 6.50299215 epoch total loss 6.37445831\n",
      "Trained batch 754 batch loss 6.40222836 epoch total loss 6.37449551\n",
      "Trained batch 755 batch loss 6.41080523 epoch total loss 6.37454319\n",
      "Trained batch 756 batch loss 6.3494997 epoch total loss 6.37451029\n",
      "Trained batch 757 batch loss 6.37882376 epoch total loss 6.37451601\n",
      "Trained batch 758 batch loss 6.59517527 epoch total loss 6.37480736\n",
      "Trained batch 759 batch loss 6.42932081 epoch total loss 6.37487888\n",
      "Trained batch 760 batch loss 6.33344841 epoch total loss 6.37482452\n",
      "Trained batch 761 batch loss 6.40308428 epoch total loss 6.37486219\n",
      "Trained batch 762 batch loss 6.55184507 epoch total loss 6.37509441\n",
      "Trained batch 763 batch loss 6.34870052 epoch total loss 6.3750596\n",
      "Trained batch 764 batch loss 5.98469448 epoch total loss 6.37454891\n",
      "Trained batch 765 batch loss 5.59580326 epoch total loss 6.37353086\n",
      "Trained batch 766 batch loss 6.08429432 epoch total loss 6.37315321\n",
      "Trained batch 767 batch loss 6.63174868 epoch total loss 6.37349081\n",
      "Trained batch 768 batch loss 6.61425877 epoch total loss 6.37380409\n",
      "Trained batch 769 batch loss 6.8073535 epoch total loss 6.37436771\n",
      "Trained batch 770 batch loss 6.64915276 epoch total loss 6.37472439\n",
      "Trained batch 771 batch loss 6.91133165 epoch total loss 6.37542\n",
      "Trained batch 772 batch loss 6.79875 epoch total loss 6.37596846\n",
      "Trained batch 773 batch loss 6.62210703 epoch total loss 6.37628651\n",
      "Trained batch 774 batch loss 6.55307674 epoch total loss 6.37651539\n",
      "Trained batch 775 batch loss 6.54719639 epoch total loss 6.37673569\n",
      "Trained batch 776 batch loss 6.32924938 epoch total loss 6.37667418\n",
      "Trained batch 777 batch loss 6.45247459 epoch total loss 6.37677193\n",
      "Trained batch 778 batch loss 6.5951004 epoch total loss 6.37705278\n",
      "Trained batch 779 batch loss 6.38929892 epoch total loss 6.37706852\n",
      "Trained batch 780 batch loss 6.49484921 epoch total loss 6.3772192\n",
      "Trained batch 781 batch loss 6.20801592 epoch total loss 6.37700272\n",
      "Trained batch 782 batch loss 6.54251671 epoch total loss 6.37721395\n",
      "Trained batch 783 batch loss 6.60826397 epoch total loss 6.37750959\n",
      "Trained batch 784 batch loss 6.44133759 epoch total loss 6.37759066\n",
      "Trained batch 785 batch loss 6.52958584 epoch total loss 6.37778473\n",
      "Trained batch 786 batch loss 6.62174749 epoch total loss 6.37809515\n",
      "Trained batch 787 batch loss 6.60442257 epoch total loss 6.37838268\n",
      "Trained batch 788 batch loss 6.44720316 epoch total loss 6.37847\n",
      "Trained batch 789 batch loss 6.48427677 epoch total loss 6.37860441\n",
      "Trained batch 790 batch loss 6.47234106 epoch total loss 6.37872267\n",
      "Trained batch 791 batch loss 6.57490253 epoch total loss 6.37897062\n",
      "Trained batch 792 batch loss 6.43021297 epoch total loss 6.379035\n",
      "Trained batch 793 batch loss 6.53927422 epoch total loss 6.3792367\n",
      "Trained batch 794 batch loss 6.52931213 epoch total loss 6.379426\n",
      "Trained batch 795 batch loss 6.43570042 epoch total loss 6.37949657\n",
      "Trained batch 796 batch loss 6.33921766 epoch total loss 6.37944603\n",
      "Trained batch 797 batch loss 6.16930151 epoch total loss 6.37918234\n",
      "Trained batch 798 batch loss 6.29247141 epoch total loss 6.3790741\n",
      "Trained batch 799 batch loss 6.32940102 epoch total loss 6.37901211\n",
      "Trained batch 800 batch loss 6.42479 epoch total loss 6.37906933\n",
      "Trained batch 801 batch loss 6.63969231 epoch total loss 6.37939453\n",
      "Trained batch 802 batch loss 6.13427114 epoch total loss 6.37908888\n",
      "Trained batch 803 batch loss 6.52556562 epoch total loss 6.37927103\n",
      "Trained batch 804 batch loss 6.32537937 epoch total loss 6.3792038\n",
      "Trained batch 805 batch loss 6.55882 epoch total loss 6.37942648\n",
      "Trained batch 806 batch loss 6.08008957 epoch total loss 6.3790555\n",
      "Trained batch 807 batch loss 5.98690176 epoch total loss 6.37856913\n",
      "Trained batch 808 batch loss 6.16613293 epoch total loss 6.37830639\n",
      "Trained batch 809 batch loss 6.3733778 epoch total loss 6.3783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 810 batch loss 6.07027531 epoch total loss 6.37792\n",
      "Trained batch 811 batch loss 6.41930771 epoch total loss 6.37797117\n",
      "Trained batch 812 batch loss 6.55283976 epoch total loss 6.37818623\n",
      "Trained batch 813 batch loss 6.70164776 epoch total loss 6.37858438\n",
      "Trained batch 814 batch loss 6.54056215 epoch total loss 6.37878323\n",
      "Trained batch 815 batch loss 6.03796816 epoch total loss 6.37836504\n",
      "Trained batch 816 batch loss 6.20955563 epoch total loss 6.37815809\n",
      "Trained batch 817 batch loss 6.53084278 epoch total loss 6.37834501\n",
      "Trained batch 818 batch loss 6.41396093 epoch total loss 6.37838888\n",
      "Trained batch 819 batch loss 6.73483706 epoch total loss 6.37882376\n",
      "Trained batch 820 batch loss 6.46889639 epoch total loss 6.37893343\n",
      "Trained batch 821 batch loss 6.44081736 epoch total loss 6.37900925\n",
      "Trained batch 822 batch loss 5.8173418 epoch total loss 6.37832594\n",
      "Trained batch 823 batch loss 6.35202074 epoch total loss 6.37829399\n",
      "Trained batch 824 batch loss 6.91315699 epoch total loss 6.37894297\n",
      "Trained batch 825 batch loss 7.12454844 epoch total loss 6.37984657\n",
      "Trained batch 826 batch loss 7.02115393 epoch total loss 6.38062286\n",
      "Trained batch 827 batch loss 6.58105707 epoch total loss 6.3808651\n",
      "Trained batch 828 batch loss 6.1440134 epoch total loss 6.38057947\n",
      "Trained batch 829 batch loss 6.48483467 epoch total loss 6.38070488\n",
      "Trained batch 830 batch loss 6.39243221 epoch total loss 6.38071918\n",
      "Trained batch 831 batch loss 6.13056469 epoch total loss 6.3804183\n",
      "Trained batch 832 batch loss 6.11730576 epoch total loss 6.38010168\n",
      "Trained batch 833 batch loss 6.10236 epoch total loss 6.37976837\n",
      "Trained batch 834 batch loss 6.69186783 epoch total loss 6.38014269\n",
      "Trained batch 835 batch loss 6.57047701 epoch total loss 6.38037062\n",
      "Trained batch 836 batch loss 6.64946747 epoch total loss 6.38069248\n",
      "Trained batch 837 batch loss 6.54589939 epoch total loss 6.38089\n",
      "Trained batch 838 batch loss 6.26537609 epoch total loss 6.38075161\n",
      "Trained batch 839 batch loss 6.44297409 epoch total loss 6.38082552\n",
      "Trained batch 840 batch loss 6.282619 epoch total loss 6.38070869\n",
      "Trained batch 841 batch loss 6.3769331 epoch total loss 6.3807044\n",
      "Trained batch 842 batch loss 6.62047482 epoch total loss 6.38098907\n",
      "Trained batch 843 batch loss 6.16872597 epoch total loss 6.38073778\n",
      "Trained batch 844 batch loss 6.19977427 epoch total loss 6.3805232\n",
      "Trained batch 845 batch loss 6.08386087 epoch total loss 6.38017225\n",
      "Trained batch 846 batch loss 6.62769747 epoch total loss 6.38046503\n",
      "Trained batch 847 batch loss 6.44467163 epoch total loss 6.38054132\n",
      "Trained batch 848 batch loss 6.22664 epoch total loss 6.38035965\n",
      "Trained batch 849 batch loss 5.55657434 epoch total loss 6.37938929\n",
      "Trained batch 850 batch loss 5.53769207 epoch total loss 6.3783989\n",
      "Trained batch 851 batch loss 6.2298975 epoch total loss 6.37822437\n",
      "Trained batch 852 batch loss 6.37431765 epoch total loss 6.37822\n",
      "Trained batch 853 batch loss 6.47136402 epoch total loss 6.37832928\n",
      "Trained batch 854 batch loss 6.04172564 epoch total loss 6.37793493\n",
      "Trained batch 855 batch loss 6.59236145 epoch total loss 6.37818575\n",
      "Trained batch 856 batch loss 6.36760235 epoch total loss 6.37817335\n",
      "Trained batch 857 batch loss 6.51960325 epoch total loss 6.37833834\n",
      "Trained batch 858 batch loss 6.17460299 epoch total loss 6.37810087\n",
      "Trained batch 859 batch loss 5.60649 epoch total loss 6.37720251\n",
      "Trained batch 860 batch loss 6.39967775 epoch total loss 6.37722921\n",
      "Trained batch 861 batch loss 6.3410759 epoch total loss 6.37718725\n",
      "Trained batch 862 batch loss 6.23085165 epoch total loss 6.3770175\n",
      "Trained batch 863 batch loss 6.25754452 epoch total loss 6.37687922\n",
      "Trained batch 864 batch loss 6.46347857 epoch total loss 6.37697935\n",
      "Trained batch 865 batch loss 6.3862524 epoch total loss 6.37699\n",
      "Trained batch 866 batch loss 6.34515476 epoch total loss 6.37695312\n",
      "Trained batch 867 batch loss 6.59274101 epoch total loss 6.37720203\n",
      "Trained batch 868 batch loss 6.33461714 epoch total loss 6.37715292\n",
      "Trained batch 869 batch loss 6.42519474 epoch total loss 6.37720823\n",
      "Trained batch 870 batch loss 6.48351622 epoch total loss 6.3773303\n",
      "Trained batch 871 batch loss 6.19168043 epoch total loss 6.37711716\n",
      "Trained batch 872 batch loss 6.72851801 epoch total loss 6.37752056\n",
      "Trained batch 873 batch loss 7.03057957 epoch total loss 6.37826872\n",
      "Trained batch 874 batch loss 6.94162941 epoch total loss 6.37891293\n",
      "Trained batch 875 batch loss 6.22225904 epoch total loss 6.37873363\n",
      "Trained batch 876 batch loss 6.29307365 epoch total loss 6.37863588\n",
      "Trained batch 877 batch loss 6.3301239 epoch total loss 6.37858057\n",
      "Trained batch 878 batch loss 6.48395205 epoch total loss 6.37870026\n",
      "Trained batch 879 batch loss 6.2899375 epoch total loss 6.37859964\n",
      "Trained batch 880 batch loss 6.29809856 epoch total loss 6.37850857\n",
      "Trained batch 881 batch loss 6.18829346 epoch total loss 6.37829256\n",
      "Trained batch 882 batch loss 6.38750458 epoch total loss 6.37830353\n",
      "Trained batch 883 batch loss 6.30118561 epoch total loss 6.37821627\n",
      "Trained batch 884 batch loss 6.33727932 epoch total loss 6.37817\n",
      "Trained batch 885 batch loss 6.34438 epoch total loss 6.37813139\n",
      "Trained batch 886 batch loss 6.37244558 epoch total loss 6.37812519\n",
      "Trained batch 887 batch loss 6.29252 epoch total loss 6.37802887\n",
      "Trained batch 888 batch loss 6.31870699 epoch total loss 6.37796211\n",
      "Trained batch 889 batch loss 6.35953188 epoch total loss 6.37794113\n",
      "Trained batch 890 batch loss 6.337708 epoch total loss 6.37789631\n",
      "Trained batch 891 batch loss 6.15094137 epoch total loss 6.3776412\n",
      "Trained batch 892 batch loss 6.44652653 epoch total loss 6.37771845\n",
      "Trained batch 893 batch loss 6.04460096 epoch total loss 6.37734509\n",
      "Trained batch 894 batch loss 5.62077236 epoch total loss 6.3764987\n",
      "Trained batch 895 batch loss 6.08000231 epoch total loss 6.3761673\n",
      "Trained batch 896 batch loss 5.62450361 epoch total loss 6.37532854\n",
      "Trained batch 897 batch loss 6.15959501 epoch total loss 6.37508821\n",
      "Trained batch 898 batch loss 6.33247757 epoch total loss 6.37504101\n",
      "Trained batch 899 batch loss 6.61387539 epoch total loss 6.37530613\n",
      "Trained batch 900 batch loss 6.41107464 epoch total loss 6.37534618\n",
      "Trained batch 901 batch loss 6.31297636 epoch total loss 6.37527704\n",
      "Trained batch 902 batch loss 6.31087446 epoch total loss 6.37520552\n",
      "Trained batch 903 batch loss 6.26609612 epoch total loss 6.37508488\n",
      "Trained batch 904 batch loss 5.876266 epoch total loss 6.37453318\n",
      "Trained batch 905 batch loss 6.01126623 epoch total loss 6.37413168\n",
      "Trained batch 906 batch loss 6.2585578 epoch total loss 6.37400436\n",
      "Trained batch 907 batch loss 6.56984377 epoch total loss 6.37422037\n",
      "Trained batch 908 batch loss 6.38386154 epoch total loss 6.37423086\n",
      "Trained batch 909 batch loss 6.33235121 epoch total loss 6.37418509\n",
      "Trained batch 910 batch loss 6.31943369 epoch total loss 6.374125\n",
      "Trained batch 911 batch loss 6.4164176 epoch total loss 6.37417126\n",
      "Trained batch 912 batch loss 6.46538734 epoch total loss 6.37427139\n",
      "Trained batch 913 batch loss 6.04327345 epoch total loss 6.373909\n",
      "Trained batch 914 batch loss 6.45055819 epoch total loss 6.37399292\n",
      "Trained batch 915 batch loss 6.36543 epoch total loss 6.37398338\n",
      "Trained batch 916 batch loss 6.63061 epoch total loss 6.37426329\n",
      "Trained batch 917 batch loss 6.39930105 epoch total loss 6.37429094\n",
      "Trained batch 918 batch loss 6.64497662 epoch total loss 6.37458563\n",
      "Trained batch 919 batch loss 6.08061314 epoch total loss 6.37426567\n",
      "Trained batch 920 batch loss 6.23218489 epoch total loss 6.37411165\n",
      "Trained batch 921 batch loss 6.11394453 epoch total loss 6.37382889\n",
      "Trained batch 922 batch loss 6.53202057 epoch total loss 6.37400055\n",
      "Trained batch 923 batch loss 6.66069126 epoch total loss 6.37431145\n",
      "Trained batch 924 batch loss 6.85314703 epoch total loss 6.37482929\n",
      "Trained batch 925 batch loss 6.94156027 epoch total loss 6.37544203\n",
      "Trained batch 926 batch loss 6.87535143 epoch total loss 6.37598181\n",
      "Trained batch 927 batch loss 6.90342665 epoch total loss 6.37655067\n",
      "Trained batch 928 batch loss 6.99372959 epoch total loss 6.37721586\n",
      "Trained batch 929 batch loss 5.73053932 epoch total loss 6.37651968\n",
      "Trained batch 930 batch loss 5.11321831 epoch total loss 6.37516117\n",
      "Trained batch 931 batch loss 5.4463191 epoch total loss 6.37416363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 932 batch loss 5.61665535 epoch total loss 6.37335062\n",
      "Trained batch 933 batch loss 5.76942205 epoch total loss 6.37270355\n",
      "Trained batch 934 batch loss 6.24823523 epoch total loss 6.37257\n",
      "Trained batch 935 batch loss 6.37724304 epoch total loss 6.37257528\n",
      "Trained batch 936 batch loss 6.36640358 epoch total loss 6.37256861\n",
      "Trained batch 937 batch loss 6.41916037 epoch total loss 6.3726182\n",
      "Trained batch 938 batch loss 6.41452169 epoch total loss 6.37266254\n",
      "Trained batch 939 batch loss 6.32374239 epoch total loss 6.37261057\n",
      "Trained batch 940 batch loss 5.99582863 epoch total loss 6.37220955\n",
      "Trained batch 941 batch loss 5.61211538 epoch total loss 6.37140179\n",
      "Trained batch 942 batch loss 5.01833534 epoch total loss 6.36996603\n",
      "Trained batch 943 batch loss 5.44211674 epoch total loss 6.36898184\n",
      "Trained batch 944 batch loss 6.07387972 epoch total loss 6.36866903\n",
      "Trained batch 945 batch loss 6.70523691 epoch total loss 6.36902475\n",
      "Trained batch 946 batch loss 6.21172619 epoch total loss 6.36885881\n",
      "Trained batch 947 batch loss 6.38992453 epoch total loss 6.36888123\n",
      "Trained batch 948 batch loss 6.5861578 epoch total loss 6.36911\n",
      "Trained batch 949 batch loss 6.33634424 epoch total loss 6.36907578\n",
      "Trained batch 950 batch loss 5.74003792 epoch total loss 6.36841393\n",
      "Trained batch 951 batch loss 6.03177261 epoch total loss 6.36805964\n",
      "Trained batch 952 batch loss 5.77648878 epoch total loss 6.36743832\n",
      "Trained batch 953 batch loss 5.96376562 epoch total loss 6.36701488\n",
      "Trained batch 954 batch loss 5.93946171 epoch total loss 6.36656666\n",
      "Trained batch 955 batch loss 6.01776457 epoch total loss 6.3662014\n",
      "Trained batch 956 batch loss 6.65394974 epoch total loss 6.36650229\n",
      "Trained batch 957 batch loss 7.22504377 epoch total loss 6.36739922\n",
      "Trained batch 958 batch loss 7.05503273 epoch total loss 6.36811733\n",
      "Trained batch 959 batch loss 6.82660913 epoch total loss 6.36859512\n",
      "Trained batch 960 batch loss 7.23586702 epoch total loss 6.36949873\n",
      "Trained batch 961 batch loss 6.77558708 epoch total loss 6.36992121\n",
      "Trained batch 962 batch loss 6.60053635 epoch total loss 6.37016106\n",
      "Trained batch 963 batch loss 6.23461676 epoch total loss 6.37002\n",
      "Trained batch 964 batch loss 6.96855879 epoch total loss 6.37064075\n",
      "Trained batch 965 batch loss 6.36733913 epoch total loss 6.37063742\n",
      "Trained batch 966 batch loss 6.06116104 epoch total loss 6.37031698\n",
      "Trained batch 967 batch loss 6.5949378 epoch total loss 6.37054873\n",
      "Trained batch 968 batch loss 6.48727846 epoch total loss 6.37066936\n",
      "Trained batch 969 batch loss 6.49027777 epoch total loss 6.37079287\n",
      "Trained batch 970 batch loss 6.23560715 epoch total loss 6.37065363\n",
      "Trained batch 971 batch loss 6.19606924 epoch total loss 6.37047434\n",
      "Trained batch 972 batch loss 7.0229578 epoch total loss 6.37114573\n",
      "Trained batch 973 batch loss 6.82952785 epoch total loss 6.37161684\n",
      "Trained batch 974 batch loss 6.64806 epoch total loss 6.37190056\n",
      "Trained batch 975 batch loss 6.70806789 epoch total loss 6.37224531\n",
      "Trained batch 976 batch loss 6.7976203 epoch total loss 6.37268114\n",
      "Trained batch 977 batch loss 6.56850195 epoch total loss 6.37288141\n",
      "Trained batch 978 batch loss 6.45322132 epoch total loss 6.37296343\n",
      "Trained batch 979 batch loss 6.18244362 epoch total loss 6.37276888\n",
      "Trained batch 980 batch loss 6.628613 epoch total loss 6.37302971\n",
      "Trained batch 981 batch loss 5.95750904 epoch total loss 6.37260628\n",
      "Trained batch 982 batch loss 5.99681902 epoch total loss 6.37222338\n",
      "Trained batch 983 batch loss 6.17719793 epoch total loss 6.37202501\n",
      "Trained batch 984 batch loss 6.29252386 epoch total loss 6.37194443\n",
      "Trained batch 985 batch loss 6.23908806 epoch total loss 6.37180948\n",
      "Trained batch 986 batch loss 6.40062 epoch total loss 6.37183857\n",
      "Trained batch 987 batch loss 6.58088446 epoch total loss 6.37205076\n",
      "Trained batch 988 batch loss 6.57550478 epoch total loss 6.37225676\n",
      "Trained batch 989 batch loss 6.46327972 epoch total loss 6.37234879\n",
      "Trained batch 990 batch loss 6.62888765 epoch total loss 6.37260771\n",
      "Trained batch 991 batch loss 6.53179741 epoch total loss 6.3727684\n",
      "Trained batch 992 batch loss 6.13358498 epoch total loss 6.3725276\n",
      "Trained batch 993 batch loss 6.30011082 epoch total loss 6.37245464\n",
      "Trained batch 994 batch loss 6.38272619 epoch total loss 6.37246513\n",
      "Trained batch 995 batch loss 6.38484716 epoch total loss 6.37247753\n",
      "Trained batch 996 batch loss 6.89920139 epoch total loss 6.37300682\n",
      "Trained batch 997 batch loss 6.87362671 epoch total loss 6.37350893\n",
      "Trained batch 998 batch loss 6.48075628 epoch total loss 6.37361622\n",
      "Trained batch 999 batch loss 6.57260084 epoch total loss 6.37381554\n",
      "Trained batch 1000 batch loss 6.39011574 epoch total loss 6.37383223\n",
      "Trained batch 1001 batch loss 6.3411274 epoch total loss 6.37379932\n",
      "Trained batch 1002 batch loss 6.45334196 epoch total loss 6.37387848\n",
      "Trained batch 1003 batch loss 6.26316309 epoch total loss 6.37376833\n",
      "Trained batch 1004 batch loss 6.20753574 epoch total loss 6.37360287\n",
      "Trained batch 1005 batch loss 5.99853611 epoch total loss 6.3732295\n",
      "Trained batch 1006 batch loss 6.10082674 epoch total loss 6.37295866\n",
      "Trained batch 1007 batch loss 6.57994223 epoch total loss 6.37316418\n",
      "Trained batch 1008 batch loss 6.24188757 epoch total loss 6.373034\n",
      "Trained batch 1009 batch loss 6.21519279 epoch total loss 6.3728776\n",
      "Trained batch 1010 batch loss 6.4301486 epoch total loss 6.37293434\n",
      "Trained batch 1011 batch loss 6.7150631 epoch total loss 6.37327242\n",
      "Trained batch 1012 batch loss 6.52226448 epoch total loss 6.37342\n",
      "Trained batch 1013 batch loss 6.51087379 epoch total loss 6.37355518\n",
      "Trained batch 1014 batch loss 6.67048264 epoch total loss 6.37384796\n",
      "Trained batch 1015 batch loss 6.78109598 epoch total loss 6.37424946\n",
      "Trained batch 1016 batch loss 6.30802441 epoch total loss 6.37418461\n",
      "Trained batch 1017 batch loss 6.31154919 epoch total loss 6.37412262\n",
      "Trained batch 1018 batch loss 5.86636686 epoch total loss 6.37362385\n",
      "Trained batch 1019 batch loss 6.08945274 epoch total loss 6.3733449\n",
      "Trained batch 1020 batch loss 5.50472975 epoch total loss 6.37249327\n",
      "Trained batch 1021 batch loss 5.28673315 epoch total loss 6.37143\n",
      "Trained batch 1022 batch loss 6.33549833 epoch total loss 6.37139463\n",
      "Trained batch 1023 batch loss 6.49108315 epoch total loss 6.37151194\n",
      "Trained batch 1024 batch loss 6.2927227 epoch total loss 6.37143469\n",
      "Trained batch 1025 batch loss 6.57576132 epoch total loss 6.37163401\n",
      "Trained batch 1026 batch loss 6.97360086 epoch total loss 6.37222052\n",
      "Trained batch 1027 batch loss 6.78219891 epoch total loss 6.37262\n",
      "Trained batch 1028 batch loss 6.59186745 epoch total loss 6.37283325\n",
      "Trained batch 1029 batch loss 6.84168816 epoch total loss 6.37328911\n",
      "Trained batch 1030 batch loss 6.82258606 epoch total loss 6.37372541\n",
      "Trained batch 1031 batch loss 6.66874552 epoch total loss 6.37401152\n",
      "Trained batch 1032 batch loss 6.50882053 epoch total loss 6.37414217\n",
      "Trained batch 1033 batch loss 6.39633703 epoch total loss 6.37416363\n",
      "Trained batch 1034 batch loss 5.45358467 epoch total loss 6.37327337\n",
      "Trained batch 1035 batch loss 5.86609697 epoch total loss 6.37278366\n",
      "Trained batch 1036 batch loss 6.55591536 epoch total loss 6.37296057\n",
      "Trained batch 1037 batch loss 6.57647657 epoch total loss 6.37315702\n",
      "Trained batch 1038 batch loss 6.22879028 epoch total loss 6.37301826\n",
      "Trained batch 1039 batch loss 6.35615301 epoch total loss 6.37300158\n",
      "Trained batch 1040 batch loss 6.47038364 epoch total loss 6.37309504\n",
      "Trained batch 1041 batch loss 6.26835775 epoch total loss 6.3729949\n",
      "Trained batch 1042 batch loss 5.95410967 epoch total loss 6.37259293\n",
      "Trained batch 1043 batch loss 6.23545027 epoch total loss 6.37246132\n",
      "Trained batch 1044 batch loss 6.10703278 epoch total loss 6.37220669\n",
      "Trained batch 1045 batch loss 6.53498363 epoch total loss 6.37236261\n",
      "Trained batch 1046 batch loss 5.55868149 epoch total loss 6.37158489\n",
      "Trained batch 1047 batch loss 5.87925959 epoch total loss 6.37111473\n",
      "Trained batch 1048 batch loss 5.88106823 epoch total loss 6.37064695\n",
      "Trained batch 1049 batch loss 6.06054544 epoch total loss 6.37035131\n",
      "Trained batch 1050 batch loss 5.73916769 epoch total loss 6.3697505\n",
      "Trained batch 1051 batch loss 6.57904768 epoch total loss 6.36994934\n",
      "Trained batch 1052 batch loss 6.47244024 epoch total loss 6.37004709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1053 batch loss 6.36669207 epoch total loss 6.37004375\n",
      "Trained batch 1054 batch loss 6.25638723 epoch total loss 6.36993599\n",
      "Trained batch 1055 batch loss 6.05961 epoch total loss 6.36964178\n",
      "Trained batch 1056 batch loss 6.33477449 epoch total loss 6.36960888\n",
      "Trained batch 1057 batch loss 6.47109604 epoch total loss 6.3697052\n",
      "Trained batch 1058 batch loss 6.25965261 epoch total loss 6.36960125\n",
      "Trained batch 1059 batch loss 6.09459829 epoch total loss 6.36934185\n",
      "Trained batch 1060 batch loss 6.2628603 epoch total loss 6.36924124\n",
      "Trained batch 1061 batch loss 6.32438374 epoch total loss 6.3691988\n",
      "Trained batch 1062 batch loss 6.1521616 epoch total loss 6.36899424\n",
      "Trained batch 1063 batch loss 6.52296257 epoch total loss 6.36913919\n",
      "Trained batch 1064 batch loss 6.47830057 epoch total loss 6.36924219\n",
      "Trained batch 1065 batch loss 6.45310593 epoch total loss 6.36932087\n",
      "Trained batch 1066 batch loss 6.83023119 epoch total loss 6.36975288\n",
      "Trained batch 1067 batch loss 6.49060488 epoch total loss 6.36986637\n",
      "Trained batch 1068 batch loss 6.66774559 epoch total loss 6.37014532\n",
      "Trained batch 1069 batch loss 6.34762239 epoch total loss 6.37012434\n",
      "Trained batch 1070 batch loss 6.46929026 epoch total loss 6.37021732\n",
      "Trained batch 1071 batch loss 6.54437065 epoch total loss 6.37038\n",
      "Trained batch 1072 batch loss 6.61554098 epoch total loss 6.37060881\n",
      "Trained batch 1073 batch loss 6.4989481 epoch total loss 6.37072849\n",
      "Trained batch 1074 batch loss 6.32100344 epoch total loss 6.37068176\n",
      "Trained batch 1075 batch loss 6.27342415 epoch total loss 6.37059116\n",
      "Trained batch 1076 batch loss 6.33221292 epoch total loss 6.3705554\n",
      "Trained batch 1077 batch loss 6.48379135 epoch total loss 6.37066078\n",
      "Trained batch 1078 batch loss 6.54052877 epoch total loss 6.37081814\n",
      "Trained batch 1079 batch loss 6.06882191 epoch total loss 6.37053871\n",
      "Trained batch 1080 batch loss 6.5179019 epoch total loss 6.37067509\n",
      "Trained batch 1081 batch loss 6.82184076 epoch total loss 6.37109232\n",
      "Trained batch 1082 batch loss 6.7341485 epoch total loss 6.37142801\n",
      "Trained batch 1083 batch loss 6.52281237 epoch total loss 6.3715682\n",
      "Trained batch 1084 batch loss 6.48828173 epoch total loss 6.37167549\n",
      "Trained batch 1085 batch loss 6.21229362 epoch total loss 6.3715291\n",
      "Trained batch 1086 batch loss 6.31659555 epoch total loss 6.37147808\n",
      "Trained batch 1087 batch loss 6.56572294 epoch total loss 6.37165689\n",
      "Trained batch 1088 batch loss 6.29420519 epoch total loss 6.37158585\n",
      "Trained batch 1089 batch loss 6.62147617 epoch total loss 6.37181568\n",
      "Trained batch 1090 batch loss 6.48613262 epoch total loss 6.37192059\n",
      "Trained batch 1091 batch loss 6.57479095 epoch total loss 6.37210655\n",
      "Trained batch 1092 batch loss 6.30873156 epoch total loss 6.37204838\n",
      "Trained batch 1093 batch loss 6.56956816 epoch total loss 6.3722291\n",
      "Trained batch 1094 batch loss 6.14527178 epoch total loss 6.37202168\n",
      "Trained batch 1095 batch loss 6.62893105 epoch total loss 6.37225628\n",
      "Trained batch 1096 batch loss 6.19617558 epoch total loss 6.37209558\n",
      "Trained batch 1097 batch loss 5.94464779 epoch total loss 6.37170601\n",
      "Trained batch 1098 batch loss 5.68492889 epoch total loss 6.37108088\n",
      "Trained batch 1099 batch loss 6.28368759 epoch total loss 6.37100124\n",
      "Trained batch 1100 batch loss 6.39083672 epoch total loss 6.37101936\n",
      "Trained batch 1101 batch loss 6.15961361 epoch total loss 6.3708272\n",
      "Trained batch 1102 batch loss 6.55693722 epoch total loss 6.37099648\n",
      "Trained batch 1103 batch loss 6.3256464 epoch total loss 6.37095499\n",
      "Trained batch 1104 batch loss 5.82276773 epoch total loss 6.3704586\n",
      "Trained batch 1105 batch loss 5.93155527 epoch total loss 6.3700614\n",
      "Trained batch 1106 batch loss 6.3468194 epoch total loss 6.37004042\n",
      "Trained batch 1107 batch loss 6.41236973 epoch total loss 6.37007904\n",
      "Trained batch 1108 batch loss 6.2703557 epoch total loss 6.36998892\n",
      "Trained batch 1109 batch loss 6.60123825 epoch total loss 6.3701973\n",
      "Trained batch 1110 batch loss 6.6661706 epoch total loss 6.37046385\n",
      "Trained batch 1111 batch loss 6.67706823 epoch total loss 6.37074\n",
      "Trained batch 1112 batch loss 6.61301756 epoch total loss 6.37095785\n",
      "Trained batch 1113 batch loss 6.46860838 epoch total loss 6.37104559\n",
      "Trained batch 1114 batch loss 6.24186611 epoch total loss 6.37092924\n",
      "Trained batch 1115 batch loss 6.06067371 epoch total loss 6.37065125\n",
      "Trained batch 1116 batch loss 6.27405691 epoch total loss 6.37056446\n",
      "Trained batch 1117 batch loss 6.40806913 epoch total loss 6.37059784\n",
      "Trained batch 1118 batch loss 6.50722408 epoch total loss 6.37072039\n",
      "Trained batch 1119 batch loss 6.83549881 epoch total loss 6.37113571\n",
      "Trained batch 1120 batch loss 6.86349392 epoch total loss 6.37157488\n",
      "Trained batch 1121 batch loss 6.88379574 epoch total loss 6.37203217\n",
      "Trained batch 1122 batch loss 7.06510878 epoch total loss 6.37264967\n",
      "Trained batch 1123 batch loss 6.82726288 epoch total loss 6.3730545\n",
      "Trained batch 1124 batch loss 7.31294727 epoch total loss 6.3738904\n",
      "Trained batch 1125 batch loss 7.03748512 epoch total loss 6.37448025\n",
      "Trained batch 1126 batch loss 6.76302338 epoch total loss 6.37482548\n",
      "Trained batch 1127 batch loss 6.56462955 epoch total loss 6.3749938\n",
      "Trained batch 1128 batch loss 6.40365076 epoch total loss 6.37501955\n",
      "Trained batch 1129 batch loss 6.46134901 epoch total loss 6.37509584\n",
      "Trained batch 1130 batch loss 6.01049471 epoch total loss 6.37477303\n",
      "Trained batch 1131 batch loss 6.42506838 epoch total loss 6.37481785\n",
      "Trained batch 1132 batch loss 6.50870895 epoch total loss 6.3749361\n",
      "Trained batch 1133 batch loss 6.45719481 epoch total loss 6.37500858\n",
      "Trained batch 1134 batch loss 6.44301128 epoch total loss 6.37506866\n",
      "Trained batch 1135 batch loss 6.12875557 epoch total loss 6.3748517\n",
      "Trained batch 1136 batch loss 6.41134548 epoch total loss 6.37488365\n",
      "Trained batch 1137 batch loss 6.18004751 epoch total loss 6.37471247\n",
      "Trained batch 1138 batch loss 6.11838102 epoch total loss 6.37448692\n",
      "Trained batch 1139 batch loss 6.27776241 epoch total loss 6.37440205\n",
      "Trained batch 1140 batch loss 6.3532 epoch total loss 6.37438345\n",
      "Trained batch 1141 batch loss 5.9081049 epoch total loss 6.3739748\n",
      "Trained batch 1142 batch loss 6.00940609 epoch total loss 6.37365532\n",
      "Trained batch 1143 batch loss 6.24531507 epoch total loss 6.37354279\n",
      "Trained batch 1144 batch loss 6.45999861 epoch total loss 6.3736186\n",
      "Trained batch 1145 batch loss 6.55590868 epoch total loss 6.37377787\n",
      "Trained batch 1146 batch loss 6.53665733 epoch total loss 6.37392\n",
      "Trained batch 1147 batch loss 6.64808226 epoch total loss 6.37415886\n",
      "Trained batch 1148 batch loss 6.19301176 epoch total loss 6.37400103\n",
      "Trained batch 1149 batch loss 5.49545813 epoch total loss 6.37323618\n",
      "Trained batch 1150 batch loss 6.15512371 epoch total loss 6.37304688\n",
      "Trained batch 1151 batch loss 6.16557932 epoch total loss 6.37286663\n",
      "Trained batch 1152 batch loss 6.28249931 epoch total loss 6.37278843\n",
      "Trained batch 1153 batch loss 6.30003881 epoch total loss 6.37272501\n",
      "Trained batch 1154 batch loss 6.1710577 epoch total loss 6.37255\n",
      "Trained batch 1155 batch loss 5.96392822 epoch total loss 6.3721962\n",
      "Trained batch 1156 batch loss 6.1211586 epoch total loss 6.37197924\n",
      "Trained batch 1157 batch loss 6.58251381 epoch total loss 6.37216091\n",
      "Trained batch 1158 batch loss 6.36734056 epoch total loss 6.37215662\n",
      "Trained batch 1159 batch loss 5.59543753 epoch total loss 6.37148619\n",
      "Trained batch 1160 batch loss 5.6510148 epoch total loss 6.37086535\n",
      "Trained batch 1161 batch loss 5.53473282 epoch total loss 6.37014484\n",
      "Trained batch 1162 batch loss 6.11096954 epoch total loss 6.36992168\n",
      "Trained batch 1163 batch loss 6.16466379 epoch total loss 6.36974525\n",
      "Trained batch 1164 batch loss 6.05294847 epoch total loss 6.36947298\n",
      "Trained batch 1165 batch loss 6.00114775 epoch total loss 6.36915636\n",
      "Trained batch 1166 batch loss 6.28376818 epoch total loss 6.3690834\n",
      "Trained batch 1167 batch loss 6.41019535 epoch total loss 6.36911821\n",
      "Trained batch 1168 batch loss 6.27170515 epoch total loss 6.36903477\n",
      "Trained batch 1169 batch loss 6.85758781 epoch total loss 6.36945248\n",
      "Trained batch 1170 batch loss 7.43377209 epoch total loss 6.37036228\n",
      "Trained batch 1171 batch loss 6.91594124 epoch total loss 6.37082815\n",
      "Trained batch 1172 batch loss 6.29221296 epoch total loss 6.37076092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1173 batch loss 6.27877951 epoch total loss 6.37068272\n",
      "Trained batch 1174 batch loss 6.06385136 epoch total loss 6.37042141\n",
      "Trained batch 1175 batch loss 6.50065327 epoch total loss 6.37053204\n",
      "Trained batch 1176 batch loss 6.11283875 epoch total loss 6.37031269\n",
      "Trained batch 1177 batch loss 6.24143457 epoch total loss 6.37020302\n",
      "Trained batch 1178 batch loss 5.95496 epoch total loss 6.36985064\n",
      "Trained batch 1179 batch loss 6.42737913 epoch total loss 6.36989927\n",
      "Trained batch 1180 batch loss 6.32212591 epoch total loss 6.36985874\n",
      "Trained batch 1181 batch loss 6.46409 epoch total loss 6.36993837\n",
      "Trained batch 1182 batch loss 6.71600771 epoch total loss 6.37023115\n",
      "Trained batch 1183 batch loss 6.19616 epoch total loss 6.37008429\n",
      "Trained batch 1184 batch loss 5.91375113 epoch total loss 6.36969852\n",
      "Trained batch 1185 batch loss 6.63279438 epoch total loss 6.36992073\n",
      "Trained batch 1186 batch loss 6.51313448 epoch total loss 6.37004137\n",
      "Trained batch 1187 batch loss 6.53436089 epoch total loss 6.37017965\n",
      "Trained batch 1188 batch loss 6.66146088 epoch total loss 6.37042522\n",
      "Trained batch 1189 batch loss 6.23267078 epoch total loss 6.37030935\n",
      "Trained batch 1190 batch loss 6.04311037 epoch total loss 6.37003422\n",
      "Trained batch 1191 batch loss 6.25884151 epoch total loss 6.36994076\n",
      "Trained batch 1192 batch loss 5.96288061 epoch total loss 6.36959934\n",
      "Trained batch 1193 batch loss 5.84174061 epoch total loss 6.36915684\n",
      "Trained batch 1194 batch loss 5.82290363 epoch total loss 6.36869955\n",
      "Trained batch 1195 batch loss 6.26680517 epoch total loss 6.36861372\n",
      "Trained batch 1196 batch loss 6.33217573 epoch total loss 6.3685832\n",
      "Trained batch 1197 batch loss 6.24905682 epoch total loss 6.36848354\n",
      "Trained batch 1198 batch loss 6.23746967 epoch total loss 6.36837387\n",
      "Trained batch 1199 batch loss 6.33622742 epoch total loss 6.36834717\n",
      "Trained batch 1200 batch loss 6.49891376 epoch total loss 6.36845636\n",
      "Trained batch 1201 batch loss 6.55539227 epoch total loss 6.36861181\n",
      "Trained batch 1202 batch loss 6.9486 epoch total loss 6.36909437\n",
      "Trained batch 1203 batch loss 6.60216904 epoch total loss 6.36928797\n",
      "Trained batch 1204 batch loss 6.50583 epoch total loss 6.36940145\n",
      "Trained batch 1205 batch loss 6.45669079 epoch total loss 6.36947393\n",
      "Trained batch 1206 batch loss 6.11445236 epoch total loss 6.36926222\n",
      "Trained batch 1207 batch loss 6.38654232 epoch total loss 6.36927652\n",
      "Trained batch 1208 batch loss 6.55522776 epoch total loss 6.36943054\n",
      "Trained batch 1209 batch loss 6.92008448 epoch total loss 6.36988592\n",
      "Trained batch 1210 batch loss 6.81853628 epoch total loss 6.37025642\n",
      "Trained batch 1211 batch loss 5.95511723 epoch total loss 6.36991358\n",
      "Trained batch 1212 batch loss 6.37123251 epoch total loss 6.36991453\n",
      "Trained batch 1213 batch loss 6.02394295 epoch total loss 6.36962938\n",
      "Trained batch 1214 batch loss 6.04607964 epoch total loss 6.36936283\n",
      "Trained batch 1215 batch loss 6.51514244 epoch total loss 6.36948252\n",
      "Trained batch 1216 batch loss 6.97960806 epoch total loss 6.36998415\n",
      "Trained batch 1217 batch loss 6.20962763 epoch total loss 6.36985254\n",
      "Trained batch 1218 batch loss 6.38339901 epoch total loss 6.36986351\n",
      "Trained batch 1219 batch loss 6.03208208 epoch total loss 6.36958647\n",
      "Trained batch 1220 batch loss 6.04211712 epoch total loss 6.36931801\n",
      "Trained batch 1221 batch loss 6.4721117 epoch total loss 6.36940193\n",
      "Trained batch 1222 batch loss 6.60526419 epoch total loss 6.36959553\n",
      "Trained batch 1223 batch loss 6.49228716 epoch total loss 6.36969566\n",
      "Trained batch 1224 batch loss 6.32886505 epoch total loss 6.36966228\n",
      "Trained batch 1225 batch loss 6.28089428 epoch total loss 6.36959\n",
      "Trained batch 1226 batch loss 6.25631571 epoch total loss 6.3694973\n",
      "Trained batch 1227 batch loss 6.33986378 epoch total loss 6.36947346\n",
      "Trained batch 1228 batch loss 6.1046114 epoch total loss 6.36925745\n",
      "Trained batch 1229 batch loss 6.19236565 epoch total loss 6.36911345\n",
      "Trained batch 1230 batch loss 6.12608528 epoch total loss 6.36891603\n",
      "Trained batch 1231 batch loss 6.52534103 epoch total loss 6.36904287\n",
      "Trained batch 1232 batch loss 7.14127254 epoch total loss 6.36967\n",
      "Trained batch 1233 batch loss 6.96401167 epoch total loss 6.37015152\n",
      "Trained batch 1234 batch loss 6.89696026 epoch total loss 6.37057877\n",
      "Trained batch 1235 batch loss 6.56973219 epoch total loss 6.37074\n",
      "Trained batch 1236 batch loss 6.52238274 epoch total loss 6.37086248\n",
      "Trained batch 1237 batch loss 6.26556349 epoch total loss 6.37077761\n",
      "Trained batch 1238 batch loss 6.31866932 epoch total loss 6.37073565\n",
      "Trained batch 1239 batch loss 6.14667654 epoch total loss 6.37055445\n",
      "Trained batch 1240 batch loss 6.24870443 epoch total loss 6.37045622\n",
      "Trained batch 1241 batch loss 6.22149563 epoch total loss 6.37033653\n",
      "Trained batch 1242 batch loss 6.68350792 epoch total loss 6.37058878\n",
      "Trained batch 1243 batch loss 6.57236338 epoch total loss 6.3707509\n",
      "Trained batch 1244 batch loss 6.11048269 epoch total loss 6.37054157\n",
      "Trained batch 1245 batch loss 6.39561844 epoch total loss 6.3705616\n",
      "Trained batch 1246 batch loss 6.08669519 epoch total loss 6.37033367\n",
      "Trained batch 1247 batch loss 6.09700918 epoch total loss 6.3701148\n",
      "Trained batch 1248 batch loss 6.35512686 epoch total loss 6.37010288\n",
      "Trained batch 1249 batch loss 6.21671391 epoch total loss 6.36998\n",
      "Trained batch 1250 batch loss 6.1491251 epoch total loss 6.36980295\n",
      "Trained batch 1251 batch loss 6.43036 epoch total loss 6.36985159\n",
      "Trained batch 1252 batch loss 5.90572309 epoch total loss 6.36948061\n",
      "Trained batch 1253 batch loss 6.45532227 epoch total loss 6.3695488\n",
      "Trained batch 1254 batch loss 6.14379501 epoch total loss 6.36936903\n",
      "Trained batch 1255 batch loss 6.33191299 epoch total loss 6.36933899\n",
      "Trained batch 1256 batch loss 6.89431 epoch total loss 6.36975718\n",
      "Trained batch 1257 batch loss 6.26222277 epoch total loss 6.36967182\n",
      "Trained batch 1258 batch loss 6.11240387 epoch total loss 6.36946726\n",
      "Trained batch 1259 batch loss 5.98574877 epoch total loss 6.36916256\n",
      "Trained batch 1260 batch loss 6.58014679 epoch total loss 6.36933\n",
      "Trained batch 1261 batch loss 6.6090064 epoch total loss 6.36951971\n",
      "Trained batch 1262 batch loss 6.70068216 epoch total loss 6.36978197\n",
      "Trained batch 1263 batch loss 6.75476217 epoch total loss 6.37008715\n",
      "Trained batch 1264 batch loss 6.91305923 epoch total loss 6.37051678\n",
      "Trained batch 1265 batch loss 6.64460516 epoch total loss 6.37073326\n",
      "Trained batch 1266 batch loss 6.42528534 epoch total loss 6.37077618\n",
      "Trained batch 1267 batch loss 6.57651758 epoch total loss 6.37093878\n",
      "Trained batch 1268 batch loss 6.80802 epoch total loss 6.37128353\n",
      "Trained batch 1269 batch loss 6.85329962 epoch total loss 6.37166357\n",
      "Trained batch 1270 batch loss 6.59653854 epoch total loss 6.37184095\n",
      "Trained batch 1271 batch loss 6.49508381 epoch total loss 6.37193775\n",
      "Trained batch 1272 batch loss 6.71455479 epoch total loss 6.37220716\n",
      "Trained batch 1273 batch loss 6.20718288 epoch total loss 6.37207699\n",
      "Trained batch 1274 batch loss 6.75628 epoch total loss 6.37237883\n",
      "Trained batch 1275 batch loss 6.44242096 epoch total loss 6.37243366\n",
      "Trained batch 1276 batch loss 6.42219448 epoch total loss 6.37247276\n",
      "Trained batch 1277 batch loss 6.81586075 epoch total loss 6.37282\n",
      "Trained batch 1278 batch loss 5.68995 epoch total loss 6.37228584\n",
      "Trained batch 1279 batch loss 5.78717422 epoch total loss 6.37182808\n",
      "Trained batch 1280 batch loss 5.85997868 epoch total loss 6.37142849\n",
      "Trained batch 1281 batch loss 6.24733 epoch total loss 6.37133169\n",
      "Trained batch 1282 batch loss 6.22624731 epoch total loss 6.3712182\n",
      "Trained batch 1283 batch loss 6.18925095 epoch total loss 6.37107658\n",
      "Trained batch 1284 batch loss 6.25236177 epoch total loss 6.37098408\n",
      "Trained batch 1285 batch loss 5.75051 epoch total loss 6.37050152\n",
      "Trained batch 1286 batch loss 6.33467388 epoch total loss 6.37047338\n",
      "Trained batch 1287 batch loss 5.87401438 epoch total loss 6.37008762\n",
      "Trained batch 1288 batch loss 6.27257061 epoch total loss 6.37001181\n",
      "Trained batch 1289 batch loss 5.85037661 epoch total loss 6.36960888\n",
      "Trained batch 1290 batch loss 5.74993753 epoch total loss 6.3691287\n",
      "Trained batch 1291 batch loss 5.86716413 epoch total loss 6.3687396\n",
      "Trained batch 1292 batch loss 5.52321911 epoch total loss 6.36808538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1293 batch loss 5.97021484 epoch total loss 6.3677783\n",
      "Trained batch 1294 batch loss 6.21384954 epoch total loss 6.36765909\n",
      "Trained batch 1295 batch loss 6.38851 epoch total loss 6.3676753\n",
      "Trained batch 1296 batch loss 6.46550512 epoch total loss 6.36775112\n",
      "Trained batch 1297 batch loss 6.50036097 epoch total loss 6.36785316\n",
      "Trained batch 1298 batch loss 6.54024029 epoch total loss 6.36798573\n",
      "Trained batch 1299 batch loss 6.7021656 epoch total loss 6.36824322\n",
      "Trained batch 1300 batch loss 6.57734394 epoch total loss 6.36840391\n",
      "Trained batch 1301 batch loss 6.56222296 epoch total loss 6.36855268\n",
      "Trained batch 1302 batch loss 6.66002131 epoch total loss 6.3687768\n",
      "Trained batch 1303 batch loss 6.0301857 epoch total loss 6.36851692\n",
      "Trained batch 1304 batch loss 5.91968775 epoch total loss 6.36817312\n",
      "Trained batch 1305 batch loss 6.47604799 epoch total loss 6.36825514\n",
      "Trained batch 1306 batch loss 6.49695921 epoch total loss 6.36835384\n",
      "Trained batch 1307 batch loss 6.26059723 epoch total loss 6.36827183\n",
      "Trained batch 1308 batch loss 6.39541769 epoch total loss 6.36829233\n",
      "Trained batch 1309 batch loss 6.30622244 epoch total loss 6.3682456\n",
      "Trained batch 1310 batch loss 6.65462589 epoch total loss 6.36846352\n",
      "Trained batch 1311 batch loss 6.65773773 epoch total loss 6.36868477\n",
      "Trained batch 1312 batch loss 6.75342894 epoch total loss 6.3689785\n",
      "Trained batch 1313 batch loss 6.23978424 epoch total loss 6.36888027\n",
      "Trained batch 1314 batch loss 6.4938612 epoch total loss 6.36897564\n",
      "Trained batch 1315 batch loss 6.25750399 epoch total loss 6.36889124\n",
      "Trained batch 1316 batch loss 6.13307571 epoch total loss 6.36871147\n",
      "Trained batch 1317 batch loss 6.06399107 epoch total loss 6.36848068\n",
      "Trained batch 1318 batch loss 5.93382597 epoch total loss 6.36815071\n",
      "Trained batch 1319 batch loss 6.19442368 epoch total loss 6.3680191\n",
      "Trained batch 1320 batch loss 6.39678955 epoch total loss 6.36804056\n",
      "Trained batch 1321 batch loss 6.06414557 epoch total loss 6.36781073\n",
      "Trained batch 1322 batch loss 6.30017519 epoch total loss 6.36775923\n",
      "Trained batch 1323 batch loss 6.63773537 epoch total loss 6.36796331\n",
      "Trained batch 1324 batch loss 6.76314831 epoch total loss 6.36826134\n",
      "Trained batch 1325 batch loss 6.90457535 epoch total loss 6.36866617\n",
      "Trained batch 1326 batch loss 6.69576693 epoch total loss 6.36891222\n",
      "Trained batch 1327 batch loss 6.47114 epoch total loss 6.36898899\n",
      "Trained batch 1328 batch loss 6.37063456 epoch total loss 6.36899042\n",
      "Trained batch 1329 batch loss 6.38363552 epoch total loss 6.36900187\n",
      "Trained batch 1330 batch loss 6.5098815 epoch total loss 6.36910772\n",
      "Trained batch 1331 batch loss 6.52910376 epoch total loss 6.36922789\n",
      "Trained batch 1332 batch loss 7.0805912 epoch total loss 6.36976242\n",
      "Trained batch 1333 batch loss 7.03676319 epoch total loss 6.3702631\n",
      "Trained batch 1334 batch loss 6.8067441 epoch total loss 6.37059\n",
      "Trained batch 1335 batch loss 6.70602846 epoch total loss 6.3708415\n",
      "Trained batch 1336 batch loss 6.64393616 epoch total loss 6.37104559\n",
      "Trained batch 1337 batch loss 6.26208258 epoch total loss 6.37096357\n",
      "Trained batch 1338 batch loss 6.06176853 epoch total loss 6.37073231\n",
      "Trained batch 1339 batch loss 6.77356243 epoch total loss 6.37103319\n",
      "Trained batch 1340 batch loss 6.3487978 epoch total loss 6.3710165\n",
      "Trained batch 1341 batch loss 6.53252316 epoch total loss 6.37113667\n",
      "Trained batch 1342 batch loss 6.6516695 epoch total loss 6.37134552\n",
      "Trained batch 1343 batch loss 6.36816835 epoch total loss 6.37134314\n",
      "Trained batch 1344 batch loss 6.34982 epoch total loss 6.37132692\n",
      "Trained batch 1345 batch loss 6.52003574 epoch total loss 6.37143803\n",
      "Trained batch 1346 batch loss 6.08041716 epoch total loss 6.37122154\n",
      "Trained batch 1347 batch loss 5.87256098 epoch total loss 6.37085152\n",
      "Trained batch 1348 batch loss 5.45687056 epoch total loss 6.37017393\n",
      "Trained batch 1349 batch loss 5.3562746 epoch total loss 6.36942244\n",
      "Trained batch 1350 batch loss 5.79759836 epoch total loss 6.368999\n",
      "Trained batch 1351 batch loss 6.06609058 epoch total loss 6.36877489\n",
      "Trained batch 1352 batch loss 6.28444576 epoch total loss 6.36871243\n",
      "Trained batch 1353 batch loss 6.83914757 epoch total loss 6.36905956\n",
      "Trained batch 1354 batch loss 6.80983543 epoch total loss 6.36938524\n",
      "Trained batch 1355 batch loss 6.73839474 epoch total loss 6.36965752\n",
      "Trained batch 1356 batch loss 7.32305765 epoch total loss 6.37036037\n",
      "Trained batch 1357 batch loss 6.40312624 epoch total loss 6.37038469\n",
      "Trained batch 1358 batch loss 6.76612854 epoch total loss 6.37067652\n",
      "Trained batch 1359 batch loss 6.56992674 epoch total loss 6.37082338\n",
      "Trained batch 1360 batch loss 6.5853343 epoch total loss 6.37098122\n",
      "Trained batch 1361 batch loss 6.7367487 epoch total loss 6.37124968\n",
      "Trained batch 1362 batch loss 6.60690403 epoch total loss 6.37142229\n",
      "Trained batch 1363 batch loss 6.50573301 epoch total loss 6.371521\n",
      "Trained batch 1364 batch loss 6.41790915 epoch total loss 6.37155485\n",
      "Trained batch 1365 batch loss 6.05270147 epoch total loss 6.3713212\n",
      "Trained batch 1366 batch loss 5.42143059 epoch total loss 6.37062597\n",
      "Trained batch 1367 batch loss 5.786798 epoch total loss 6.3701992\n",
      "Trained batch 1368 batch loss 5.95388556 epoch total loss 6.36989498\n",
      "Trained batch 1369 batch loss 6.1045084 epoch total loss 6.36970139\n",
      "Trained batch 1370 batch loss 6.6083889 epoch total loss 6.36987543\n",
      "Trained batch 1371 batch loss 6.38484383 epoch total loss 6.3698864\n",
      "Trained batch 1372 batch loss 6.42910099 epoch total loss 6.36992931\n",
      "Trained batch 1373 batch loss 5.85795355 epoch total loss 6.3695569\n",
      "Trained batch 1374 batch loss 5.90026522 epoch total loss 6.36921549\n",
      "Trained batch 1375 batch loss 6.25016 epoch total loss 6.3691287\n",
      "Trained batch 1376 batch loss 6.50733709 epoch total loss 6.36922932\n",
      "Trained batch 1377 batch loss 6.67365837 epoch total loss 6.36945057\n",
      "Trained batch 1378 batch loss 6.42142677 epoch total loss 6.36948872\n",
      "Trained batch 1379 batch loss 6.21054411 epoch total loss 6.3693738\n",
      "Trained batch 1380 batch loss 6.13551188 epoch total loss 6.36920452\n",
      "Trained batch 1381 batch loss 6.73341703 epoch total loss 6.36946821\n",
      "Trained batch 1382 batch loss 6.5637269 epoch total loss 6.3696084\n",
      "Trained batch 1383 batch loss 6.86521101 epoch total loss 6.36996698\n",
      "Trained batch 1384 batch loss 6.40270901 epoch total loss 6.36999035\n",
      "Trained batch 1385 batch loss 6.35493183 epoch total loss 6.3699789\n",
      "Trained batch 1386 batch loss 6.33523655 epoch total loss 6.36995363\n",
      "Trained batch 1387 batch loss 6.16653633 epoch total loss 6.36980724\n",
      "Trained batch 1388 batch loss 6.28947973 epoch total loss 6.36974907\n",
      "Epoch 2 train loss 6.369749069213867\n",
      "Validated batch 1 batch loss 6.53607559\n",
      "Validated batch 2 batch loss 6.63422966\n",
      "Validated batch 3 batch loss 6.14749289\n",
      "Validated batch 4 batch loss 5.7399354\n",
      "Validated batch 5 batch loss 6.24043798\n",
      "Validated batch 6 batch loss 6.27592707\n",
      "Validated batch 7 batch loss 5.90297508\n",
      "Validated batch 8 batch loss 5.99082804\n",
      "Validated batch 9 batch loss 6.27210665\n",
      "Validated batch 10 batch loss 6.29854965\n",
      "Validated batch 11 batch loss 6.09070492\n",
      "Validated batch 12 batch loss 5.86561298\n",
      "Validated batch 13 batch loss 6.18602705\n",
      "Validated batch 14 batch loss 6.18923044\n",
      "Validated batch 15 batch loss 6.22786379\n",
      "Validated batch 16 batch loss 6.24238539\n",
      "Validated batch 17 batch loss 6.26561069\n",
      "Validated batch 18 batch loss 6.49982309\n",
      "Validated batch 19 batch loss 6.27183247\n",
      "Validated batch 20 batch loss 6.10204411\n",
      "Validated batch 21 batch loss 6.53795052\n",
      "Validated batch 22 batch loss 5.39582634\n",
      "Validated batch 23 batch loss 6.72869682\n",
      "Validated batch 24 batch loss 6.33150578\n",
      "Validated batch 25 batch loss 6.403018\n",
      "Validated batch 26 batch loss 5.97421217\n",
      "Validated batch 27 batch loss 6.16560221\n",
      "Validated batch 28 batch loss 6.46301794\n",
      "Validated batch 29 batch loss 6.29051399\n",
      "Validated batch 30 batch loss 6.37401676\n",
      "Validated batch 31 batch loss 6.22508812\n",
      "Validated batch 32 batch loss 6.14819\n",
      "Validated batch 33 batch loss 6.65926123\n",
      "Validated batch 34 batch loss 6.68387938\n",
      "Validated batch 35 batch loss 6.40144587\n",
      "Validated batch 36 batch loss 6.41187429\n",
      "Validated batch 37 batch loss 6.22410488\n",
      "Validated batch 38 batch loss 6.38955688\n",
      "Validated batch 39 batch loss 6.30434513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 40 batch loss 6.35591\n",
      "Validated batch 41 batch loss 6.40416288\n",
      "Validated batch 42 batch loss 6.15048313\n",
      "Validated batch 43 batch loss 6.4275074\n",
      "Validated batch 44 batch loss 5.9737525\n",
      "Validated batch 45 batch loss 6.54948282\n",
      "Validated batch 46 batch loss 6.60977602\n",
      "Validated batch 47 batch loss 5.85377216\n",
      "Validated batch 48 batch loss 6.6917367\n",
      "Validated batch 49 batch loss 6.34179831\n",
      "Validated batch 50 batch loss 6.53554773\n",
      "Validated batch 51 batch loss 6.38359737\n",
      "Validated batch 52 batch loss 6.69207144\n",
      "Validated batch 53 batch loss 6.16965961\n",
      "Validated batch 54 batch loss 6.70266533\n",
      "Validated batch 55 batch loss 6.0956707\n",
      "Validated batch 56 batch loss 6.39698839\n",
      "Validated batch 57 batch loss 6.40013313\n",
      "Validated batch 58 batch loss 5.80464172\n",
      "Validated batch 59 batch loss 5.733428\n",
      "Validated batch 60 batch loss 6.52589226\n",
      "Validated batch 61 batch loss 6.48918533\n",
      "Validated batch 62 batch loss 6.04877234\n",
      "Validated batch 63 batch loss 6.50506783\n",
      "Validated batch 64 batch loss 6.52941036\n",
      "Validated batch 65 batch loss 6.43492794\n",
      "Validated batch 66 batch loss 6.59439945\n",
      "Validated batch 67 batch loss 6.53433\n",
      "Validated batch 68 batch loss 6.22591829\n",
      "Validated batch 69 batch loss 5.92886\n",
      "Validated batch 70 batch loss 6.70250416\n",
      "Validated batch 71 batch loss 6.38782501\n",
      "Validated batch 72 batch loss 6.4427247\n",
      "Validated batch 73 batch loss 6.32660294\n",
      "Validated batch 74 batch loss 6.34210682\n",
      "Validated batch 75 batch loss 6.62406826\n",
      "Validated batch 76 batch loss 6.25959539\n",
      "Validated batch 77 batch loss 5.99392414\n",
      "Validated batch 78 batch loss 6.11629725\n",
      "Validated batch 79 batch loss 6.21156263\n",
      "Validated batch 80 batch loss 6.47351885\n",
      "Validated batch 81 batch loss 6.14216375\n",
      "Validated batch 82 batch loss 6.06820726\n",
      "Validated batch 83 batch loss 5.98235655\n",
      "Validated batch 84 batch loss 6.28649139\n",
      "Validated batch 85 batch loss 6.46239185\n",
      "Validated batch 86 batch loss 6.53996277\n",
      "Validated batch 87 batch loss 6.50965452\n",
      "Validated batch 88 batch loss 6.90785217\n",
      "Validated batch 89 batch loss 7.20753288\n",
      "Validated batch 90 batch loss 6.82398701\n",
      "Validated batch 91 batch loss 6.3088913\n",
      "Validated batch 92 batch loss 6.01433086\n",
      "Validated batch 93 batch loss 6.45643044\n",
      "Validated batch 94 batch loss 6.29577446\n",
      "Validated batch 95 batch loss 6.13499737\n",
      "Validated batch 96 batch loss 6.02914238\n",
      "Validated batch 97 batch loss 6.11033916\n",
      "Validated batch 98 batch loss 6.43778944\n",
      "Validated batch 99 batch loss 6.03550816\n",
      "Validated batch 100 batch loss 6.33737755\n",
      "Validated batch 101 batch loss 6.42221928\n",
      "Validated batch 102 batch loss 6.36558342\n",
      "Validated batch 103 batch loss 6.16202641\n",
      "Validated batch 104 batch loss 5.86136436\n",
      "Validated batch 105 batch loss 6.36886644\n",
      "Validated batch 106 batch loss 6.54981136\n",
      "Validated batch 107 batch loss 6.20703173\n",
      "Validated batch 108 batch loss 6.27621317\n",
      "Validated batch 109 batch loss 6.24599838\n",
      "Validated batch 110 batch loss 5.80629921\n",
      "Validated batch 111 batch loss 5.8527441\n",
      "Validated batch 112 batch loss 6.34544754\n",
      "Validated batch 113 batch loss 5.92075968\n",
      "Validated batch 114 batch loss 6.16258049\n",
      "Validated batch 115 batch loss 6.2844739\n",
      "Validated batch 116 batch loss 6.37836695\n",
      "Validated batch 117 batch loss 6.40374088\n",
      "Validated batch 118 batch loss 6.09971333\n",
      "Validated batch 119 batch loss 6.65595293\n",
      "Validated batch 120 batch loss 6.72278214\n",
      "Validated batch 121 batch loss 6.43235254\n",
      "Validated batch 122 batch loss 6.45735\n",
      "Validated batch 123 batch loss 6.50348043\n",
      "Validated batch 124 batch loss 6.52904463\n",
      "Validated batch 125 batch loss 6.41360712\n",
      "Validated batch 126 batch loss 6.82242537\n",
      "Validated batch 127 batch loss 6.79631376\n",
      "Validated batch 128 batch loss 5.97708511\n",
      "Validated batch 129 batch loss 6.620296\n",
      "Validated batch 130 batch loss 6.23148108\n",
      "Validated batch 131 batch loss 6.4596858\n",
      "Validated batch 132 batch loss 6.57623672\n",
      "Validated batch 133 batch loss 5.84220457\n",
      "Validated batch 134 batch loss 6.00794649\n",
      "Validated batch 135 batch loss 6.4593854\n",
      "Validated batch 136 batch loss 6.19726133\n",
      "Validated batch 137 batch loss 6.88875\n",
      "Validated batch 138 batch loss 6.17396498\n",
      "Validated batch 139 batch loss 6.19637346\n",
      "Validated batch 140 batch loss 6.05610418\n",
      "Validated batch 141 batch loss 6.28680182\n",
      "Validated batch 142 batch loss 6.2921629\n",
      "Validated batch 143 batch loss 5.95042324\n",
      "Validated batch 144 batch loss 6.68907499\n",
      "Validated batch 145 batch loss 6.40764618\n",
      "Validated batch 146 batch loss 6.29056931\n",
      "Validated batch 147 batch loss 6.46412468\n",
      "Validated batch 148 batch loss 6.54217434\n",
      "Validated batch 149 batch loss 6.29708767\n",
      "Validated batch 150 batch loss 6.29295588\n",
      "Validated batch 151 batch loss 6.20462799\n",
      "Validated batch 152 batch loss 6.52036333\n",
      "Validated batch 153 batch loss 6.64637232\n",
      "Validated batch 154 batch loss 6.41499043\n",
      "Validated batch 155 batch loss 6.11996365\n",
      "Validated batch 156 batch loss 5.94514179\n",
      "Validated batch 157 batch loss 6.30235386\n",
      "Validated batch 158 batch loss 6.35194778\n",
      "Validated batch 159 batch loss 6.41270494\n",
      "Validated batch 160 batch loss 6.44862\n",
      "Validated batch 161 batch loss 6.20570421\n",
      "Validated batch 162 batch loss 6.20471334\n",
      "Validated batch 163 batch loss 6.38069868\n",
      "Validated batch 164 batch loss 6.15418196\n",
      "Validated batch 165 batch loss 5.55031729\n",
      "Validated batch 166 batch loss 6.25124264\n",
      "Validated batch 167 batch loss 6.63158369\n",
      "Validated batch 168 batch loss 5.93676805\n",
      "Validated batch 169 batch loss 6.26919031\n",
      "Validated batch 170 batch loss 6.37920094\n",
      "Validated batch 171 batch loss 6.45554686\n",
      "Validated batch 172 batch loss 6.64405966\n",
      "Validated batch 173 batch loss 6.39228058\n",
      "Validated batch 174 batch loss 6.02268\n",
      "Validated batch 175 batch loss 6.44430304\n",
      "Validated batch 176 batch loss 6.48271084\n",
      "Validated batch 177 batch loss 6.77275276\n",
      "Validated batch 178 batch loss 6.35653877\n",
      "Validated batch 179 batch loss 6.52551317\n",
      "Validated batch 180 batch loss 6.43194485\n",
      "Validated batch 181 batch loss 6.03642\n",
      "Validated batch 182 batch loss 6.33872747\n",
      "Validated batch 183 batch loss 6.24680901\n",
      "Validated batch 184 batch loss 6.65333319\n",
      "Validated batch 185 batch loss 3.31559896\n",
      "Epoch 2 val loss 6.301888942718506\n",
      "Model .//simplebaseline_model-epoch-2-loss-6.3019.h5 saved.\n",
      "Start epoch 3 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 6.3815484 epoch total loss 6.3815484\n",
      "Trained batch 2 batch loss 6.56435156 epoch total loss 6.47295\n",
      "Trained batch 3 batch loss 6.36126804 epoch total loss 6.43572235\n",
      "Trained batch 4 batch loss 6.45703125 epoch total loss 6.44104958\n",
      "Trained batch 5 batch loss 6.52577257 epoch total loss 6.45799398\n",
      "Trained batch 6 batch loss 6.80402 epoch total loss 6.51566505\n",
      "Trained batch 7 batch loss 6.08656836 epoch total loss 6.45436525\n",
      "Trained batch 8 batch loss 6.35538816 epoch total loss 6.44199324\n",
      "Trained batch 9 batch loss 6.73661947 epoch total loss 6.47472954\n",
      "Trained batch 10 batch loss 6.63930559 epoch total loss 6.4911871\n",
      "Trained batch 11 batch loss 6.35735321 epoch total loss 6.4790206\n",
      "Trained batch 12 batch loss 6.12977171 epoch total loss 6.44991636\n",
      "Trained batch 13 batch loss 6.01863575 epoch total loss 6.41674089\n",
      "Trained batch 14 batch loss 6.34182072 epoch total loss 6.41138935\n",
      "Trained batch 15 batch loss 6.58696413 epoch total loss 6.42309475\n",
      "Trained batch 16 batch loss 6.27676058 epoch total loss 6.41394901\n",
      "Trained batch 17 batch loss 6.18281841 epoch total loss 6.40035295\n",
      "Trained batch 18 batch loss 6.33633137 epoch total loss 6.39679623\n",
      "Trained batch 19 batch loss 6.40683556 epoch total loss 6.39732504\n",
      "Trained batch 20 batch loss 6.28042412 epoch total loss 6.39148\n",
      "Trained batch 21 batch loss 6.45139933 epoch total loss 6.39433289\n",
      "Trained batch 22 batch loss 6.09919071 epoch total loss 6.38091755\n",
      "Trained batch 23 batch loss 6.55626774 epoch total loss 6.3885417\n",
      "Trained batch 24 batch loss 6.30702209 epoch total loss 6.38514519\n",
      "Trained batch 25 batch loss 6.17082 epoch total loss 6.37657213\n",
      "Trained batch 26 batch loss 6.1863966 epoch total loss 6.36925793\n",
      "Trained batch 27 batch loss 6.29555511 epoch total loss 6.36652851\n",
      "Trained batch 28 batch loss 6.56556463 epoch total loss 6.3736372\n",
      "Trained batch 29 batch loss 6.76865721 epoch total loss 6.38725853\n",
      "Trained batch 30 batch loss 6.51138878 epoch total loss 6.39139605\n",
      "Trained batch 31 batch loss 6.63787699 epoch total loss 6.39934731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 32 batch loss 6.69036865 epoch total loss 6.40844154\n",
      "Trained batch 33 batch loss 6.35343695 epoch total loss 6.406775\n",
      "Trained batch 34 batch loss 6.5071 epoch total loss 6.40972519\n",
      "Trained batch 35 batch loss 6.73268461 epoch total loss 6.41895294\n",
      "Trained batch 36 batch loss 6.45219707 epoch total loss 6.4198761\n",
      "Trained batch 37 batch loss 6.32258415 epoch total loss 6.41724682\n",
      "Trained batch 38 batch loss 6.7828908 epoch total loss 6.42686892\n",
      "Trained batch 39 batch loss 6.44552517 epoch total loss 6.42734766\n",
      "Trained batch 40 batch loss 6.15968227 epoch total loss 6.42065573\n",
      "Trained batch 41 batch loss 6.54828453 epoch total loss 6.42376852\n",
      "Trained batch 42 batch loss 6.582304 epoch total loss 6.42754316\n",
      "Trained batch 43 batch loss 6.89204645 epoch total loss 6.43834591\n",
      "Trained batch 44 batch loss 6.61080551 epoch total loss 6.44226551\n",
      "Trained batch 45 batch loss 6.58613968 epoch total loss 6.44546318\n",
      "Trained batch 46 batch loss 6.28165531 epoch total loss 6.44190168\n",
      "Trained batch 47 batch loss 6.41936159 epoch total loss 6.44142246\n",
      "Trained batch 48 batch loss 6.30462551 epoch total loss 6.43857241\n",
      "Trained batch 49 batch loss 6.28938293 epoch total loss 6.43552828\n",
      "Trained batch 50 batch loss 6.45555592 epoch total loss 6.43592882\n",
      "Trained batch 51 batch loss 6.46505499 epoch total loss 6.4365\n",
      "Trained batch 52 batch loss 6.12928915 epoch total loss 6.43059254\n",
      "Trained batch 53 batch loss 6.47017288 epoch total loss 6.43133926\n",
      "Trained batch 54 batch loss 6.51002598 epoch total loss 6.43279696\n",
      "Trained batch 55 batch loss 6.31019354 epoch total loss 6.43056774\n",
      "Trained batch 56 batch loss 6.34646845 epoch total loss 6.4290657\n",
      "Trained batch 57 batch loss 6.46673822 epoch total loss 6.4297266\n",
      "Trained batch 58 batch loss 6.54504061 epoch total loss 6.43171501\n",
      "Trained batch 59 batch loss 6.64016342 epoch total loss 6.4352479\n",
      "Trained batch 60 batch loss 6.79381132 epoch total loss 6.4412241\n",
      "Trained batch 61 batch loss 6.48475075 epoch total loss 6.44193745\n",
      "Trained batch 62 batch loss 6.56821156 epoch total loss 6.44397402\n",
      "Trained batch 63 batch loss 6.62898827 epoch total loss 6.44691086\n",
      "Trained batch 64 batch loss 6.53504753 epoch total loss 6.44828796\n",
      "Trained batch 65 batch loss 6.65078306 epoch total loss 6.45140314\n",
      "Trained batch 66 batch loss 6.29767513 epoch total loss 6.44907379\n",
      "Trained batch 67 batch loss 6.18677139 epoch total loss 6.44515896\n",
      "Trained batch 68 batch loss 6.05168295 epoch total loss 6.43937302\n",
      "Trained batch 69 batch loss 6.25946569 epoch total loss 6.43676519\n",
      "Trained batch 70 batch loss 6.19754171 epoch total loss 6.4333477\n",
      "Trained batch 71 batch loss 6.39932 epoch total loss 6.43286848\n",
      "Trained batch 72 batch loss 6.59155464 epoch total loss 6.43507242\n",
      "Trained batch 73 batch loss 6.53538179 epoch total loss 6.43644667\n",
      "Trained batch 74 batch loss 6.07448816 epoch total loss 6.43155527\n",
      "Trained batch 75 batch loss 6.04076624 epoch total loss 6.42634487\n",
      "Trained batch 76 batch loss 6.49331 epoch total loss 6.42722607\n",
      "Trained batch 77 batch loss 6.39158964 epoch total loss 6.42676353\n",
      "Trained batch 78 batch loss 6.08346081 epoch total loss 6.42236233\n",
      "Trained batch 79 batch loss 6.06955 epoch total loss 6.41789627\n",
      "Trained batch 80 batch loss 6.27124929 epoch total loss 6.41606283\n",
      "Trained batch 81 batch loss 6.14626884 epoch total loss 6.41273165\n",
      "Trained batch 82 batch loss 6.40200663 epoch total loss 6.41260052\n",
      "Trained batch 83 batch loss 6.38179874 epoch total loss 6.41222906\n",
      "Trained batch 84 batch loss 6.17331362 epoch total loss 6.4093852\n",
      "Trained batch 85 batch loss 6.54895353 epoch total loss 6.41102743\n",
      "Trained batch 86 batch loss 6.2330265 epoch total loss 6.40895748\n",
      "Trained batch 87 batch loss 5.96619225 epoch total loss 6.4038682\n",
      "Trained batch 88 batch loss 6.39664888 epoch total loss 6.40378618\n",
      "Trained batch 89 batch loss 6.32881546 epoch total loss 6.40294361\n",
      "Trained batch 90 batch loss 6.64792776 epoch total loss 6.40566587\n",
      "Trained batch 91 batch loss 6.58753443 epoch total loss 6.40766478\n",
      "Trained batch 92 batch loss 6.56293344 epoch total loss 6.4093523\n",
      "Trained batch 93 batch loss 6.44286203 epoch total loss 6.40971279\n",
      "Trained batch 94 batch loss 6.64939117 epoch total loss 6.41226244\n",
      "Trained batch 95 batch loss 6.31780624 epoch total loss 6.41126823\n",
      "Trained batch 96 batch loss 6.60520744 epoch total loss 6.41328859\n",
      "Trained batch 97 batch loss 6.74071598 epoch total loss 6.41666412\n",
      "Trained batch 98 batch loss 6.20208454 epoch total loss 6.41447496\n",
      "Trained batch 99 batch loss 5.92788219 epoch total loss 6.40955925\n",
      "Trained batch 100 batch loss 6.28105497 epoch total loss 6.40827465\n",
      "Trained batch 101 batch loss 6.64434242 epoch total loss 6.41061211\n",
      "Trained batch 102 batch loss 6.50604105 epoch total loss 6.41154766\n",
      "Trained batch 103 batch loss 6.41563082 epoch total loss 6.41158724\n",
      "Trained batch 104 batch loss 6.54982376 epoch total loss 6.41291618\n",
      "Trained batch 105 batch loss 6.75410128 epoch total loss 6.41616583\n",
      "Trained batch 106 batch loss 6.6806221 epoch total loss 6.41866\n",
      "Trained batch 107 batch loss 6.5380168 epoch total loss 6.41977596\n",
      "Trained batch 108 batch loss 6.40880537 epoch total loss 6.4196744\n",
      "Trained batch 109 batch loss 6.71216583 epoch total loss 6.42235756\n",
      "Trained batch 110 batch loss 6.6582284 epoch total loss 6.4245019\n",
      "Trained batch 111 batch loss 6.90515137 epoch total loss 6.42883205\n",
      "Trained batch 112 batch loss 6.74640417 epoch total loss 6.43166733\n",
      "Trained batch 113 batch loss 6.58669662 epoch total loss 6.43303919\n",
      "Trained batch 114 batch loss 6.36131477 epoch total loss 6.43241\n",
      "Trained batch 115 batch loss 6.515553 epoch total loss 6.43313313\n",
      "Trained batch 116 batch loss 6.42688942 epoch total loss 6.43307924\n",
      "Trained batch 117 batch loss 6.2696 epoch total loss 6.43168163\n",
      "Trained batch 118 batch loss 6.16215706 epoch total loss 6.42939806\n",
      "Trained batch 119 batch loss 6.43649149 epoch total loss 6.42945719\n",
      "Trained batch 120 batch loss 6.193717 epoch total loss 6.42749262\n",
      "Trained batch 121 batch loss 6.38729191 epoch total loss 6.42716026\n",
      "Trained batch 122 batch loss 6.09603643 epoch total loss 6.42444611\n",
      "Trained batch 123 batch loss 6.27604246 epoch total loss 6.42323971\n",
      "Trained batch 124 batch loss 6.1713872 epoch total loss 6.42120838\n",
      "Trained batch 125 batch loss 6.1900816 epoch total loss 6.41935921\n",
      "Trained batch 126 batch loss 6.2614665 epoch total loss 6.41810656\n",
      "Trained batch 127 batch loss 6.16546774 epoch total loss 6.41611719\n",
      "Trained batch 128 batch loss 5.84841967 epoch total loss 6.41168213\n",
      "Trained batch 129 batch loss 5.93931 epoch total loss 6.4080205\n",
      "Trained batch 130 batch loss 6.11436272 epoch total loss 6.40576172\n",
      "Trained batch 131 batch loss 6.28183365 epoch total loss 6.40481567\n",
      "Trained batch 132 batch loss 6.39643621 epoch total loss 6.40475225\n",
      "Trained batch 133 batch loss 6.14192677 epoch total loss 6.40277624\n",
      "Trained batch 134 batch loss 5.59274721 epoch total loss 6.39673138\n",
      "Trained batch 135 batch loss 5.55488062 epoch total loss 6.3904953\n",
      "Trained batch 136 batch loss 6.07555437 epoch total loss 6.38818\n",
      "Trained batch 137 batch loss 5.90383196 epoch total loss 6.38464403\n",
      "Trained batch 138 batch loss 6.52041435 epoch total loss 6.38562775\n",
      "Trained batch 139 batch loss 6.25662327 epoch total loss 6.3847\n",
      "Trained batch 140 batch loss 6.60536575 epoch total loss 6.38627577\n",
      "Trained batch 141 batch loss 6.35221434 epoch total loss 6.38603449\n",
      "Trained batch 142 batch loss 6.6483779 epoch total loss 6.38788176\n",
      "Trained batch 143 batch loss 6.55039644 epoch total loss 6.38901854\n",
      "Trained batch 144 batch loss 6.35958672 epoch total loss 6.38881397\n",
      "Trained batch 145 batch loss 6.40387 epoch total loss 6.38891792\n",
      "Trained batch 146 batch loss 6.2821784 epoch total loss 6.38818645\n",
      "Trained batch 147 batch loss 6.13539362 epoch total loss 6.3864665\n",
      "Trained batch 148 batch loss 6.08281851 epoch total loss 6.38441515\n",
      "Trained batch 149 batch loss 6.28988886 epoch total loss 6.38378096\n",
      "Trained batch 150 batch loss 6.04557037 epoch total loss 6.38152647\n",
      "Trained batch 151 batch loss 5.71980572 epoch total loss 6.37714386\n",
      "Trained batch 152 batch loss 6.07525635 epoch total loss 6.37515783\n",
      "Trained batch 153 batch loss 6.07072163 epoch total loss 6.37316799\n",
      "Trained batch 154 batch loss 6.41239262 epoch total loss 6.3734231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 155 batch loss 6.52044392 epoch total loss 6.37437153\n",
      "Trained batch 156 batch loss 6.48971415 epoch total loss 6.37511063\n",
      "Trained batch 157 batch loss 6.73181391 epoch total loss 6.37738276\n",
      "Trained batch 158 batch loss 6.26518154 epoch total loss 6.37667274\n",
      "Trained batch 159 batch loss 6.42134619 epoch total loss 6.3769536\n",
      "Trained batch 160 batch loss 6.85702324 epoch total loss 6.37995386\n",
      "Trained batch 161 batch loss 6.19256783 epoch total loss 6.37879038\n",
      "Trained batch 162 batch loss 6.15209246 epoch total loss 6.37739086\n",
      "Trained batch 163 batch loss 6.38081837 epoch total loss 6.37741232\n",
      "Trained batch 164 batch loss 6.00003 epoch total loss 6.3751111\n",
      "Trained batch 165 batch loss 5.88649559 epoch total loss 6.37214947\n",
      "Trained batch 166 batch loss 5.74041367 epoch total loss 6.36834335\n",
      "Trained batch 167 batch loss 6.4967804 epoch total loss 6.36911297\n",
      "Trained batch 168 batch loss 6.36649609 epoch total loss 6.36909723\n",
      "Trained batch 169 batch loss 6.14867353 epoch total loss 6.36779261\n",
      "Trained batch 170 batch loss 6.49392 epoch total loss 6.36853456\n",
      "Trained batch 171 batch loss 6.02767467 epoch total loss 6.36654139\n",
      "Trained batch 172 batch loss 6.04439831 epoch total loss 6.36466885\n",
      "Trained batch 173 batch loss 6.55740499 epoch total loss 6.36578274\n",
      "Trained batch 174 batch loss 6.58771229 epoch total loss 6.36705828\n",
      "Trained batch 175 batch loss 6.52075338 epoch total loss 6.36793661\n",
      "Trained batch 176 batch loss 6.1583972 epoch total loss 6.36674643\n",
      "Trained batch 177 batch loss 6.09357786 epoch total loss 6.36520338\n",
      "Trained batch 178 batch loss 6.35951471 epoch total loss 6.36517143\n",
      "Trained batch 179 batch loss 6.36903763 epoch total loss 6.36519289\n",
      "Trained batch 180 batch loss 6.36417389 epoch total loss 6.36518669\n",
      "Trained batch 181 batch loss 6.26215744 epoch total loss 6.36461782\n",
      "Trained batch 182 batch loss 6.42788076 epoch total loss 6.36496544\n",
      "Trained batch 183 batch loss 6.53589249 epoch total loss 6.36589956\n",
      "Trained batch 184 batch loss 5.81224871 epoch total loss 6.36289072\n",
      "Trained batch 185 batch loss 6.56508684 epoch total loss 6.36398315\n",
      "Trained batch 186 batch loss 6.63287258 epoch total loss 6.36542845\n",
      "Trained batch 187 batch loss 6.75459862 epoch total loss 6.36751\n",
      "Trained batch 188 batch loss 6.84658909 epoch total loss 6.37005806\n",
      "Trained batch 189 batch loss 6.74994326 epoch total loss 6.37206841\n",
      "Trained batch 190 batch loss 6.29221392 epoch total loss 6.37164831\n",
      "Trained batch 191 batch loss 5.50211239 epoch total loss 6.36709547\n",
      "Trained batch 192 batch loss 6.03745937 epoch total loss 6.36537886\n",
      "Trained batch 193 batch loss 6.41116619 epoch total loss 6.36561584\n",
      "Trained batch 194 batch loss 6.65932465 epoch total loss 6.36713\n",
      "Trained batch 195 batch loss 6.01086187 epoch total loss 6.36530256\n",
      "Trained batch 196 batch loss 5.48390198 epoch total loss 6.36080551\n",
      "Trained batch 197 batch loss 5.6531992 epoch total loss 6.3572135\n",
      "Trained batch 198 batch loss 6.53009844 epoch total loss 6.35808706\n",
      "Trained batch 199 batch loss 6.37770271 epoch total loss 6.35818577\n",
      "Trained batch 200 batch loss 6.89016533 epoch total loss 6.36084557\n",
      "Trained batch 201 batch loss 6.60075283 epoch total loss 6.36203861\n",
      "Trained batch 202 batch loss 6.75546646 epoch total loss 6.36398649\n",
      "Trained batch 203 batch loss 6.71737576 epoch total loss 6.36572742\n",
      "Trained batch 204 batch loss 6.51844406 epoch total loss 6.36647606\n",
      "Trained batch 205 batch loss 6.84811974 epoch total loss 6.36882544\n",
      "Trained batch 206 batch loss 6.80478668 epoch total loss 6.37094212\n",
      "Trained batch 207 batch loss 6.74900866 epoch total loss 6.3727684\n",
      "Trained batch 208 batch loss 6.39189625 epoch total loss 6.37286043\n",
      "Trained batch 209 batch loss 6.1305 epoch total loss 6.37170076\n",
      "Trained batch 210 batch loss 5.72043371 epoch total loss 6.36859941\n",
      "Trained batch 211 batch loss 6.06589603 epoch total loss 6.36716509\n",
      "Trained batch 212 batch loss 6.19016695 epoch total loss 6.36633\n",
      "Trained batch 213 batch loss 6.74814558 epoch total loss 6.36812258\n",
      "Trained batch 214 batch loss 6.08857489 epoch total loss 6.36681652\n",
      "Trained batch 215 batch loss 6.50652552 epoch total loss 6.36746645\n",
      "Trained batch 216 batch loss 6.3509593 epoch total loss 6.36738968\n",
      "Trained batch 217 batch loss 6.28016376 epoch total loss 6.36698771\n",
      "Trained batch 218 batch loss 6.23814631 epoch total loss 6.3663969\n",
      "Trained batch 219 batch loss 6.48556662 epoch total loss 6.36694098\n",
      "Trained batch 220 batch loss 6.66849804 epoch total loss 6.36831188\n",
      "Trained batch 221 batch loss 7.38780928 epoch total loss 6.3729248\n",
      "Trained batch 222 batch loss 7.31053066 epoch total loss 6.37714815\n",
      "Trained batch 223 batch loss 7.00979233 epoch total loss 6.37998533\n",
      "Trained batch 224 batch loss 6.97408199 epoch total loss 6.3826375\n",
      "Trained batch 225 batch loss 6.63924313 epoch total loss 6.3837781\n",
      "Trained batch 226 batch loss 5.92866802 epoch total loss 6.38176441\n",
      "Trained batch 227 batch loss 6.32324791 epoch total loss 6.38150692\n",
      "Trained batch 228 batch loss 6.56835 epoch total loss 6.38232613\n",
      "Trained batch 229 batch loss 6.92267704 epoch total loss 6.38468599\n",
      "Trained batch 230 batch loss 6.23559093 epoch total loss 6.38403797\n",
      "Trained batch 231 batch loss 6.16501331 epoch total loss 6.38309\n",
      "Trained batch 232 batch loss 6.01532078 epoch total loss 6.38150501\n",
      "Trained batch 233 batch loss 6.48127794 epoch total loss 6.38193321\n",
      "Trained batch 234 batch loss 6.02344418 epoch total loss 6.38040113\n",
      "Trained batch 235 batch loss 6.34814548 epoch total loss 6.38026428\n",
      "Trained batch 236 batch loss 6.11062574 epoch total loss 6.3791213\n",
      "Trained batch 237 batch loss 6.16463518 epoch total loss 6.37821674\n",
      "Trained batch 238 batch loss 6.41830111 epoch total loss 6.37838507\n",
      "Trained batch 239 batch loss 6.48637152 epoch total loss 6.37883663\n",
      "Trained batch 240 batch loss 6.22836304 epoch total loss 6.37821\n",
      "Trained batch 241 batch loss 6.40173 epoch total loss 6.37830734\n",
      "Trained batch 242 batch loss 6.1803441 epoch total loss 6.37748909\n",
      "Trained batch 243 batch loss 6.20006132 epoch total loss 6.37675905\n",
      "Trained batch 244 batch loss 6.32302094 epoch total loss 6.37653875\n",
      "Trained batch 245 batch loss 5.90658045 epoch total loss 6.37462091\n",
      "Trained batch 246 batch loss 5.9888 epoch total loss 6.37305212\n",
      "Trained batch 247 batch loss 6.18504906 epoch total loss 6.37229109\n",
      "Trained batch 248 batch loss 5.94595 epoch total loss 6.37057209\n",
      "Trained batch 249 batch loss 5.94569588 epoch total loss 6.36886549\n",
      "Trained batch 250 batch loss 6.37097549 epoch total loss 6.36887407\n",
      "Trained batch 251 batch loss 6.60578728 epoch total loss 6.36981821\n",
      "Trained batch 252 batch loss 6.31729174 epoch total loss 6.36960936\n",
      "Trained batch 253 batch loss 6.5799036 epoch total loss 6.37044096\n",
      "Trained batch 254 batch loss 6.16971207 epoch total loss 6.36965036\n",
      "Trained batch 255 batch loss 5.99724722 epoch total loss 6.36819\n",
      "Trained batch 256 batch loss 6.53726912 epoch total loss 6.36885\n",
      "Trained batch 257 batch loss 6.01472616 epoch total loss 6.36747265\n",
      "Trained batch 258 batch loss 6.29095411 epoch total loss 6.36717558\n",
      "Trained batch 259 batch loss 6.32520819 epoch total loss 6.36701345\n",
      "Trained batch 260 batch loss 6.16567707 epoch total loss 6.36623907\n",
      "Trained batch 261 batch loss 6.72375679 epoch total loss 6.36760902\n",
      "Trained batch 262 batch loss 5.84232712 epoch total loss 6.36560392\n",
      "Trained batch 263 batch loss 6.30926085 epoch total loss 6.36538935\n",
      "Trained batch 264 batch loss 5.97774506 epoch total loss 6.36392117\n",
      "Trained batch 265 batch loss 5.75566339 epoch total loss 6.36162567\n",
      "Trained batch 266 batch loss 6.33624792 epoch total loss 6.3615303\n",
      "Trained batch 267 batch loss 6.04360104 epoch total loss 6.36033964\n",
      "Trained batch 268 batch loss 5.40651321 epoch total loss 6.35678053\n",
      "Trained batch 269 batch loss 5.1102953 epoch total loss 6.3521471\n",
      "Trained batch 270 batch loss 5.50199461 epoch total loss 6.34899807\n",
      "Trained batch 271 batch loss 6.16926622 epoch total loss 6.34833527\n",
      "Trained batch 272 batch loss 5.50961876 epoch total loss 6.34525156\n",
      "Trained batch 273 batch loss 5.72640657 epoch total loss 6.34298515\n",
      "Trained batch 274 batch loss 6.0280633 epoch total loss 6.3418355\n",
      "Trained batch 275 batch loss 6.12592459 epoch total loss 6.34105062\n",
      "Trained batch 276 batch loss 5.88796902 epoch total loss 6.33940887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 277 batch loss 5.78692 epoch total loss 6.33741426\n",
      "Trained batch 278 batch loss 6.05123138 epoch total loss 6.33638477\n",
      "Trained batch 279 batch loss 6.41264343 epoch total loss 6.336658\n",
      "Trained batch 280 batch loss 6.6163125 epoch total loss 6.33765697\n",
      "Trained batch 281 batch loss 6.75099 epoch total loss 6.33912802\n",
      "Trained batch 282 batch loss 6.63660097 epoch total loss 6.34018278\n",
      "Trained batch 283 batch loss 6.34967899 epoch total loss 6.34021664\n",
      "Trained batch 284 batch loss 6.54189491 epoch total loss 6.34092665\n",
      "Trained batch 285 batch loss 6.70293427 epoch total loss 6.34219646\n",
      "Trained batch 286 batch loss 6.43960047 epoch total loss 6.34253693\n",
      "Trained batch 287 batch loss 6.83611441 epoch total loss 6.3442564\n",
      "Trained batch 288 batch loss 6.88023138 epoch total loss 6.3461175\n",
      "Trained batch 289 batch loss 6.54548502 epoch total loss 6.34680748\n",
      "Trained batch 290 batch loss 6.63576651 epoch total loss 6.34780407\n",
      "Trained batch 291 batch loss 6.40639 epoch total loss 6.34800529\n",
      "Trained batch 292 batch loss 6.84865236 epoch total loss 6.34971952\n",
      "Trained batch 293 batch loss 6.70466042 epoch total loss 6.35093117\n",
      "Trained batch 294 batch loss 6.66524649 epoch total loss 6.35200071\n",
      "Trained batch 295 batch loss 6.5900259 epoch total loss 6.35280704\n",
      "Trained batch 296 batch loss 6.83159256 epoch total loss 6.35442448\n",
      "Trained batch 297 batch loss 6.25816059 epoch total loss 6.3541007\n",
      "Trained batch 298 batch loss 6.51483488 epoch total loss 6.35464\n",
      "Trained batch 299 batch loss 6.50176954 epoch total loss 6.3551321\n",
      "Trained batch 300 batch loss 6.52176285 epoch total loss 6.35568714\n",
      "Trained batch 301 batch loss 6.78280067 epoch total loss 6.35710621\n",
      "Trained batch 302 batch loss 5.93147326 epoch total loss 6.35569715\n",
      "Trained batch 303 batch loss 5.413095 epoch total loss 6.35258627\n",
      "Trained batch 304 batch loss 5.89951324 epoch total loss 6.35109568\n",
      "Trained batch 305 batch loss 6.36877823 epoch total loss 6.35115385\n",
      "Trained batch 306 batch loss 6.33096838 epoch total loss 6.35108757\n",
      "Trained batch 307 batch loss 6.30943632 epoch total loss 6.35095215\n",
      "Trained batch 308 batch loss 6.52398872 epoch total loss 6.35151434\n",
      "Trained batch 309 batch loss 6.46869087 epoch total loss 6.35189342\n",
      "Trained batch 310 batch loss 6.08497143 epoch total loss 6.35103226\n",
      "Trained batch 311 batch loss 6.54013872 epoch total loss 6.3516407\n",
      "Trained batch 312 batch loss 5.71889782 epoch total loss 6.34961271\n",
      "Trained batch 313 batch loss 5.43989944 epoch total loss 6.34670639\n",
      "Trained batch 314 batch loss 6.44086599 epoch total loss 6.34700632\n",
      "Trained batch 315 batch loss 6.35862637 epoch total loss 6.34704304\n",
      "Trained batch 316 batch loss 6.35126829 epoch total loss 6.34705687\n",
      "Trained batch 317 batch loss 6.65742111 epoch total loss 6.34803581\n",
      "Trained batch 318 batch loss 6.52664757 epoch total loss 6.34859753\n",
      "Trained batch 319 batch loss 6.43249512 epoch total loss 6.34886026\n",
      "Trained batch 320 batch loss 6.25627756 epoch total loss 6.34857082\n",
      "Trained batch 321 batch loss 6.59233904 epoch total loss 6.34933043\n",
      "Trained batch 322 batch loss 6.20434856 epoch total loss 6.34888\n",
      "Trained batch 323 batch loss 6.14176941 epoch total loss 6.34823895\n",
      "Trained batch 324 batch loss 6.45219803 epoch total loss 6.34856\n",
      "Trained batch 325 batch loss 6.68731356 epoch total loss 6.34960175\n",
      "Trained batch 326 batch loss 6.68386364 epoch total loss 6.35062695\n",
      "Trained batch 327 batch loss 6.70913029 epoch total loss 6.35172367\n",
      "Trained batch 328 batch loss 6.75581264 epoch total loss 6.35295582\n",
      "Trained batch 329 batch loss 6.50883 epoch total loss 6.35342932\n",
      "Trained batch 330 batch loss 6.44268322 epoch total loss 6.35369968\n",
      "Trained batch 331 batch loss 6.48745346 epoch total loss 6.35410404\n",
      "Trained batch 332 batch loss 6.39788 epoch total loss 6.35423613\n",
      "Trained batch 333 batch loss 6.58297586 epoch total loss 6.35492325\n",
      "Trained batch 334 batch loss 6.77296066 epoch total loss 6.35617495\n",
      "Trained batch 335 batch loss 6.89750814 epoch total loss 6.35779047\n",
      "Trained batch 336 batch loss 6.86626387 epoch total loss 6.35930395\n",
      "Trained batch 337 batch loss 6.75075245 epoch total loss 6.36046553\n",
      "Trained batch 338 batch loss 6.63056707 epoch total loss 6.36126471\n",
      "Trained batch 339 batch loss 6.5211668 epoch total loss 6.3617363\n",
      "Trained batch 340 batch loss 6.41576624 epoch total loss 6.36189556\n",
      "Trained batch 341 batch loss 6.47629309 epoch total loss 6.36223078\n",
      "Trained batch 342 batch loss 6.54652739 epoch total loss 6.36277\n",
      "Trained batch 343 batch loss 6.52822113 epoch total loss 6.36325264\n",
      "Trained batch 344 batch loss 6.62951851 epoch total loss 6.36402702\n",
      "Trained batch 345 batch loss 6.09677649 epoch total loss 6.36325216\n",
      "Trained batch 346 batch loss 6.59784889 epoch total loss 6.36393\n",
      "Trained batch 347 batch loss 6.15365744 epoch total loss 6.36332417\n",
      "Trained batch 348 batch loss 6.32349586 epoch total loss 6.36320972\n",
      "Trained batch 349 batch loss 6.99555826 epoch total loss 6.36502171\n",
      "Trained batch 350 batch loss 7.09681511 epoch total loss 6.36711264\n",
      "Trained batch 351 batch loss 6.92740965 epoch total loss 6.36870956\n",
      "Trained batch 352 batch loss 6.46036339 epoch total loss 6.36897\n",
      "Trained batch 353 batch loss 7.22975111 epoch total loss 6.37140846\n",
      "Trained batch 354 batch loss 6.53508711 epoch total loss 6.37187099\n",
      "Trained batch 355 batch loss 6.64674 epoch total loss 6.37264538\n",
      "Trained batch 356 batch loss 6.51505899 epoch total loss 6.37304544\n",
      "Trained batch 357 batch loss 6.53615618 epoch total loss 6.37350225\n",
      "Trained batch 358 batch loss 6.13970566 epoch total loss 6.37284899\n",
      "Trained batch 359 batch loss 6.14187098 epoch total loss 6.37220573\n",
      "Trained batch 360 batch loss 6.78208399 epoch total loss 6.37334394\n",
      "Trained batch 361 batch loss 6.30870295 epoch total loss 6.37316465\n",
      "Trained batch 362 batch loss 6.43125296 epoch total loss 6.37332487\n",
      "Trained batch 363 batch loss 6.2859273 epoch total loss 6.37308407\n",
      "Trained batch 364 batch loss 6.45350027 epoch total loss 6.37330532\n",
      "Trained batch 365 batch loss 6.69419 epoch total loss 6.37418413\n",
      "Trained batch 366 batch loss 6.5018115 epoch total loss 6.37453222\n",
      "Trained batch 367 batch loss 6.9059782 epoch total loss 6.37598038\n",
      "Trained batch 368 batch loss 6.68116903 epoch total loss 6.3768096\n",
      "Trained batch 369 batch loss 6.21475 epoch total loss 6.37637091\n",
      "Trained batch 370 batch loss 6.60648537 epoch total loss 6.3769927\n",
      "Trained batch 371 batch loss 6.44271469 epoch total loss 6.37716961\n",
      "Trained batch 372 batch loss 6.38295794 epoch total loss 6.37718534\n",
      "Trained batch 373 batch loss 6.51452303 epoch total loss 6.37755346\n",
      "Trained batch 374 batch loss 6.57940245 epoch total loss 6.37809277\n",
      "Trained batch 375 batch loss 6.57018089 epoch total loss 6.37860489\n",
      "Trained batch 376 batch loss 6.37432 epoch total loss 6.37859344\n",
      "Trained batch 377 batch loss 6.31180429 epoch total loss 6.37841606\n",
      "Trained batch 378 batch loss 6.47222948 epoch total loss 6.37866402\n",
      "Trained batch 379 batch loss 6.69162893 epoch total loss 6.37949\n",
      "Trained batch 380 batch loss 6.58967113 epoch total loss 6.38004255\n",
      "Trained batch 381 batch loss 6.3449192 epoch total loss 6.37995052\n",
      "Trained batch 382 batch loss 6.31925583 epoch total loss 6.37979221\n",
      "Trained batch 383 batch loss 6.17056417 epoch total loss 6.37924623\n",
      "Trained batch 384 batch loss 5.81513262 epoch total loss 6.3777771\n",
      "Trained batch 385 batch loss 6.19541025 epoch total loss 6.37730312\n",
      "Trained batch 386 batch loss 6.30097771 epoch total loss 6.37710571\n",
      "Trained batch 387 batch loss 6.51609564 epoch total loss 6.37746477\n",
      "Trained batch 388 batch loss 6.57448673 epoch total loss 6.3779726\n",
      "Trained batch 389 batch loss 5.95978975 epoch total loss 6.37689734\n",
      "Trained batch 390 batch loss 6.11897278 epoch total loss 6.37623596\n",
      "Trained batch 391 batch loss 5.82408237 epoch total loss 6.37482309\n",
      "Trained batch 392 batch loss 5.87950134 epoch total loss 6.37355947\n",
      "Trained batch 393 batch loss 6.42133904 epoch total loss 6.37368107\n",
      "Trained batch 394 batch loss 6.48179913 epoch total loss 6.37395525\n",
      "Trained batch 395 batch loss 6.39524412 epoch total loss 6.37400913\n",
      "Trained batch 396 batch loss 6.21821499 epoch total loss 6.37361574\n",
      "Trained batch 397 batch loss 6.47589731 epoch total loss 6.37387323\n",
      "Trained batch 398 batch loss 6.46456337 epoch total loss 6.37410116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 399 batch loss 6.77975416 epoch total loss 6.37511826\n",
      "Trained batch 400 batch loss 6.5758934 epoch total loss 6.37562\n",
      "Trained batch 401 batch loss 6.62522268 epoch total loss 6.37624264\n",
      "Trained batch 402 batch loss 6.35011101 epoch total loss 6.37617779\n",
      "Trained batch 403 batch loss 6.54079199 epoch total loss 6.37658596\n",
      "Trained batch 404 batch loss 6.35006762 epoch total loss 6.37652063\n",
      "Trained batch 405 batch loss 6.36416531 epoch total loss 6.37649\n",
      "Trained batch 406 batch loss 6.7177 epoch total loss 6.37733078\n",
      "Trained batch 407 batch loss 6.60503387 epoch total loss 6.37789\n",
      "Trained batch 408 batch loss 6.403615 epoch total loss 6.37795305\n",
      "Trained batch 409 batch loss 6.04249668 epoch total loss 6.37713289\n",
      "Trained batch 410 batch loss 6.18872643 epoch total loss 6.37667322\n",
      "Trained batch 411 batch loss 6.17453241 epoch total loss 6.3761816\n",
      "Trained batch 412 batch loss 6.66915941 epoch total loss 6.37689257\n",
      "Trained batch 413 batch loss 6.32220078 epoch total loss 6.37676048\n",
      "Trained batch 414 batch loss 6.26485205 epoch total loss 6.37649\n",
      "Trained batch 415 batch loss 6.18481302 epoch total loss 6.37602854\n",
      "Trained batch 416 batch loss 6.12312174 epoch total loss 6.37542\n",
      "Trained batch 417 batch loss 6.35809326 epoch total loss 6.37537861\n",
      "Trained batch 418 batch loss 6.7725296 epoch total loss 6.37632895\n",
      "Trained batch 419 batch loss 6.75245571 epoch total loss 6.37722635\n",
      "Trained batch 420 batch loss 6.79131317 epoch total loss 6.37821198\n",
      "Trained batch 421 batch loss 6.75795221 epoch total loss 6.37911463\n",
      "Trained batch 422 batch loss 6.50499296 epoch total loss 6.37941265\n",
      "Trained batch 423 batch loss 6.69089794 epoch total loss 6.38014889\n",
      "Trained batch 424 batch loss 6.58000278 epoch total loss 6.38062048\n",
      "Trained batch 425 batch loss 6.20761967 epoch total loss 6.38021326\n",
      "Trained batch 426 batch loss 6.61542416 epoch total loss 6.38076544\n",
      "Trained batch 427 batch loss 6.21656847 epoch total loss 6.38038063\n",
      "Trained batch 428 batch loss 6.80029058 epoch total loss 6.38136196\n",
      "Trained batch 429 batch loss 6.78430557 epoch total loss 6.38230133\n",
      "Trained batch 430 batch loss 6.72160196 epoch total loss 6.3830905\n",
      "Trained batch 431 batch loss 6.21128798 epoch total loss 6.38269186\n",
      "Trained batch 432 batch loss 6.30473709 epoch total loss 6.38251114\n",
      "Trained batch 433 batch loss 6.15737104 epoch total loss 6.38199139\n",
      "Trained batch 434 batch loss 6.2052145 epoch total loss 6.38158464\n",
      "Trained batch 435 batch loss 6.43988514 epoch total loss 6.38171864\n",
      "Trained batch 436 batch loss 6.17052746 epoch total loss 6.38123417\n",
      "Trained batch 437 batch loss 6.49891329 epoch total loss 6.38150358\n",
      "Trained batch 438 batch loss 6.39698505 epoch total loss 6.38153887\n",
      "Trained batch 439 batch loss 6.44321728 epoch total loss 6.38167906\n",
      "Trained batch 440 batch loss 6.38893509 epoch total loss 6.38169575\n",
      "Trained batch 441 batch loss 6.27562 epoch total loss 6.38145494\n",
      "Trained batch 442 batch loss 6.13986588 epoch total loss 6.38090849\n",
      "Trained batch 443 batch loss 6.09359741 epoch total loss 6.38026\n",
      "Trained batch 444 batch loss 6.23437595 epoch total loss 6.37993097\n",
      "Trained batch 445 batch loss 6.77100372 epoch total loss 6.38081\n",
      "Trained batch 446 batch loss 6.29085302 epoch total loss 6.38060808\n",
      "Trained batch 447 batch loss 6.56941938 epoch total loss 6.38103056\n",
      "Trained batch 448 batch loss 6.88839197 epoch total loss 6.38216305\n",
      "Trained batch 449 batch loss 6.09696865 epoch total loss 6.38152742\n",
      "Trained batch 450 batch loss 6.23869419 epoch total loss 6.38121033\n",
      "Trained batch 451 batch loss 6.4432168 epoch total loss 6.38134766\n",
      "Trained batch 452 batch loss 6.18651772 epoch total loss 6.3809166\n",
      "Trained batch 453 batch loss 6.03782558 epoch total loss 6.38015938\n",
      "Trained batch 454 batch loss 6.29486656 epoch total loss 6.3799715\n",
      "Trained batch 455 batch loss 6.39393091 epoch total loss 6.3800025\n",
      "Trained batch 456 batch loss 6.00896931 epoch total loss 6.37918901\n",
      "Trained batch 457 batch loss 6.15085363 epoch total loss 6.37868929\n",
      "Trained batch 458 batch loss 5.99923563 epoch total loss 6.37786102\n",
      "Trained batch 459 batch loss 6.24175167 epoch total loss 6.37756443\n",
      "Trained batch 460 batch loss 6.28545237 epoch total loss 6.37736368\n",
      "Trained batch 461 batch loss 5.8983531 epoch total loss 6.37632513\n",
      "Trained batch 462 batch loss 6.41842 epoch total loss 6.37641621\n",
      "Trained batch 463 batch loss 6.34066963 epoch total loss 6.37633896\n",
      "Trained batch 464 batch loss 6.59626722 epoch total loss 6.37681246\n",
      "Trained batch 465 batch loss 5.76500368 epoch total loss 6.37549686\n",
      "Trained batch 466 batch loss 5.12430429 epoch total loss 6.37281179\n",
      "Trained batch 467 batch loss 5.62235975 epoch total loss 6.37120438\n",
      "Trained batch 468 batch loss 6.32585669 epoch total loss 6.37110806\n",
      "Trained batch 469 batch loss 6.08089542 epoch total loss 6.37048864\n",
      "Trained batch 470 batch loss 5.91605377 epoch total loss 6.36952209\n",
      "Trained batch 471 batch loss 6.19667816 epoch total loss 6.36915493\n",
      "Trained batch 472 batch loss 6.09993935 epoch total loss 6.36858463\n",
      "Trained batch 473 batch loss 6.21389341 epoch total loss 6.36825752\n",
      "Trained batch 474 batch loss 6.55147791 epoch total loss 6.36864424\n",
      "Trained batch 475 batch loss 6.47225237 epoch total loss 6.36886215\n",
      "Trained batch 476 batch loss 7.06436157 epoch total loss 6.37032318\n",
      "Trained batch 477 batch loss 7.58722496 epoch total loss 6.37287426\n",
      "Trained batch 478 batch loss 7.55199766 epoch total loss 6.37534142\n",
      "Trained batch 479 batch loss 6.92144918 epoch total loss 6.37648106\n",
      "Trained batch 480 batch loss 6.34766054 epoch total loss 6.37642097\n",
      "Trained batch 481 batch loss 6.34042692 epoch total loss 6.37634611\n",
      "Trained batch 482 batch loss 6.38982248 epoch total loss 6.37637424\n",
      "Trained batch 483 batch loss 6.63806534 epoch total loss 6.37691641\n",
      "Trained batch 484 batch loss 6.05749655 epoch total loss 6.37625647\n",
      "Trained batch 485 batch loss 6.13008 epoch total loss 6.37574911\n",
      "Trained batch 486 batch loss 6.18921852 epoch total loss 6.37536526\n",
      "Trained batch 487 batch loss 6.39818525 epoch total loss 6.37541199\n",
      "Trained batch 488 batch loss 6.85466337 epoch total loss 6.37639427\n",
      "Trained batch 489 batch loss 6.6569643 epoch total loss 6.37696791\n",
      "Trained batch 490 batch loss 6.57893 epoch total loss 6.37738\n",
      "Trained batch 491 batch loss 6.36938715 epoch total loss 6.37736368\n",
      "Trained batch 492 batch loss 6.49799 epoch total loss 6.37760925\n",
      "Trained batch 493 batch loss 6.24535179 epoch total loss 6.37734079\n",
      "Trained batch 494 batch loss 6.6028 epoch total loss 6.37779713\n",
      "Trained batch 495 batch loss 6.35368061 epoch total loss 6.37774849\n",
      "Trained batch 496 batch loss 6.24123478 epoch total loss 6.37747335\n",
      "Trained batch 497 batch loss 6.24647284 epoch total loss 6.37721\n",
      "Trained batch 498 batch loss 6.33519888 epoch total loss 6.37712574\n",
      "Trained batch 499 batch loss 6.07407331 epoch total loss 6.37651825\n",
      "Trained batch 500 batch loss 6.46438742 epoch total loss 6.37669373\n",
      "Trained batch 501 batch loss 5.98453903 epoch total loss 6.37591124\n",
      "Trained batch 502 batch loss 6.31462526 epoch total loss 6.37578917\n",
      "Trained batch 503 batch loss 6.33041716 epoch total loss 6.37569904\n",
      "Trained batch 504 batch loss 6.19279 epoch total loss 6.37533617\n",
      "Trained batch 505 batch loss 6.32397747 epoch total loss 6.3752346\n",
      "Trained batch 506 batch loss 6.28254461 epoch total loss 6.37505102\n",
      "Trained batch 507 batch loss 6.49857044 epoch total loss 6.37529469\n",
      "Trained batch 508 batch loss 6.8353076 epoch total loss 6.3762\n",
      "Trained batch 509 batch loss 6.72466278 epoch total loss 6.37688446\n",
      "Trained batch 510 batch loss 6.63229799 epoch total loss 6.37738562\n",
      "Trained batch 511 batch loss 6.31092501 epoch total loss 6.37725544\n",
      "Trained batch 512 batch loss 6.36577702 epoch total loss 6.37723303\n",
      "Trained batch 513 batch loss 6.18711662 epoch total loss 6.37686205\n",
      "Trained batch 514 batch loss 6.12572479 epoch total loss 6.37637377\n",
      "Trained batch 515 batch loss 6.18609238 epoch total loss 6.37600422\n",
      "Trained batch 516 batch loss 6.06706762 epoch total loss 6.37540531\n",
      "Trained batch 517 batch loss 6.10943556 epoch total loss 6.3748908\n",
      "Trained batch 518 batch loss 6.34183264 epoch total loss 6.37482691\n",
      "Trained batch 519 batch loss 6.23081 epoch total loss 6.37454939\n",
      "Trained batch 520 batch loss 6.30236864 epoch total loss 6.37441063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 521 batch loss 6.23787212 epoch total loss 6.37414837\n",
      "Trained batch 522 batch loss 6.57409286 epoch total loss 6.37453127\n",
      "Trained batch 523 batch loss 6.2733283 epoch total loss 6.37433815\n",
      "Trained batch 524 batch loss 6.31828451 epoch total loss 6.37423134\n",
      "Trained batch 525 batch loss 6.29383 epoch total loss 6.37407827\n",
      "Trained batch 526 batch loss 6.56467 epoch total loss 6.37444067\n",
      "Trained batch 527 batch loss 6.57219028 epoch total loss 6.37481594\n",
      "Trained batch 528 batch loss 6.53238583 epoch total loss 6.37511444\n",
      "Trained batch 529 batch loss 6.13290596 epoch total loss 6.37465668\n",
      "Trained batch 530 batch loss 5.79109478 epoch total loss 6.37355566\n",
      "Trained batch 531 batch loss 6.33250952 epoch total loss 6.37347794\n",
      "Trained batch 532 batch loss 6.28624439 epoch total loss 6.3733139\n",
      "Trained batch 533 batch loss 6.16191387 epoch total loss 6.37291718\n",
      "Trained batch 534 batch loss 6.2928 epoch total loss 6.37276697\n",
      "Trained batch 535 batch loss 6.67466593 epoch total loss 6.37333107\n",
      "Trained batch 536 batch loss 6.56167936 epoch total loss 6.3736825\n",
      "Trained batch 537 batch loss 6.6541481 epoch total loss 6.37420464\n",
      "Trained batch 538 batch loss 5.64054585 epoch total loss 6.37284136\n",
      "Trained batch 539 batch loss 6.11496449 epoch total loss 6.37236309\n",
      "Trained batch 540 batch loss 6.45509195 epoch total loss 6.37251616\n",
      "Trained batch 541 batch loss 6.55387354 epoch total loss 6.37285137\n",
      "Trained batch 542 batch loss 6.4766655 epoch total loss 6.37304258\n",
      "Trained batch 543 batch loss 6.58766699 epoch total loss 6.37343788\n",
      "Trained batch 544 batch loss 6.00276661 epoch total loss 6.37275648\n",
      "Trained batch 545 batch loss 6.27790546 epoch total loss 6.37258244\n",
      "Trained batch 546 batch loss 6.22193289 epoch total loss 6.37230635\n",
      "Trained batch 547 batch loss 5.85442877 epoch total loss 6.37136\n",
      "Trained batch 548 batch loss 6.2121892 epoch total loss 6.37106943\n",
      "Trained batch 549 batch loss 6.14831114 epoch total loss 6.37066317\n",
      "Trained batch 550 batch loss 6.42930508 epoch total loss 6.3707695\n",
      "Trained batch 551 batch loss 6.3399992 epoch total loss 6.37071419\n",
      "Trained batch 552 batch loss 6.32306814 epoch total loss 6.3706274\n",
      "Trained batch 553 batch loss 6.54249287 epoch total loss 6.3709383\n",
      "Trained batch 554 batch loss 6.59291553 epoch total loss 6.37133932\n",
      "Trained batch 555 batch loss 5.87170076 epoch total loss 6.37043858\n",
      "Trained batch 556 batch loss 6.39334536 epoch total loss 6.37048\n",
      "Trained batch 557 batch loss 6.53392887 epoch total loss 6.37077332\n",
      "Trained batch 558 batch loss 6.3720212 epoch total loss 6.3707757\n",
      "Trained batch 559 batch loss 6.55199575 epoch total loss 6.3711\n",
      "Trained batch 560 batch loss 6.57360363 epoch total loss 6.37146139\n",
      "Trained batch 561 batch loss 6.30346727 epoch total loss 6.37134027\n",
      "Trained batch 562 batch loss 5.98051167 epoch total loss 6.37064457\n",
      "Trained batch 563 batch loss 6.2437911 epoch total loss 6.3704195\n",
      "Trained batch 564 batch loss 6.67824745 epoch total loss 6.370965\n",
      "Trained batch 565 batch loss 6.45888138 epoch total loss 6.37112093\n",
      "Trained batch 566 batch loss 6.3992753 epoch total loss 6.37117052\n",
      "Trained batch 567 batch loss 6.39411068 epoch total loss 6.37121105\n",
      "Trained batch 568 batch loss 6.29737329 epoch total loss 6.37108088\n",
      "Trained batch 569 batch loss 5.95426941 epoch total loss 6.37034845\n",
      "Trained batch 570 batch loss 6.0567441 epoch total loss 6.36979818\n",
      "Trained batch 571 batch loss 6.15078306 epoch total loss 6.36941481\n",
      "Trained batch 572 batch loss 6.23415756 epoch total loss 6.3691783\n",
      "Trained batch 573 batch loss 6.35739613 epoch total loss 6.36915779\n",
      "Trained batch 574 batch loss 6.27489662 epoch total loss 6.36899328\n",
      "Trained batch 575 batch loss 6.54257441 epoch total loss 6.36929512\n",
      "Trained batch 576 batch loss 6.31070757 epoch total loss 6.36919355\n",
      "Trained batch 577 batch loss 6.65969467 epoch total loss 6.36969709\n",
      "Trained batch 578 batch loss 6.40345144 epoch total loss 6.36975574\n",
      "Trained batch 579 batch loss 6.6406703 epoch total loss 6.37022352\n",
      "Trained batch 580 batch loss 5.9918766 epoch total loss 6.36957121\n",
      "Trained batch 581 batch loss 6.42049742 epoch total loss 6.36965895\n",
      "Trained batch 582 batch loss 6.66680956 epoch total loss 6.37016916\n",
      "Trained batch 583 batch loss 6.33028793 epoch total loss 6.37010098\n",
      "Trained batch 584 batch loss 6.84499121 epoch total loss 6.37091398\n",
      "Trained batch 585 batch loss 6.68061829 epoch total loss 6.37144327\n",
      "Trained batch 586 batch loss 6.9110589 epoch total loss 6.37236452\n",
      "Trained batch 587 batch loss 6.76553535 epoch total loss 6.37303448\n",
      "Trained batch 588 batch loss 6.78308773 epoch total loss 6.37373209\n",
      "Trained batch 589 batch loss 6.54383421 epoch total loss 6.37402105\n",
      "Trained batch 590 batch loss 6.23457 epoch total loss 6.37378454\n",
      "Trained batch 591 batch loss 6.2174511 epoch total loss 6.37352037\n",
      "Trained batch 592 batch loss 6.34119749 epoch total loss 6.37346601\n",
      "Trained batch 593 batch loss 6.80676174 epoch total loss 6.37419653\n",
      "Trained batch 594 batch loss 6.33445787 epoch total loss 6.3741293\n",
      "Trained batch 595 batch loss 6.53814554 epoch total loss 6.37440491\n",
      "Trained batch 596 batch loss 6.16228 epoch total loss 6.37404919\n",
      "Trained batch 597 batch loss 6.24777412 epoch total loss 6.37383795\n",
      "Trained batch 598 batch loss 6.56465292 epoch total loss 6.37415695\n",
      "Trained batch 599 batch loss 6.62196 epoch total loss 6.37457085\n",
      "Trained batch 600 batch loss 6.56946087 epoch total loss 6.37489605\n",
      "Trained batch 601 batch loss 6.57947302 epoch total loss 6.37523651\n",
      "Trained batch 602 batch loss 6.47532892 epoch total loss 6.37540293\n",
      "Trained batch 603 batch loss 6.42839193 epoch total loss 6.37549067\n",
      "Trained batch 604 batch loss 6.33150053 epoch total loss 6.37541819\n",
      "Trained batch 605 batch loss 6.38438225 epoch total loss 6.37543249\n",
      "Trained batch 606 batch loss 6.47574711 epoch total loss 6.37559843\n",
      "Trained batch 607 batch loss 6.51935673 epoch total loss 6.37583494\n",
      "Trained batch 608 batch loss 6.73391771 epoch total loss 6.37642384\n",
      "Trained batch 609 batch loss 6.13854313 epoch total loss 6.37603331\n",
      "Trained batch 610 batch loss 5.34434271 epoch total loss 6.37434149\n",
      "Trained batch 611 batch loss 5.35378695 epoch total loss 6.37267113\n",
      "Trained batch 612 batch loss 5.34272909 epoch total loss 6.37098837\n",
      "Trained batch 613 batch loss 5.79146433 epoch total loss 6.37004328\n",
      "Trained batch 614 batch loss 6.17759705 epoch total loss 6.36972952\n",
      "Trained batch 615 batch loss 6.4356 epoch total loss 6.36983633\n",
      "Trained batch 616 batch loss 7.09647751 epoch total loss 6.37101603\n",
      "Trained batch 617 batch loss 6.74576139 epoch total loss 6.37162352\n",
      "Trained batch 618 batch loss 6.45130348 epoch total loss 6.37175274\n",
      "Trained batch 619 batch loss 7.0410533 epoch total loss 6.37283373\n",
      "Trained batch 620 batch loss 6.83454704 epoch total loss 6.37357855\n",
      "Trained batch 621 batch loss 6.72592592 epoch total loss 6.37414551\n",
      "Trained batch 622 batch loss 6.65584898 epoch total loss 6.3745985\n",
      "Trained batch 623 batch loss 6.64999247 epoch total loss 6.37504053\n",
      "Trained batch 624 batch loss 6.81229115 epoch total loss 6.375741\n",
      "Trained batch 625 batch loss 6.46959686 epoch total loss 6.37589121\n",
      "Trained batch 626 batch loss 6.52850485 epoch total loss 6.37613487\n",
      "Trained batch 627 batch loss 5.89703751 epoch total loss 6.3753705\n",
      "Trained batch 628 batch loss 5.98172808 epoch total loss 6.37474394\n",
      "Trained batch 629 batch loss 5.89690924 epoch total loss 6.37398434\n",
      "Trained batch 630 batch loss 6.13775301 epoch total loss 6.37360907\n",
      "Trained batch 631 batch loss 5.91812658 epoch total loss 6.37288761\n",
      "Trained batch 632 batch loss 6.37499094 epoch total loss 6.37289095\n",
      "Trained batch 633 batch loss 6.28034878 epoch total loss 6.37274456\n",
      "Trained batch 634 batch loss 6.06998682 epoch total loss 6.37226725\n",
      "Trained batch 635 batch loss 5.95412207 epoch total loss 6.37160873\n",
      "Trained batch 636 batch loss 6.22267485 epoch total loss 6.37137413\n",
      "Trained batch 637 batch loss 6.40066433 epoch total loss 6.37142038\n",
      "Trained batch 638 batch loss 6.27716827 epoch total loss 6.37127256\n",
      "Trained batch 639 batch loss 6.81199169 epoch total loss 6.37196207\n",
      "Trained batch 640 batch loss 6.49073696 epoch total loss 6.37214756\n",
      "Trained batch 641 batch loss 6.34001446 epoch total loss 6.37209797\n",
      "Trained batch 642 batch loss 6.11829853 epoch total loss 6.37170267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 643 batch loss 6.33340549 epoch total loss 6.37164259\n",
      "Trained batch 644 batch loss 6.52542734 epoch total loss 6.37188148\n",
      "Trained batch 645 batch loss 6.45235443 epoch total loss 6.37200594\n",
      "Trained batch 646 batch loss 6.60215187 epoch total loss 6.37236214\n",
      "Trained batch 647 batch loss 6.06425619 epoch total loss 6.37188625\n",
      "Trained batch 648 batch loss 6.34766483 epoch total loss 6.37184858\n",
      "Trained batch 649 batch loss 6.0394454 epoch total loss 6.37133694\n",
      "Trained batch 650 batch loss 5.86714792 epoch total loss 6.37056112\n",
      "Trained batch 651 batch loss 5.33598328 epoch total loss 6.36897182\n",
      "Trained batch 652 batch loss 5.96187449 epoch total loss 6.36834764\n",
      "Trained batch 653 batch loss 6.29310036 epoch total loss 6.36823225\n",
      "Trained batch 654 batch loss 6.27191067 epoch total loss 6.36808491\n",
      "Trained batch 655 batch loss 6.37815285 epoch total loss 6.3681\n",
      "Trained batch 656 batch loss 6.52219868 epoch total loss 6.36833429\n",
      "Trained batch 657 batch loss 6.3688221 epoch total loss 6.36833477\n",
      "Trained batch 658 batch loss 5.8250289 epoch total loss 6.36750937\n",
      "Trained batch 659 batch loss 6.11891031 epoch total loss 6.36713266\n",
      "Trained batch 660 batch loss 6.23426247 epoch total loss 6.36693144\n",
      "Trained batch 661 batch loss 6.56815767 epoch total loss 6.36723614\n",
      "Trained batch 662 batch loss 6.66218281 epoch total loss 6.3676815\n",
      "Trained batch 663 batch loss 6.05777311 epoch total loss 6.3672142\n",
      "Trained batch 664 batch loss 6.50627 epoch total loss 6.36742353\n",
      "Trained batch 665 batch loss 6.05794954 epoch total loss 6.36695862\n",
      "Trained batch 666 batch loss 6.58414841 epoch total loss 6.3672843\n",
      "Trained batch 667 batch loss 6.74237156 epoch total loss 6.36784649\n",
      "Trained batch 668 batch loss 6.69540596 epoch total loss 6.36833668\n",
      "Trained batch 669 batch loss 6.27296782 epoch total loss 6.3681941\n",
      "Trained batch 670 batch loss 6.31014 epoch total loss 6.36810732\n",
      "Trained batch 671 batch loss 6.32112169 epoch total loss 6.36803722\n",
      "Trained batch 672 batch loss 6.35086536 epoch total loss 6.36801243\n",
      "Trained batch 673 batch loss 6.23166513 epoch total loss 6.3678093\n",
      "Trained batch 674 batch loss 6.11132 epoch total loss 6.36742878\n",
      "Trained batch 675 batch loss 6.43222618 epoch total loss 6.36752462\n",
      "Trained batch 676 batch loss 6.54328394 epoch total loss 6.36778498\n",
      "Trained batch 677 batch loss 6.72548628 epoch total loss 6.36831331\n",
      "Trained batch 678 batch loss 6.84721088 epoch total loss 6.36901951\n",
      "Trained batch 679 batch loss 7.1259141 epoch total loss 6.37013435\n",
      "Trained batch 680 batch loss 6.19370508 epoch total loss 6.36987543\n",
      "Trained batch 681 batch loss 5.80851 epoch total loss 6.36905098\n",
      "Trained batch 682 batch loss 6.36927605 epoch total loss 6.36905098\n",
      "Trained batch 683 batch loss 6.0271244 epoch total loss 6.36855078\n",
      "Trained batch 684 batch loss 6.54624128 epoch total loss 6.36881065\n",
      "Trained batch 685 batch loss 6.51589441 epoch total loss 6.36902571\n",
      "Trained batch 686 batch loss 6.49309063 epoch total loss 6.36920691\n",
      "Trained batch 687 batch loss 6.39751816 epoch total loss 6.36924791\n",
      "Trained batch 688 batch loss 6.40102911 epoch total loss 6.36929369\n",
      "Trained batch 689 batch loss 6.34349298 epoch total loss 6.36925602\n",
      "Trained batch 690 batch loss 6.35812616 epoch total loss 6.36924\n",
      "Trained batch 691 batch loss 6.32349491 epoch total loss 6.369174\n",
      "Trained batch 692 batch loss 6.3464222 epoch total loss 6.36914062\n",
      "Trained batch 693 batch loss 6.25436497 epoch total loss 6.36897516\n",
      "Trained batch 694 batch loss 5.91256809 epoch total loss 6.3683176\n",
      "Trained batch 695 batch loss 5.88906813 epoch total loss 6.3676281\n",
      "Trained batch 696 batch loss 6.53609228 epoch total loss 6.36787033\n",
      "Trained batch 697 batch loss 6.34404 epoch total loss 6.367836\n",
      "Trained batch 698 batch loss 6.33931208 epoch total loss 6.36779547\n",
      "Trained batch 699 batch loss 6.69525433 epoch total loss 6.36826372\n",
      "Trained batch 700 batch loss 6.43944359 epoch total loss 6.36836576\n",
      "Trained batch 701 batch loss 6.52442408 epoch total loss 6.36858845\n",
      "Trained batch 702 batch loss 6.10276508 epoch total loss 6.36820936\n",
      "Trained batch 703 batch loss 6.51894903 epoch total loss 6.36842394\n",
      "Trained batch 704 batch loss 6.38060284 epoch total loss 6.36844063\n",
      "Trained batch 705 batch loss 6.54923 epoch total loss 6.36869717\n",
      "Trained batch 706 batch loss 6.76163054 epoch total loss 6.36925411\n",
      "Trained batch 707 batch loss 6.12738657 epoch total loss 6.36891222\n",
      "Trained batch 708 batch loss 5.99136972 epoch total loss 6.36837864\n",
      "Trained batch 709 batch loss 5.94973946 epoch total loss 6.36778784\n",
      "Trained batch 710 batch loss 5.54959679 epoch total loss 6.3666358\n",
      "Trained batch 711 batch loss 5.72450733 epoch total loss 6.36573315\n",
      "Trained batch 712 batch loss 6.28907871 epoch total loss 6.36562538\n",
      "Trained batch 713 batch loss 6.42030525 epoch total loss 6.36570215\n",
      "Trained batch 714 batch loss 6.73255682 epoch total loss 6.36621571\n",
      "Trained batch 715 batch loss 7.11883163 epoch total loss 6.36726809\n",
      "Trained batch 716 batch loss 6.6259985 epoch total loss 6.36762953\n",
      "Trained batch 717 batch loss 6.47016191 epoch total loss 6.36777258\n",
      "Trained batch 718 batch loss 5.80522823 epoch total loss 6.36698914\n",
      "Trained batch 719 batch loss 6.40069485 epoch total loss 6.36703587\n",
      "Trained batch 720 batch loss 6.30921888 epoch total loss 6.36695576\n",
      "Trained batch 721 batch loss 5.62753439 epoch total loss 6.36593\n",
      "Trained batch 722 batch loss 5.70970106 epoch total loss 6.36502075\n",
      "Trained batch 723 batch loss 5.68678331 epoch total loss 6.36408281\n",
      "Trained batch 724 batch loss 5.83901739 epoch total loss 6.36335754\n",
      "Trained batch 725 batch loss 6.15596 epoch total loss 6.36307096\n",
      "Trained batch 726 batch loss 6.44157887 epoch total loss 6.36317921\n",
      "Trained batch 727 batch loss 6.7273097 epoch total loss 6.36368036\n",
      "Trained batch 728 batch loss 6.22833872 epoch total loss 6.3634944\n",
      "Trained batch 729 batch loss 5.8342824 epoch total loss 6.36276865\n",
      "Trained batch 730 batch loss 6.41136789 epoch total loss 6.36283493\n",
      "Trained batch 731 batch loss 6.44561529 epoch total loss 6.36294842\n",
      "Trained batch 732 batch loss 6.09716463 epoch total loss 6.36258554\n",
      "Trained batch 733 batch loss 6.46154594 epoch total loss 6.36272049\n",
      "Trained batch 734 batch loss 6.18139505 epoch total loss 6.36247301\n",
      "Trained batch 735 batch loss 6.26986361 epoch total loss 6.36234713\n",
      "Trained batch 736 batch loss 6.26360512 epoch total loss 6.36221313\n",
      "Trained batch 737 batch loss 6.18629885 epoch total loss 6.36197472\n",
      "Trained batch 738 batch loss 6.27213097 epoch total loss 6.36185265\n",
      "Trained batch 739 batch loss 6.11332321 epoch total loss 6.36151648\n",
      "Trained batch 740 batch loss 6.45931768 epoch total loss 6.36164904\n",
      "Trained batch 741 batch loss 6.26576424 epoch total loss 6.36151934\n",
      "Trained batch 742 batch loss 5.93063879 epoch total loss 6.36093855\n",
      "Trained batch 743 batch loss 5.82631 epoch total loss 6.360219\n",
      "Trained batch 744 batch loss 6.26061726 epoch total loss 6.36008501\n",
      "Trained batch 745 batch loss 6.3916111 epoch total loss 6.36012745\n",
      "Trained batch 746 batch loss 6.53652906 epoch total loss 6.36036396\n",
      "Trained batch 747 batch loss 6.51402855 epoch total loss 6.36057\n",
      "Trained batch 748 batch loss 6.62521839 epoch total loss 6.36092329\n",
      "Trained batch 749 batch loss 7.01336384 epoch total loss 6.36179447\n",
      "Trained batch 750 batch loss 6.82181883 epoch total loss 6.36240768\n",
      "Trained batch 751 batch loss 6.55984879 epoch total loss 6.3626709\n",
      "Trained batch 752 batch loss 6.40177202 epoch total loss 6.36272287\n",
      "Trained batch 753 batch loss 6.46474838 epoch total loss 6.3628583\n",
      "Trained batch 754 batch loss 6.49989462 epoch total loss 6.36304045\n",
      "Trained batch 755 batch loss 6.2376442 epoch total loss 6.36287451\n",
      "Trained batch 756 batch loss 6.5755682 epoch total loss 6.36315584\n",
      "Trained batch 757 batch loss 6.47117758 epoch total loss 6.36329889\n",
      "Trained batch 758 batch loss 6.4848156 epoch total loss 6.36345911\n",
      "Trained batch 759 batch loss 6.62300968 epoch total loss 6.363801\n",
      "Trained batch 760 batch loss 6.72961 epoch total loss 6.36428213\n",
      "Trained batch 761 batch loss 7.02557325 epoch total loss 6.36515093\n",
      "Trained batch 762 batch loss 6.27025032 epoch total loss 6.365026\n",
      "Trained batch 763 batch loss 6.70485973 epoch total loss 6.36547184\n",
      "Trained batch 764 batch loss 6.83487749 epoch total loss 6.36608648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 765 batch loss 6.36329317 epoch total loss 6.36608267\n",
      "Trained batch 766 batch loss 6.35115433 epoch total loss 6.36606312\n",
      "Trained batch 767 batch loss 6.34156466 epoch total loss 6.36603165\n",
      "Trained batch 768 batch loss 6.21317101 epoch total loss 6.36583281\n",
      "Trained batch 769 batch loss 6.3824563 epoch total loss 6.36585426\n",
      "Trained batch 770 batch loss 6.5964632 epoch total loss 6.36615372\n",
      "Trained batch 771 batch loss 6.53492594 epoch total loss 6.36637306\n",
      "Trained batch 772 batch loss 6.54888439 epoch total loss 6.36660957\n",
      "Trained batch 773 batch loss 6.65435934 epoch total loss 6.36698151\n",
      "Trained batch 774 batch loss 6.64889145 epoch total loss 6.36734581\n",
      "Trained batch 775 batch loss 6.33895159 epoch total loss 6.36730909\n",
      "Trained batch 776 batch loss 6.63973665 epoch total loss 6.36766\n",
      "Trained batch 777 batch loss 6.31712389 epoch total loss 6.36759472\n",
      "Trained batch 778 batch loss 5.90467882 epoch total loss 6.36699963\n",
      "Trained batch 779 batch loss 6.12949181 epoch total loss 6.36669493\n",
      "Trained batch 780 batch loss 6.2910347 epoch total loss 6.36659765\n",
      "Trained batch 781 batch loss 6.23971081 epoch total loss 6.36643553\n",
      "Trained batch 782 batch loss 6.40901947 epoch total loss 6.36649\n",
      "Trained batch 783 batch loss 6.53783655 epoch total loss 6.36670876\n",
      "Trained batch 784 batch loss 6.6504693 epoch total loss 6.36707\n",
      "Trained batch 785 batch loss 6.51540422 epoch total loss 6.3672595\n",
      "Trained batch 786 batch loss 6.3506732 epoch total loss 6.36723852\n",
      "Trained batch 787 batch loss 6.43424416 epoch total loss 6.3673234\n",
      "Trained batch 788 batch loss 6.26429319 epoch total loss 6.36719227\n",
      "Trained batch 789 batch loss 6.468503 epoch total loss 6.36732054\n",
      "Trained batch 790 batch loss 5.96688366 epoch total loss 6.36681366\n",
      "Trained batch 791 batch loss 6.21419573 epoch total loss 6.36662102\n",
      "Trained batch 792 batch loss 6.33969641 epoch total loss 6.36658716\n",
      "Trained batch 793 batch loss 6.24091053 epoch total loss 6.36642838\n",
      "Trained batch 794 batch loss 5.99113 epoch total loss 6.36595583\n",
      "Trained batch 795 batch loss 6.22108936 epoch total loss 6.36577368\n",
      "Trained batch 796 batch loss 6.15704 epoch total loss 6.36551142\n",
      "Trained batch 797 batch loss 6.36574 epoch total loss 6.36551189\n",
      "Trained batch 798 batch loss 6.09737778 epoch total loss 6.36517572\n",
      "Trained batch 799 batch loss 5.98000431 epoch total loss 6.36469364\n",
      "Trained batch 800 batch loss 6.34098721 epoch total loss 6.3646636\n",
      "Trained batch 801 batch loss 6.07385778 epoch total loss 6.36430025\n",
      "Trained batch 802 batch loss 6.37201309 epoch total loss 6.36431026\n",
      "Trained batch 803 batch loss 6.28084421 epoch total loss 6.36420631\n",
      "Trained batch 804 batch loss 6.25793552 epoch total loss 6.36407375\n",
      "Trained batch 805 batch loss 6.60668468 epoch total loss 6.36437511\n",
      "Trained batch 806 batch loss 6.39944839 epoch total loss 6.36441851\n",
      "Trained batch 807 batch loss 6.50708914 epoch total loss 6.36459541\n",
      "Trained batch 808 batch loss 6.48526049 epoch total loss 6.36474466\n",
      "Trained batch 809 batch loss 6.34144592 epoch total loss 6.36471605\n",
      "Trained batch 810 batch loss 6.36688423 epoch total loss 6.36471844\n",
      "Trained batch 811 batch loss 6.45560312 epoch total loss 6.36483049\n",
      "Trained batch 812 batch loss 6.35698938 epoch total loss 6.36482048\n",
      "Trained batch 813 batch loss 6.50199938 epoch total loss 6.36498928\n",
      "Trained batch 814 batch loss 6.44288778 epoch total loss 6.36508512\n",
      "Trained batch 815 batch loss 6.12092876 epoch total loss 6.36478567\n",
      "Trained batch 816 batch loss 6.52294493 epoch total loss 6.36497927\n",
      "Trained batch 817 batch loss 6.9307003 epoch total loss 6.36567163\n",
      "Trained batch 818 batch loss 6.77433395 epoch total loss 6.36617136\n",
      "Trained batch 819 batch loss 6.93486547 epoch total loss 6.36686611\n",
      "Trained batch 820 batch loss 7.51895761 epoch total loss 6.36827135\n",
      "Trained batch 821 batch loss 7.37475204 epoch total loss 6.36949682\n",
      "Trained batch 822 batch loss 7.38351679 epoch total loss 6.3707304\n",
      "Trained batch 823 batch loss 6.91046858 epoch total loss 6.37138605\n",
      "Trained batch 824 batch loss 6.42244816 epoch total loss 6.37144804\n",
      "Trained batch 825 batch loss 7.01837635 epoch total loss 6.37223244\n",
      "Trained batch 826 batch loss 6.72271395 epoch total loss 6.37265682\n",
      "Trained batch 827 batch loss 6.65062141 epoch total loss 6.37299252\n",
      "Trained batch 828 batch loss 6.68064833 epoch total loss 6.37336397\n",
      "Trained batch 829 batch loss 6.40416431 epoch total loss 6.37340164\n",
      "Trained batch 830 batch loss 6.43172312 epoch total loss 6.37347174\n",
      "Trained batch 831 batch loss 6.19715166 epoch total loss 6.37325954\n",
      "Trained batch 832 batch loss 6.56349039 epoch total loss 6.37348843\n",
      "Trained batch 833 batch loss 6.41663551 epoch total loss 6.37354\n",
      "Trained batch 834 batch loss 6.58105326 epoch total loss 6.37378883\n",
      "Trained batch 835 batch loss 6.41380262 epoch total loss 6.37383652\n",
      "Trained batch 836 batch loss 6.41970778 epoch total loss 6.37389135\n",
      "Trained batch 837 batch loss 6.4134779 epoch total loss 6.37393904\n",
      "Trained batch 838 batch loss 6.59316349 epoch total loss 6.37420034\n",
      "Trained batch 839 batch loss 6.50643206 epoch total loss 6.37435818\n",
      "Trained batch 840 batch loss 6.36860609 epoch total loss 6.3743515\n",
      "Trained batch 841 batch loss 6.29623652 epoch total loss 6.37425852\n",
      "Trained batch 842 batch loss 6.41895437 epoch total loss 6.37431145\n",
      "Trained batch 843 batch loss 6.17848158 epoch total loss 6.3740797\n",
      "Trained batch 844 batch loss 6.24962807 epoch total loss 6.37393188\n",
      "Trained batch 845 batch loss 5.70826244 epoch total loss 6.37314463\n",
      "Trained batch 846 batch loss 5.97069359 epoch total loss 6.37266874\n",
      "Trained batch 847 batch loss 6.49746037 epoch total loss 6.37281609\n",
      "Trained batch 848 batch loss 6.18113613 epoch total loss 6.37259\n",
      "Trained batch 849 batch loss 6.64763212 epoch total loss 6.37291384\n",
      "Trained batch 850 batch loss 6.58619928 epoch total loss 6.37316513\n",
      "Trained batch 851 batch loss 6.50712347 epoch total loss 6.37332296\n",
      "Trained batch 852 batch loss 6.55161095 epoch total loss 6.3735323\n",
      "Trained batch 853 batch loss 6.64702511 epoch total loss 6.37385273\n",
      "Trained batch 854 batch loss 6.71969128 epoch total loss 6.37425804\n",
      "Trained batch 855 batch loss 6.54550362 epoch total loss 6.37445784\n",
      "Trained batch 856 batch loss 6.52857161 epoch total loss 6.37463856\n",
      "Trained batch 857 batch loss 6.42479706 epoch total loss 6.37469673\n",
      "Trained batch 858 batch loss 6.43258619 epoch total loss 6.37476444\n",
      "Trained batch 859 batch loss 6.52335453 epoch total loss 6.37493753\n",
      "Trained batch 860 batch loss 6.44641399 epoch total loss 6.3750205\n",
      "Trained batch 861 batch loss 6.48545313 epoch total loss 6.37514877\n",
      "Trained batch 862 batch loss 6.27831 epoch total loss 6.37503624\n",
      "Trained batch 863 batch loss 6.46517 epoch total loss 6.37514067\n",
      "Trained batch 864 batch loss 6.28489065 epoch total loss 6.37503624\n",
      "Trained batch 865 batch loss 6.41180849 epoch total loss 6.37507868\n",
      "Trained batch 866 batch loss 6.38606501 epoch total loss 6.37509155\n",
      "Trained batch 867 batch loss 6.55596399 epoch total loss 6.37530041\n",
      "Trained batch 868 batch loss 6.22389269 epoch total loss 6.37512589\n",
      "Trained batch 869 batch loss 6.583148 epoch total loss 6.37536526\n",
      "Trained batch 870 batch loss 6.06672812 epoch total loss 6.37501049\n",
      "Trained batch 871 batch loss 6.41128 epoch total loss 6.37505198\n",
      "Trained batch 872 batch loss 6.30726147 epoch total loss 6.37497425\n",
      "Trained batch 873 batch loss 6.37247 epoch total loss 6.37497139\n",
      "Trained batch 874 batch loss 6.38874578 epoch total loss 6.37498713\n",
      "Trained batch 875 batch loss 6.31884527 epoch total loss 6.37492323\n",
      "Trained batch 876 batch loss 6.15759659 epoch total loss 6.37467527\n",
      "Trained batch 877 batch loss 6.11452579 epoch total loss 6.37437868\n",
      "Trained batch 878 batch loss 6.32190657 epoch total loss 6.3743186\n",
      "Trained batch 879 batch loss 6.04166698 epoch total loss 6.37394\n",
      "Trained batch 880 batch loss 6.4330368 epoch total loss 6.37400723\n",
      "Trained batch 881 batch loss 5.90635252 epoch total loss 6.37347651\n",
      "Trained batch 882 batch loss 5.8454175 epoch total loss 6.3728776\n",
      "Trained batch 883 batch loss 6.23614168 epoch total loss 6.37272263\n",
      "Trained batch 884 batch loss 6.60685396 epoch total loss 6.37298775\n",
      "Trained batch 885 batch loss 6.72915173 epoch total loss 6.37339\n",
      "Trained batch 886 batch loss 6.59856892 epoch total loss 6.37364435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 887 batch loss 6.59187078 epoch total loss 6.3738904\n",
      "Trained batch 888 batch loss 6.13285446 epoch total loss 6.3736186\n",
      "Trained batch 889 batch loss 5.5796051 epoch total loss 6.37272549\n",
      "Trained batch 890 batch loss 5.96460152 epoch total loss 6.37226725\n",
      "Trained batch 891 batch loss 6.29985046 epoch total loss 6.37218571\n",
      "Trained batch 892 batch loss 6.53972864 epoch total loss 6.37237358\n",
      "Trained batch 893 batch loss 6.26442766 epoch total loss 6.37225294\n",
      "Trained batch 894 batch loss 6.6305871 epoch total loss 6.37254143\n",
      "Trained batch 895 batch loss 6.61352968 epoch total loss 6.37281132\n",
      "Trained batch 896 batch loss 6.85828638 epoch total loss 6.373353\n",
      "Trained batch 897 batch loss 6.29341412 epoch total loss 6.37326384\n",
      "Trained batch 898 batch loss 6.27327 epoch total loss 6.37315273\n",
      "Trained batch 899 batch loss 6.2474761 epoch total loss 6.37301302\n",
      "Trained batch 900 batch loss 5.50719833 epoch total loss 6.37205124\n",
      "Trained batch 901 batch loss 6.75387764 epoch total loss 6.37247515\n",
      "Trained batch 902 batch loss 6.16308594 epoch total loss 6.37224293\n",
      "Trained batch 903 batch loss 6.49979258 epoch total loss 6.37238455\n",
      "Trained batch 904 batch loss 6.56338501 epoch total loss 6.37259579\n",
      "Trained batch 905 batch loss 6.20076847 epoch total loss 6.37240601\n",
      "Trained batch 906 batch loss 6.17200375 epoch total loss 6.37218475\n",
      "Trained batch 907 batch loss 6.63490486 epoch total loss 6.37247419\n",
      "Trained batch 908 batch loss 6.14920282 epoch total loss 6.37222862\n",
      "Trained batch 909 batch loss 6.70749187 epoch total loss 6.37259722\n",
      "Trained batch 910 batch loss 6.61562395 epoch total loss 6.37286425\n",
      "Trained batch 911 batch loss 6.39831924 epoch total loss 6.37289238\n",
      "Trained batch 912 batch loss 6.23803091 epoch total loss 6.37274456\n",
      "Trained batch 913 batch loss 6.65846825 epoch total loss 6.37305737\n",
      "Trained batch 914 batch loss 6.26679516 epoch total loss 6.37294102\n",
      "Trained batch 915 batch loss 6.4000988 epoch total loss 6.37297058\n",
      "Trained batch 916 batch loss 5.72816324 epoch total loss 6.37226629\n",
      "Trained batch 917 batch loss 6.33021688 epoch total loss 6.37222052\n",
      "Trained batch 918 batch loss 6.38327551 epoch total loss 6.37223244\n",
      "Trained batch 919 batch loss 6.81633806 epoch total loss 6.37271595\n",
      "Trained batch 920 batch loss 6.23423815 epoch total loss 6.37256527\n",
      "Trained batch 921 batch loss 6.43809319 epoch total loss 6.37263632\n",
      "Trained batch 922 batch loss 6.14610481 epoch total loss 6.37239075\n",
      "Trained batch 923 batch loss 6.42806292 epoch total loss 6.37245131\n",
      "Trained batch 924 batch loss 6.04017258 epoch total loss 6.37209129\n",
      "Trained batch 925 batch loss 5.96670771 epoch total loss 6.37165308\n",
      "Trained batch 926 batch loss 6.3673315 epoch total loss 6.37164831\n",
      "Trained batch 927 batch loss 6.51348591 epoch total loss 6.37180185\n",
      "Trained batch 928 batch loss 6.73497391 epoch total loss 6.37219286\n",
      "Trained batch 929 batch loss 6.97134066 epoch total loss 6.37283754\n",
      "Trained batch 930 batch loss 7.205 epoch total loss 6.37373257\n",
      "Trained batch 931 batch loss 6.53784084 epoch total loss 6.37390852\n",
      "Trained batch 932 batch loss 6.48078966 epoch total loss 6.37402344\n",
      "Trained batch 933 batch loss 6.64019442 epoch total loss 6.37430859\n",
      "Trained batch 934 batch loss 6.49558973 epoch total loss 6.37443876\n",
      "Trained batch 935 batch loss 5.72940302 epoch total loss 6.37374878\n",
      "Trained batch 936 batch loss 5.60410166 epoch total loss 6.37292624\n",
      "Trained batch 937 batch loss 5.65305901 epoch total loss 6.37215805\n",
      "Trained batch 938 batch loss 6.39252138 epoch total loss 6.37217951\n",
      "Trained batch 939 batch loss 6.32913589 epoch total loss 6.37213373\n",
      "Trained batch 940 batch loss 6.51676 epoch total loss 6.37228727\n",
      "Trained batch 941 batch loss 6.47328 epoch total loss 6.37239456\n",
      "Trained batch 942 batch loss 6.24345303 epoch total loss 6.37225819\n",
      "Trained batch 943 batch loss 6.30986214 epoch total loss 6.37219191\n",
      "Trained batch 944 batch loss 6.37056112 epoch total loss 6.37219048\n",
      "Trained batch 945 batch loss 6.32971144 epoch total loss 6.37214518\n",
      "Trained batch 946 batch loss 6.08297062 epoch total loss 6.37183952\n",
      "Trained batch 947 batch loss 6.25896883 epoch total loss 6.37172031\n",
      "Trained batch 948 batch loss 6.07089758 epoch total loss 6.37140274\n",
      "Trained batch 949 batch loss 6.38493 epoch total loss 6.37141705\n",
      "Trained batch 950 batch loss 6.48619843 epoch total loss 6.37153769\n",
      "Trained batch 951 batch loss 6.36650372 epoch total loss 6.37153292\n",
      "Trained batch 952 batch loss 6.1993432 epoch total loss 6.37135172\n",
      "Trained batch 953 batch loss 6.29184675 epoch total loss 6.37126827\n",
      "Trained batch 954 batch loss 6.21747828 epoch total loss 6.3711071\n",
      "Trained batch 955 batch loss 5.83342457 epoch total loss 6.37054396\n",
      "Trained batch 956 batch loss 6.16351461 epoch total loss 6.37032747\n",
      "Trained batch 957 batch loss 6.09141684 epoch total loss 6.37003613\n",
      "Trained batch 958 batch loss 6.29532146 epoch total loss 6.3699584\n",
      "Trained batch 959 batch loss 6.15353251 epoch total loss 6.36973238\n",
      "Trained batch 960 batch loss 6.42259169 epoch total loss 6.36978722\n",
      "Trained batch 961 batch loss 6.45431376 epoch total loss 6.36987495\n",
      "Trained batch 962 batch loss 6.43053961 epoch total loss 6.3699379\n",
      "Trained batch 963 batch loss 6.61585474 epoch total loss 6.37019348\n",
      "Trained batch 964 batch loss 6.33451128 epoch total loss 6.37015629\n",
      "Trained batch 965 batch loss 6.43957186 epoch total loss 6.37022781\n",
      "Trained batch 966 batch loss 6.31653929 epoch total loss 6.3701725\n",
      "Trained batch 967 batch loss 6.41105747 epoch total loss 6.37021446\n",
      "Trained batch 968 batch loss 6.68723822 epoch total loss 6.37054205\n",
      "Trained batch 969 batch loss 6.53125477 epoch total loss 6.37070799\n",
      "Trained batch 970 batch loss 6.50457382 epoch total loss 6.37084579\n",
      "Trained batch 971 batch loss 6.45765543 epoch total loss 6.37093496\n",
      "Trained batch 972 batch loss 6.38529778 epoch total loss 6.37094975\n",
      "Trained batch 973 batch loss 6.22997379 epoch total loss 6.37080479\n",
      "Trained batch 974 batch loss 6.54373217 epoch total loss 6.37098265\n",
      "Trained batch 975 batch loss 6.32237434 epoch total loss 6.37093258\n",
      "Trained batch 976 batch loss 6.71139336 epoch total loss 6.37128115\n",
      "Trained batch 977 batch loss 6.3142004 epoch total loss 6.3712225\n",
      "Trained batch 978 batch loss 6.48525906 epoch total loss 6.37133932\n",
      "Trained batch 979 batch loss 6.61608076 epoch total loss 6.37158966\n",
      "Trained batch 980 batch loss 6.1489377 epoch total loss 6.37136221\n",
      "Trained batch 981 batch loss 5.92835426 epoch total loss 6.37091064\n",
      "Trained batch 982 batch loss 6.21306276 epoch total loss 6.37074947\n",
      "Trained batch 983 batch loss 6.06898499 epoch total loss 6.37044239\n",
      "Trained batch 984 batch loss 5.97033167 epoch total loss 6.37003565\n",
      "Trained batch 985 batch loss 5.94175625 epoch total loss 6.36960125\n",
      "Trained batch 986 batch loss 5.85721111 epoch total loss 6.3690815\n",
      "Trained batch 987 batch loss 5.74764156 epoch total loss 6.36845207\n",
      "Trained batch 988 batch loss 5.66068459 epoch total loss 6.36773539\n",
      "Trained batch 989 batch loss 5.80701399 epoch total loss 6.3671689\n",
      "Trained batch 990 batch loss 6.1841855 epoch total loss 6.36698389\n",
      "Trained batch 991 batch loss 6.13632965 epoch total loss 6.36675072\n",
      "Trained batch 992 batch loss 6.29208565 epoch total loss 6.36667538\n",
      "Trained batch 993 batch loss 6.50124788 epoch total loss 6.36681128\n",
      "Trained batch 994 batch loss 6.57178688 epoch total loss 6.36701775\n",
      "Trained batch 995 batch loss 6.57311392 epoch total loss 6.36722469\n",
      "Trained batch 996 batch loss 6.65257359 epoch total loss 6.36751127\n",
      "Trained batch 997 batch loss 6.72401285 epoch total loss 6.3678689\n",
      "Trained batch 998 batch loss 6.00695276 epoch total loss 6.36750698\n",
      "Trained batch 999 batch loss 5.7134676 epoch total loss 6.36685228\n",
      "Trained batch 1000 batch loss 6.25364351 epoch total loss 6.3667388\n",
      "Trained batch 1001 batch loss 6.65763569 epoch total loss 6.36702967\n",
      "Trained batch 1002 batch loss 6.35523891 epoch total loss 6.36701775\n",
      "Trained batch 1003 batch loss 6.48097086 epoch total loss 6.36713171\n",
      "Trained batch 1004 batch loss 5.96320534 epoch total loss 6.36672926\n",
      "Trained batch 1005 batch loss 6.11224604 epoch total loss 6.36647606\n",
      "Trained batch 1006 batch loss 6.37311411 epoch total loss 6.36648273\n",
      "Trained batch 1007 batch loss 5.91754723 epoch total loss 6.36603689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1008 batch loss 6.08310795 epoch total loss 6.36575603\n",
      "Trained batch 1009 batch loss 6.1981 epoch total loss 6.36559\n",
      "Trained batch 1010 batch loss 5.78681707 epoch total loss 6.36501694\n",
      "Trained batch 1011 batch loss 5.9653945 epoch total loss 6.36462164\n",
      "Trained batch 1012 batch loss 5.89692116 epoch total loss 6.36415958\n",
      "Trained batch 1013 batch loss 6.15812731 epoch total loss 6.36395597\n",
      "Trained batch 1014 batch loss 6.39788 epoch total loss 6.36399\n",
      "Trained batch 1015 batch loss 6.14413643 epoch total loss 6.36377287\n",
      "Trained batch 1016 batch loss 6.48569965 epoch total loss 6.36389303\n",
      "Trained batch 1017 batch loss 6.60551167 epoch total loss 6.3641305\n",
      "Trained batch 1018 batch loss 6.53839302 epoch total loss 6.36430216\n",
      "Trained batch 1019 batch loss 6.33297539 epoch total loss 6.36427116\n",
      "Trained batch 1020 batch loss 6.02558661 epoch total loss 6.36393881\n",
      "Trained batch 1021 batch loss 5.94202185 epoch total loss 6.36352587\n",
      "Trained batch 1022 batch loss 6.33063555 epoch total loss 6.36349344\n",
      "Trained batch 1023 batch loss 6.36167 epoch total loss 6.36349154\n",
      "Trained batch 1024 batch loss 6.64558506 epoch total loss 6.36376715\n",
      "Trained batch 1025 batch loss 6.75779104 epoch total loss 6.36415148\n",
      "Trained batch 1026 batch loss 6.33393574 epoch total loss 6.36412239\n",
      "Trained batch 1027 batch loss 6.24550581 epoch total loss 6.364007\n",
      "Trained batch 1028 batch loss 6.19558048 epoch total loss 6.36384296\n",
      "Trained batch 1029 batch loss 6.11484671 epoch total loss 6.36360121\n",
      "Trained batch 1030 batch loss 6.05264425 epoch total loss 6.36329937\n",
      "Trained batch 1031 batch loss 6.38702679 epoch total loss 6.36332226\n",
      "Trained batch 1032 batch loss 6.55219173 epoch total loss 6.36350536\n",
      "Trained batch 1033 batch loss 6.51639605 epoch total loss 6.36365366\n",
      "Trained batch 1034 batch loss 6.72578812 epoch total loss 6.36400366\n",
      "Trained batch 1035 batch loss 6.91834259 epoch total loss 6.36453962\n",
      "Trained batch 1036 batch loss 6.84093428 epoch total loss 6.36499929\n",
      "Trained batch 1037 batch loss 6.74024534 epoch total loss 6.36536121\n",
      "Trained batch 1038 batch loss 6.46800041 epoch total loss 6.36546\n",
      "Trained batch 1039 batch loss 6.34190273 epoch total loss 6.36543703\n",
      "Trained batch 1040 batch loss 6.52451134 epoch total loss 6.36558962\n",
      "Trained batch 1041 batch loss 5.84187174 epoch total loss 6.36508656\n",
      "Trained batch 1042 batch loss 5.93832588 epoch total loss 6.36467743\n",
      "Trained batch 1043 batch loss 5.14904261 epoch total loss 6.36351156\n",
      "Trained batch 1044 batch loss 6.00668478 epoch total loss 6.36317\n",
      "Trained batch 1045 batch loss 5.98799276 epoch total loss 6.36281061\n",
      "Trained batch 1046 batch loss 6.42871332 epoch total loss 6.36287355\n",
      "Trained batch 1047 batch loss 6.40835238 epoch total loss 6.36291695\n",
      "Trained batch 1048 batch loss 6.53812361 epoch total loss 6.36308432\n",
      "Trained batch 1049 batch loss 6.3061924 epoch total loss 6.36303\n",
      "Trained batch 1050 batch loss 6.03996 epoch total loss 6.3627224\n",
      "Trained batch 1051 batch loss 6.58678 epoch total loss 6.36293554\n",
      "Trained batch 1052 batch loss 6.05418634 epoch total loss 6.36264229\n",
      "Trained batch 1053 batch loss 6.21517134 epoch total loss 6.3625021\n",
      "Trained batch 1054 batch loss 6.3377471 epoch total loss 6.36247873\n",
      "Trained batch 1055 batch loss 6.12602472 epoch total loss 6.36225462\n",
      "Trained batch 1056 batch loss 6.30895042 epoch total loss 6.36220455\n",
      "Trained batch 1057 batch loss 6.45112133 epoch total loss 6.36228848\n",
      "Trained batch 1058 batch loss 7.1610961 epoch total loss 6.36304379\n",
      "Trained batch 1059 batch loss 6.60112333 epoch total loss 6.36326838\n",
      "Trained batch 1060 batch loss 6.23918772 epoch total loss 6.36315155\n",
      "Trained batch 1061 batch loss 5.71913195 epoch total loss 6.36254454\n",
      "Trained batch 1062 batch loss 5.68502188 epoch total loss 6.36190653\n",
      "Trained batch 1063 batch loss 6.40882111 epoch total loss 6.3619504\n",
      "Trained batch 1064 batch loss 6.06187677 epoch total loss 6.36166859\n",
      "Trained batch 1065 batch loss 6.28771162 epoch total loss 6.36159897\n",
      "Trained batch 1066 batch loss 5.52021122 epoch total loss 6.36081\n",
      "Trained batch 1067 batch loss 5.44068527 epoch total loss 6.35994768\n",
      "Trained batch 1068 batch loss 6.1735611 epoch total loss 6.35977268\n",
      "Trained batch 1069 batch loss 6.96830273 epoch total loss 6.36034203\n",
      "Trained batch 1070 batch loss 6.9891634 epoch total loss 6.36093\n",
      "Trained batch 1071 batch loss 6.7853303 epoch total loss 6.36132574\n",
      "Trained batch 1072 batch loss 6.79020786 epoch total loss 6.36172581\n",
      "Trained batch 1073 batch loss 6.96795368 epoch total loss 6.36229038\n",
      "Trained batch 1074 batch loss 6.78044748 epoch total loss 6.36268\n",
      "Trained batch 1075 batch loss 6.4555006 epoch total loss 6.36276627\n",
      "Trained batch 1076 batch loss 6.41653109 epoch total loss 6.36281633\n",
      "Trained batch 1077 batch loss 6.61830759 epoch total loss 6.36305332\n",
      "Trained batch 1078 batch loss 6.50251055 epoch total loss 6.36318254\n",
      "Trained batch 1079 batch loss 6.5352025 epoch total loss 6.36334181\n",
      "Trained batch 1080 batch loss 6.54036951 epoch total loss 6.36350584\n",
      "Trained batch 1081 batch loss 6.4702282 epoch total loss 6.36360455\n",
      "Trained batch 1082 batch loss 6.35582209 epoch total loss 6.36359739\n",
      "Trained batch 1083 batch loss 6.48514032 epoch total loss 6.36371\n",
      "Trained batch 1084 batch loss 6.45992184 epoch total loss 6.36379862\n",
      "Trained batch 1085 batch loss 6.20485449 epoch total loss 6.36365271\n",
      "Trained batch 1086 batch loss 6.45050144 epoch total loss 6.36373281\n",
      "Trained batch 1087 batch loss 6.49552488 epoch total loss 6.36385393\n",
      "Trained batch 1088 batch loss 6.49654436 epoch total loss 6.363976\n",
      "Trained batch 1089 batch loss 6.57633829 epoch total loss 6.36417103\n",
      "Trained batch 1090 batch loss 6.59402037 epoch total loss 6.36438179\n",
      "Trained batch 1091 batch loss 6.46912336 epoch total loss 6.36447811\n",
      "Trained batch 1092 batch loss 6.15766525 epoch total loss 6.36428881\n",
      "Trained batch 1093 batch loss 6.20711279 epoch total loss 6.3641448\n",
      "Trained batch 1094 batch loss 6.10739708 epoch total loss 6.36391\n",
      "Trained batch 1095 batch loss 6.58462286 epoch total loss 6.36411142\n",
      "Trained batch 1096 batch loss 6.64515734 epoch total loss 6.36436796\n",
      "Trained batch 1097 batch loss 6.66404676 epoch total loss 6.36464119\n",
      "Trained batch 1098 batch loss 6.86697435 epoch total loss 6.36509895\n",
      "Trained batch 1099 batch loss 6.75944281 epoch total loss 6.36545753\n",
      "Trained batch 1100 batch loss 7.0569315 epoch total loss 6.36608601\n",
      "Trained batch 1101 batch loss 6.73803329 epoch total loss 6.36642361\n",
      "Trained batch 1102 batch loss 7.33720207 epoch total loss 6.3673048\n",
      "Trained batch 1103 batch loss 6.94468451 epoch total loss 6.36782837\n",
      "Trained batch 1104 batch loss 6.84314299 epoch total loss 6.36825895\n",
      "Trained batch 1105 batch loss 6.41449833 epoch total loss 6.36830091\n",
      "Trained batch 1106 batch loss 6.55089808 epoch total loss 6.3684659\n",
      "Trained batch 1107 batch loss 6.79671335 epoch total loss 6.36885309\n",
      "Trained batch 1108 batch loss 6.42720604 epoch total loss 6.36890554\n",
      "Trained batch 1109 batch loss 6.20168734 epoch total loss 6.36875486\n",
      "Trained batch 1110 batch loss 6.59152269 epoch total loss 6.36895561\n",
      "Trained batch 1111 batch loss 6.54220247 epoch total loss 6.36911106\n",
      "Trained batch 1112 batch loss 6.56303835 epoch total loss 6.36928558\n",
      "Trained batch 1113 batch loss 6.45636702 epoch total loss 6.36936378\n",
      "Trained batch 1114 batch loss 6.23552799 epoch total loss 6.36924362\n",
      "Trained batch 1115 batch loss 5.90764236 epoch total loss 6.36882973\n",
      "Trained batch 1116 batch loss 5.75644112 epoch total loss 6.36828089\n",
      "Trained batch 1117 batch loss 5.51904917 epoch total loss 6.36752081\n",
      "Trained batch 1118 batch loss 5.7766223 epoch total loss 6.36699247\n",
      "Trained batch 1119 batch loss 5.39475679 epoch total loss 6.3661232\n",
      "Trained batch 1120 batch loss 5.24992085 epoch total loss 6.36512661\n",
      "Trained batch 1121 batch loss 4.99968719 epoch total loss 6.36390829\n",
      "Trained batch 1122 batch loss 5.22728348 epoch total loss 6.36289501\n",
      "Trained batch 1123 batch loss 5.95361328 epoch total loss 6.36253071\n",
      "Trained batch 1124 batch loss 6.62860441 epoch total loss 6.36276722\n",
      "Trained batch 1125 batch loss 6.30198336 epoch total loss 6.36271334\n",
      "Trained batch 1126 batch loss 6.48100901 epoch total loss 6.36281824\n",
      "Trained batch 1127 batch loss 6.42222595 epoch total loss 6.36287117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1128 batch loss 6.31150723 epoch total loss 6.36282539\n",
      "Trained batch 1129 batch loss 6.31285477 epoch total loss 6.36278152\n",
      "Trained batch 1130 batch loss 6.06507587 epoch total loss 6.36251783\n",
      "Trained batch 1131 batch loss 6.21212816 epoch total loss 6.36238432\n",
      "Trained batch 1132 batch loss 6.39131546 epoch total loss 6.36241\n",
      "Trained batch 1133 batch loss 6.01650858 epoch total loss 6.36210489\n",
      "Trained batch 1134 batch loss 5.94616842 epoch total loss 6.3617382\n",
      "Trained batch 1135 batch loss 6.3133688 epoch total loss 6.36169529\n",
      "Trained batch 1136 batch loss 6.35564947 epoch total loss 6.36169\n",
      "Trained batch 1137 batch loss 6.47181559 epoch total loss 6.36178684\n",
      "Trained batch 1138 batch loss 6.27607679 epoch total loss 6.3617115\n",
      "Trained batch 1139 batch loss 6.38833189 epoch total loss 6.36173439\n",
      "Trained batch 1140 batch loss 6.31147051 epoch total loss 6.36169052\n",
      "Trained batch 1141 batch loss 6.29440212 epoch total loss 6.36163139\n",
      "Trained batch 1142 batch loss 6.10379267 epoch total loss 6.36140585\n",
      "Trained batch 1143 batch loss 5.95997572 epoch total loss 6.3610549\n",
      "Trained batch 1144 batch loss 6.22374964 epoch total loss 6.36093473\n",
      "Trained batch 1145 batch loss 6.25583935 epoch total loss 6.3608427\n",
      "Trained batch 1146 batch loss 6.41753292 epoch total loss 6.3608923\n",
      "Trained batch 1147 batch loss 6.51884651 epoch total loss 6.36103\n",
      "Trained batch 1148 batch loss 6.14261246 epoch total loss 6.36084\n",
      "Trained batch 1149 batch loss 6.15553045 epoch total loss 6.36066151\n",
      "Trained batch 1150 batch loss 6.33322048 epoch total loss 6.36063719\n",
      "Trained batch 1151 batch loss 6.50346279 epoch total loss 6.36076117\n",
      "Trained batch 1152 batch loss 6.11116362 epoch total loss 6.36054468\n",
      "Trained batch 1153 batch loss 6.00639153 epoch total loss 6.3602376\n",
      "Trained batch 1154 batch loss 6.80001974 epoch total loss 6.36061859\n",
      "Trained batch 1155 batch loss 6.59085369 epoch total loss 6.36081791\n",
      "Trained batch 1156 batch loss 6.46702194 epoch total loss 6.36090946\n",
      "Trained batch 1157 batch loss 6.24313593 epoch total loss 6.3608079\n",
      "Trained batch 1158 batch loss 6.05455971 epoch total loss 6.36054325\n",
      "Trained batch 1159 batch loss 5.98225212 epoch total loss 6.36021709\n",
      "Trained batch 1160 batch loss 6.36352491 epoch total loss 6.36022\n",
      "Trained batch 1161 batch loss 6.64668322 epoch total loss 6.36046648\n",
      "Trained batch 1162 batch loss 6.75300074 epoch total loss 6.36080408\n",
      "Trained batch 1163 batch loss 6.74504519 epoch total loss 6.36113453\n",
      "Trained batch 1164 batch loss 6.98048449 epoch total loss 6.36166668\n",
      "Trained batch 1165 batch loss 7.0367322 epoch total loss 6.36224604\n",
      "Trained batch 1166 batch loss 6.85665131 epoch total loss 6.36267\n",
      "Trained batch 1167 batch loss 6.12428474 epoch total loss 6.36246586\n",
      "Trained batch 1168 batch loss 5.59002733 epoch total loss 6.36180449\n",
      "Trained batch 1169 batch loss 5.42799282 epoch total loss 6.36100578\n",
      "Trained batch 1170 batch loss 5.27960205 epoch total loss 6.36008167\n",
      "Trained batch 1171 batch loss 5.83493328 epoch total loss 6.35963297\n",
      "Trained batch 1172 batch loss 5.7844429 epoch total loss 6.3591423\n",
      "Trained batch 1173 batch loss 6.21517324 epoch total loss 6.35901976\n",
      "Trained batch 1174 batch loss 6.47434521 epoch total loss 6.35911798\n",
      "Trained batch 1175 batch loss 6.38369656 epoch total loss 6.35913897\n",
      "Trained batch 1176 batch loss 6.37949753 epoch total loss 6.35915613\n",
      "Trained batch 1177 batch loss 6.53168201 epoch total loss 6.359303\n",
      "Trained batch 1178 batch loss 6.39274788 epoch total loss 6.35933113\n",
      "Trained batch 1179 batch loss 5.34182739 epoch total loss 6.35846806\n",
      "Trained batch 1180 batch loss 5.33400106 epoch total loss 6.35759974\n",
      "Trained batch 1181 batch loss 5.19449425 epoch total loss 6.35661459\n",
      "Trained batch 1182 batch loss 5.66901922 epoch total loss 6.35603285\n",
      "Trained batch 1183 batch loss 6.70627308 epoch total loss 6.35632896\n",
      "Trained batch 1184 batch loss 6.53562832 epoch total loss 6.3564806\n",
      "Trained batch 1185 batch loss 6.20656157 epoch total loss 6.35635376\n",
      "Trained batch 1186 batch loss 6.17956829 epoch total loss 6.35620499\n",
      "Trained batch 1187 batch loss 6.52855492 epoch total loss 6.35635\n",
      "Trained batch 1188 batch loss 5.89186811 epoch total loss 6.35595894\n",
      "Trained batch 1189 batch loss 6.01314211 epoch total loss 6.35567093\n",
      "Trained batch 1190 batch loss 5.98110437 epoch total loss 6.35535574\n",
      "Trained batch 1191 batch loss 5.94386816 epoch total loss 6.35501051\n",
      "Trained batch 1192 batch loss 5.99029 epoch total loss 6.35470438\n",
      "Trained batch 1193 batch loss 6.04628563 epoch total loss 6.35444593\n",
      "Trained batch 1194 batch loss 6.21073866 epoch total loss 6.35432577\n",
      "Trained batch 1195 batch loss 6.49919033 epoch total loss 6.35444689\n",
      "Trained batch 1196 batch loss 6.4163208 epoch total loss 6.35449886\n",
      "Trained batch 1197 batch loss 6.36250544 epoch total loss 6.35450506\n",
      "Trained batch 1198 batch loss 6.40930462 epoch total loss 6.35455084\n",
      "Trained batch 1199 batch loss 6.47245741 epoch total loss 6.35464954\n",
      "Trained batch 1200 batch loss 6.30494881 epoch total loss 6.35460806\n",
      "Trained batch 1201 batch loss 6.598804 epoch total loss 6.35481119\n",
      "Trained batch 1202 batch loss 6.64626217 epoch total loss 6.3550539\n",
      "Trained batch 1203 batch loss 6.49888563 epoch total loss 6.35517359\n",
      "Trained batch 1204 batch loss 6.26941442 epoch total loss 6.35510254\n",
      "Trained batch 1205 batch loss 5.42175341 epoch total loss 6.35432816\n",
      "Trained batch 1206 batch loss 5.46965742 epoch total loss 6.35359478\n",
      "Trained batch 1207 batch loss 5.774652 epoch total loss 6.3531146\n",
      "Trained batch 1208 batch loss 6.27132463 epoch total loss 6.35304737\n",
      "Trained batch 1209 batch loss 6.13656139 epoch total loss 6.35286808\n",
      "Trained batch 1210 batch loss 6.01690483 epoch total loss 6.35259056\n",
      "Trained batch 1211 batch loss 5.79939795 epoch total loss 6.35213375\n",
      "Trained batch 1212 batch loss 6.09717417 epoch total loss 6.35192347\n",
      "Trained batch 1213 batch loss 6.20088863 epoch total loss 6.35179901\n",
      "Trained batch 1214 batch loss 5.99880362 epoch total loss 6.35150814\n",
      "Trained batch 1215 batch loss 6.63027668 epoch total loss 6.3517375\n",
      "Trained batch 1216 batch loss 6.06572866 epoch total loss 6.3515029\n",
      "Trained batch 1217 batch loss 6.36344957 epoch total loss 6.35151243\n",
      "Trained batch 1218 batch loss 6.34476662 epoch total loss 6.35150671\n",
      "Trained batch 1219 batch loss 6.32976341 epoch total loss 6.35148859\n",
      "Trained batch 1220 batch loss 6.23032618 epoch total loss 6.35138941\n",
      "Trained batch 1221 batch loss 6.25789165 epoch total loss 6.35131311\n",
      "Trained batch 1222 batch loss 6.07747316 epoch total loss 6.351089\n",
      "Trained batch 1223 batch loss 6.29720354 epoch total loss 6.35104513\n",
      "Trained batch 1224 batch loss 5.99476528 epoch total loss 6.35075378\n",
      "Trained batch 1225 batch loss 6.57387781 epoch total loss 6.35093594\n",
      "Trained batch 1226 batch loss 6.34668589 epoch total loss 6.3509326\n",
      "Trained batch 1227 batch loss 6.43653536 epoch total loss 6.35100222\n",
      "Trained batch 1228 batch loss 6.43769836 epoch total loss 6.35107279\n",
      "Trained batch 1229 batch loss 6.55736732 epoch total loss 6.35124\n",
      "Trained batch 1230 batch loss 6.65199232 epoch total loss 6.35148478\n",
      "Trained batch 1231 batch loss 6.8922081 epoch total loss 6.35192394\n",
      "Trained batch 1232 batch loss 6.52903175 epoch total loss 6.35206747\n",
      "Trained batch 1233 batch loss 6.39933634 epoch total loss 6.35210562\n",
      "Trained batch 1234 batch loss 6.31578588 epoch total loss 6.35207653\n",
      "Trained batch 1235 batch loss 6.09310436 epoch total loss 6.35186672\n",
      "Trained batch 1236 batch loss 6.02667713 epoch total loss 6.35160398\n",
      "Trained batch 1237 batch loss 6.34169245 epoch total loss 6.35159588\n",
      "Trained batch 1238 batch loss 6.24276924 epoch total loss 6.35150814\n",
      "Trained batch 1239 batch loss 5.72911358 epoch total loss 6.35100555\n",
      "Trained batch 1240 batch loss 5.53102541 epoch total loss 6.35034466\n",
      "Trained batch 1241 batch loss 5.80919 epoch total loss 6.34990835\n",
      "Trained batch 1242 batch loss 6.00625 epoch total loss 6.34963179\n",
      "Trained batch 1243 batch loss 6.05042791 epoch total loss 6.34939098\n",
      "Trained batch 1244 batch loss 6.15835142 epoch total loss 6.34923744\n",
      "Trained batch 1245 batch loss 5.99733067 epoch total loss 6.34895468\n",
      "Trained batch 1246 batch loss 6.20325851 epoch total loss 6.34883785\n",
      "Trained batch 1247 batch loss 6.36892891 epoch total loss 6.34885406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1248 batch loss 6.42228031 epoch total loss 6.34891272\n",
      "Trained batch 1249 batch loss 6.49785376 epoch total loss 6.3490324\n",
      "Trained batch 1250 batch loss 6.54011774 epoch total loss 6.34918499\n",
      "Trained batch 1251 batch loss 5.61597919 epoch total loss 6.34859943\n",
      "Trained batch 1252 batch loss 4.67433786 epoch total loss 6.34726191\n",
      "Trained batch 1253 batch loss 4.9554019 epoch total loss 6.34615135\n",
      "Trained batch 1254 batch loss 6.73122787 epoch total loss 6.34645844\n",
      "Trained batch 1255 batch loss 7.32914114 epoch total loss 6.3472414\n",
      "Trained batch 1256 batch loss 7.3878231 epoch total loss 6.34806967\n",
      "Trained batch 1257 batch loss 6.66363859 epoch total loss 6.34832096\n",
      "Trained batch 1258 batch loss 5.74510574 epoch total loss 6.34784126\n",
      "Trained batch 1259 batch loss 5.36800957 epoch total loss 6.34706306\n",
      "Trained batch 1260 batch loss 5.23065138 epoch total loss 6.3461771\n",
      "Trained batch 1261 batch loss 5.6189127 epoch total loss 6.34560061\n",
      "Trained batch 1262 batch loss 6.60463095 epoch total loss 6.34580564\n",
      "Trained batch 1263 batch loss 7.02246332 epoch total loss 6.34634161\n",
      "Trained batch 1264 batch loss 6.36987686 epoch total loss 6.34636\n",
      "Trained batch 1265 batch loss 6.23989964 epoch total loss 6.34627581\n",
      "Trained batch 1266 batch loss 6.43989897 epoch total loss 6.34634972\n",
      "Trained batch 1267 batch loss 6.08454 epoch total loss 6.34614325\n",
      "Trained batch 1268 batch loss 6.44398451 epoch total loss 6.34622049\n",
      "Trained batch 1269 batch loss 6.54894066 epoch total loss 6.34637976\n",
      "Trained batch 1270 batch loss 6.3888936 epoch total loss 6.34641314\n",
      "Trained batch 1271 batch loss 6.58341694 epoch total loss 6.34659958\n",
      "Trained batch 1272 batch loss 6.78144693 epoch total loss 6.34694147\n",
      "Trained batch 1273 batch loss 6.47822809 epoch total loss 6.34704447\n",
      "Trained batch 1274 batch loss 6.51254082 epoch total loss 6.34717464\n",
      "Trained batch 1275 batch loss 6.12832689 epoch total loss 6.34700298\n",
      "Trained batch 1276 batch loss 6.74585772 epoch total loss 6.34731579\n",
      "Trained batch 1277 batch loss 6.65927792 epoch total loss 6.34756\n",
      "Trained batch 1278 batch loss 6.35314083 epoch total loss 6.34756422\n",
      "Trained batch 1279 batch loss 5.84464836 epoch total loss 6.34717083\n",
      "Trained batch 1280 batch loss 6.11486483 epoch total loss 6.34698963\n",
      "Trained batch 1281 batch loss 6.39587736 epoch total loss 6.34702778\n",
      "Trained batch 1282 batch loss 6.21587324 epoch total loss 6.34692526\n",
      "Trained batch 1283 batch loss 6.11977768 epoch total loss 6.34674835\n",
      "Trained batch 1284 batch loss 6.65188885 epoch total loss 6.34698582\n",
      "Trained batch 1285 batch loss 6.40745354 epoch total loss 6.34703255\n",
      "Trained batch 1286 batch loss 6.37909412 epoch total loss 6.34705734\n",
      "Trained batch 1287 batch loss 6.29728031 epoch total loss 6.34701872\n",
      "Trained batch 1288 batch loss 6.19027185 epoch total loss 6.34689713\n",
      "Trained batch 1289 batch loss 6.42931175 epoch total loss 6.34696102\n",
      "Trained batch 1290 batch loss 7.03146839 epoch total loss 6.34749174\n",
      "Trained batch 1291 batch loss 6.57393122 epoch total loss 6.34766674\n",
      "Trained batch 1292 batch loss 6.56486 epoch total loss 6.34783459\n",
      "Trained batch 1293 batch loss 6.62304878 epoch total loss 6.34804726\n",
      "Trained batch 1294 batch loss 6.82727623 epoch total loss 6.34841776\n",
      "Trained batch 1295 batch loss 6.68735504 epoch total loss 6.34867954\n",
      "Trained batch 1296 batch loss 6.47005129 epoch total loss 6.348773\n",
      "Trained batch 1297 batch loss 6.96153212 epoch total loss 6.34924555\n",
      "Trained batch 1298 batch loss 6.72595453 epoch total loss 6.34953547\n",
      "Trained batch 1299 batch loss 6.69589853 epoch total loss 6.34980249\n",
      "Trained batch 1300 batch loss 6.56987143 epoch total loss 6.34997225\n",
      "Trained batch 1301 batch loss 6.4414711 epoch total loss 6.35004234\n",
      "Trained batch 1302 batch loss 6.37043142 epoch total loss 6.35005808\n",
      "Trained batch 1303 batch loss 6.27198696 epoch total loss 6.34999847\n",
      "Trained batch 1304 batch loss 6.39972305 epoch total loss 6.35003614\n",
      "Trained batch 1305 batch loss 5.75963593 epoch total loss 6.3495841\n",
      "Trained batch 1306 batch loss 5.62195206 epoch total loss 6.34902668\n",
      "Trained batch 1307 batch loss 5.8569212 epoch total loss 6.34865\n",
      "Trained batch 1308 batch loss 5.9991169 epoch total loss 6.34838247\n",
      "Trained batch 1309 batch loss 5.94268847 epoch total loss 6.34807253\n",
      "Trained batch 1310 batch loss 6.30301571 epoch total loss 6.34803772\n",
      "Trained batch 1311 batch loss 6.64925861 epoch total loss 6.34826803\n",
      "Trained batch 1312 batch loss 6.35443783 epoch total loss 6.34827232\n",
      "Trained batch 1313 batch loss 6.44600439 epoch total loss 6.34834719\n",
      "Trained batch 1314 batch loss 6.36168289 epoch total loss 6.3483572\n",
      "Trained batch 1315 batch loss 6.18952227 epoch total loss 6.34823608\n",
      "Trained batch 1316 batch loss 5.76285744 epoch total loss 6.34779119\n",
      "Trained batch 1317 batch loss 5.87705088 epoch total loss 6.34743357\n",
      "Trained batch 1318 batch loss 6.53954 epoch total loss 6.347579\n",
      "Trained batch 1319 batch loss 6.57902527 epoch total loss 6.34775496\n",
      "Trained batch 1320 batch loss 6.30083132 epoch total loss 6.34771919\n",
      "Trained batch 1321 batch loss 6.50098896 epoch total loss 6.34783506\n",
      "Trained batch 1322 batch loss 6.32577705 epoch total loss 6.34781885\n",
      "Trained batch 1323 batch loss 6.49450922 epoch total loss 6.34792948\n",
      "Trained batch 1324 batch loss 6.46894169 epoch total loss 6.34802055\n",
      "Trained batch 1325 batch loss 7.09202 epoch total loss 6.34858179\n",
      "Trained batch 1326 batch loss 6.87939548 epoch total loss 6.34898281\n",
      "Trained batch 1327 batch loss 7.05856705 epoch total loss 6.34951735\n",
      "Trained batch 1328 batch loss 6.97931433 epoch total loss 6.3499918\n",
      "Trained batch 1329 batch loss 6.80182886 epoch total loss 6.35033178\n",
      "Trained batch 1330 batch loss 6.21284437 epoch total loss 6.35022831\n",
      "Trained batch 1331 batch loss 6.40616274 epoch total loss 6.35027027\n",
      "Trained batch 1332 batch loss 6.70514393 epoch total loss 6.35053682\n",
      "Trained batch 1333 batch loss 6.95336342 epoch total loss 6.35098886\n",
      "Trained batch 1334 batch loss 6.57407904 epoch total loss 6.35115623\n",
      "Trained batch 1335 batch loss 6.33345366 epoch total loss 6.35114241\n",
      "Trained batch 1336 batch loss 6.54575348 epoch total loss 6.35128832\n",
      "Trained batch 1337 batch loss 6.51067543 epoch total loss 6.35140753\n",
      "Trained batch 1338 batch loss 6.58380604 epoch total loss 6.35158157\n",
      "Trained batch 1339 batch loss 6.85893631 epoch total loss 6.35196066\n",
      "Trained batch 1340 batch loss 7.15985775 epoch total loss 6.35256386\n",
      "Trained batch 1341 batch loss 6.8369832 epoch total loss 6.35292482\n",
      "Trained batch 1342 batch loss 6.19954205 epoch total loss 6.35281038\n",
      "Trained batch 1343 batch loss 6.76529551 epoch total loss 6.35311794\n",
      "Trained batch 1344 batch loss 7.14503479 epoch total loss 6.35370731\n",
      "Trained batch 1345 batch loss 6.89090443 epoch total loss 6.35410643\n",
      "Trained batch 1346 batch loss 6.5510931 epoch total loss 6.35425282\n",
      "Trained batch 1347 batch loss 6.79589653 epoch total loss 6.3545804\n",
      "Trained batch 1348 batch loss 6.61423874 epoch total loss 6.35477304\n",
      "Trained batch 1349 batch loss 6.69028902 epoch total loss 6.35502195\n",
      "Trained batch 1350 batch loss 6.90499401 epoch total loss 6.35542965\n",
      "Trained batch 1351 batch loss 6.46311283 epoch total loss 6.35550928\n",
      "Trained batch 1352 batch loss 5.96237659 epoch total loss 6.35521793\n",
      "Trained batch 1353 batch loss 6.40876484 epoch total loss 6.35525799\n",
      "Trained batch 1354 batch loss 5.58235168 epoch total loss 6.35468674\n",
      "Trained batch 1355 batch loss 5.72759342 epoch total loss 6.35422421\n",
      "Trained batch 1356 batch loss 6.25996447 epoch total loss 6.35415459\n",
      "Trained batch 1357 batch loss 5.48151398 epoch total loss 6.35351133\n",
      "Trained batch 1358 batch loss 4.95922232 epoch total loss 6.35248423\n",
      "Trained batch 1359 batch loss 5.13626 epoch total loss 6.35158968\n",
      "Trained batch 1360 batch loss 5.50538826 epoch total loss 6.35096788\n",
      "Trained batch 1361 batch loss 5.91375303 epoch total loss 6.35064697\n",
      "Trained batch 1362 batch loss 6.21600676 epoch total loss 6.35054779\n",
      "Trained batch 1363 batch loss 6.33164406 epoch total loss 6.35053444\n",
      "Trained batch 1364 batch loss 6.82709169 epoch total loss 6.35088396\n",
      "Trained batch 1365 batch loss 6.73909712 epoch total loss 6.35116816\n",
      "Trained batch 1366 batch loss 6.94136 epoch total loss 6.3516\n",
      "Trained batch 1367 batch loss 6.689291 epoch total loss 6.35184765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1368 batch loss 6.42256 epoch total loss 6.35189962\n",
      "Trained batch 1369 batch loss 6.42765951 epoch total loss 6.35195494\n",
      "Trained batch 1370 batch loss 6.62322617 epoch total loss 6.35215282\n",
      "Trained batch 1371 batch loss 6.47570801 epoch total loss 6.35224295\n",
      "Trained batch 1372 batch loss 6.22606421 epoch total loss 6.35215044\n",
      "Trained batch 1373 batch loss 6.53089809 epoch total loss 6.35228109\n",
      "Trained batch 1374 batch loss 6.5073328 epoch total loss 6.3523941\n",
      "Trained batch 1375 batch loss 6.4584465 epoch total loss 6.35247087\n",
      "Trained batch 1376 batch loss 6.50805473 epoch total loss 6.35258389\n",
      "Trained batch 1377 batch loss 6.76324558 epoch total loss 6.35288239\n",
      "Trained batch 1378 batch loss 6.81292677 epoch total loss 6.35321569\n",
      "Trained batch 1379 batch loss 6.7889061 epoch total loss 6.35353184\n",
      "Trained batch 1380 batch loss 6.87438774 epoch total loss 6.35390902\n",
      "Trained batch 1381 batch loss 6.76996422 epoch total loss 6.35421\n",
      "Trained batch 1382 batch loss 6.85949564 epoch total loss 6.35457563\n",
      "Trained batch 1383 batch loss 6.83582687 epoch total loss 6.35492373\n",
      "Trained batch 1384 batch loss 6.35018826 epoch total loss 6.35492039\n",
      "Trained batch 1385 batch loss 6.76990461 epoch total loss 6.35522\n",
      "Trained batch 1386 batch loss 6.50462198 epoch total loss 6.35532761\n",
      "Trained batch 1387 batch loss 6.3346734 epoch total loss 6.3553133\n",
      "Trained batch 1388 batch loss 6.51499748 epoch total loss 6.35542774\n",
      "Epoch 3 train loss 6.3554277420043945\n",
      "Validated batch 1 batch loss 5.99807644\n",
      "Validated batch 2 batch loss 6.60405445\n",
      "Validated batch 3 batch loss 6.41604\n",
      "Validated batch 4 batch loss 6.49100256\n",
      "Validated batch 5 batch loss 6.45529842\n",
      "Validated batch 6 batch loss 6.64311504\n",
      "Validated batch 7 batch loss 6.28133392\n",
      "Validated batch 8 batch loss 6.59041643\n",
      "Validated batch 9 batch loss 6.19514132\n",
      "Validated batch 10 batch loss 6.42409706\n",
      "Validated batch 11 batch loss 6.36452055\n",
      "Validated batch 12 batch loss 5.73772144\n",
      "Validated batch 13 batch loss 5.94456959\n",
      "Validated batch 14 batch loss 6.57753849\n",
      "Validated batch 15 batch loss 6.51765728\n",
      "Validated batch 16 batch loss 6.17727423\n",
      "Validated batch 17 batch loss 6.51389885\n",
      "Validated batch 18 batch loss 6.48168039\n",
      "Validated batch 19 batch loss 6.45948887\n",
      "Validated batch 20 batch loss 6.60388231\n",
      "Validated batch 21 batch loss 6.53102303\n",
      "Validated batch 22 batch loss 6.26172829\n",
      "Validated batch 23 batch loss 6.01669693\n",
      "Validated batch 24 batch loss 6.38924265\n",
      "Validated batch 25 batch loss 6.672822\n",
      "Validated batch 26 batch loss 6.24662\n",
      "Validated batch 27 batch loss 5.77137041\n",
      "Validated batch 28 batch loss 6.19966364\n",
      "Validated batch 29 batch loss 6.36633\n",
      "Validated batch 30 batch loss 5.87629223\n",
      "Validated batch 31 batch loss 6.10408163\n",
      "Validated batch 32 batch loss 6.06690788\n",
      "Validated batch 33 batch loss 6.4165349\n",
      "Validated batch 34 batch loss 6.09209204\n",
      "Validated batch 35 batch loss 5.94844913\n",
      "Validated batch 36 batch loss 6.22267866\n",
      "Validated batch 37 batch loss 6.22509623\n",
      "Validated batch 38 batch loss 6.25419378\n",
      "Validated batch 39 batch loss 6.30329609\n",
      "Validated batch 40 batch loss 6.3439517\n",
      "Validated batch 41 batch loss 6.47796297\n",
      "Validated batch 42 batch loss 6.33230591\n",
      "Validated batch 43 batch loss 6.13929796\n",
      "Validated batch 44 batch loss 6.58637238\n",
      "Validated batch 45 batch loss 5.44734335\n",
      "Validated batch 46 batch loss 6.72447443\n",
      "Validated batch 47 batch loss 6.29278612\n",
      "Validated batch 48 batch loss 6.39482689\n",
      "Validated batch 49 batch loss 6.01617289\n",
      "Validated batch 50 batch loss 5.8336277\n",
      "Validated batch 51 batch loss 6.08330536\n",
      "Validated batch 52 batch loss 6.44338131\n",
      "Validated batch 53 batch loss 6.26443529\n",
      "Validated batch 54 batch loss 6.32079506\n",
      "Validated batch 55 batch loss 6.44913292\n",
      "Validated batch 56 batch loss 6.4871788\n",
      "Validated batch 57 batch loss 6.45989609\n",
      "Validated batch 58 batch loss 6.23549604\n",
      "Validated batch 59 batch loss 6.50177383\n",
      "Validated batch 60 batch loss 6.2774539\n",
      "Validated batch 61 batch loss 6.57309866\n",
      "Validated batch 62 batch loss 6.66079044\n",
      "Validated batch 63 batch loss 6.18184137\n",
      "Validated batch 64 batch loss 6.70084572\n",
      "Validated batch 65 batch loss 6.24248362\n",
      "Validated batch 66 batch loss 6.18459034\n",
      "Validated batch 67 batch loss 6.26875782\n",
      "Validated batch 68 batch loss 6.41290188\n",
      "Validated batch 69 batch loss 6.60845661\n",
      "Validated batch 70 batch loss 6.23536301\n",
      "Validated batch 71 batch loss 5.96870136\n",
      "Validated batch 72 batch loss 6.3341136\n",
      "Validated batch 73 batch loss 6.33671618\n",
      "Validated batch 74 batch loss 6.15794468\n",
      "Validated batch 75 batch loss 6.59724045\n",
      "Validated batch 76 batch loss 6.37239313\n",
      "Validated batch 77 batch loss 6.42092037\n",
      "Validated batch 78 batch loss 6.34686661\n",
      "Validated batch 79 batch loss 6.50229692\n",
      "Validated batch 80 batch loss 6.14598179\n",
      "Validated batch 81 batch loss 6.46132231\n",
      "Validated batch 82 batch loss 6.12831163\n",
      "Validated batch 83 batch loss 6.62749052\n",
      "Validated batch 84 batch loss 6.65296268\n",
      "Validated batch 85 batch loss 6.22936296\n",
      "Validated batch 86 batch loss 6.37746286\n",
      "Validated batch 87 batch loss 5.95343256\n",
      "Validated batch 88 batch loss 6.38739347\n",
      "Validated batch 89 batch loss 6.43117714\n",
      "Validated batch 90 batch loss 6.39685202\n",
      "Validated batch 91 batch loss 6.41012859\n",
      "Validated batch 92 batch loss 6.12262821\n",
      "Validated batch 93 batch loss 6.48581839\n",
      "Validated batch 94 batch loss 6.25657558\n",
      "Validated batch 95 batch loss 6.1540823\n",
      "Validated batch 96 batch loss 6.09785509\n",
      "Validated batch 97 batch loss 6.4619875\n",
      "Validated batch 98 batch loss 6.3332181\n",
      "Validated batch 99 batch loss 6.39802933\n",
      "Validated batch 100 batch loss 6.34627914\n",
      "Validated batch 101 batch loss 6.13001919\n",
      "Validated batch 102 batch loss 6.47831869\n",
      "Validated batch 103 batch loss 6.80826283\n",
      "Validated batch 104 batch loss 6.43696404\n",
      "Validated batch 105 batch loss 6.3931284\n",
      "Validated batch 106 batch loss 6.25742579\n",
      "Validated batch 107 batch loss 6.47817898\n",
      "Validated batch 108 batch loss 6.2069912\n",
      "Validated batch 109 batch loss 6.61875677\n",
      "Validated batch 110 batch loss 6.09389544\n",
      "Validated batch 111 batch loss 6.32534122\n",
      "Validated batch 112 batch loss 6.29095554\n",
      "Validated batch 113 batch loss 6.19521809\n",
      "Validated batch 114 batch loss 6.4996562\n",
      "Validated batch 115 batch loss 6.47874212\n",
      "Validated batch 116 batch loss 6.32356739\n",
      "Validated batch 117 batch loss 6.42821455\n",
      "Validated batch 118 batch loss 6.12345409\n",
      "Validated batch 119 batch loss 6.67212677\n",
      "Validated batch 120 batch loss 6.69756699\n",
      "Validated batch 121 batch loss 6.44539165\n",
      "Validated batch 122 batch loss 6.45480824\n",
      "Validated batch 123 batch loss 6.42604542\n",
      "Validated batch 124 batch loss 6.55856085\n",
      "Validated batch 125 batch loss 6.40747261\n",
      "Validated batch 126 batch loss 6.7313695\n",
      "Validated batch 127 batch loss 6.75983429\n",
      "Validated batch 128 batch loss 6.02856159\n",
      "Validated batch 129 batch loss 6.61837482\n",
      "Validated batch 130 batch loss 6.20530844\n",
      "Validated batch 131 batch loss 6.55209541\n",
      "Validated batch 132 batch loss 6.61267805\n",
      "Validated batch 133 batch loss 5.8985076\n",
      "Validated batch 134 batch loss 6.07045937\n",
      "Validated batch 135 batch loss 6.45811033\n",
      "Validated batch 136 batch loss 6.21626377\n",
      "Validated batch 137 batch loss 6.79505682\n",
      "Validated batch 138 batch loss 6.22260189\n",
      "Validated batch 139 batch loss 6.80043364\n",
      "Validated batch 140 batch loss 6.42421389\n",
      "Validated batch 141 batch loss 6.49357939\n",
      "Validated batch 142 batch loss 6.36990356\n",
      "Validated batch 143 batch loss 6.36212826\n",
      "Validated batch 144 batch loss 6.56256342\n",
      "Validated batch 145 batch loss 6.2977581\n",
      "Validated batch 146 batch loss 6.08626699\n",
      "Validated batch 147 batch loss 6.14892721\n",
      "Validated batch 148 batch loss 6.36153269\n",
      "Validated batch 149 batch loss 6.29961777\n",
      "Validated batch 150 batch loss 6.23901224\n",
      "Validated batch 151 batch loss 6.21122551\n",
      "Validated batch 152 batch loss 6.09714222\n",
      "Validated batch 153 batch loss 6.20903349\n",
      "Validated batch 154 batch loss 6.38011074\n",
      "Validated batch 155 batch loss 6.46940231\n",
      "Validated batch 156 batch loss 6.59931517\n",
      "Validated batch 157 batch loss 6.76588535\n",
      "Validated batch 158 batch loss 7.21596813\n",
      "Validated batch 159 batch loss 6.8374567\n",
      "Validated batch 160 batch loss 6.33059454\n",
      "Validated batch 161 batch loss 5.98602533\n",
      "Validated batch 162 batch loss 6.34971952\n",
      "Validated batch 163 batch loss 6.38053846\n",
      "Validated batch 164 batch loss 6.29017544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 165 batch loss 6.12607288\n",
      "Validated batch 166 batch loss 6.13941479\n",
      "Validated batch 167 batch loss 6.39980459\n",
      "Validated batch 168 batch loss 6.11761951\n",
      "Validated batch 169 batch loss 6.21668768\n",
      "Validated batch 170 batch loss 6.40309906\n",
      "Validated batch 171 batch loss 6.46090126\n",
      "Validated batch 172 batch loss 6.15557337\n",
      "Validated batch 173 batch loss 6.16800833\n",
      "Validated batch 174 batch loss 6.01588631\n",
      "Validated batch 175 batch loss 6.6806674\n",
      "Validated batch 176 batch loss 6.2374754\n",
      "Validated batch 177 batch loss 6.26475334\n",
      "Validated batch 178 batch loss 6.38232803\n",
      "Validated batch 179 batch loss 5.92709923\n",
      "Validated batch 180 batch loss 5.79614258\n",
      "Validated batch 181 batch loss 6.2477107\n",
      "Validated batch 182 batch loss 6.28242064\n",
      "Validated batch 183 batch loss 5.83788776\n",
      "Validated batch 184 batch loss 6.51874208\n",
      "Validated batch 185 batch loss 3.28005433\n",
      "Epoch 3 val loss 6.3196940422058105\n",
      "Start epoch 4 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 6.60141 epoch total loss 6.60141\n",
      "Trained batch 2 batch loss 6.37535667 epoch total loss 6.48838329\n",
      "Trained batch 3 batch loss 6.3847971 epoch total loss 6.45385504\n",
      "Trained batch 4 batch loss 6.2626195 epoch total loss 6.40604591\n",
      "Trained batch 5 batch loss 6.34968424 epoch total loss 6.39477348\n",
      "Trained batch 6 batch loss 6.60394716 epoch total loss 6.429636\n",
      "Trained batch 7 batch loss 6.75060177 epoch total loss 6.47548819\n",
      "Trained batch 8 batch loss 6.59817743 epoch total loss 6.4908247\n",
      "Trained batch 9 batch loss 6.57911396 epoch total loss 6.50063467\n",
      "Trained batch 10 batch loss 6.67195797 epoch total loss 6.51776648\n",
      "Trained batch 11 batch loss 6.65835619 epoch total loss 6.53054762\n",
      "Trained batch 12 batch loss 6.62752 epoch total loss 6.5386281\n",
      "Trained batch 13 batch loss 6.50072813 epoch total loss 6.53571272\n",
      "Trained batch 14 batch loss 6.33022594 epoch total loss 6.52103472\n",
      "Trained batch 15 batch loss 6.45014954 epoch total loss 6.51630926\n",
      "Trained batch 16 batch loss 6.70018291 epoch total loss 6.52780104\n",
      "Trained batch 17 batch loss 6.60483694 epoch total loss 6.53233242\n",
      "Trained batch 18 batch loss 6.14735031 epoch total loss 6.51094484\n",
      "Trained batch 19 batch loss 6.25264502 epoch total loss 6.49735\n",
      "Trained batch 20 batch loss 6.44108677 epoch total loss 6.49453735\n",
      "Trained batch 21 batch loss 6.81952524 epoch total loss 6.51001263\n",
      "Trained batch 22 batch loss 6.60080051 epoch total loss 6.51413918\n",
      "Trained batch 23 batch loss 6.31540918 epoch total loss 6.50549936\n",
      "Trained batch 24 batch loss 6.34504318 epoch total loss 6.49881363\n",
      "Trained batch 25 batch loss 5.93690586 epoch total loss 6.47633743\n",
      "Trained batch 26 batch loss 6.30399323 epoch total loss 6.46970892\n",
      "Trained batch 27 batch loss 5.99744844 epoch total loss 6.45221806\n",
      "Trained batch 28 batch loss 6.24441576 epoch total loss 6.44479656\n",
      "Trained batch 29 batch loss 6.47381163 epoch total loss 6.44579697\n",
      "Trained batch 30 batch loss 6.37905359 epoch total loss 6.44357252\n",
      "Trained batch 31 batch loss 6.2857213 epoch total loss 6.43848038\n",
      "Trained batch 32 batch loss 5.10021305 epoch total loss 6.39666\n",
      "Trained batch 33 batch loss 5.69467211 epoch total loss 6.37538767\n",
      "Trained batch 34 batch loss 5.68215466 epoch total loss 6.35499859\n",
      "Trained batch 35 batch loss 6.18024778 epoch total loss 6.35000563\n",
      "Trained batch 36 batch loss 5.97095966 epoch total loss 6.33947659\n",
      "Trained batch 37 batch loss 6.15616798 epoch total loss 6.33452272\n",
      "Trained batch 38 batch loss 6.29853344 epoch total loss 6.33357573\n",
      "Trained batch 39 batch loss 6.65918732 epoch total loss 6.34192419\n",
      "Trained batch 40 batch loss 6.0793252 epoch total loss 6.33535957\n",
      "Trained batch 41 batch loss 6.39955854 epoch total loss 6.33692503\n",
      "Trained batch 42 batch loss 7.23758745 epoch total loss 6.35836935\n",
      "Trained batch 43 batch loss 6.90110254 epoch total loss 6.37099075\n",
      "Trained batch 44 batch loss 6.30274248 epoch total loss 6.3694396\n",
      "Trained batch 45 batch loss 6.55262423 epoch total loss 6.37351\n",
      "Trained batch 46 batch loss 7.13155937 epoch total loss 6.38998938\n",
      "Trained batch 47 batch loss 6.98632336 epoch total loss 6.40267754\n",
      "Trained batch 48 batch loss 6.66936159 epoch total loss 6.40823364\n",
      "Trained batch 49 batch loss 7.1283679 epoch total loss 6.42293024\n",
      "Trained batch 50 batch loss 6.88949394 epoch total loss 6.43226147\n",
      "Trained batch 51 batch loss 6.54891109 epoch total loss 6.43454885\n",
      "Trained batch 52 batch loss 6.28250313 epoch total loss 6.43162489\n",
      "Trained batch 53 batch loss 6.55020428 epoch total loss 6.43386221\n",
      "Trained batch 54 batch loss 6.47187662 epoch total loss 6.43456602\n",
      "Trained batch 55 batch loss 6.30241871 epoch total loss 6.43216324\n",
      "Trained batch 56 batch loss 6.48191833 epoch total loss 6.43305206\n",
      "Trained batch 57 batch loss 6.58090496 epoch total loss 6.43564606\n",
      "Trained batch 58 batch loss 6.01918793 epoch total loss 6.42846584\n",
      "Trained batch 59 batch loss 5.93789291 epoch total loss 6.42015123\n",
      "Trained batch 60 batch loss 6.72317505 epoch total loss 6.42520142\n",
      "Trained batch 61 batch loss 6.77737856 epoch total loss 6.43097496\n",
      "Trained batch 62 batch loss 6.36749458 epoch total loss 6.42995071\n",
      "Trained batch 63 batch loss 6.93023968 epoch total loss 6.43789196\n",
      "Trained batch 64 batch loss 6.84070921 epoch total loss 6.44418573\n",
      "Trained batch 65 batch loss 6.91355324 epoch total loss 6.45140648\n",
      "Trained batch 66 batch loss 6.51985931 epoch total loss 6.45244408\n",
      "Trained batch 67 batch loss 6.2480135 epoch total loss 6.4493928\n",
      "Trained batch 68 batch loss 6.42668533 epoch total loss 6.44905901\n",
      "Trained batch 69 batch loss 6.52941179 epoch total loss 6.45022345\n",
      "Trained batch 70 batch loss 6.40506315 epoch total loss 6.44957829\n",
      "Trained batch 71 batch loss 5.87622213 epoch total loss 6.44150305\n",
      "Trained batch 72 batch loss 6.14554119 epoch total loss 6.43739223\n",
      "Trained batch 73 batch loss 6.22586441 epoch total loss 6.4344945\n",
      "Trained batch 74 batch loss 6.31278229 epoch total loss 6.43285\n",
      "Trained batch 75 batch loss 6.19062662 epoch total loss 6.42962\n",
      "Trained batch 76 batch loss 6.63714123 epoch total loss 6.43235064\n",
      "Trained batch 77 batch loss 6.24665928 epoch total loss 6.42993927\n",
      "Trained batch 78 batch loss 6.70570517 epoch total loss 6.43347502\n",
      "Trained batch 79 batch loss 6.75106478 epoch total loss 6.43749475\n",
      "Trained batch 80 batch loss 6.41253042 epoch total loss 6.43718243\n",
      "Trained batch 81 batch loss 6.15649176 epoch total loss 6.43371725\n",
      "Trained batch 82 batch loss 6.29043722 epoch total loss 6.4319706\n",
      "Trained batch 83 batch loss 6.15792274 epoch total loss 6.4286685\n",
      "Trained batch 84 batch loss 6.43175 epoch total loss 6.42870522\n",
      "Trained batch 85 batch loss 6.92580652 epoch total loss 6.43455315\n",
      "Trained batch 86 batch loss 6.5946846 epoch total loss 6.43641472\n",
      "Trained batch 87 batch loss 6.68930912 epoch total loss 6.43932199\n",
      "Trained batch 88 batch loss 6.47983742 epoch total loss 6.43978262\n",
      "Trained batch 89 batch loss 6.1345005 epoch total loss 6.43635273\n",
      "Trained batch 90 batch loss 6.31588268 epoch total loss 6.43501377\n",
      "Trained batch 91 batch loss 6.75183535 epoch total loss 6.43849516\n",
      "Trained batch 92 batch loss 6.35687971 epoch total loss 6.43760824\n",
      "Trained batch 93 batch loss 6.31740236 epoch total loss 6.43631554\n",
      "Trained batch 94 batch loss 6.38840151 epoch total loss 6.4358058\n",
      "Trained batch 95 batch loss 6.10841656 epoch total loss 6.4323597\n",
      "Trained batch 96 batch loss 6.22223616 epoch total loss 6.43017054\n",
      "Trained batch 97 batch loss 6.07197094 epoch total loss 6.42647791\n",
      "Trained batch 98 batch loss 6.52834463 epoch total loss 6.42751694\n",
      "Trained batch 99 batch loss 6.58247757 epoch total loss 6.42908192\n",
      "Trained batch 100 batch loss 6.74780273 epoch total loss 6.4322691\n",
      "Trained batch 101 batch loss 6.94724321 epoch total loss 6.43736839\n",
      "Trained batch 102 batch loss 7.03678083 epoch total loss 6.44324493\n",
      "Trained batch 103 batch loss 6.74049091 epoch total loss 6.44613075\n",
      "Trained batch 104 batch loss 6.36187363 epoch total loss 6.44532061\n",
      "Trained batch 105 batch loss 5.76190472 epoch total loss 6.43881178\n",
      "Trained batch 106 batch loss 5.31903219 epoch total loss 6.42824793\n",
      "Trained batch 107 batch loss 5.28618526 epoch total loss 6.41757441\n",
      "Trained batch 108 batch loss 5.40510702 epoch total loss 6.4082\n",
      "Trained batch 109 batch loss 6.4172864 epoch total loss 6.40828323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 110 batch loss 6.19611454 epoch total loss 6.40635443\n",
      "Trained batch 111 batch loss 6.3273859 epoch total loss 6.40564299\n",
      "Trained batch 112 batch loss 6.18959379 epoch total loss 6.4037137\n",
      "Trained batch 113 batch loss 6.68191528 epoch total loss 6.40617561\n",
      "Trained batch 114 batch loss 6.48804188 epoch total loss 6.40689373\n",
      "Trained batch 115 batch loss 6.4165554 epoch total loss 6.40697765\n",
      "Trained batch 116 batch loss 6.24804401 epoch total loss 6.4056077\n",
      "Trained batch 117 batch loss 6.3357 epoch total loss 6.40501\n",
      "Trained batch 118 batch loss 6.45728636 epoch total loss 6.40545273\n",
      "Trained batch 119 batch loss 6.47370863 epoch total loss 6.40602636\n",
      "Trained batch 120 batch loss 6.53841209 epoch total loss 6.40712929\n",
      "Trained batch 121 batch loss 6.06835508 epoch total loss 6.40433\n",
      "Trained batch 122 batch loss 6.61172295 epoch total loss 6.40602922\n",
      "Trained batch 123 batch loss 6.84183025 epoch total loss 6.4095726\n",
      "Trained batch 124 batch loss 6.63032293 epoch total loss 6.41135263\n",
      "Trained batch 125 batch loss 7.19894218 epoch total loss 6.41765308\n",
      "Trained batch 126 batch loss 7.46517181 epoch total loss 6.42596674\n",
      "Trained batch 127 batch loss 7.3696146 epoch total loss 6.43339729\n",
      "Trained batch 128 batch loss 6.97972965 epoch total loss 6.43766546\n",
      "Trained batch 129 batch loss 6.9565835 epoch total loss 6.44168806\n",
      "Trained batch 130 batch loss 5.98258 epoch total loss 6.4381566\n",
      "Trained batch 131 batch loss 6.22397614 epoch total loss 6.43652201\n",
      "Trained batch 132 batch loss 6.32201433 epoch total loss 6.43565464\n",
      "Trained batch 133 batch loss 6.54639482 epoch total loss 6.4364872\n",
      "Trained batch 134 batch loss 6.22521067 epoch total loss 6.43491077\n",
      "Trained batch 135 batch loss 6.49222898 epoch total loss 6.43533516\n",
      "Trained batch 136 batch loss 6.65284157 epoch total loss 6.43693447\n",
      "Trained batch 137 batch loss 6.58729 epoch total loss 6.43803215\n",
      "Trained batch 138 batch loss 6.43131876 epoch total loss 6.43798351\n",
      "Trained batch 139 batch loss 6.55145359 epoch total loss 6.4388\n",
      "Trained batch 140 batch loss 6.33146811 epoch total loss 6.4380331\n",
      "Trained batch 141 batch loss 5.76714468 epoch total loss 6.43327522\n",
      "Trained batch 142 batch loss 6.36091423 epoch total loss 6.43276548\n",
      "Trained batch 143 batch loss 6.34581 epoch total loss 6.43215752\n",
      "Trained batch 144 batch loss 6.32831669 epoch total loss 6.43143654\n",
      "Trained batch 145 batch loss 6.47038317 epoch total loss 6.431705\n",
      "Trained batch 146 batch loss 6.58554173 epoch total loss 6.43275881\n",
      "Trained batch 147 batch loss 6.45933771 epoch total loss 6.43294\n",
      "Trained batch 148 batch loss 6.60221863 epoch total loss 6.43408346\n",
      "Trained batch 149 batch loss 6.11498356 epoch total loss 6.43194199\n",
      "Trained batch 150 batch loss 6.57940054 epoch total loss 6.43292522\n",
      "Trained batch 151 batch loss 6.61229134 epoch total loss 6.43411303\n",
      "Trained batch 152 batch loss 6.36577749 epoch total loss 6.43366385\n",
      "Trained batch 153 batch loss 6.37464 epoch total loss 6.43327761\n",
      "Trained batch 154 batch loss 5.95759916 epoch total loss 6.43018866\n",
      "Trained batch 155 batch loss 6.36737728 epoch total loss 6.42978334\n",
      "Trained batch 156 batch loss 6.40560532 epoch total loss 6.42962837\n",
      "Trained batch 157 batch loss 6.43555927 epoch total loss 6.42966604\n",
      "Trained batch 158 batch loss 6.32320595 epoch total loss 6.42899227\n",
      "Trained batch 159 batch loss 6.37426615 epoch total loss 6.42864799\n",
      "Trained batch 160 batch loss 5.99991179 epoch total loss 6.42596817\n",
      "Trained batch 161 batch loss 6.2785759 epoch total loss 6.42505264\n",
      "Trained batch 162 batch loss 6.39683199 epoch total loss 6.4248786\n",
      "Trained batch 163 batch loss 6.24145842 epoch total loss 6.42375326\n",
      "Trained batch 164 batch loss 6.05421829 epoch total loss 6.42149973\n",
      "Trained batch 165 batch loss 5.83292675 epoch total loss 6.41793251\n",
      "Trained batch 166 batch loss 5.90498447 epoch total loss 6.41484261\n",
      "Trained batch 167 batch loss 6.32272243 epoch total loss 6.41429138\n",
      "Trained batch 168 batch loss 6.43009138 epoch total loss 6.41438532\n",
      "Trained batch 169 batch loss 6.57008028 epoch total loss 6.41530609\n",
      "Trained batch 170 batch loss 6.70393038 epoch total loss 6.41700459\n",
      "Trained batch 171 batch loss 6.52264881 epoch total loss 6.41762257\n",
      "Trained batch 172 batch loss 6.10104036 epoch total loss 6.41578197\n",
      "Trained batch 173 batch loss 6.07192469 epoch total loss 6.41379452\n",
      "Trained batch 174 batch loss 6.3058753 epoch total loss 6.41317415\n",
      "Trained batch 175 batch loss 5.85459757 epoch total loss 6.40998268\n",
      "Trained batch 176 batch loss 6.35479116 epoch total loss 6.40966845\n",
      "Trained batch 177 batch loss 5.94213 epoch total loss 6.40702724\n",
      "Trained batch 178 batch loss 6.1242609 epoch total loss 6.4054389\n",
      "Trained batch 179 batch loss 6.75792885 epoch total loss 6.40740776\n",
      "Trained batch 180 batch loss 6.81705141 epoch total loss 6.4096837\n",
      "Trained batch 181 batch loss 7.265306 epoch total loss 6.41441059\n",
      "Trained batch 182 batch loss 6.82712889 epoch total loss 6.41667843\n",
      "Trained batch 183 batch loss 6.60680866 epoch total loss 6.41771746\n",
      "Trained batch 184 batch loss 6.37935686 epoch total loss 6.41750908\n",
      "Trained batch 185 batch loss 6.35011339 epoch total loss 6.41714478\n",
      "Trained batch 186 batch loss 6.39099073 epoch total loss 6.41700411\n",
      "Trained batch 187 batch loss 6.56287241 epoch total loss 6.41778421\n",
      "Trained batch 188 batch loss 6.42669153 epoch total loss 6.41783094\n",
      "Trained batch 189 batch loss 6.55114 epoch total loss 6.41853666\n",
      "Trained batch 190 batch loss 6.83904505 epoch total loss 6.42074919\n",
      "Trained batch 191 batch loss 6.57689476 epoch total loss 6.42156696\n",
      "Trained batch 192 batch loss 6.30677605 epoch total loss 6.42096901\n",
      "Trained batch 193 batch loss 6.14772654 epoch total loss 6.41955328\n",
      "Trained batch 194 batch loss 6.27132225 epoch total loss 6.41878939\n",
      "Trained batch 195 batch loss 5.97160339 epoch total loss 6.4164958\n",
      "Trained batch 196 batch loss 6.08601 epoch total loss 6.4148097\n",
      "Trained batch 197 batch loss 6.40115643 epoch total loss 6.41474056\n",
      "Trained batch 198 batch loss 6.11853933 epoch total loss 6.41324425\n",
      "Trained batch 199 batch loss 6.55357409 epoch total loss 6.41394949\n",
      "Trained batch 200 batch loss 6.53664732 epoch total loss 6.41456318\n",
      "Trained batch 201 batch loss 6.54177427 epoch total loss 6.41519594\n",
      "Trained batch 202 batch loss 6.48552418 epoch total loss 6.41554356\n",
      "Trained batch 203 batch loss 6.41861343 epoch total loss 6.41555882\n",
      "Trained batch 204 batch loss 6.38956785 epoch total loss 6.41543102\n",
      "Trained batch 205 batch loss 6.52292728 epoch total loss 6.41595554\n",
      "Trained batch 206 batch loss 6.55211926 epoch total loss 6.41661644\n",
      "Trained batch 207 batch loss 6.46548271 epoch total loss 6.41685247\n",
      "Trained batch 208 batch loss 6.4533987 epoch total loss 6.41702795\n",
      "Trained batch 209 batch loss 6.21980476 epoch total loss 6.41608477\n",
      "Trained batch 210 batch loss 6.04819155 epoch total loss 6.41433287\n",
      "Trained batch 211 batch loss 5.86993122 epoch total loss 6.41175222\n",
      "Trained batch 212 batch loss 6.51246929 epoch total loss 6.41222763\n",
      "Trained batch 213 batch loss 6.41007805 epoch total loss 6.41221714\n",
      "Trained batch 214 batch loss 6.34226131 epoch total loss 6.41189051\n",
      "Trained batch 215 batch loss 6.39359617 epoch total loss 6.41180515\n",
      "Trained batch 216 batch loss 6.1905489 epoch total loss 6.41078091\n",
      "Trained batch 217 batch loss 5.66490841 epoch total loss 6.40734339\n",
      "Trained batch 218 batch loss 5.637012 epoch total loss 6.40380955\n",
      "Trained batch 219 batch loss 6.00973749 epoch total loss 6.40201044\n",
      "Trained batch 220 batch loss 6.39200497 epoch total loss 6.40196466\n",
      "Trained batch 221 batch loss 6.56892395 epoch total loss 6.40272045\n",
      "Trained batch 222 batch loss 6.34256411 epoch total loss 6.40244913\n",
      "Trained batch 223 batch loss 6.30456495 epoch total loss 6.40201044\n",
      "Trained batch 224 batch loss 5.84342289 epoch total loss 6.39951658\n",
      "Trained batch 225 batch loss 6.08664322 epoch total loss 6.39812613\n",
      "Trained batch 226 batch loss 6.37657642 epoch total loss 6.39803076\n",
      "Trained batch 227 batch loss 6.66826057 epoch total loss 6.39922094\n",
      "Trained batch 228 batch loss 6.38045 epoch total loss 6.39913893\n",
      "Trained batch 229 batch loss 6.19099903 epoch total loss 6.39823\n",
      "Trained batch 230 batch loss 6.3821826 epoch total loss 6.39816046\n",
      "Trained batch 231 batch loss 6.27747536 epoch total loss 6.39763784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 232 batch loss 6.63899899 epoch total loss 6.3986783\n",
      "Trained batch 233 batch loss 6.52521706 epoch total loss 6.3992219\n",
      "Trained batch 234 batch loss 6.53993034 epoch total loss 6.39982319\n",
      "Trained batch 235 batch loss 6.48504353 epoch total loss 6.40018559\n",
      "Trained batch 236 batch loss 6.39429951 epoch total loss 6.40016031\n",
      "Trained batch 237 batch loss 6.14422369 epoch total loss 6.39908028\n",
      "Trained batch 238 batch loss 6.07691145 epoch total loss 6.39772654\n",
      "Trained batch 239 batch loss 5.73016405 epoch total loss 6.3949337\n",
      "Trained batch 240 batch loss 5.24494791 epoch total loss 6.39014244\n",
      "Trained batch 241 batch loss 6.5271759 epoch total loss 6.39071131\n",
      "Trained batch 242 batch loss 6.45593882 epoch total loss 6.39098072\n",
      "Trained batch 243 batch loss 6.40615749 epoch total loss 6.39104319\n",
      "Trained batch 244 batch loss 6.50464916 epoch total loss 6.39150858\n",
      "Trained batch 245 batch loss 6.71383381 epoch total loss 6.39282417\n",
      "Trained batch 246 batch loss 6.87274456 epoch total loss 6.39477539\n",
      "Trained batch 247 batch loss 6.73277903 epoch total loss 6.39614391\n",
      "Trained batch 248 batch loss 6.62562847 epoch total loss 6.39706898\n",
      "Trained batch 249 batch loss 6.92829466 epoch total loss 6.39920282\n",
      "Trained batch 250 batch loss 6.91850424 epoch total loss 6.40128\n",
      "Trained batch 251 batch loss 6.33738852 epoch total loss 6.4010253\n",
      "Trained batch 252 batch loss 6.50724411 epoch total loss 6.40144682\n",
      "Trained batch 253 batch loss 5.79427624 epoch total loss 6.3990469\n",
      "Trained batch 254 batch loss 6.02045298 epoch total loss 6.39755678\n",
      "Trained batch 255 batch loss 6.09900951 epoch total loss 6.39638567\n",
      "Trained batch 256 batch loss 6.59975052 epoch total loss 6.39718\n",
      "Trained batch 257 batch loss 6.27102137 epoch total loss 6.39668894\n",
      "Trained batch 258 batch loss 6.29015493 epoch total loss 6.396276\n",
      "Trained batch 259 batch loss 6.23208904 epoch total loss 6.39564228\n",
      "Trained batch 260 batch loss 6.44962215 epoch total loss 6.3958497\n",
      "Trained batch 261 batch loss 6.35574913 epoch total loss 6.39569569\n",
      "Trained batch 262 batch loss 7.13742352 epoch total loss 6.39852715\n",
      "Trained batch 263 batch loss 6.74746943 epoch total loss 6.39985371\n",
      "Trained batch 264 batch loss 6.33509445 epoch total loss 6.39960814\n",
      "Trained batch 265 batch loss 6.35620117 epoch total loss 6.39944458\n",
      "Trained batch 266 batch loss 6.26102924 epoch total loss 6.39892387\n",
      "Trained batch 267 batch loss 6.22975779 epoch total loss 6.39829\n",
      "Trained batch 268 batch loss 6.83648825 epoch total loss 6.39992571\n",
      "Trained batch 269 batch loss 6.34331274 epoch total loss 6.39971495\n",
      "Trained batch 270 batch loss 6.75057888 epoch total loss 6.40101433\n",
      "Trained batch 271 batch loss 5.9337306 epoch total loss 6.39929\n",
      "Trained batch 272 batch loss 6.43773556 epoch total loss 6.39943171\n",
      "Trained batch 273 batch loss 6.33146572 epoch total loss 6.39918232\n",
      "Trained batch 274 batch loss 6.73655558 epoch total loss 6.40041399\n",
      "Trained batch 275 batch loss 6.6670742 epoch total loss 6.4013834\n",
      "Trained batch 276 batch loss 6.5959692 epoch total loss 6.40208864\n",
      "Trained batch 277 batch loss 6.4550705 epoch total loss 6.40228\n",
      "Trained batch 278 batch loss 6.3457303 epoch total loss 6.40207624\n",
      "Trained batch 279 batch loss 6.34338379 epoch total loss 6.40186596\n",
      "Trained batch 280 batch loss 6.30215883 epoch total loss 6.40151\n",
      "Trained batch 281 batch loss 6.92933846 epoch total loss 6.40338802\n",
      "Trained batch 282 batch loss 6.53335667 epoch total loss 6.40384865\n",
      "Trained batch 283 batch loss 6.62548494 epoch total loss 6.40463209\n",
      "Trained batch 284 batch loss 6.76540756 epoch total loss 6.40590239\n",
      "Trained batch 285 batch loss 6.54936886 epoch total loss 6.40640545\n",
      "Trained batch 286 batch loss 6.61661196 epoch total loss 6.40714025\n",
      "Trained batch 287 batch loss 7.25263262 epoch total loss 6.41008663\n",
      "Trained batch 288 batch loss 6.52551079 epoch total loss 6.41048717\n",
      "Trained batch 289 batch loss 6.0804882 epoch total loss 6.40934515\n",
      "Trained batch 290 batch loss 6.35682964 epoch total loss 6.40916395\n",
      "Trained batch 291 batch loss 5.55389786 epoch total loss 6.4062252\n",
      "Trained batch 292 batch loss 5.74132109 epoch total loss 6.40394831\n",
      "Trained batch 293 batch loss 6.09904242 epoch total loss 6.40290737\n",
      "Trained batch 294 batch loss 5.75332785 epoch total loss 6.40069771\n",
      "Trained batch 295 batch loss 5.26265144 epoch total loss 6.39684\n",
      "Trained batch 296 batch loss 5.24999046 epoch total loss 6.39296579\n",
      "Trained batch 297 batch loss 5.07450342 epoch total loss 6.38852644\n",
      "Trained batch 298 batch loss 5.54740667 epoch total loss 6.38570356\n",
      "Trained batch 299 batch loss 6.18545866 epoch total loss 6.38503408\n",
      "Trained batch 300 batch loss 6.17990446 epoch total loss 6.3843503\n",
      "Trained batch 301 batch loss 6.62473106 epoch total loss 6.385149\n",
      "Trained batch 302 batch loss 6.92229271 epoch total loss 6.38692713\n",
      "Trained batch 303 batch loss 6.72392845 epoch total loss 6.38803959\n",
      "Trained batch 304 batch loss 6.22111654 epoch total loss 6.38749027\n",
      "Trained batch 305 batch loss 5.25398302 epoch total loss 6.3837738\n",
      "Trained batch 306 batch loss 6.01040363 epoch total loss 6.38255358\n",
      "Trained batch 307 batch loss 6.57050514 epoch total loss 6.38316584\n",
      "Trained batch 308 batch loss 6.35096169 epoch total loss 6.38306141\n",
      "Trained batch 309 batch loss 6.44981909 epoch total loss 6.38327742\n",
      "Trained batch 310 batch loss 6.56853628 epoch total loss 6.38387489\n",
      "Trained batch 311 batch loss 6.48478699 epoch total loss 6.38419914\n",
      "Trained batch 312 batch loss 6.55390739 epoch total loss 6.38474321\n",
      "Trained batch 313 batch loss 6.48936844 epoch total loss 6.38507748\n",
      "Trained batch 314 batch loss 6.49237728 epoch total loss 6.38541937\n",
      "Trained batch 315 batch loss 6.35087681 epoch total loss 6.3853097\n",
      "Trained batch 316 batch loss 6.23832703 epoch total loss 6.3848443\n",
      "Trained batch 317 batch loss 6.75906277 epoch total loss 6.38602495\n",
      "Trained batch 318 batch loss 6.62688112 epoch total loss 6.38678217\n",
      "Trained batch 319 batch loss 6.35900688 epoch total loss 6.38669491\n",
      "Trained batch 320 batch loss 6.32947 epoch total loss 6.38651609\n",
      "Trained batch 321 batch loss 6.30437899 epoch total loss 6.38626\n",
      "Trained batch 322 batch loss 6.31994724 epoch total loss 6.38605452\n",
      "Trained batch 323 batch loss 6.2960391 epoch total loss 6.38577604\n",
      "Trained batch 324 batch loss 6.65124 epoch total loss 6.38659525\n",
      "Trained batch 325 batch loss 6.63324833 epoch total loss 6.38735437\n",
      "Trained batch 326 batch loss 6.10697079 epoch total loss 6.38649416\n",
      "Trained batch 327 batch loss 6.16991806 epoch total loss 6.38583183\n",
      "Trained batch 328 batch loss 6.0421 epoch total loss 6.38478327\n",
      "Trained batch 329 batch loss 6.54729605 epoch total loss 6.38527775\n",
      "Trained batch 330 batch loss 6.51396179 epoch total loss 6.38566732\n",
      "Trained batch 331 batch loss 6.89456272 epoch total loss 6.38720465\n",
      "Trained batch 332 batch loss 6.64430475 epoch total loss 6.38797903\n",
      "Trained batch 333 batch loss 6.15929413 epoch total loss 6.38729191\n",
      "Trained batch 334 batch loss 6.10296392 epoch total loss 6.38644075\n",
      "Trained batch 335 batch loss 6.19570971 epoch total loss 6.38587189\n",
      "Trained batch 336 batch loss 6.286057 epoch total loss 6.38557529\n",
      "Trained batch 337 batch loss 6.47005081 epoch total loss 6.38582563\n",
      "Trained batch 338 batch loss 6.57958 epoch total loss 6.38639879\n",
      "Trained batch 339 batch loss 6.61624193 epoch total loss 6.38707685\n",
      "Trained batch 340 batch loss 6.8993907 epoch total loss 6.38858366\n",
      "Trained batch 341 batch loss 6.63020134 epoch total loss 6.38929176\n",
      "Trained batch 342 batch loss 7.0270853 epoch total loss 6.39115667\n",
      "Trained batch 343 batch loss 6.62625265 epoch total loss 6.39184237\n",
      "Trained batch 344 batch loss 6.54165173 epoch total loss 6.39227772\n",
      "Trained batch 345 batch loss 6.44276381 epoch total loss 6.39242458\n",
      "Trained batch 346 batch loss 6.43400192 epoch total loss 6.39254475\n",
      "Trained batch 347 batch loss 6.81623077 epoch total loss 6.39376593\n",
      "Trained batch 348 batch loss 6.75764084 epoch total loss 6.39481115\n",
      "Trained batch 349 batch loss 6.73693895 epoch total loss 6.39579201\n",
      "Trained batch 350 batch loss 6.72112751 epoch total loss 6.39672136\n",
      "Trained batch 351 batch loss 6.83234549 epoch total loss 6.39796257\n",
      "Trained batch 352 batch loss 6.39927626 epoch total loss 6.39796591\n",
      "Trained batch 353 batch loss 6.42038155 epoch total loss 6.39802933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 354 batch loss 6.48788118 epoch total loss 6.398283\n",
      "Trained batch 355 batch loss 6.56170321 epoch total loss 6.39874363\n",
      "Trained batch 356 batch loss 6.11131144 epoch total loss 6.39793634\n",
      "Trained batch 357 batch loss 6.26028538 epoch total loss 6.39755058\n",
      "Trained batch 358 batch loss 6.53921843 epoch total loss 6.39794636\n",
      "Trained batch 359 batch loss 6.15528727 epoch total loss 6.39727068\n",
      "Trained batch 360 batch loss 6.38744068 epoch total loss 6.39724302\n",
      "Trained batch 361 batch loss 6.32025 epoch total loss 6.39703035\n",
      "Trained batch 362 batch loss 6.12604618 epoch total loss 6.39628124\n",
      "Trained batch 363 batch loss 6.23840714 epoch total loss 6.39584684\n",
      "Trained batch 364 batch loss 6.15062 epoch total loss 6.39517307\n",
      "Trained batch 365 batch loss 6.16674614 epoch total loss 6.39454746\n",
      "Trained batch 366 batch loss 6.09311438 epoch total loss 6.39372349\n",
      "Trained batch 367 batch loss 5.77370882 epoch total loss 6.39203405\n",
      "Trained batch 368 batch loss 6.17536449 epoch total loss 6.39144516\n",
      "Trained batch 369 batch loss 6.16188335 epoch total loss 6.39082289\n",
      "Trained batch 370 batch loss 6.19265032 epoch total loss 6.3902874\n",
      "Trained batch 371 batch loss 6.09173584 epoch total loss 6.3894825\n",
      "Trained batch 372 batch loss 6.46797657 epoch total loss 6.38969374\n",
      "Trained batch 373 batch loss 6.37397337 epoch total loss 6.38965178\n",
      "Trained batch 374 batch loss 6.6305666 epoch total loss 6.39029598\n",
      "Trained batch 375 batch loss 6.51090717 epoch total loss 6.39061785\n",
      "Trained batch 376 batch loss 5.77312851 epoch total loss 6.38897562\n",
      "Trained batch 377 batch loss 5.42013311 epoch total loss 6.38640594\n",
      "Trained batch 378 batch loss 5.32447433 epoch total loss 6.38359642\n",
      "Trained batch 379 batch loss 5.47692442 epoch total loss 6.38120413\n",
      "Trained batch 380 batch loss 6.28301859 epoch total loss 6.38094568\n",
      "Trained batch 381 batch loss 6.1722188 epoch total loss 6.38039732\n",
      "Trained batch 382 batch loss 6.5589 epoch total loss 6.38086462\n",
      "Trained batch 383 batch loss 6.35977 epoch total loss 6.38081\n",
      "Trained batch 384 batch loss 6.20210218 epoch total loss 6.38034439\n",
      "Trained batch 385 batch loss 6.24634838 epoch total loss 6.3799963\n",
      "Trained batch 386 batch loss 5.92407084 epoch total loss 6.37881517\n",
      "Trained batch 387 batch loss 6.12363386 epoch total loss 6.37815571\n",
      "Trained batch 388 batch loss 5.98132181 epoch total loss 6.37713242\n",
      "Trained batch 389 batch loss 5.97286177 epoch total loss 6.37609339\n",
      "Trained batch 390 batch loss 6.17560101 epoch total loss 6.37557888\n",
      "Trained batch 391 batch loss 6.6779809 epoch total loss 6.37635231\n",
      "Trained batch 392 batch loss 6.89586687 epoch total loss 6.37767744\n",
      "Trained batch 393 batch loss 6.34302902 epoch total loss 6.37758923\n",
      "Trained batch 394 batch loss 6.51775217 epoch total loss 6.37794495\n",
      "Trained batch 395 batch loss 6.56947136 epoch total loss 6.37843037\n",
      "Trained batch 396 batch loss 6.29798269 epoch total loss 6.37822723\n",
      "Trained batch 397 batch loss 6.49089766 epoch total loss 6.37851143\n",
      "Trained batch 398 batch loss 6.42092037 epoch total loss 6.37861776\n",
      "Trained batch 399 batch loss 6.58452415 epoch total loss 6.3791337\n",
      "Trained batch 400 batch loss 6.31678391 epoch total loss 6.37897825\n",
      "Trained batch 401 batch loss 6.49478388 epoch total loss 6.37926722\n",
      "Trained batch 402 batch loss 6.41788626 epoch total loss 6.37936354\n",
      "Trained batch 403 batch loss 6.52013 epoch total loss 6.37971258\n",
      "Trained batch 404 batch loss 6.71925211 epoch total loss 6.38055277\n",
      "Trained batch 405 batch loss 6.48435307 epoch total loss 6.38080931\n",
      "Trained batch 406 batch loss 6.58981419 epoch total loss 6.38132429\n",
      "Trained batch 407 batch loss 6.41697025 epoch total loss 6.38141203\n",
      "Trained batch 408 batch loss 6.20747614 epoch total loss 6.38098574\n",
      "Trained batch 409 batch loss 6.11986446 epoch total loss 6.38034725\n",
      "Trained batch 410 batch loss 6.62174511 epoch total loss 6.38093615\n",
      "Trained batch 411 batch loss 6.13454199 epoch total loss 6.38033676\n",
      "Trained batch 412 batch loss 6.35238171 epoch total loss 6.38026857\n",
      "Trained batch 413 batch loss 6.36753511 epoch total loss 6.38023758\n",
      "Trained batch 414 batch loss 6.30438805 epoch total loss 6.38005447\n",
      "Trained batch 415 batch loss 6.61755419 epoch total loss 6.38062716\n",
      "Trained batch 416 batch loss 6.23465967 epoch total loss 6.3802762\n",
      "Trained batch 417 batch loss 5.58426905 epoch total loss 6.37836695\n",
      "Trained batch 418 batch loss 6.48202515 epoch total loss 6.3786149\n",
      "Trained batch 419 batch loss 6.27336788 epoch total loss 6.37836361\n",
      "Trained batch 420 batch loss 6.77128553 epoch total loss 6.37929916\n",
      "Trained batch 421 batch loss 6.45129108 epoch total loss 6.37947\n",
      "Trained batch 422 batch loss 6.178267 epoch total loss 6.37899303\n",
      "Trained batch 423 batch loss 6.01434517 epoch total loss 6.37813091\n",
      "Trained batch 424 batch loss 6.45613289 epoch total loss 6.37831497\n",
      "Trained batch 425 batch loss 6.06014252 epoch total loss 6.37756586\n",
      "Trained batch 426 batch loss 6.22484541 epoch total loss 6.37720776\n",
      "Trained batch 427 batch loss 6.18935299 epoch total loss 6.37676811\n",
      "Trained batch 428 batch loss 6.33157635 epoch total loss 6.37666225\n",
      "Trained batch 429 batch loss 6.71308088 epoch total loss 6.37744665\n",
      "Trained batch 430 batch loss 7.12712049 epoch total loss 6.37919\n",
      "Trained batch 431 batch loss 7.01739073 epoch total loss 6.38067055\n",
      "Trained batch 432 batch loss 6.7101655 epoch total loss 6.38143349\n",
      "Trained batch 433 batch loss 6.68191814 epoch total loss 6.38212729\n",
      "Trained batch 434 batch loss 6.16102076 epoch total loss 6.38161802\n",
      "Trained batch 435 batch loss 6.501194 epoch total loss 6.38189316\n",
      "Trained batch 436 batch loss 6.30617523 epoch total loss 6.38171959\n",
      "Trained batch 437 batch loss 6.36196899 epoch total loss 6.38167429\n",
      "Trained batch 438 batch loss 6.34967375 epoch total loss 6.38160133\n",
      "Trained batch 439 batch loss 6.38757229 epoch total loss 6.38161469\n",
      "Trained batch 440 batch loss 6.22375965 epoch total loss 6.3812561\n",
      "Trained batch 441 batch loss 6.21704578 epoch total loss 6.38088369\n",
      "Trained batch 442 batch loss 5.80367136 epoch total loss 6.37957811\n",
      "Trained batch 443 batch loss 5.90324 epoch total loss 6.37850285\n",
      "Trained batch 444 batch loss 6.15849686 epoch total loss 6.37800741\n",
      "Trained batch 445 batch loss 6.36223793 epoch total loss 6.37797213\n",
      "Trained batch 446 batch loss 6.29588699 epoch total loss 6.37778807\n",
      "Trained batch 447 batch loss 6.56921 epoch total loss 6.37821579\n",
      "Trained batch 448 batch loss 6.42723 epoch total loss 6.37832546\n",
      "Trained batch 449 batch loss 6.46305132 epoch total loss 6.37851429\n",
      "Trained batch 450 batch loss 6.23287964 epoch total loss 6.37819052\n",
      "Trained batch 451 batch loss 6.53576088 epoch total loss 6.37853956\n",
      "Trained batch 452 batch loss 6.53014612 epoch total loss 6.37887478\n",
      "Trained batch 453 batch loss 6.0273633 epoch total loss 6.37809896\n",
      "Trained batch 454 batch loss 6.35948849 epoch total loss 6.37805748\n",
      "Trained batch 455 batch loss 6.72783232 epoch total loss 6.37882614\n",
      "Trained batch 456 batch loss 6.36304 epoch total loss 6.37879181\n",
      "Trained batch 457 batch loss 6.19546223 epoch total loss 6.37839079\n",
      "Trained batch 458 batch loss 6.36388969 epoch total loss 6.37835884\n",
      "Trained batch 459 batch loss 6.64039326 epoch total loss 6.37892962\n",
      "Trained batch 460 batch loss 6.40815401 epoch total loss 6.37899351\n",
      "Trained batch 461 batch loss 6.18791866 epoch total loss 6.37857914\n",
      "Trained batch 462 batch loss 6.33166361 epoch total loss 6.3784771\n",
      "Trained batch 463 batch loss 6.27976322 epoch total loss 6.37826395\n",
      "Trained batch 464 batch loss 6.60916 epoch total loss 6.37876177\n",
      "Trained batch 465 batch loss 6.26902914 epoch total loss 6.37852573\n",
      "Trained batch 466 batch loss 6.4117136 epoch total loss 6.37859678\n",
      "Trained batch 467 batch loss 6.3723712 epoch total loss 6.37858295\n",
      "Trained batch 468 batch loss 6.1647191 epoch total loss 6.37812614\n",
      "Trained batch 469 batch loss 6.44708061 epoch total loss 6.37827349\n",
      "Trained batch 470 batch loss 6.69816 epoch total loss 6.37895393\n",
      "Trained batch 471 batch loss 6.58446407 epoch total loss 6.37939024\n",
      "Trained batch 472 batch loss 6.78941059 epoch total loss 6.38025904\n",
      "Trained batch 473 batch loss 6.3103776 epoch total loss 6.38011074\n",
      "Trained batch 474 batch loss 6.3703475 epoch total loss 6.38009\n",
      "Trained batch 475 batch loss 6.62397099 epoch total loss 6.38060379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 476 batch loss 6.87300587 epoch total loss 6.38163853\n",
      "Trained batch 477 batch loss 6.61006308 epoch total loss 6.38211727\n",
      "Trained batch 478 batch loss 6.42171764 epoch total loss 6.38220024\n",
      "Trained batch 479 batch loss 6.29439402 epoch total loss 6.38201666\n",
      "Trained batch 480 batch loss 6.37982321 epoch total loss 6.38201237\n",
      "Trained batch 481 batch loss 6.43413448 epoch total loss 6.38212061\n",
      "Trained batch 482 batch loss 6.54697323 epoch total loss 6.3824625\n",
      "Trained batch 483 batch loss 6.62977123 epoch total loss 6.38297462\n",
      "Trained batch 484 batch loss 5.85967541 epoch total loss 6.38189363\n",
      "Trained batch 485 batch loss 6.66513634 epoch total loss 6.38247728\n",
      "Trained batch 486 batch loss 6.80630302 epoch total loss 6.38334942\n",
      "Trained batch 487 batch loss 6.42032337 epoch total loss 6.38342571\n",
      "Trained batch 488 batch loss 6.05459881 epoch total loss 6.38275194\n",
      "Trained batch 489 batch loss 6.28981209 epoch total loss 6.38256168\n",
      "Trained batch 490 batch loss 5.98132515 epoch total loss 6.38174343\n",
      "Trained batch 491 batch loss 6.28805161 epoch total loss 6.3815527\n",
      "Trained batch 492 batch loss 6.5108304 epoch total loss 6.38181496\n",
      "Trained batch 493 batch loss 6.24819756 epoch total loss 6.38154411\n",
      "Trained batch 494 batch loss 6.56334209 epoch total loss 6.38191223\n",
      "Trained batch 495 batch loss 6.05678082 epoch total loss 6.38125563\n",
      "Trained batch 496 batch loss 6.37737417 epoch total loss 6.38124752\n",
      "Trained batch 497 batch loss 6.50449371 epoch total loss 6.38149548\n",
      "Trained batch 498 batch loss 5.92133474 epoch total loss 6.38057137\n",
      "Trained batch 499 batch loss 6.24242496 epoch total loss 6.3802948\n",
      "Trained batch 500 batch loss 6.2359724 epoch total loss 6.38000631\n",
      "Trained batch 501 batch loss 6.03687954 epoch total loss 6.37932158\n",
      "Trained batch 502 batch loss 6.09814501 epoch total loss 6.37876129\n",
      "Trained batch 503 batch loss 6.41944647 epoch total loss 6.37884235\n",
      "Trained batch 504 batch loss 6.12999249 epoch total loss 6.37834835\n",
      "Trained batch 505 batch loss 6.00861931 epoch total loss 6.37761593\n",
      "Trained batch 506 batch loss 6.13817739 epoch total loss 6.37714291\n",
      "Trained batch 507 batch loss 6.21275806 epoch total loss 6.37681818\n",
      "Trained batch 508 batch loss 6.19348 epoch total loss 6.37645721\n",
      "Trained batch 509 batch loss 6.4535079 epoch total loss 6.37660885\n",
      "Trained batch 510 batch loss 6.88655 epoch total loss 6.3776083\n",
      "Trained batch 511 batch loss 6.18078947 epoch total loss 6.37722349\n",
      "Trained batch 512 batch loss 6.1517477 epoch total loss 6.37678337\n",
      "Trained batch 513 batch loss 6.55029106 epoch total loss 6.37712145\n",
      "Trained batch 514 batch loss 6.48624516 epoch total loss 6.37733412\n",
      "Trained batch 515 batch loss 6.57498 epoch total loss 6.37771797\n",
      "Trained batch 516 batch loss 6.50902939 epoch total loss 6.37797213\n",
      "Trained batch 517 batch loss 5.85560751 epoch total loss 6.37696218\n",
      "Trained batch 518 batch loss 6.11228228 epoch total loss 6.37645102\n",
      "Trained batch 519 batch loss 6.40076256 epoch total loss 6.37649822\n",
      "Trained batch 520 batch loss 5.87020683 epoch total loss 6.37552452\n",
      "Trained batch 521 batch loss 6.24896717 epoch total loss 6.37528181\n",
      "Trained batch 522 batch loss 6.22410488 epoch total loss 6.37499189\n",
      "Trained batch 523 batch loss 6.4128747 epoch total loss 6.37506437\n",
      "Trained batch 524 batch loss 6.29176 epoch total loss 6.37490559\n",
      "Trained batch 525 batch loss 6.52405834 epoch total loss 6.37519\n",
      "Trained batch 526 batch loss 6.47341537 epoch total loss 6.37537622\n",
      "Trained batch 527 batch loss 6.61205482 epoch total loss 6.37582541\n",
      "Trained batch 528 batch loss 6.4081316 epoch total loss 6.37588692\n",
      "Trained batch 529 batch loss 6.64437246 epoch total loss 6.37639427\n",
      "Trained batch 530 batch loss 6.47855 epoch total loss 6.37658691\n",
      "Trained batch 531 batch loss 6.2166419 epoch total loss 6.37628555\n",
      "Trained batch 532 batch loss 6.11421633 epoch total loss 6.37579298\n",
      "Trained batch 533 batch loss 5.8817091 epoch total loss 6.37486601\n",
      "Trained batch 534 batch loss 5.63623428 epoch total loss 6.3734827\n",
      "Trained batch 535 batch loss 6.04840708 epoch total loss 6.37287474\n",
      "Trained batch 536 batch loss 6.34186697 epoch total loss 6.37281704\n",
      "Trained batch 537 batch loss 6.30395031 epoch total loss 6.37268877\n",
      "Trained batch 538 batch loss 6.38308668 epoch total loss 6.37270784\n",
      "Trained batch 539 batch loss 6.59541655 epoch total loss 6.37312126\n",
      "Trained batch 540 batch loss 5.80292082 epoch total loss 6.37206554\n",
      "Trained batch 541 batch loss 6.43830395 epoch total loss 6.37218761\n",
      "Trained batch 542 batch loss 6.25097466 epoch total loss 6.37196398\n",
      "Trained batch 543 batch loss 6.00367832 epoch total loss 6.37128592\n",
      "Trained batch 544 batch loss 6.49084568 epoch total loss 6.37150574\n",
      "Trained batch 545 batch loss 6.28836298 epoch total loss 6.37135315\n",
      "Trained batch 546 batch loss 6.517694 epoch total loss 6.37162113\n",
      "Trained batch 547 batch loss 6.74885273 epoch total loss 6.37231064\n",
      "Trained batch 548 batch loss 6.70108509 epoch total loss 6.3729105\n",
      "Trained batch 549 batch loss 6.40800095 epoch total loss 6.3729744\n",
      "Trained batch 550 batch loss 6.05353832 epoch total loss 6.37239361\n",
      "Trained batch 551 batch loss 6.22045374 epoch total loss 6.37211752\n",
      "Trained batch 552 batch loss 6.36509418 epoch total loss 6.37210464\n",
      "Trained batch 553 batch loss 6.23688221 epoch total loss 6.37186\n",
      "Trained batch 554 batch loss 6.57608223 epoch total loss 6.3722291\n",
      "Trained batch 555 batch loss 6.75586271 epoch total loss 6.37292\n",
      "Trained batch 556 batch loss 6.95603275 epoch total loss 6.37396908\n",
      "Trained batch 557 batch loss 7.02204514 epoch total loss 6.37513256\n",
      "Trained batch 558 batch loss 6.72674894 epoch total loss 6.37576246\n",
      "Trained batch 559 batch loss 6.81810284 epoch total loss 6.37655401\n",
      "Trained batch 560 batch loss 7.43748522 epoch total loss 6.37844849\n",
      "Trained batch 561 batch loss 6.97318697 epoch total loss 6.3795085\n",
      "Trained batch 562 batch loss 6.8101511 epoch total loss 6.38027477\n",
      "Trained batch 563 batch loss 6.49245 epoch total loss 6.38047409\n",
      "Trained batch 564 batch loss 6.07253027 epoch total loss 6.37992764\n",
      "Trained batch 565 batch loss 6.19405603 epoch total loss 6.37959909\n",
      "Trained batch 566 batch loss 6.39626932 epoch total loss 6.37962818\n",
      "Trained batch 567 batch loss 6.4499445 epoch total loss 6.37975216\n",
      "Trained batch 568 batch loss 6.50518322 epoch total loss 6.37997293\n",
      "Trained batch 569 batch loss 6.20621681 epoch total loss 6.37966776\n",
      "Trained batch 570 batch loss 6.46833277 epoch total loss 6.37982321\n",
      "Trained batch 571 batch loss 6.32565069 epoch total loss 6.37972832\n",
      "Trained batch 572 batch loss 6.49020958 epoch total loss 6.37992144\n",
      "Trained batch 573 batch loss 6.72261047 epoch total loss 6.38052\n",
      "Trained batch 574 batch loss 6.38187218 epoch total loss 6.38052225\n",
      "Trained batch 575 batch loss 6.21906424 epoch total loss 6.38024139\n",
      "Trained batch 576 batch loss 5.79888248 epoch total loss 6.37923193\n",
      "Trained batch 577 batch loss 5.40026808 epoch total loss 6.37753487\n",
      "Trained batch 578 batch loss 5.77063036 epoch total loss 6.37648535\n",
      "Trained batch 579 batch loss 5.94613791 epoch total loss 6.37574148\n",
      "Trained batch 580 batch loss 6.47496748 epoch total loss 6.37591267\n",
      "Trained batch 581 batch loss 5.89161873 epoch total loss 6.37507915\n",
      "Trained batch 582 batch loss 5.79225206 epoch total loss 6.37407732\n",
      "Trained batch 583 batch loss 5.96377039 epoch total loss 6.37337399\n",
      "Trained batch 584 batch loss 6.34349728 epoch total loss 6.37332296\n",
      "Trained batch 585 batch loss 5.8099494 epoch total loss 6.37235975\n",
      "Trained batch 586 batch loss 6.4691577 epoch total loss 6.37252522\n",
      "Trained batch 587 batch loss 5.95920658 epoch total loss 6.3718214\n",
      "Trained batch 588 batch loss 6.09513235 epoch total loss 6.37135077\n",
      "Trained batch 589 batch loss 5.92209196 epoch total loss 6.37058783\n",
      "Trained batch 590 batch loss 6.41966391 epoch total loss 6.37067127\n",
      "Trained batch 591 batch loss 6.53741884 epoch total loss 6.37095308\n",
      "Trained batch 592 batch loss 6.37897968 epoch total loss 6.37096691\n",
      "Trained batch 593 batch loss 6.40389109 epoch total loss 6.37102222\n",
      "Trained batch 594 batch loss 6.3181448 epoch total loss 6.37093306\n",
      "Trained batch 595 batch loss 6.26239872 epoch total loss 6.3707509\n",
      "Trained batch 596 batch loss 6.30589199 epoch total loss 6.37064171\n",
      "Trained batch 597 batch loss 6.32885456 epoch total loss 6.37057209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 598 batch loss 6.26229858 epoch total loss 6.37039089\n",
      "Trained batch 599 batch loss 5.9954443 epoch total loss 6.3697648\n",
      "Trained batch 600 batch loss 6.23662186 epoch total loss 6.3695426\n",
      "Trained batch 601 batch loss 6.27777147 epoch total loss 6.36939\n",
      "Trained batch 602 batch loss 6.3852582 epoch total loss 6.36941624\n",
      "Trained batch 603 batch loss 6.44662523 epoch total loss 6.36954451\n",
      "Trained batch 604 batch loss 6.13694191 epoch total loss 6.36915922\n",
      "Trained batch 605 batch loss 6.2518611 epoch total loss 6.36896563\n",
      "Trained batch 606 batch loss 6.48934 epoch total loss 6.36916399\n",
      "Trained batch 607 batch loss 5.88258171 epoch total loss 6.36836243\n",
      "Trained batch 608 batch loss 5.32444239 epoch total loss 6.36664534\n",
      "Trained batch 609 batch loss 6.50919533 epoch total loss 6.36687946\n",
      "Trained batch 610 batch loss 6.43672705 epoch total loss 6.36699438\n",
      "Trained batch 611 batch loss 6.3743782 epoch total loss 6.3670063\n",
      "Trained batch 612 batch loss 6.55401 epoch total loss 6.36731148\n",
      "Trained batch 613 batch loss 6.38703 epoch total loss 6.36734343\n",
      "Trained batch 614 batch loss 6.4028573 epoch total loss 6.3674016\n",
      "Trained batch 615 batch loss 6.09208441 epoch total loss 6.36695385\n",
      "Trained batch 616 batch loss 6.45290947 epoch total loss 6.36709309\n",
      "Trained batch 617 batch loss 6.65053225 epoch total loss 6.36755276\n",
      "Trained batch 618 batch loss 6.42349863 epoch total loss 6.36764336\n",
      "Trained batch 619 batch loss 6.48698092 epoch total loss 6.36783648\n",
      "Trained batch 620 batch loss 6.41474342 epoch total loss 6.36791182\n",
      "Trained batch 621 batch loss 5.6177 epoch total loss 6.36670399\n",
      "Trained batch 622 batch loss 5.93947411 epoch total loss 6.36601686\n",
      "Trained batch 623 batch loss 5.64443159 epoch total loss 6.3648591\n",
      "Trained batch 624 batch loss 5.53348589 epoch total loss 6.36352634\n",
      "Trained batch 625 batch loss 6.16328382 epoch total loss 6.36320639\n",
      "Trained batch 626 batch loss 6.43242121 epoch total loss 6.36331654\n",
      "Trained batch 627 batch loss 6.52871752 epoch total loss 6.3635807\n",
      "Trained batch 628 batch loss 6.90919495 epoch total loss 6.3644495\n",
      "Trained batch 629 batch loss 6.83839417 epoch total loss 6.3652029\n",
      "Trained batch 630 batch loss 6.81429911 epoch total loss 6.36591578\n",
      "Trained batch 631 batch loss 5.81143856 epoch total loss 6.36503696\n",
      "Trained batch 632 batch loss 5.86432457 epoch total loss 6.36424446\n",
      "Trained batch 633 batch loss 5.49619102 epoch total loss 6.36287308\n",
      "Trained batch 634 batch loss 5.85671806 epoch total loss 6.36207485\n",
      "Trained batch 635 batch loss 7.02845 epoch total loss 6.36312437\n",
      "Trained batch 636 batch loss 6.91822147 epoch total loss 6.36399698\n",
      "Trained batch 637 batch loss 7.0315547 epoch total loss 6.36504507\n",
      "Trained batch 638 batch loss 6.72955561 epoch total loss 6.36561632\n",
      "Trained batch 639 batch loss 6.78394175 epoch total loss 6.36627102\n",
      "Trained batch 640 batch loss 6.58851433 epoch total loss 6.36661816\n",
      "Trained batch 641 batch loss 6.66764307 epoch total loss 6.36708832\n",
      "Trained batch 642 batch loss 6.48190546 epoch total loss 6.36726713\n",
      "Trained batch 643 batch loss 6.47246599 epoch total loss 6.36743069\n",
      "Trained batch 644 batch loss 6.65937376 epoch total loss 6.36788368\n",
      "Trained batch 645 batch loss 6.41177845 epoch total loss 6.36795139\n",
      "Trained batch 646 batch loss 6.52539444 epoch total loss 6.36819506\n",
      "Trained batch 647 batch loss 6.4044733 epoch total loss 6.36825085\n",
      "Trained batch 648 batch loss 6.50987864 epoch total loss 6.36846924\n",
      "Trained batch 649 batch loss 6.43545914 epoch total loss 6.36857271\n",
      "Trained batch 650 batch loss 6.62608385 epoch total loss 6.36896849\n",
      "Trained batch 651 batch loss 5.3579216 epoch total loss 6.36741543\n",
      "Trained batch 652 batch loss 5.63278532 epoch total loss 6.36628866\n",
      "Trained batch 653 batch loss 5.90162373 epoch total loss 6.3655777\n",
      "Trained batch 654 batch loss 5.88193798 epoch total loss 6.36483812\n",
      "Trained batch 655 batch loss 6.31364393 epoch total loss 6.36475945\n",
      "Trained batch 656 batch loss 6.34409952 epoch total loss 6.36472845\n",
      "Trained batch 657 batch loss 6.35301781 epoch total loss 6.36471033\n",
      "Trained batch 658 batch loss 6.16189528 epoch total loss 6.36440229\n",
      "Trained batch 659 batch loss 6.48477173 epoch total loss 6.3645854\n",
      "Trained batch 660 batch loss 6.39515257 epoch total loss 6.36463165\n",
      "Trained batch 661 batch loss 6.24380684 epoch total loss 6.36444855\n",
      "Trained batch 662 batch loss 6.12736082 epoch total loss 6.36409044\n",
      "Trained batch 663 batch loss 6.14893866 epoch total loss 6.36376572\n",
      "Trained batch 664 batch loss 6.06909323 epoch total loss 6.36332226\n",
      "Trained batch 665 batch loss 6.12433529 epoch total loss 6.3629632\n",
      "Trained batch 666 batch loss 6.53447676 epoch total loss 6.36322117\n",
      "Trained batch 667 batch loss 6.71314 epoch total loss 6.36374617\n",
      "Trained batch 668 batch loss 6.64049339 epoch total loss 6.36416054\n",
      "Trained batch 669 batch loss 6.79860306 epoch total loss 6.36481047\n",
      "Trained batch 670 batch loss 5.83703136 epoch total loss 6.36402225\n",
      "Trained batch 671 batch loss 5.96921301 epoch total loss 6.36343384\n",
      "Trained batch 672 batch loss 5.77504826 epoch total loss 6.36255836\n",
      "Trained batch 673 batch loss 6.42230701 epoch total loss 6.36264706\n",
      "Trained batch 674 batch loss 6.17335367 epoch total loss 6.3623662\n",
      "Trained batch 675 batch loss 6.30360317 epoch total loss 6.36227942\n",
      "Trained batch 676 batch loss 6.26945353 epoch total loss 6.36214209\n",
      "Trained batch 677 batch loss 6.43221188 epoch total loss 6.36224556\n",
      "Trained batch 678 batch loss 6.72918129 epoch total loss 6.36278629\n",
      "Trained batch 679 batch loss 6.61882687 epoch total loss 6.36316347\n",
      "Trained batch 680 batch loss 6.20104074 epoch total loss 6.36292505\n",
      "Trained batch 681 batch loss 6.26668262 epoch total loss 6.36278343\n",
      "Trained batch 682 batch loss 5.94004154 epoch total loss 6.36216354\n",
      "Trained batch 683 batch loss 6.14673328 epoch total loss 6.36184835\n",
      "Trained batch 684 batch loss 6.82003498 epoch total loss 6.36251831\n",
      "Trained batch 685 batch loss 6.15395832 epoch total loss 6.36221361\n",
      "Trained batch 686 batch loss 6.64342642 epoch total loss 6.36262369\n",
      "Trained batch 687 batch loss 6.17519045 epoch total loss 6.36235094\n",
      "Trained batch 688 batch loss 6.64942217 epoch total loss 6.36276817\n",
      "Trained batch 689 batch loss 6.48349667 epoch total loss 6.36294317\n",
      "Trained batch 690 batch loss 5.83971786 epoch total loss 6.362185\n",
      "Trained batch 691 batch loss 6.30378437 epoch total loss 6.3621006\n",
      "Trained batch 692 batch loss 6.31673717 epoch total loss 6.36203527\n",
      "Trained batch 693 batch loss 6.42942238 epoch total loss 6.36213207\n",
      "Trained batch 694 batch loss 6.61832237 epoch total loss 6.36250114\n",
      "Trained batch 695 batch loss 6.54984951 epoch total loss 6.36277056\n",
      "Trained batch 696 batch loss 6.73581934 epoch total loss 6.36330652\n",
      "Trained batch 697 batch loss 7.05652046 epoch total loss 6.3643012\n",
      "Trained batch 698 batch loss 7.21274376 epoch total loss 6.36551714\n",
      "Trained batch 699 batch loss 6.8816247 epoch total loss 6.36625576\n",
      "Trained batch 700 batch loss 6.50288534 epoch total loss 6.36645079\n",
      "Trained batch 701 batch loss 6.28621483 epoch total loss 6.36633635\n",
      "Trained batch 702 batch loss 6.49396229 epoch total loss 6.3665185\n",
      "Trained batch 703 batch loss 6.79882526 epoch total loss 6.36713314\n",
      "Trained batch 704 batch loss 6.84435797 epoch total loss 6.3678112\n",
      "Trained batch 705 batch loss 6.69159 epoch total loss 6.36827\n",
      "Trained batch 706 batch loss 6.60257912 epoch total loss 6.3686018\n",
      "Trained batch 707 batch loss 6.25033379 epoch total loss 6.36843491\n",
      "Trained batch 708 batch loss 6.60532093 epoch total loss 6.36876965\n",
      "Trained batch 709 batch loss 6.5142231 epoch total loss 6.36897469\n",
      "Trained batch 710 batch loss 6.99691486 epoch total loss 6.36985922\n",
      "Trained batch 711 batch loss 6.95087051 epoch total loss 6.37067604\n",
      "Trained batch 712 batch loss 6.943048 epoch total loss 6.37148\n",
      "Trained batch 713 batch loss 6.61484 epoch total loss 6.37182093\n",
      "Trained batch 714 batch loss 6.48938847 epoch total loss 6.37198544\n",
      "Trained batch 715 batch loss 6.75460052 epoch total loss 6.37252045\n",
      "Trained batch 716 batch loss 6.86389256 epoch total loss 6.37320662\n",
      "Trained batch 717 batch loss 6.23868465 epoch total loss 6.37301874\n",
      "Trained batch 718 batch loss 6.06718683 epoch total loss 6.3725934\n",
      "Trained batch 719 batch loss 6.35847473 epoch total loss 6.37257338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 720 batch loss 6.31090784 epoch total loss 6.37248802\n",
      "Trained batch 721 batch loss 6.19232893 epoch total loss 6.37223816\n",
      "Trained batch 722 batch loss 5.91104078 epoch total loss 6.37159967\n",
      "Trained batch 723 batch loss 6.32708 epoch total loss 6.37153816\n",
      "Trained batch 724 batch loss 6.46471691 epoch total loss 6.37166691\n",
      "Trained batch 725 batch loss 6.20273066 epoch total loss 6.37143373\n",
      "Trained batch 726 batch loss 6.63200474 epoch total loss 6.37179232\n",
      "Trained batch 727 batch loss 6.58483934 epoch total loss 6.37208557\n",
      "Trained batch 728 batch loss 6.17142439 epoch total loss 6.37181\n",
      "Trained batch 729 batch loss 6.14377832 epoch total loss 6.37149715\n",
      "Trained batch 730 batch loss 6.53291178 epoch total loss 6.37171793\n",
      "Trained batch 731 batch loss 6.79076958 epoch total loss 6.37229061\n",
      "Trained batch 732 batch loss 6.36129093 epoch total loss 6.37227583\n",
      "Trained batch 733 batch loss 6.54873514 epoch total loss 6.37251663\n",
      "Trained batch 734 batch loss 6.35564709 epoch total loss 6.37249327\n",
      "Trained batch 735 batch loss 6.03609943 epoch total loss 6.37203598\n",
      "Trained batch 736 batch loss 5.84376669 epoch total loss 6.37131786\n",
      "Trained batch 737 batch loss 6.77321815 epoch total loss 6.37186384\n",
      "Trained batch 738 batch loss 6.43210268 epoch total loss 6.37194538\n",
      "Trained batch 739 batch loss 6.65035486 epoch total loss 6.37232208\n",
      "Trained batch 740 batch loss 6.38277721 epoch total loss 6.37233639\n",
      "Trained batch 741 batch loss 5.94374371 epoch total loss 6.37175798\n",
      "Trained batch 742 batch loss 6.26537037 epoch total loss 6.37161446\n",
      "Trained batch 743 batch loss 6.34536695 epoch total loss 6.37157869\n",
      "Trained batch 744 batch loss 6.14096975 epoch total loss 6.37126875\n",
      "Trained batch 745 batch loss 6.21362495 epoch total loss 6.37105751\n",
      "Trained batch 746 batch loss 6.63441467 epoch total loss 6.37141037\n",
      "Trained batch 747 batch loss 5.86712408 epoch total loss 6.37073565\n",
      "Trained batch 748 batch loss 6.71034288 epoch total loss 6.37118959\n",
      "Trained batch 749 batch loss 6.61921215 epoch total loss 6.37152052\n",
      "Trained batch 750 batch loss 6.69232512 epoch total loss 6.37194872\n",
      "Trained batch 751 batch loss 6.6150651 epoch total loss 6.37227249\n",
      "Trained batch 752 batch loss 6.64131308 epoch total loss 6.37263\n",
      "Trained batch 753 batch loss 6.8018508 epoch total loss 6.3732\n",
      "Trained batch 754 batch loss 5.79194975 epoch total loss 6.37242889\n",
      "Trained batch 755 batch loss 5.34675026 epoch total loss 6.37107038\n",
      "Trained batch 756 batch loss 6.43329048 epoch total loss 6.3711524\n",
      "Trained batch 757 batch loss 6.39108896 epoch total loss 6.3711791\n",
      "Trained batch 758 batch loss 6.72754955 epoch total loss 6.37164879\n",
      "Trained batch 759 batch loss 6.40542459 epoch total loss 6.37169313\n",
      "Trained batch 760 batch loss 6.39023256 epoch total loss 6.37171745\n",
      "Trained batch 761 batch loss 6.37072277 epoch total loss 6.37171602\n",
      "Trained batch 762 batch loss 6.57302427 epoch total loss 6.37198067\n",
      "Trained batch 763 batch loss 6.61895561 epoch total loss 6.37230444\n",
      "Trained batch 764 batch loss 6.51075172 epoch total loss 6.37248564\n",
      "Trained batch 765 batch loss 6.45318651 epoch total loss 6.37259102\n",
      "Trained batch 766 batch loss 6.59852886 epoch total loss 6.37288618\n",
      "Trained batch 767 batch loss 6.46058321 epoch total loss 6.37300062\n",
      "Trained batch 768 batch loss 6.56105423 epoch total loss 6.37324524\n",
      "Trained batch 769 batch loss 6.46812248 epoch total loss 6.37336874\n",
      "Trained batch 770 batch loss 6.40674686 epoch total loss 6.37341213\n",
      "Trained batch 771 batch loss 6.08091974 epoch total loss 6.37303305\n",
      "Trained batch 772 batch loss 6.33002663 epoch total loss 6.37297726\n",
      "Trained batch 773 batch loss 6.50056791 epoch total loss 6.37314224\n",
      "Trained batch 774 batch loss 6.55611753 epoch total loss 6.37337875\n",
      "Trained batch 775 batch loss 6.28412199 epoch total loss 6.37326384\n",
      "Trained batch 776 batch loss 6.56766129 epoch total loss 6.37351418\n",
      "Trained batch 777 batch loss 6.65260506 epoch total loss 6.37387371\n",
      "Trained batch 778 batch loss 6.50440693 epoch total loss 6.37404156\n",
      "Trained batch 779 batch loss 6.61119175 epoch total loss 6.37434626\n",
      "Trained batch 780 batch loss 6.36777735 epoch total loss 6.37433767\n",
      "Trained batch 781 batch loss 6.42937374 epoch total loss 6.37440777\n",
      "Trained batch 782 batch loss 6.48846531 epoch total loss 6.37455368\n",
      "Trained batch 783 batch loss 6.47489786 epoch total loss 6.37468195\n",
      "Trained batch 784 batch loss 6.47422647 epoch total loss 6.37480879\n",
      "Trained batch 785 batch loss 6.48681831 epoch total loss 6.37495136\n",
      "Trained batch 786 batch loss 6.36730909 epoch total loss 6.37494183\n",
      "Trained batch 787 batch loss 6.46299171 epoch total loss 6.37505341\n",
      "Trained batch 788 batch loss 6.17807961 epoch total loss 6.37480354\n",
      "Trained batch 789 batch loss 6.37611437 epoch total loss 6.37480497\n",
      "Trained batch 790 batch loss 6.11178493 epoch total loss 6.37447214\n",
      "Trained batch 791 batch loss 6.14206171 epoch total loss 6.37417841\n",
      "Trained batch 792 batch loss 5.86440611 epoch total loss 6.37353468\n",
      "Trained batch 793 batch loss 6.33328819 epoch total loss 6.37348413\n",
      "Trained batch 794 batch loss 6.20114231 epoch total loss 6.37326717\n",
      "Trained batch 795 batch loss 6.20935917 epoch total loss 6.37306118\n",
      "Trained batch 796 batch loss 6.10890579 epoch total loss 6.3727293\n",
      "Trained batch 797 batch loss 6.04013538 epoch total loss 6.37231159\n",
      "Trained batch 798 batch loss 6.18979931 epoch total loss 6.37208319\n",
      "Trained batch 799 batch loss 6.0099268 epoch total loss 6.37162971\n",
      "Trained batch 800 batch loss 6.22475481 epoch total loss 6.37144613\n",
      "Trained batch 801 batch loss 6.1130619 epoch total loss 6.37112379\n",
      "Trained batch 802 batch loss 6.46578121 epoch total loss 6.37124157\n",
      "Trained batch 803 batch loss 6.29019356 epoch total loss 6.37114048\n",
      "Trained batch 804 batch loss 6.29396343 epoch total loss 6.37104464\n",
      "Trained batch 805 batch loss 6.47257614 epoch total loss 6.371171\n",
      "Trained batch 806 batch loss 6.4699707 epoch total loss 6.37129354\n",
      "Trained batch 807 batch loss 6.95389605 epoch total loss 6.37201595\n",
      "Trained batch 808 batch loss 6.92030907 epoch total loss 6.37269449\n",
      "Trained batch 809 batch loss 6.96912098 epoch total loss 6.37343216\n",
      "Trained batch 810 batch loss 6.20821285 epoch total loss 6.3732276\n",
      "Trained batch 811 batch loss 6.00680208 epoch total loss 6.37277603\n",
      "Trained batch 812 batch loss 6.31615162 epoch total loss 6.37270594\n",
      "Trained batch 813 batch loss 6.26889467 epoch total loss 6.37257862\n",
      "Trained batch 814 batch loss 6.22861433 epoch total loss 6.37240124\n",
      "Trained batch 815 batch loss 6.49099588 epoch total loss 6.37254715\n",
      "Trained batch 816 batch loss 6.32757235 epoch total loss 6.37249231\n",
      "Trained batch 817 batch loss 6.38696527 epoch total loss 6.37251\n",
      "Trained batch 818 batch loss 6.29755068 epoch total loss 6.3724184\n",
      "Trained batch 819 batch loss 6.22405243 epoch total loss 6.37223721\n",
      "Trained batch 820 batch loss 6.23566628 epoch total loss 6.37207079\n",
      "Trained batch 821 batch loss 6.31035376 epoch total loss 6.37199593\n",
      "Trained batch 822 batch loss 6.30686235 epoch total loss 6.37191629\n",
      "Trained batch 823 batch loss 6.25340319 epoch total loss 6.37177229\n",
      "Trained batch 824 batch loss 6.20198059 epoch total loss 6.37156677\n",
      "Trained batch 825 batch loss 6.18994045 epoch total loss 6.37134647\n",
      "Trained batch 826 batch loss 6.28387737 epoch total loss 6.37124\n",
      "Trained batch 827 batch loss 6.36729479 epoch total loss 6.37123537\n",
      "Trained batch 828 batch loss 6.35569382 epoch total loss 6.3712163\n",
      "Trained batch 829 batch loss 6.53733492 epoch total loss 6.37141657\n",
      "Trained batch 830 batch loss 6.18918133 epoch total loss 6.37119675\n",
      "Trained batch 831 batch loss 6.36870384 epoch total loss 6.37119341\n",
      "Trained batch 832 batch loss 6.23460484 epoch total loss 6.37102938\n",
      "Trained batch 833 batch loss 6.43161488 epoch total loss 6.37110186\n",
      "Trained batch 834 batch loss 6.67814541 epoch total loss 6.37147\n",
      "Trained batch 835 batch loss 6.59838057 epoch total loss 6.37174177\n",
      "Trained batch 836 batch loss 6.63127422 epoch total loss 6.37205219\n",
      "Trained batch 837 batch loss 6.75160742 epoch total loss 6.37250566\n",
      "Trained batch 838 batch loss 6.70912313 epoch total loss 6.37290716\n",
      "Trained batch 839 batch loss 6.79224825 epoch total loss 6.37340689\n",
      "Trained batch 840 batch loss 6.4951129 epoch total loss 6.37355185\n",
      "Trained batch 841 batch loss 6.29933834 epoch total loss 6.37346363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 842 batch loss 7.05465889 epoch total loss 6.37427282\n",
      "Trained batch 843 batch loss 6.82559443 epoch total loss 6.37480831\n",
      "Trained batch 844 batch loss 6.7828083 epoch total loss 6.37529135\n",
      "Trained batch 845 batch loss 6.60435677 epoch total loss 6.37556267\n",
      "Trained batch 846 batch loss 6.48004055 epoch total loss 6.37568617\n",
      "Trained batch 847 batch loss 6.04078102 epoch total loss 6.37529135\n",
      "Trained batch 848 batch loss 6.27600145 epoch total loss 6.37517405\n",
      "Trained batch 849 batch loss 6.31907892 epoch total loss 6.37510777\n",
      "Trained batch 850 batch loss 6.38844967 epoch total loss 6.3751235\n",
      "Trained batch 851 batch loss 6.01258373 epoch total loss 6.37469769\n",
      "Trained batch 852 batch loss 6.28468657 epoch total loss 6.37459183\n",
      "Trained batch 853 batch loss 5.86614132 epoch total loss 6.37399578\n",
      "Trained batch 854 batch loss 5.91962242 epoch total loss 6.37346363\n",
      "Trained batch 855 batch loss 5.91646528 epoch total loss 6.3729291\n",
      "Trained batch 856 batch loss 5.94163513 epoch total loss 6.37242508\n",
      "Trained batch 857 batch loss 6.44633961 epoch total loss 6.37251139\n",
      "Trained batch 858 batch loss 6.52749825 epoch total loss 6.37269163\n",
      "Trained batch 859 batch loss 6.48203 epoch total loss 6.37281895\n",
      "Trained batch 860 batch loss 6.47721148 epoch total loss 6.37294\n",
      "Trained batch 861 batch loss 6.3583951 epoch total loss 6.37292337\n",
      "Trained batch 862 batch loss 6.17091036 epoch total loss 6.37268877\n",
      "Trained batch 863 batch loss 6.21453285 epoch total loss 6.37250519\n",
      "Trained batch 864 batch loss 6.19750786 epoch total loss 6.37230253\n",
      "Trained batch 865 batch loss 6.05654716 epoch total loss 6.37193775\n",
      "Trained batch 866 batch loss 6.116014 epoch total loss 6.37164259\n",
      "Trained batch 867 batch loss 6.41745806 epoch total loss 6.37169504\n",
      "Trained batch 868 batch loss 6.88735437 epoch total loss 6.37228918\n",
      "Trained batch 869 batch loss 6.81794214 epoch total loss 6.37280178\n",
      "Trained batch 870 batch loss 6.81267071 epoch total loss 6.37330723\n",
      "Trained batch 871 batch loss 6.28688049 epoch total loss 6.37320852\n",
      "Trained batch 872 batch loss 6.71926451 epoch total loss 6.37360525\n",
      "Trained batch 873 batch loss 6.20723677 epoch total loss 6.37341452\n",
      "Trained batch 874 batch loss 6.33522129 epoch total loss 6.37337112\n",
      "Trained batch 875 batch loss 6.4792161 epoch total loss 6.37349176\n",
      "Trained batch 876 batch loss 6.50783777 epoch total loss 6.37364483\n",
      "Trained batch 877 batch loss 6.46497059 epoch total loss 6.37374878\n",
      "Trained batch 878 batch loss 6.32041 epoch total loss 6.37368822\n",
      "Trained batch 879 batch loss 6.25057697 epoch total loss 6.37354803\n",
      "Trained batch 880 batch loss 6.37240219 epoch total loss 6.3735466\n",
      "Trained batch 881 batch loss 6.52907181 epoch total loss 6.37372351\n",
      "Trained batch 882 batch loss 6.3881712 epoch total loss 6.37374\n",
      "Trained batch 883 batch loss 6.35553885 epoch total loss 6.37371922\n",
      "Trained batch 884 batch loss 6.3331418 epoch total loss 6.37367344\n",
      "Trained batch 885 batch loss 6.25780249 epoch total loss 6.37354231\n",
      "Trained batch 886 batch loss 6.36764765 epoch total loss 6.37353563\n",
      "Trained batch 887 batch loss 6.29748201 epoch total loss 6.37345\n",
      "Trained batch 888 batch loss 5.69475174 epoch total loss 6.37268543\n",
      "Trained batch 889 batch loss 5.77297306 epoch total loss 6.37201118\n",
      "Trained batch 890 batch loss 6.17146397 epoch total loss 6.37178564\n",
      "Trained batch 891 batch loss 6.28044367 epoch total loss 6.37168264\n",
      "Trained batch 892 batch loss 5.86675739 epoch total loss 6.37111664\n",
      "Trained batch 893 batch loss 5.45026 epoch total loss 6.37008524\n",
      "Trained batch 894 batch loss 5.77034378 epoch total loss 6.36941481\n",
      "Trained batch 895 batch loss 6.01728 epoch total loss 6.36902094\n",
      "Trained batch 896 batch loss 5.89697647 epoch total loss 6.36849451\n",
      "Trained batch 897 batch loss 6.34923267 epoch total loss 6.36847258\n",
      "Trained batch 898 batch loss 6.22871399 epoch total loss 6.36831665\n",
      "Trained batch 899 batch loss 6.36377716 epoch total loss 6.36831188\n",
      "Trained batch 900 batch loss 6.06520605 epoch total loss 6.36797523\n",
      "Trained batch 901 batch loss 6.2933712 epoch total loss 6.36789274\n",
      "Trained batch 902 batch loss 6.37361431 epoch total loss 6.36789894\n",
      "Trained batch 903 batch loss 6.60351706 epoch total loss 6.36816\n",
      "Trained batch 904 batch loss 5.58016 epoch total loss 6.36728811\n",
      "Trained batch 905 batch loss 5.20943069 epoch total loss 6.36600876\n",
      "Trained batch 906 batch loss 4.90294838 epoch total loss 6.36439371\n",
      "Trained batch 907 batch loss 6.08723164 epoch total loss 6.36408806\n",
      "Trained batch 908 batch loss 7.2311039 epoch total loss 6.36504316\n",
      "Trained batch 909 batch loss 7.16695 epoch total loss 6.36592531\n",
      "Trained batch 910 batch loss 6.80463934 epoch total loss 6.36640739\n",
      "Trained batch 911 batch loss 6.4185195 epoch total loss 6.36646461\n",
      "Trained batch 912 batch loss 5.02291775 epoch total loss 6.36499119\n",
      "Trained batch 913 batch loss 4.97647381 epoch total loss 6.36347055\n",
      "Trained batch 914 batch loss 5.90370274 epoch total loss 6.36296749\n",
      "Trained batch 915 batch loss 6.4858222 epoch total loss 6.36310196\n",
      "Trained batch 916 batch loss 7.01529455 epoch total loss 6.36381388\n",
      "Trained batch 917 batch loss 6.74035358 epoch total loss 6.36422443\n",
      "Trained batch 918 batch loss 6.13315344 epoch total loss 6.36397266\n",
      "Trained batch 919 batch loss 6.07506227 epoch total loss 6.36365843\n",
      "Trained batch 920 batch loss 6.37745476 epoch total loss 6.36367369\n",
      "Trained batch 921 batch loss 6.46435785 epoch total loss 6.36378288\n",
      "Trained batch 922 batch loss 6.50111246 epoch total loss 6.36393166\n",
      "Trained batch 923 batch loss 6.45581532 epoch total loss 6.36403131\n",
      "Trained batch 924 batch loss 6.60308838 epoch total loss 6.36429\n",
      "Trained batch 925 batch loss 6.57355213 epoch total loss 6.36451626\n",
      "Trained batch 926 batch loss 6.57205629 epoch total loss 6.36474085\n",
      "Trained batch 927 batch loss 6.41233 epoch total loss 6.36479187\n",
      "Trained batch 928 batch loss 6.27880573 epoch total loss 6.36469936\n",
      "Trained batch 929 batch loss 6.64596748 epoch total loss 6.36500216\n",
      "Trained batch 930 batch loss 6.49492311 epoch total loss 6.36514187\n",
      "Trained batch 931 batch loss 6.35454416 epoch total loss 6.36513042\n",
      "Trained batch 932 batch loss 5.77679491 epoch total loss 6.36449957\n",
      "Trained batch 933 batch loss 6.12538099 epoch total loss 6.36424303\n",
      "Trained batch 934 batch loss 6.30020094 epoch total loss 6.36417484\n",
      "Trained batch 935 batch loss 6.67013645 epoch total loss 6.36450148\n",
      "Trained batch 936 batch loss 6.22393 epoch total loss 6.36435175\n",
      "Trained batch 937 batch loss 6.19267273 epoch total loss 6.36416864\n",
      "Trained batch 938 batch loss 6.41575909 epoch total loss 6.36422348\n",
      "Trained batch 939 batch loss 6.25240469 epoch total loss 6.36410427\n",
      "Trained batch 940 batch loss 5.84321976 epoch total loss 6.36355\n",
      "Trained batch 941 batch loss 6.40057802 epoch total loss 6.36358929\n",
      "Trained batch 942 batch loss 6.34009838 epoch total loss 6.36356497\n",
      "Trained batch 943 batch loss 6.28248024 epoch total loss 6.36347914\n",
      "Trained batch 944 batch loss 6.34208918 epoch total loss 6.36345673\n",
      "Trained batch 945 batch loss 6.27945328 epoch total loss 6.36336756\n",
      "Trained batch 946 batch loss 6.6996417 epoch total loss 6.36372328\n",
      "Trained batch 947 batch loss 6.49085331 epoch total loss 6.36385727\n",
      "Trained batch 948 batch loss 6.68514538 epoch total loss 6.36419582\n",
      "Trained batch 949 batch loss 6.51157141 epoch total loss 6.36435127\n",
      "Trained batch 950 batch loss 6.33678102 epoch total loss 6.36432266\n",
      "Trained batch 951 batch loss 6.57773495 epoch total loss 6.36454678\n",
      "Trained batch 952 batch loss 6.33440304 epoch total loss 6.3645153\n",
      "Trained batch 953 batch loss 5.8874836 epoch total loss 6.3640151\n",
      "Trained batch 954 batch loss 6.40878153 epoch total loss 6.36406183\n",
      "Trained batch 955 batch loss 7.20667839 epoch total loss 6.36494398\n",
      "Trained batch 956 batch loss 6.97917175 epoch total loss 6.36558628\n",
      "Trained batch 957 batch loss 6.23610258 epoch total loss 6.36545134\n",
      "Trained batch 958 batch loss 6.48322392 epoch total loss 6.36557436\n",
      "Trained batch 959 batch loss 6.35293293 epoch total loss 6.36556149\n",
      "Trained batch 960 batch loss 6.20008659 epoch total loss 6.36538887\n",
      "Trained batch 961 batch loss 6.173388 epoch total loss 6.36518908\n",
      "Trained batch 962 batch loss 6.26863813 epoch total loss 6.36508894\n",
      "Trained batch 963 batch loss 5.89912891 epoch total loss 6.36460447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 964 batch loss 6.45512104 epoch total loss 6.36469841\n",
      "Trained batch 965 batch loss 6.75470781 epoch total loss 6.36510277\n",
      "Trained batch 966 batch loss 6.63920689 epoch total loss 6.36538649\n",
      "Trained batch 967 batch loss 6.50663567 epoch total loss 6.36553288\n",
      "Trained batch 968 batch loss 6.32190704 epoch total loss 6.36548758\n",
      "Trained batch 969 batch loss 6.23712254 epoch total loss 6.36535549\n",
      "Trained batch 970 batch loss 6.36235619 epoch total loss 6.36535215\n",
      "Trained batch 971 batch loss 6.6391983 epoch total loss 6.36563396\n",
      "Trained batch 972 batch loss 6.24892044 epoch total loss 6.36551428\n",
      "Trained batch 973 batch loss 6.37602282 epoch total loss 6.36552477\n",
      "Trained batch 974 batch loss 6.08514547 epoch total loss 6.36523676\n",
      "Trained batch 975 batch loss 6.23708677 epoch total loss 6.36510563\n",
      "Trained batch 976 batch loss 5.9770093 epoch total loss 6.36470795\n",
      "Trained batch 977 batch loss 6.54776478 epoch total loss 6.36489534\n",
      "Trained batch 978 batch loss 6.19126701 epoch total loss 6.36471796\n",
      "Trained batch 979 batch loss 6.32897902 epoch total loss 6.36468172\n",
      "Trained batch 980 batch loss 6.17052698 epoch total loss 6.36448336\n",
      "Trained batch 981 batch loss 6.45424318 epoch total loss 6.36457491\n",
      "Trained batch 982 batch loss 6.30683184 epoch total loss 6.36451578\n",
      "Trained batch 983 batch loss 6.09064817 epoch total loss 6.36423731\n",
      "Trained batch 984 batch loss 6.6640749 epoch total loss 6.36454201\n",
      "Trained batch 985 batch loss 6.65216732 epoch total loss 6.36483431\n",
      "Trained batch 986 batch loss 6.99618387 epoch total loss 6.3654747\n",
      "Trained batch 987 batch loss 6.47139883 epoch total loss 6.36558151\n",
      "Trained batch 988 batch loss 6.17790937 epoch total loss 6.36539173\n",
      "Trained batch 989 batch loss 6.44888306 epoch total loss 6.36547565\n",
      "Trained batch 990 batch loss 6.07307959 epoch total loss 6.36518049\n",
      "Trained batch 991 batch loss 6.26494789 epoch total loss 6.36508\n",
      "Trained batch 992 batch loss 6.02555847 epoch total loss 6.36473703\n",
      "Trained batch 993 batch loss 6.28994751 epoch total loss 6.36466217\n",
      "Trained batch 994 batch loss 6.06019354 epoch total loss 6.36435556\n",
      "Trained batch 995 batch loss 6.33460617 epoch total loss 6.36432552\n",
      "Trained batch 996 batch loss 6.21755552 epoch total loss 6.36417818\n",
      "Trained batch 997 batch loss 6.17505741 epoch total loss 6.36398888\n",
      "Trained batch 998 batch loss 6.08447552 epoch total loss 6.36370897\n",
      "Trained batch 999 batch loss 6.05443907 epoch total loss 6.36339903\n",
      "Trained batch 1000 batch loss 6.26041126 epoch total loss 6.36329603\n",
      "Trained batch 1001 batch loss 6.10380745 epoch total loss 6.36303663\n",
      "Trained batch 1002 batch loss 6.28229618 epoch total loss 6.36295605\n",
      "Trained batch 1003 batch loss 6.27980089 epoch total loss 6.36287308\n",
      "Trained batch 1004 batch loss 6.45738363 epoch total loss 6.36296749\n",
      "Trained batch 1005 batch loss 6.28854609 epoch total loss 6.36289358\n",
      "Trained batch 1006 batch loss 6.85815716 epoch total loss 6.36338615\n",
      "Trained batch 1007 batch loss 6.54824448 epoch total loss 6.36356974\n",
      "Trained batch 1008 batch loss 6.57025576 epoch total loss 6.36377478\n",
      "Trained batch 1009 batch loss 6.48100805 epoch total loss 6.36389112\n",
      "Trained batch 1010 batch loss 6.37919617 epoch total loss 6.36390638\n",
      "Trained batch 1011 batch loss 6.36994028 epoch total loss 6.36391258\n",
      "Trained batch 1012 batch loss 6.16305 epoch total loss 6.36371422\n",
      "Trained batch 1013 batch loss 6.62279558 epoch total loss 6.36397\n",
      "Trained batch 1014 batch loss 6.83621168 epoch total loss 6.36443567\n",
      "Trained batch 1015 batch loss 6.4926157 epoch total loss 6.36456203\n",
      "Trained batch 1016 batch loss 6.13452339 epoch total loss 6.36433554\n",
      "Trained batch 1017 batch loss 6.43621397 epoch total loss 6.36440611\n",
      "Trained batch 1018 batch loss 5.82231 epoch total loss 6.36387348\n",
      "Trained batch 1019 batch loss 6.31116199 epoch total loss 6.36382198\n",
      "Trained batch 1020 batch loss 6.73263693 epoch total loss 6.36418295\n",
      "Trained batch 1021 batch loss 6.22342587 epoch total loss 6.36404562\n",
      "Trained batch 1022 batch loss 6.50635338 epoch total loss 6.36418486\n",
      "Trained batch 1023 batch loss 6.52266359 epoch total loss 6.36433935\n",
      "Trained batch 1024 batch loss 5.72001886 epoch total loss 6.3637104\n",
      "Trained batch 1025 batch loss 6.00996351 epoch total loss 6.36336517\n",
      "Trained batch 1026 batch loss 6.21832323 epoch total loss 6.36322355\n",
      "Trained batch 1027 batch loss 5.75562811 epoch total loss 6.36263227\n",
      "Trained batch 1028 batch loss 6.12002516 epoch total loss 6.36239624\n",
      "Trained batch 1029 batch loss 6.06140947 epoch total loss 6.36210394\n",
      "Trained batch 1030 batch loss 5.58711147 epoch total loss 6.36135149\n",
      "Trained batch 1031 batch loss 5.81950569 epoch total loss 6.36082554\n",
      "Trained batch 1032 batch loss 5.72814083 epoch total loss 6.36021233\n",
      "Trained batch 1033 batch loss 5.88675117 epoch total loss 6.35975409\n",
      "Trained batch 1034 batch loss 6.36963129 epoch total loss 6.35976362\n",
      "Trained batch 1035 batch loss 6.26411247 epoch total loss 6.35967112\n",
      "Trained batch 1036 batch loss 6.41092396 epoch total loss 6.35972071\n",
      "Trained batch 1037 batch loss 6.48385525 epoch total loss 6.35984087\n",
      "Trained batch 1038 batch loss 6.48585033 epoch total loss 6.35996199\n",
      "Trained batch 1039 batch loss 6.68806791 epoch total loss 6.36027765\n",
      "Trained batch 1040 batch loss 6.57163525 epoch total loss 6.36048126\n",
      "Trained batch 1041 batch loss 6.60764837 epoch total loss 6.36071825\n",
      "Trained batch 1042 batch loss 6.8339262 epoch total loss 6.36117268\n",
      "Trained batch 1043 batch loss 6.7135334 epoch total loss 6.36151028\n",
      "Trained batch 1044 batch loss 6.75481796 epoch total loss 6.36188698\n",
      "Trained batch 1045 batch loss 6.41492081 epoch total loss 6.361938\n",
      "Trained batch 1046 batch loss 6.31158972 epoch total loss 6.36189\n",
      "Trained batch 1047 batch loss 6.48809385 epoch total loss 6.36201048\n",
      "Trained batch 1048 batch loss 6.49366188 epoch total loss 6.36213589\n",
      "Trained batch 1049 batch loss 6.67081738 epoch total loss 6.36243057\n",
      "Trained batch 1050 batch loss 7.08405733 epoch total loss 6.36311769\n",
      "Trained batch 1051 batch loss 6.81775761 epoch total loss 6.36355\n",
      "Trained batch 1052 batch loss 6.64135122 epoch total loss 6.36381388\n",
      "Trained batch 1053 batch loss 6.46713161 epoch total loss 6.36391258\n",
      "Trained batch 1054 batch loss 6.74033499 epoch total loss 6.36426926\n",
      "Trained batch 1055 batch loss 6.3662262 epoch total loss 6.36427116\n",
      "Trained batch 1056 batch loss 6.31802702 epoch total loss 6.36422729\n",
      "Trained batch 1057 batch loss 6.60387897 epoch total loss 6.36445427\n",
      "Trained batch 1058 batch loss 6.52236843 epoch total loss 6.36460352\n",
      "Trained batch 1059 batch loss 6.61440086 epoch total loss 6.36483908\n",
      "Trained batch 1060 batch loss 6.58548069 epoch total loss 6.36504745\n",
      "Trained batch 1061 batch loss 6.37736416 epoch total loss 6.3650589\n",
      "Trained batch 1062 batch loss 6.25974 epoch total loss 6.36495972\n",
      "Trained batch 1063 batch loss 6.42215061 epoch total loss 6.36501408\n",
      "Trained batch 1064 batch loss 6.36682415 epoch total loss 6.36501551\n",
      "Trained batch 1065 batch loss 6.82134628 epoch total loss 6.36544371\n",
      "Trained batch 1066 batch loss 6.55285645 epoch total loss 6.36561966\n",
      "Trained batch 1067 batch loss 6.69401312 epoch total loss 6.36592722\n",
      "Trained batch 1068 batch loss 6.65844631 epoch total loss 6.36620092\n",
      "Trained batch 1069 batch loss 6.84414768 epoch total loss 6.3666482\n",
      "Trained batch 1070 batch loss 6.4469223 epoch total loss 6.36672306\n",
      "Trained batch 1071 batch loss 6.43274784 epoch total loss 6.36678457\n",
      "Trained batch 1072 batch loss 6.62685442 epoch total loss 6.36702728\n",
      "Trained batch 1073 batch loss 6.79985523 epoch total loss 6.36743069\n",
      "Trained batch 1074 batch loss 6.74375868 epoch total loss 6.36778069\n",
      "Trained batch 1075 batch loss 6.44991779 epoch total loss 6.36785698\n",
      "Trained batch 1076 batch loss 6.65251303 epoch total loss 6.36812162\n",
      "Trained batch 1077 batch loss 6.70407343 epoch total loss 6.36843348\n",
      "Trained batch 1078 batch loss 6.51503325 epoch total loss 6.36856937\n",
      "Trained batch 1079 batch loss 6.28954268 epoch total loss 6.36849642\n",
      "Trained batch 1080 batch loss 6.395895 epoch total loss 6.36852169\n",
      "Trained batch 1081 batch loss 6.43930101 epoch total loss 6.36858749\n",
      "Trained batch 1082 batch loss 6.35063744 epoch total loss 6.3685708\n",
      "Trained batch 1083 batch loss 5.67531252 epoch total loss 6.36793041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1084 batch loss 5.70530605 epoch total loss 6.36731911\n",
      "Trained batch 1085 batch loss 6.16906166 epoch total loss 6.367136\n",
      "Trained batch 1086 batch loss 6.37167311 epoch total loss 6.36714029\n",
      "Trained batch 1087 batch loss 6.36214209 epoch total loss 6.367136\n",
      "Trained batch 1088 batch loss 5.77874136 epoch total loss 6.36659527\n",
      "Trained batch 1089 batch loss 5.55662203 epoch total loss 6.3658514\n",
      "Trained batch 1090 batch loss 5.71468401 epoch total loss 6.36525393\n",
      "Trained batch 1091 batch loss 6.02579117 epoch total loss 6.36494303\n",
      "Trained batch 1092 batch loss 6.442523 epoch total loss 6.36501408\n",
      "Trained batch 1093 batch loss 6.49886274 epoch total loss 6.36513662\n",
      "Trained batch 1094 batch loss 6.56682062 epoch total loss 6.36532116\n",
      "Trained batch 1095 batch loss 6.23051262 epoch total loss 6.36519766\n",
      "Trained batch 1096 batch loss 6.53101778 epoch total loss 6.36534929\n",
      "Trained batch 1097 batch loss 6.62023 epoch total loss 6.36558151\n",
      "Trained batch 1098 batch loss 6.27489376 epoch total loss 6.36549902\n",
      "Trained batch 1099 batch loss 6.36527681 epoch total loss 6.36549854\n",
      "Trained batch 1100 batch loss 6.27873325 epoch total loss 6.36542\n",
      "Trained batch 1101 batch loss 6.08204174 epoch total loss 6.36516237\n",
      "Trained batch 1102 batch loss 6.12313557 epoch total loss 6.36494303\n",
      "Trained batch 1103 batch loss 6.05307198 epoch total loss 6.36466026\n",
      "Trained batch 1104 batch loss 6.23969364 epoch total loss 6.36454725\n",
      "Trained batch 1105 batch loss 5.93465376 epoch total loss 6.36415815\n",
      "Trained batch 1106 batch loss 6.08870125 epoch total loss 6.36390924\n",
      "Trained batch 1107 batch loss 6.67698 epoch total loss 6.36419153\n",
      "Trained batch 1108 batch loss 6.26006842 epoch total loss 6.36409807\n",
      "Trained batch 1109 batch loss 5.35807323 epoch total loss 6.36319065\n",
      "Trained batch 1110 batch loss 5.52727461 epoch total loss 6.36243773\n",
      "Trained batch 1111 batch loss 6.01665878 epoch total loss 6.36212635\n",
      "Trained batch 1112 batch loss 6.30493402 epoch total loss 6.36207485\n",
      "Trained batch 1113 batch loss 6.48098612 epoch total loss 6.36218166\n",
      "Trained batch 1114 batch loss 6.18431091 epoch total loss 6.36202192\n",
      "Trained batch 1115 batch loss 6.30157232 epoch total loss 6.36196804\n",
      "Trained batch 1116 batch loss 6.45093107 epoch total loss 6.36204767\n",
      "Trained batch 1117 batch loss 6.24561834 epoch total loss 6.36194372\n",
      "Trained batch 1118 batch loss 6.53465509 epoch total loss 6.36209822\n",
      "Trained batch 1119 batch loss 6.16212 epoch total loss 6.3619194\n",
      "Trained batch 1120 batch loss 6.02405262 epoch total loss 6.36161757\n",
      "Trained batch 1121 batch loss 6.05682611 epoch total loss 6.36134577\n",
      "Trained batch 1122 batch loss 6.27017117 epoch total loss 6.36126423\n",
      "Trained batch 1123 batch loss 6.33819962 epoch total loss 6.36124372\n",
      "Trained batch 1124 batch loss 6.27047586 epoch total loss 6.36116314\n",
      "Trained batch 1125 batch loss 6.26523638 epoch total loss 6.36107779\n",
      "Trained batch 1126 batch loss 6.43071508 epoch total loss 6.3611393\n",
      "Trained batch 1127 batch loss 6.27951145 epoch total loss 6.36106682\n",
      "Trained batch 1128 batch loss 6.26264954 epoch total loss 6.36097956\n",
      "Trained batch 1129 batch loss 5.86466 epoch total loss 6.36054\n",
      "Trained batch 1130 batch loss 6.6544795 epoch total loss 6.36080027\n",
      "Trained batch 1131 batch loss 6.20718098 epoch total loss 6.36066437\n",
      "Trained batch 1132 batch loss 6.29231453 epoch total loss 6.36060381\n",
      "Trained batch 1133 batch loss 6.41964054 epoch total loss 6.36065578\n",
      "Trained batch 1134 batch loss 5.77467 epoch total loss 6.36013937\n",
      "Trained batch 1135 batch loss 6.1614809 epoch total loss 6.35996437\n",
      "Trained batch 1136 batch loss 6.219666 epoch total loss 6.35984087\n",
      "Trained batch 1137 batch loss 5.89945745 epoch total loss 6.35943604\n",
      "Trained batch 1138 batch loss 5.96878815 epoch total loss 6.35909271\n",
      "Trained batch 1139 batch loss 6.0821991 epoch total loss 6.35884953\n",
      "Trained batch 1140 batch loss 6.06803131 epoch total loss 6.35859394\n",
      "Trained batch 1141 batch loss 6.04747772 epoch total loss 6.35832119\n",
      "Trained batch 1142 batch loss 5.92185 epoch total loss 6.35793924\n",
      "Trained batch 1143 batch loss 6.24271297 epoch total loss 6.35783815\n",
      "Trained batch 1144 batch loss 6.13291311 epoch total loss 6.3576417\n",
      "Trained batch 1145 batch loss 5.79193544 epoch total loss 6.35714769\n",
      "Trained batch 1146 batch loss 6.59314394 epoch total loss 6.35735369\n",
      "Trained batch 1147 batch loss 6.51468563 epoch total loss 6.35749102\n",
      "Trained batch 1148 batch loss 6.52082777 epoch total loss 6.35763311\n",
      "Trained batch 1149 batch loss 6.31777 epoch total loss 6.35759878\n",
      "Trained batch 1150 batch loss 6.28606653 epoch total loss 6.35753632\n",
      "Trained batch 1151 batch loss 6.85828543 epoch total loss 6.35797167\n",
      "Trained batch 1152 batch loss 6.37986183 epoch total loss 6.35799074\n",
      "Trained batch 1153 batch loss 7.07190657 epoch total loss 6.35860968\n",
      "Trained batch 1154 batch loss 7.26866436 epoch total loss 6.35939837\n",
      "Trained batch 1155 batch loss 6.98399973 epoch total loss 6.3599391\n",
      "Trained batch 1156 batch loss 7.18208838 epoch total loss 6.36065\n",
      "Trained batch 1157 batch loss 6.48734 epoch total loss 6.36075974\n",
      "Trained batch 1158 batch loss 6.50217533 epoch total loss 6.36088181\n",
      "Trained batch 1159 batch loss 5.7840476 epoch total loss 6.36038399\n",
      "Trained batch 1160 batch loss 6.28596973 epoch total loss 6.36032\n",
      "Trained batch 1161 batch loss 6.76463604 epoch total loss 6.36066818\n",
      "Trained batch 1162 batch loss 6.62740421 epoch total loss 6.36089802\n",
      "Trained batch 1163 batch loss 6.2273097 epoch total loss 6.3607831\n",
      "Trained batch 1164 batch loss 6.24436426 epoch total loss 6.36068296\n",
      "Trained batch 1165 batch loss 6.48326826 epoch total loss 6.36078835\n",
      "Trained batch 1166 batch loss 6.16292048 epoch total loss 6.36061859\n",
      "Trained batch 1167 batch loss 6.14000607 epoch total loss 6.36043\n",
      "Trained batch 1168 batch loss 6.06253099 epoch total loss 6.36017466\n",
      "Trained batch 1169 batch loss 6.13600063 epoch total loss 6.35998297\n",
      "Trained batch 1170 batch loss 6.35831833 epoch total loss 6.35998201\n",
      "Trained batch 1171 batch loss 6.63688564 epoch total loss 6.36021805\n",
      "Trained batch 1172 batch loss 6.29954576 epoch total loss 6.36016607\n",
      "Trained batch 1173 batch loss 6.14986897 epoch total loss 6.35998678\n",
      "Trained batch 1174 batch loss 6.64171267 epoch total loss 6.36022663\n",
      "Trained batch 1175 batch loss 6.36014509 epoch total loss 6.36022711\n",
      "Trained batch 1176 batch loss 6.48505831 epoch total loss 6.36033297\n",
      "Trained batch 1177 batch loss 6.40589285 epoch total loss 6.36037159\n",
      "Trained batch 1178 batch loss 6.33492374 epoch total loss 6.36035\n",
      "Trained batch 1179 batch loss 6.36868429 epoch total loss 6.36035681\n",
      "Trained batch 1180 batch loss 5.81391144 epoch total loss 6.3598938\n",
      "Trained batch 1181 batch loss 6.35736465 epoch total loss 6.35989189\n",
      "Trained batch 1182 batch loss 6.34794807 epoch total loss 6.35988188\n",
      "Trained batch 1183 batch loss 6.71745205 epoch total loss 6.36018419\n",
      "Trained batch 1184 batch loss 6.76971054 epoch total loss 6.36053\n",
      "Trained batch 1185 batch loss 6.68711615 epoch total loss 6.36080503\n",
      "Trained batch 1186 batch loss 6.32963181 epoch total loss 6.36077881\n",
      "Trained batch 1187 batch loss 6.01294565 epoch total loss 6.36048603\n",
      "Trained batch 1188 batch loss 6.38702488 epoch total loss 6.36050844\n",
      "Trained batch 1189 batch loss 6.19051886 epoch total loss 6.36036539\n",
      "Trained batch 1190 batch loss 6.24890232 epoch total loss 6.36027193\n",
      "Trained batch 1191 batch loss 6.18563843 epoch total loss 6.36012506\n",
      "Trained batch 1192 batch loss 6.27705097 epoch total loss 6.36005545\n",
      "Trained batch 1193 batch loss 6.13810444 epoch total loss 6.35986948\n",
      "Trained batch 1194 batch loss 6.40725279 epoch total loss 6.35990906\n",
      "Trained batch 1195 batch loss 5.93950605 epoch total loss 6.35955715\n",
      "Trained batch 1196 batch loss 6.49752903 epoch total loss 6.35967255\n",
      "Trained batch 1197 batch loss 6.52738857 epoch total loss 6.35981274\n",
      "Trained batch 1198 batch loss 5.60315514 epoch total loss 6.35918093\n",
      "Trained batch 1199 batch loss 5.98295927 epoch total loss 6.35886717\n",
      "Trained batch 1200 batch loss 6.32486057 epoch total loss 6.35883856\n",
      "Trained batch 1201 batch loss 6.34117794 epoch total loss 6.35882425\n",
      "Trained batch 1202 batch loss 6.59022141 epoch total loss 6.3590169\n",
      "Trained batch 1203 batch loss 6.316 epoch total loss 6.35898066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1204 batch loss 6.21777391 epoch total loss 6.35886383\n",
      "Trained batch 1205 batch loss 6.37248373 epoch total loss 6.3588748\n",
      "Trained batch 1206 batch loss 6.17413664 epoch total loss 6.35872173\n",
      "Trained batch 1207 batch loss 6.15874863 epoch total loss 6.35855627\n",
      "Trained batch 1208 batch loss 6.23833895 epoch total loss 6.35845661\n",
      "Trained batch 1209 batch loss 6.11989689 epoch total loss 6.35825968\n",
      "Trained batch 1210 batch loss 6.15312624 epoch total loss 6.35809\n",
      "Trained batch 1211 batch loss 6.21741915 epoch total loss 6.35797405\n",
      "Trained batch 1212 batch loss 6.54208565 epoch total loss 6.35812569\n",
      "Trained batch 1213 batch loss 6.29845715 epoch total loss 6.35807657\n",
      "Trained batch 1214 batch loss 6.06823158 epoch total loss 6.35783768\n",
      "Trained batch 1215 batch loss 6.32266331 epoch total loss 6.35780907\n",
      "Trained batch 1216 batch loss 5.98551273 epoch total loss 6.35750246\n",
      "Trained batch 1217 batch loss 6.69233942 epoch total loss 6.3577776\n",
      "Trained batch 1218 batch loss 5.93393469 epoch total loss 6.35743\n",
      "Trained batch 1219 batch loss 5.44733667 epoch total loss 6.35668325\n",
      "Trained batch 1220 batch loss 6.35329771 epoch total loss 6.35668087\n",
      "Trained batch 1221 batch loss 5.75446796 epoch total loss 6.35618734\n",
      "Trained batch 1222 batch loss 5.82254791 epoch total loss 6.35575104\n",
      "Trained batch 1223 batch loss 5.11277628 epoch total loss 6.35473442\n",
      "Trained batch 1224 batch loss 5.53053904 epoch total loss 6.35406113\n",
      "Trained batch 1225 batch loss 5.70191288 epoch total loss 6.35352898\n",
      "Trained batch 1226 batch loss 5.64813948 epoch total loss 6.35295343\n",
      "Trained batch 1227 batch loss 5.725667 epoch total loss 6.35244226\n",
      "Trained batch 1228 batch loss 6.39822292 epoch total loss 6.35248\n",
      "Trained batch 1229 batch loss 5.79607487 epoch total loss 6.35202694\n",
      "Trained batch 1230 batch loss 5.81565094 epoch total loss 6.35159063\n",
      "Trained batch 1231 batch loss 6.092628 epoch total loss 6.35138035\n",
      "Trained batch 1232 batch loss 5.85399771 epoch total loss 6.35097694\n",
      "Trained batch 1233 batch loss 6.18808794 epoch total loss 6.35084438\n",
      "Trained batch 1234 batch loss 6.38279152 epoch total loss 6.35087061\n",
      "Trained batch 1235 batch loss 6.78605843 epoch total loss 6.35122299\n",
      "Trained batch 1236 batch loss 6.79344 epoch total loss 6.35158062\n",
      "Trained batch 1237 batch loss 6.19727373 epoch total loss 6.35145569\n",
      "Trained batch 1238 batch loss 6.36192942 epoch total loss 6.35146427\n",
      "Trained batch 1239 batch loss 5.99074793 epoch total loss 6.35117292\n",
      "Trained batch 1240 batch loss 5.84744644 epoch total loss 6.35076714\n",
      "Trained batch 1241 batch loss 5.51405573 epoch total loss 6.35009289\n",
      "Trained batch 1242 batch loss 6.26431274 epoch total loss 6.35002375\n",
      "Trained batch 1243 batch loss 5.43410349 epoch total loss 6.34928703\n",
      "Trained batch 1244 batch loss 6.18490314 epoch total loss 6.34915495\n",
      "Trained batch 1245 batch loss 6.31498766 epoch total loss 6.34912729\n",
      "Trained batch 1246 batch loss 6.48658514 epoch total loss 6.34923792\n",
      "Trained batch 1247 batch loss 6.19157934 epoch total loss 6.34911108\n",
      "Trained batch 1248 batch loss 6.21425486 epoch total loss 6.34900331\n",
      "Trained batch 1249 batch loss 6.33480453 epoch total loss 6.34899187\n",
      "Trained batch 1250 batch loss 6.18041658 epoch total loss 6.34885693\n",
      "Trained batch 1251 batch loss 5.94937372 epoch total loss 6.34853745\n",
      "Trained batch 1252 batch loss 5.79464293 epoch total loss 6.34809494\n",
      "Trained batch 1253 batch loss 6.47056627 epoch total loss 6.34819269\n",
      "Trained batch 1254 batch loss 6.60107756 epoch total loss 6.34839439\n",
      "Trained batch 1255 batch loss 6.54067469 epoch total loss 6.34854746\n",
      "Trained batch 1256 batch loss 6.32802439 epoch total loss 6.34853125\n",
      "Trained batch 1257 batch loss 6.33015823 epoch total loss 6.34851646\n",
      "Trained batch 1258 batch loss 6.28387976 epoch total loss 6.34846497\n",
      "Trained batch 1259 batch loss 6.62958 epoch total loss 6.34868813\n",
      "Trained batch 1260 batch loss 6.24239969 epoch total loss 6.34860373\n",
      "Trained batch 1261 batch loss 6.49982405 epoch total loss 6.34872389\n",
      "Trained batch 1262 batch loss 6.28334141 epoch total loss 6.34867191\n",
      "Trained batch 1263 batch loss 5.58642101 epoch total loss 6.34806824\n",
      "Trained batch 1264 batch loss 5.55267382 epoch total loss 6.34743929\n",
      "Trained batch 1265 batch loss 6.22609138 epoch total loss 6.34734344\n",
      "Trained batch 1266 batch loss 6.77860403 epoch total loss 6.34768391\n",
      "Trained batch 1267 batch loss 6.317379 epoch total loss 6.34766\n",
      "Trained batch 1268 batch loss 6.79198742 epoch total loss 6.34801054\n",
      "Trained batch 1269 batch loss 6.77267122 epoch total loss 6.3483448\n",
      "Trained batch 1270 batch loss 6.59535 epoch total loss 6.34853935\n",
      "Trained batch 1271 batch loss 6.81970024 epoch total loss 6.34891033\n",
      "Trained batch 1272 batch loss 6.63927078 epoch total loss 6.34913826\n",
      "Trained batch 1273 batch loss 6.17401 epoch total loss 6.34900045\n",
      "Trained batch 1274 batch loss 6.5829134 epoch total loss 6.34918451\n",
      "Trained batch 1275 batch loss 6.26745033 epoch total loss 6.34912\n",
      "Trained batch 1276 batch loss 6.60585213 epoch total loss 6.34932184\n",
      "Trained batch 1277 batch loss 6.63363647 epoch total loss 6.34954453\n",
      "Trained batch 1278 batch loss 6.58042145 epoch total loss 6.34972525\n",
      "Trained batch 1279 batch loss 6.2478528 epoch total loss 6.34964561\n",
      "Trained batch 1280 batch loss 6.36688948 epoch total loss 6.34965897\n",
      "Trained batch 1281 batch loss 6.55285931 epoch total loss 6.34981728\n",
      "Trained batch 1282 batch loss 6.42533064 epoch total loss 6.3498764\n",
      "Trained batch 1283 batch loss 6.48484468 epoch total loss 6.34998178\n",
      "Trained batch 1284 batch loss 6.6815834 epoch total loss 6.35023975\n",
      "Trained batch 1285 batch loss 6.47056723 epoch total loss 6.35033369\n",
      "Trained batch 1286 batch loss 6.63581944 epoch total loss 6.35055542\n",
      "Trained batch 1287 batch loss 6.38314915 epoch total loss 6.35058117\n",
      "Trained batch 1288 batch loss 6.45540142 epoch total loss 6.35066271\n",
      "Trained batch 1289 batch loss 6.46424294 epoch total loss 6.35075092\n",
      "Trained batch 1290 batch loss 6.4815855 epoch total loss 6.35085249\n",
      "Trained batch 1291 batch loss 6.77054501 epoch total loss 6.35117769\n",
      "Trained batch 1292 batch loss 6.31273127 epoch total loss 6.35114765\n",
      "Trained batch 1293 batch loss 6.32682467 epoch total loss 6.35112906\n",
      "Trained batch 1294 batch loss 6.4289 epoch total loss 6.35118914\n",
      "Trained batch 1295 batch loss 6.28355789 epoch total loss 6.35113621\n",
      "Trained batch 1296 batch loss 6.18157864 epoch total loss 6.35100555\n",
      "Trained batch 1297 batch loss 6.43876743 epoch total loss 6.35107327\n",
      "Trained batch 1298 batch loss 6.39603853 epoch total loss 6.35110807\n",
      "Trained batch 1299 batch loss 6.36427879 epoch total loss 6.35111809\n",
      "Trained batch 1300 batch loss 6.26559401 epoch total loss 6.35105228\n",
      "Trained batch 1301 batch loss 6.33148909 epoch total loss 6.35103703\n",
      "Trained batch 1302 batch loss 6.82229805 epoch total loss 6.35139894\n",
      "Trained batch 1303 batch loss 6.28750563 epoch total loss 6.35135\n",
      "Trained batch 1304 batch loss 6.43035936 epoch total loss 6.35141039\n",
      "Trained batch 1305 batch loss 6.6011095 epoch total loss 6.35160208\n",
      "Trained batch 1306 batch loss 6.63359261 epoch total loss 6.35181808\n",
      "Trained batch 1307 batch loss 6.42070341 epoch total loss 6.35187101\n",
      "Trained batch 1308 batch loss 6.43039417 epoch total loss 6.3519311\n",
      "Trained batch 1309 batch loss 6.448 epoch total loss 6.352005\n",
      "Trained batch 1310 batch loss 6.54902172 epoch total loss 6.35215521\n",
      "Trained batch 1311 batch loss 6.57837915 epoch total loss 6.35232735\n",
      "Trained batch 1312 batch loss 6.73816252 epoch total loss 6.35262156\n",
      "Trained batch 1313 batch loss 6.90623856 epoch total loss 6.35304356\n",
      "Trained batch 1314 batch loss 6.93621492 epoch total loss 6.35348749\n",
      "Trained batch 1315 batch loss 6.7843585 epoch total loss 6.35381508\n",
      "Trained batch 1316 batch loss 6.72688341 epoch total loss 6.35409832\n",
      "Trained batch 1317 batch loss 6.85629559 epoch total loss 6.35448\n",
      "Trained batch 1318 batch loss 6.84430122 epoch total loss 6.35485172\n",
      "Trained batch 1319 batch loss 6.51883888 epoch total loss 6.3549757\n",
      "Trained batch 1320 batch loss 6.59989262 epoch total loss 6.35516119\n",
      "Trained batch 1321 batch loss 6.56841516 epoch total loss 6.35532236\n",
      "Trained batch 1322 batch loss 6.44114 epoch total loss 6.35538769\n",
      "Trained batch 1323 batch loss 6.39309502 epoch total loss 6.3554163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1324 batch loss 6.09907961 epoch total loss 6.35522223\n",
      "Trained batch 1325 batch loss 5.88032103 epoch total loss 6.35486364\n",
      "Trained batch 1326 batch loss 6.74288464 epoch total loss 6.35515642\n",
      "Trained batch 1327 batch loss 6.21575975 epoch total loss 6.35505152\n",
      "Trained batch 1328 batch loss 5.97862101 epoch total loss 6.3547678\n",
      "Trained batch 1329 batch loss 5.83388758 epoch total loss 6.35437632\n",
      "Trained batch 1330 batch loss 5.50415134 epoch total loss 6.3537364\n",
      "Trained batch 1331 batch loss 5.73198414 epoch total loss 6.35327\n",
      "Trained batch 1332 batch loss 6.17155457 epoch total loss 6.35313368\n",
      "Trained batch 1333 batch loss 6.43619537 epoch total loss 6.35319614\n",
      "Trained batch 1334 batch loss 6.40182304 epoch total loss 6.35323238\n",
      "Trained batch 1335 batch loss 6.32302809 epoch total loss 6.35321\n",
      "Trained batch 1336 batch loss 6.15321159 epoch total loss 6.35306025\n",
      "Trained batch 1337 batch loss 6.09308147 epoch total loss 6.3528657\n",
      "Trained batch 1338 batch loss 6.42386675 epoch total loss 6.35291862\n",
      "Trained batch 1339 batch loss 6.27943563 epoch total loss 6.35286379\n",
      "Trained batch 1340 batch loss 6.36422539 epoch total loss 6.35287189\n",
      "Trained batch 1341 batch loss 6.1607933 epoch total loss 6.35272932\n",
      "Trained batch 1342 batch loss 6.24577904 epoch total loss 6.35264969\n",
      "Trained batch 1343 batch loss 6.2193594 epoch total loss 6.35255051\n",
      "Trained batch 1344 batch loss 6.24383163 epoch total loss 6.35247\n",
      "Trained batch 1345 batch loss 6.16228485 epoch total loss 6.3523283\n",
      "Trained batch 1346 batch loss 6.26414394 epoch total loss 6.3522625\n",
      "Trained batch 1347 batch loss 5.36581564 epoch total loss 6.35153055\n",
      "Trained batch 1348 batch loss 5.41955519 epoch total loss 6.35083961\n",
      "Trained batch 1349 batch loss 5.31804657 epoch total loss 6.35007429\n",
      "Trained batch 1350 batch loss 5.67564726 epoch total loss 6.34957457\n",
      "Trained batch 1351 batch loss 6.24238 epoch total loss 6.34949493\n",
      "Trained batch 1352 batch loss 6.36817312 epoch total loss 6.34950876\n",
      "Trained batch 1353 batch loss 6.71364069 epoch total loss 6.34977818\n",
      "Trained batch 1354 batch loss 7.01704073 epoch total loss 6.35027075\n",
      "Trained batch 1355 batch loss 6.68007374 epoch total loss 6.35051394\n",
      "Trained batch 1356 batch loss 6.98811579 epoch total loss 6.3509841\n",
      "Trained batch 1357 batch loss 6.51317358 epoch total loss 6.35110331\n",
      "Trained batch 1358 batch loss 6.78881454 epoch total loss 6.35142565\n",
      "Trained batch 1359 batch loss 6.71947765 epoch total loss 6.35169697\n",
      "Trained batch 1360 batch loss 6.66827059 epoch total loss 6.35192966\n",
      "Trained batch 1361 batch loss 6.86838865 epoch total loss 6.35230875\n",
      "Trained batch 1362 batch loss 6.37651825 epoch total loss 6.35232687\n",
      "Trained batch 1363 batch loss 6.55748034 epoch total loss 6.35247755\n",
      "Trained batch 1364 batch loss 6.38558 epoch total loss 6.35250187\n",
      "Trained batch 1365 batch loss 5.95266199 epoch total loss 6.35220909\n",
      "Trained batch 1366 batch loss 6.11899424 epoch total loss 6.35203886\n",
      "Trained batch 1367 batch loss 6.54283428 epoch total loss 6.3521781\n",
      "Trained batch 1368 batch loss 6.21300507 epoch total loss 6.35207653\n",
      "Trained batch 1369 batch loss 6.60139894 epoch total loss 6.35225868\n",
      "Trained batch 1370 batch loss 6.33766174 epoch total loss 6.35224819\n",
      "Trained batch 1371 batch loss 6.45320415 epoch total loss 6.35232162\n",
      "Trained batch 1372 batch loss 6.65581 epoch total loss 6.35254335\n",
      "Trained batch 1373 batch loss 6.3531146 epoch total loss 6.35254383\n",
      "Trained batch 1374 batch loss 6.32883072 epoch total loss 6.35252714\n",
      "Trained batch 1375 batch loss 5.95557499 epoch total loss 6.35223866\n",
      "Trained batch 1376 batch loss 5.77332592 epoch total loss 6.35181808\n",
      "Trained batch 1377 batch loss 5.5652566 epoch total loss 6.35124683\n",
      "Trained batch 1378 batch loss 5.6020236 epoch total loss 6.35070276\n",
      "Trained batch 1379 batch loss 5.30061436 epoch total loss 6.34994173\n",
      "Trained batch 1380 batch loss 5.21240139 epoch total loss 6.3491168\n",
      "Trained batch 1381 batch loss 5.19060659 epoch total loss 6.34827805\n",
      "Trained batch 1382 batch loss 4.80557108 epoch total loss 6.34716177\n",
      "Trained batch 1383 batch loss 6.30030346 epoch total loss 6.34712791\n",
      "Trained batch 1384 batch loss 6.03394 epoch total loss 6.34690189\n",
      "Trained batch 1385 batch loss 6.45271206 epoch total loss 6.34697866\n",
      "Trained batch 1386 batch loss 6.48491812 epoch total loss 6.34707832\n",
      "Trained batch 1387 batch loss 6.2365346 epoch total loss 6.34699869\n",
      "Trained batch 1388 batch loss 6.60905218 epoch total loss 6.34718752\n",
      "Epoch 4 train loss 6.347187519073486\n",
      "Validated batch 1 batch loss 6.32215738\n",
      "Validated batch 2 batch loss 6.40837097\n",
      "Validated batch 3 batch loss 6.04609919\n",
      "Validated batch 4 batch loss 6.25972319\n",
      "Validated batch 5 batch loss 6.49550104\n",
      "Validated batch 6 batch loss 6.41593456\n",
      "Validated batch 7 batch loss 6.24959564\n",
      "Validated batch 8 batch loss 6.38035917\n",
      "Validated batch 9 batch loss 6.24670601\n",
      "Validated batch 10 batch loss 6.8286891\n",
      "Validated batch 11 batch loss 6.61442566\n",
      "Validated batch 12 batch loss 6.41123247\n",
      "Validated batch 13 batch loss 6.54157162\n",
      "Validated batch 14 batch loss 6.25973129\n",
      "Validated batch 15 batch loss 6.48563766\n",
      "Validated batch 16 batch loss 6.30616093\n",
      "Validated batch 17 batch loss 6.46985531\n",
      "Validated batch 18 batch loss 6.37299919\n",
      "Validated batch 19 batch loss 6.15420341\n",
      "Validated batch 20 batch loss 6.47372961\n",
      "Validated batch 21 batch loss 6.051054\n",
      "Validated batch 22 batch loss 6.63211727\n",
      "Validated batch 23 batch loss 6.65792274\n",
      "Validated batch 24 batch loss 6.86720896\n",
      "Validated batch 25 batch loss 6.50877857\n",
      "Validated batch 26 batch loss 6.46766853\n",
      "Validated batch 27 batch loss 6.31491\n",
      "Validated batch 28 batch loss 6.4155426\n",
      "Validated batch 29 batch loss 6.51935101\n",
      "Validated batch 30 batch loss 6.47072792\n",
      "Validated batch 31 batch loss 5.94343567\n",
      "Validated batch 32 batch loss 6.27458382\n",
      "Validated batch 33 batch loss 6.20752335\n",
      "Validated batch 34 batch loss 6.43338919\n",
      "Validated batch 35 batch loss 6.06536913\n",
      "Validated batch 36 batch loss 6.21011209\n",
      "Validated batch 37 batch loss 6.04890966\n",
      "Validated batch 38 batch loss 6.32686138\n",
      "Validated batch 39 batch loss 6.48530293\n",
      "Validated batch 40 batch loss 6.6401186\n",
      "Validated batch 41 batch loss 6.62391043\n",
      "Validated batch 42 batch loss 7.07747555\n",
      "Validated batch 43 batch loss 7.25169945\n",
      "Validated batch 44 batch loss 6.80121517\n",
      "Validated batch 45 batch loss 6.33228683\n",
      "Validated batch 46 batch loss 5.98060083\n",
      "Validated batch 47 batch loss 5.7654829\n",
      "Validated batch 48 batch loss 6.7490654\n",
      "Validated batch 49 batch loss 6.41183853\n",
      "Validated batch 50 batch loss 6.61160851\n",
      "Validated batch 51 batch loss 6.44500542\n",
      "Validated batch 52 batch loss 6.77714968\n",
      "Validated batch 53 batch loss 6.19076872\n",
      "Validated batch 54 batch loss 6.7934227\n",
      "Validated batch 55 batch loss 6.0774045\n",
      "Validated batch 56 batch loss 6.45797157\n",
      "Validated batch 57 batch loss 6.47050953\n",
      "Validated batch 58 batch loss 5.78186512\n",
      "Validated batch 59 batch loss 5.71702957\n",
      "Validated batch 60 batch loss 6.59820223\n",
      "Validated batch 61 batch loss 6.55683708\n",
      "Validated batch 62 batch loss 6.07594252\n",
      "Validated batch 63 batch loss 6.59206295\n",
      "Validated batch 64 batch loss 6.60263157\n",
      "Validated batch 65 batch loss 6.50592804\n",
      "Validated batch 66 batch loss 6.65673351\n",
      "Validated batch 67 batch loss 6.61425209\n",
      "Validated batch 68 batch loss 6.25359201\n",
      "Validated batch 69 batch loss 5.92832136\n",
      "Validated batch 70 batch loss 6.5280571\n",
      "Validated batch 71 batch loss 6.29080629\n",
      "Validated batch 72 batch loss 6.16216421\n",
      "Validated batch 73 batch loss 5.99398088\n",
      "Validated batch 74 batch loss 6.14031124\n",
      "Validated batch 75 batch loss 6.515\n",
      "Validated batch 76 batch loss 6.17903137\n",
      "Validated batch 77 batch loss 6.39703703\n",
      "Validated batch 78 batch loss 6.5254\n",
      "Validated batch 79 batch loss 6.37478542\n",
      "Validated batch 80 batch loss 6.21623087\n",
      "Validated batch 81 batch loss 5.75759077\n",
      "Validated batch 82 batch loss 6.60902\n",
      "Validated batch 83 batch loss 6.45900059\n",
      "Validated batch 84 batch loss 6.29225683\n",
      "Validated batch 85 batch loss 6.35946751\n",
      "Validated batch 86 batch loss 6.31002903\n",
      "Validated batch 87 batch loss 5.74628\n",
      "Validated batch 88 batch loss 5.924191\n",
      "Validated batch 89 batch loss 6.35434055\n",
      "Validated batch 90 batch loss 5.98604298\n",
      "Validated batch 91 batch loss 6.21363\n",
      "Validated batch 92 batch loss 6.28595448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 93 batch loss 6.57529926\n",
      "Validated batch 94 batch loss 6.98606396\n",
      "Validated batch 95 batch loss 6.23311\n",
      "Validated batch 96 batch loss 5.79490185\n",
      "Validated batch 97 batch loss 6.48048258\n",
      "Validated batch 98 batch loss 6.16297436\n",
      "Validated batch 99 batch loss 5.81057644\n",
      "Validated batch 100 batch loss 6.25115633\n",
      "Validated batch 101 batch loss 5.7881093\n",
      "Validated batch 102 batch loss 6.66740322\n",
      "Validated batch 103 batch loss 6.13403\n",
      "Validated batch 104 batch loss 6.01745462\n",
      "Validated batch 105 batch loss 5.95880604\n",
      "Validated batch 106 batch loss 6.47461605\n",
      "Validated batch 107 batch loss 6.07669687\n",
      "Validated batch 108 batch loss 6.38575315\n",
      "Validated batch 109 batch loss 6.18662\n",
      "Validated batch 110 batch loss 6.61682796\n",
      "Validated batch 111 batch loss 6.21483374\n",
      "Validated batch 112 batch loss 6.40419483\n",
      "Validated batch 113 batch loss 6.56262255\n",
      "Validated batch 114 batch loss 5.38194704\n",
      "Validated batch 115 batch loss 6.57706\n",
      "Validated batch 116 batch loss 6.30581665\n",
      "Validated batch 117 batch loss 5.89554882\n",
      "Validated batch 118 batch loss 6.40653\n",
      "Validated batch 119 batch loss 6.37748814\n",
      "Validated batch 120 batch loss 6.02727\n",
      "Validated batch 121 batch loss 6.76168823\n",
      "Validated batch 122 batch loss 6.46903324\n",
      "Validated batch 123 batch loss 6.29220247\n",
      "Validated batch 124 batch loss 6.54375076\n",
      "Validated batch 125 batch loss 6.55348825\n",
      "Validated batch 126 batch loss 6.31128263\n",
      "Validated batch 127 batch loss 6.43749428\n",
      "Validated batch 128 batch loss 6.09741402\n",
      "Validated batch 129 batch loss 6.73592138\n",
      "Validated batch 130 batch loss 6.75555468\n",
      "Validated batch 131 batch loss 6.40729809\n",
      "Validated batch 132 batch loss 6.24761152\n",
      "Validated batch 133 batch loss 5.96672821\n",
      "Validated batch 134 batch loss 6.34566641\n",
      "Validated batch 135 batch loss 6.42050648\n",
      "Validated batch 136 batch loss 6.46356392\n",
      "Validated batch 137 batch loss 6.54146814\n",
      "Validated batch 138 batch loss 6.11296844\n",
      "Validated batch 139 batch loss 6.34018183\n",
      "Validated batch 140 batch loss 6.41975117\n",
      "Validated batch 141 batch loss 6.23473692\n",
      "Validated batch 142 batch loss 5.57280254\n",
      "Validated batch 143 batch loss 6.19087791\n",
      "Validated batch 144 batch loss 6.72695446\n",
      "Validated batch 145 batch loss 5.9816494\n",
      "Validated batch 146 batch loss 6.36298609\n",
      "Validated batch 147 batch loss 6.47127247\n",
      "Validated batch 148 batch loss 6.56141\n",
      "Validated batch 149 batch loss 6.62771463\n",
      "Validated batch 150 batch loss 6.50059795\n",
      "Validated batch 151 batch loss 6.00539446\n",
      "Validated batch 152 batch loss 6.4846034\n",
      "Validated batch 153 batch loss 6.59169292\n",
      "Validated batch 154 batch loss 6.81976366\n",
      "Validated batch 155 batch loss 6.31040525\n",
      "Validated batch 156 batch loss 6.66425228\n",
      "Validated batch 157 batch loss 6.52150679\n",
      "Validated batch 158 batch loss 6.1008215\n",
      "Validated batch 159 batch loss 6.33619595\n",
      "Validated batch 160 batch loss 6.37636852\n",
      "Validated batch 161 batch loss 6.73197794\n",
      "Validated batch 162 batch loss 6.35868216\n",
      "Validated batch 163 batch loss 6.3822403\n",
      "Validated batch 164 batch loss 6.40289\n",
      "Validated batch 165 batch loss 6.44461346\n",
      "Validated batch 166 batch loss 6.83314705\n",
      "Validated batch 167 batch loss 6.50523615\n",
      "Validated batch 168 batch loss 6.58033228\n",
      "Validated batch 169 batch loss 6.45947742\n",
      "Validated batch 170 batch loss 6.61154938\n",
      "Validated batch 171 batch loss 6.51027393\n",
      "Validated batch 172 batch loss 6.8048048\n",
      "Validated batch 173 batch loss 6.9521389\n",
      "Validated batch 174 batch loss 5.97533512\n",
      "Validated batch 175 batch loss 6.65744591\n",
      "Validated batch 176 batch loss 6.52295971\n",
      "Validated batch 177 batch loss 6.35365629\n",
      "Validated batch 178 batch loss 6.47761631\n",
      "Validated batch 179 batch loss 5.97926331\n",
      "Validated batch 180 batch loss 5.9738121\n",
      "Validated batch 181 batch loss 6.56933403\n",
      "Validated batch 182 batch loss 6.35143137\n",
      "Validated batch 183 batch loss 6.85748291\n",
      "Validated batch 184 batch loss 6.28585815\n",
      "Validated batch 185 batch loss 3.27043915\n",
      "Epoch 4 val loss 6.35068416595459\n",
      "Start epoch 5 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 4.87011099 epoch total loss 4.87011099\n",
      "Trained batch 2 batch loss 4.60885811 epoch total loss 4.73948479\n",
      "Trained batch 3 batch loss 5.92867 epoch total loss 5.13588\n",
      "Trained batch 4 batch loss 6.30949402 epoch total loss 5.42928314\n",
      "Trained batch 5 batch loss 7.21648741 epoch total loss 5.78672409\n",
      "Trained batch 6 batch loss 6.87800217 epoch total loss 5.96860361\n",
      "Trained batch 7 batch loss 6.38941574 epoch total loss 6.02872\n",
      "Trained batch 8 batch loss 6.04576397 epoch total loss 6.03085041\n",
      "Trained batch 9 batch loss 6.05878544 epoch total loss 6.03395414\n",
      "Trained batch 10 batch loss 6.47944355 epoch total loss 6.07850313\n",
      "Trained batch 11 batch loss 6.48294401 epoch total loss 6.11527061\n",
      "Trained batch 12 batch loss 6.4433074 epoch total loss 6.14260674\n",
      "Trained batch 13 batch loss 6.5095644 epoch total loss 6.17083454\n",
      "Trained batch 14 batch loss 6.50365973 epoch total loss 6.19460773\n",
      "Trained batch 15 batch loss 6.4357357 epoch total loss 6.21068335\n",
      "Trained batch 16 batch loss 6.47382069 epoch total loss 6.22712946\n",
      "Trained batch 17 batch loss 6.42850304 epoch total loss 6.23897505\n",
      "Trained batch 18 batch loss 6.42083 epoch total loss 6.24907827\n",
      "Trained batch 19 batch loss 6.43220043 epoch total loss 6.25871611\n",
      "Trained batch 20 batch loss 6.75999737 epoch total loss 6.28378\n",
      "Trained batch 21 batch loss 6.01607609 epoch total loss 6.27103233\n",
      "Trained batch 22 batch loss 5.68970871 epoch total loss 6.24460888\n",
      "Trained batch 23 batch loss 6.16803932 epoch total loss 6.24128\n",
      "Trained batch 24 batch loss 6.53748608 epoch total loss 6.25362206\n",
      "Trained batch 25 batch loss 6.34966516 epoch total loss 6.25746393\n",
      "Trained batch 26 batch loss 6.48436832 epoch total loss 6.26619148\n",
      "Trained batch 27 batch loss 6.66880941 epoch total loss 6.28110313\n",
      "Trained batch 28 batch loss 6.5816884 epoch total loss 6.29183865\n",
      "Trained batch 29 batch loss 6.2008028 epoch total loss 6.28869963\n",
      "Trained batch 30 batch loss 6.56698513 epoch total loss 6.29797554\n",
      "Trained batch 31 batch loss 6.17801666 epoch total loss 6.29410601\n",
      "Trained batch 32 batch loss 6.31740713 epoch total loss 6.29483414\n",
      "Trained batch 33 batch loss 6.30253077 epoch total loss 6.29506731\n",
      "Trained batch 34 batch loss 6.19833326 epoch total loss 6.2922225\n",
      "Trained batch 35 batch loss 6.03785515 epoch total loss 6.28495502\n",
      "Trained batch 36 batch loss 6.32713413 epoch total loss 6.28612661\n",
      "Trained batch 37 batch loss 6.60176134 epoch total loss 6.29465723\n",
      "Trained batch 38 batch loss 6.39793491 epoch total loss 6.29737473\n",
      "Trained batch 39 batch loss 6.26256418 epoch total loss 6.29648209\n",
      "Trained batch 40 batch loss 6.06026745 epoch total loss 6.29057693\n",
      "Trained batch 41 batch loss 6.44383764 epoch total loss 6.29431534\n",
      "Trained batch 42 batch loss 6.37225342 epoch total loss 6.29617071\n",
      "Trained batch 43 batch loss 6.27809143 epoch total loss 6.29575062\n",
      "Trained batch 44 batch loss 6.90150166 epoch total loss 6.30951738\n",
      "Trained batch 45 batch loss 6.74110365 epoch total loss 6.31910801\n",
      "Trained batch 46 batch loss 6.43218422 epoch total loss 6.32156658\n",
      "Trained batch 47 batch loss 6.52865601 epoch total loss 6.32597256\n",
      "Trained batch 48 batch loss 6.45303535 epoch total loss 6.32861948\n",
      "Trained batch 49 batch loss 6.48106194 epoch total loss 6.33173037\n",
      "Trained batch 50 batch loss 6.41285419 epoch total loss 6.33335257\n",
      "Trained batch 51 batch loss 6.22788477 epoch total loss 6.33128452\n",
      "Trained batch 52 batch loss 6.66150045 epoch total loss 6.33763456\n",
      "Trained batch 53 batch loss 6.42687178 epoch total loss 6.33931875\n",
      "Trained batch 54 batch loss 6.2454772 epoch total loss 6.33758116\n",
      "Trained batch 55 batch loss 6.21367216 epoch total loss 6.3353281\n",
      "Trained batch 56 batch loss 6.56620359 epoch total loss 6.33945084\n",
      "Trained batch 57 batch loss 6.24757242 epoch total loss 6.33783865\n",
      "Trained batch 58 batch loss 6.56021881 epoch total loss 6.3416729\n",
      "Trained batch 59 batch loss 6.35966873 epoch total loss 6.34197807\n",
      "Trained batch 60 batch loss 6.31063557 epoch total loss 6.34145546\n",
      "Trained batch 61 batch loss 6.4059267 epoch total loss 6.34251213\n",
      "Trained batch 62 batch loss 5.98379183 epoch total loss 6.33672667\n",
      "Trained batch 63 batch loss 6.00132036 epoch total loss 6.3314023\n",
      "Trained batch 64 batch loss 6.16216135 epoch total loss 6.32875824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 65 batch loss 5.71183586 epoch total loss 6.3192668\n",
      "Trained batch 66 batch loss 5.40082788 epoch total loss 6.30535126\n",
      "Trained batch 67 batch loss 6.20716667 epoch total loss 6.30388546\n",
      "Trained batch 68 batch loss 6.5210948 epoch total loss 6.30707932\n",
      "Trained batch 69 batch loss 6.40571165 epoch total loss 6.30850887\n",
      "Trained batch 70 batch loss 6.4916172 epoch total loss 6.31112432\n",
      "Trained batch 71 batch loss 6.10787725 epoch total loss 6.30826187\n",
      "Trained batch 72 batch loss 5.88805914 epoch total loss 6.30242586\n",
      "Trained batch 73 batch loss 6.09002638 epoch total loss 6.2995162\n",
      "Trained batch 74 batch loss 6.07656288 epoch total loss 6.29650354\n",
      "Trained batch 75 batch loss 6.08614492 epoch total loss 6.29369879\n",
      "Trained batch 76 batch loss 6.20845175 epoch total loss 6.29257727\n",
      "Trained batch 77 batch loss 5.85141802 epoch total loss 6.28684759\n",
      "Trained batch 78 batch loss 5.94040298 epoch total loss 6.28240633\n",
      "Trained batch 79 batch loss 5.34608459 epoch total loss 6.27055407\n",
      "Trained batch 80 batch loss 6.26521492 epoch total loss 6.27048779\n",
      "Trained batch 81 batch loss 6.45735168 epoch total loss 6.27279472\n",
      "Trained batch 82 batch loss 6.01071262 epoch total loss 6.26959896\n",
      "Trained batch 83 batch loss 6.3263855 epoch total loss 6.27028275\n",
      "Trained batch 84 batch loss 6.53456 epoch total loss 6.27342892\n",
      "Trained batch 85 batch loss 6.56932449 epoch total loss 6.27691\n",
      "Trained batch 86 batch loss 6.24614477 epoch total loss 6.2765522\n",
      "Trained batch 87 batch loss 6.39406919 epoch total loss 6.27790308\n",
      "Trained batch 88 batch loss 6.40907669 epoch total loss 6.2793932\n",
      "Trained batch 89 batch loss 6.16745234 epoch total loss 6.27813578\n",
      "Trained batch 90 batch loss 6.09082413 epoch total loss 6.27605438\n",
      "Trained batch 91 batch loss 6.58029699 epoch total loss 6.27939796\n",
      "Trained batch 92 batch loss 6.6306653 epoch total loss 6.28321648\n",
      "Trained batch 93 batch loss 6.57611132 epoch total loss 6.28636599\n",
      "Trained batch 94 batch loss 6.17795897 epoch total loss 6.28521252\n",
      "Trained batch 95 batch loss 6.10094833 epoch total loss 6.28327322\n",
      "Trained batch 96 batch loss 5.87694216 epoch total loss 6.27904081\n",
      "Trained batch 97 batch loss 6.39905071 epoch total loss 6.28027773\n",
      "Trained batch 98 batch loss 6.55624294 epoch total loss 6.28309345\n",
      "Trained batch 99 batch loss 6.48295116 epoch total loss 6.28511238\n",
      "Trained batch 100 batch loss 6.87471533 epoch total loss 6.29100847\n",
      "Trained batch 101 batch loss 6.98173237 epoch total loss 6.29784727\n",
      "Trained batch 102 batch loss 6.9113555 epoch total loss 6.30386209\n",
      "Trained batch 103 batch loss 6.70353 epoch total loss 6.3077426\n",
      "Trained batch 104 batch loss 6.39870834 epoch total loss 6.30861712\n",
      "Trained batch 105 batch loss 5.90500832 epoch total loss 6.30477333\n",
      "Trained batch 106 batch loss 5.22293806 epoch total loss 6.29456758\n",
      "Trained batch 107 batch loss 5.24921751 epoch total loss 6.28479815\n",
      "Trained batch 108 batch loss 5.4027319 epoch total loss 6.2766304\n",
      "Trained batch 109 batch loss 5.86520624 epoch total loss 6.27285624\n",
      "Trained batch 110 batch loss 5.7568121 epoch total loss 6.26816511\n",
      "Trained batch 111 batch loss 6.29297 epoch total loss 6.26838875\n",
      "Trained batch 112 batch loss 6.47600126 epoch total loss 6.27024221\n",
      "Trained batch 113 batch loss 6.43996191 epoch total loss 6.27174425\n",
      "Trained batch 114 batch loss 6.59828234 epoch total loss 6.27460861\n",
      "Trained batch 115 batch loss 6.44533205 epoch total loss 6.27609301\n",
      "Trained batch 116 batch loss 5.90844488 epoch total loss 6.27292347\n",
      "Trained batch 117 batch loss 6.24011946 epoch total loss 6.27264309\n",
      "Trained batch 118 batch loss 6.08472586 epoch total loss 6.27105045\n",
      "Trained batch 119 batch loss 6.32373333 epoch total loss 6.27149296\n",
      "Trained batch 120 batch loss 6.41817379 epoch total loss 6.27271509\n",
      "Trained batch 121 batch loss 6.41531944 epoch total loss 6.27389383\n",
      "Trained batch 122 batch loss 6.32054377 epoch total loss 6.27427626\n",
      "Trained batch 123 batch loss 6.83934164 epoch total loss 6.27887058\n",
      "Trained batch 124 batch loss 6.78799343 epoch total loss 6.28297615\n",
      "Trained batch 125 batch loss 7.08450413 epoch total loss 6.28938866\n",
      "Trained batch 126 batch loss 6.4616394 epoch total loss 6.29075575\n",
      "Trained batch 127 batch loss 6.50282049 epoch total loss 6.29242563\n",
      "Trained batch 128 batch loss 6.49856 epoch total loss 6.29403591\n",
      "Trained batch 129 batch loss 6.4441371 epoch total loss 6.29519939\n",
      "Trained batch 130 batch loss 6.48793125 epoch total loss 6.29668188\n",
      "Trained batch 131 batch loss 6.55135584 epoch total loss 6.29862595\n",
      "Trained batch 132 batch loss 6.35249853 epoch total loss 6.29903412\n",
      "Trained batch 133 batch loss 6.31596 epoch total loss 6.29916143\n",
      "Trained batch 134 batch loss 6.33738565 epoch total loss 6.29944658\n",
      "Trained batch 135 batch loss 5.65317965 epoch total loss 6.29465961\n",
      "Trained batch 136 batch loss 6.19935608 epoch total loss 6.29395866\n",
      "Trained batch 137 batch loss 6.55506372 epoch total loss 6.29586458\n",
      "Trained batch 138 batch loss 6.54629421 epoch total loss 6.29767895\n",
      "Trained batch 139 batch loss 6.4135766 epoch total loss 6.29851294\n",
      "Trained batch 140 batch loss 6.3004 epoch total loss 6.29852629\n",
      "Trained batch 141 batch loss 6.17742634 epoch total loss 6.2976675\n",
      "Trained batch 142 batch loss 6.28217793 epoch total loss 6.29755831\n",
      "Trained batch 143 batch loss 6.10964 epoch total loss 6.29624414\n",
      "Trained batch 144 batch loss 6.1446619 epoch total loss 6.29519129\n",
      "Trained batch 145 batch loss 6.13491964 epoch total loss 6.29408646\n",
      "Trained batch 146 batch loss 6.34955025 epoch total loss 6.29446602\n",
      "Trained batch 147 batch loss 6.83285856 epoch total loss 6.29812908\n",
      "Trained batch 148 batch loss 7.07401896 epoch total loss 6.30337143\n",
      "Trained batch 149 batch loss 7.0891242 epoch total loss 6.30864477\n",
      "Trained batch 150 batch loss 6.87034893 epoch total loss 6.31239\n",
      "Trained batch 151 batch loss 6.33037949 epoch total loss 6.31250906\n",
      "Trained batch 152 batch loss 6.60447168 epoch total loss 6.31442976\n",
      "Trained batch 153 batch loss 6.77738667 epoch total loss 6.31745577\n",
      "Trained batch 154 batch loss 6.81819725 epoch total loss 6.32070732\n",
      "Trained batch 155 batch loss 6.66155338 epoch total loss 6.32290649\n",
      "Trained batch 156 batch loss 6.37398863 epoch total loss 6.3232336\n",
      "Trained batch 157 batch loss 6.28512907 epoch total loss 6.32299089\n",
      "Trained batch 158 batch loss 6.14307833 epoch total loss 6.32185221\n",
      "Trained batch 159 batch loss 6.55162525 epoch total loss 6.3232975\n",
      "Trained batch 160 batch loss 6.81469 epoch total loss 6.32636881\n",
      "Trained batch 161 batch loss 6.84261703 epoch total loss 6.32957506\n",
      "Trained batch 162 batch loss 6.74301577 epoch total loss 6.33212709\n",
      "Trained batch 163 batch loss 6.86333132 epoch total loss 6.3353858\n",
      "Trained batch 164 batch loss 6.65607405 epoch total loss 6.33734131\n",
      "Trained batch 165 batch loss 6.512568 epoch total loss 6.33840322\n",
      "Trained batch 166 batch loss 6.11286402 epoch total loss 6.33704519\n",
      "Trained batch 167 batch loss 6.48028469 epoch total loss 6.33790255\n",
      "Trained batch 168 batch loss 6.71284676 epoch total loss 6.34013462\n",
      "Trained batch 169 batch loss 6.34880495 epoch total loss 6.34018517\n",
      "Trained batch 170 batch loss 6.70807076 epoch total loss 6.34235\n",
      "Trained batch 171 batch loss 6.57731056 epoch total loss 6.34372377\n",
      "Trained batch 172 batch loss 6.46305132 epoch total loss 6.3444171\n",
      "Trained batch 173 batch loss 6.38553286 epoch total loss 6.34465456\n",
      "Trained batch 174 batch loss 6.51637459 epoch total loss 6.34564161\n",
      "Trained batch 175 batch loss 6.53078032 epoch total loss 6.34669924\n",
      "Trained batch 176 batch loss 6.39543104 epoch total loss 6.3469758\n",
      "Trained batch 177 batch loss 5.99830818 epoch total loss 6.34500599\n",
      "Trained batch 178 batch loss 6.50227785 epoch total loss 6.34588957\n",
      "Trained batch 179 batch loss 5.92969036 epoch total loss 6.34356451\n",
      "Trained batch 180 batch loss 6.23376226 epoch total loss 6.34295464\n",
      "Trained batch 181 batch loss 6.28625584 epoch total loss 6.34264135\n",
      "Trained batch 182 batch loss 6.1236968 epoch total loss 6.34143782\n",
      "Trained batch 183 batch loss 6.18035603 epoch total loss 6.34055758\n",
      "Trained batch 184 batch loss 6.24409723 epoch total loss 6.34003353\n",
      "Trained batch 185 batch loss 6.57068443 epoch total loss 6.34128\n",
      "Trained batch 186 batch loss 6.47188139 epoch total loss 6.34198284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 187 batch loss 6.46351147 epoch total loss 6.34263229\n",
      "Trained batch 188 batch loss 6.35297394 epoch total loss 6.34268761\n",
      "Trained batch 189 batch loss 6.30329227 epoch total loss 6.34247971\n",
      "Trained batch 190 batch loss 6.37663746 epoch total loss 6.342659\n",
      "Trained batch 191 batch loss 5.94282055 epoch total loss 6.34056616\n",
      "Trained batch 192 batch loss 6.76622725 epoch total loss 6.34278297\n",
      "Trained batch 193 batch loss 6.6055851 epoch total loss 6.34414482\n",
      "Trained batch 194 batch loss 6.9900732 epoch total loss 6.34747458\n",
      "Trained batch 195 batch loss 6.50871849 epoch total loss 6.34830093\n",
      "Trained batch 196 batch loss 6.59426403 epoch total loss 6.34955597\n",
      "Trained batch 197 batch loss 6.08269024 epoch total loss 6.3482008\n",
      "Trained batch 198 batch loss 6.38750315 epoch total loss 6.34839916\n",
      "Trained batch 199 batch loss 6.45785284 epoch total loss 6.34894943\n",
      "Trained batch 200 batch loss 6.27043152 epoch total loss 6.34855652\n",
      "Trained batch 201 batch loss 6.31657028 epoch total loss 6.34839725\n",
      "Trained batch 202 batch loss 6.48373604 epoch total loss 6.34906721\n",
      "Trained batch 203 batch loss 6.13057089 epoch total loss 6.34799099\n",
      "Trained batch 204 batch loss 6.29095125 epoch total loss 6.34771109\n",
      "Trained batch 205 batch loss 6.16070652 epoch total loss 6.34679937\n",
      "Trained batch 206 batch loss 6.47539425 epoch total loss 6.34742355\n",
      "Trained batch 207 batch loss 6.48822594 epoch total loss 6.348104\n",
      "Trained batch 208 batch loss 6.63487768 epoch total loss 6.34948254\n",
      "Trained batch 209 batch loss 6.58937168 epoch total loss 6.35063028\n",
      "Trained batch 210 batch loss 6.26305103 epoch total loss 6.35021353\n",
      "Trained batch 211 batch loss 6.52398491 epoch total loss 6.35103655\n",
      "Trained batch 212 batch loss 6.091537 epoch total loss 6.34981251\n",
      "Trained batch 213 batch loss 6.39087915 epoch total loss 6.35000515\n",
      "Trained batch 214 batch loss 6.07463169 epoch total loss 6.34871817\n",
      "Trained batch 215 batch loss 6.1167264 epoch total loss 6.34763908\n",
      "Trained batch 216 batch loss 6.24807167 epoch total loss 6.34717798\n",
      "Trained batch 217 batch loss 6.50659752 epoch total loss 6.34791279\n",
      "Trained batch 218 batch loss 6.35568857 epoch total loss 6.34794855\n",
      "Trained batch 219 batch loss 6.27910757 epoch total loss 6.34763384\n",
      "Trained batch 220 batch loss 5.84670544 epoch total loss 6.34535694\n",
      "Trained batch 221 batch loss 5.5144968 epoch total loss 6.34159756\n",
      "Trained batch 222 batch loss 5.78396177 epoch total loss 6.33908558\n",
      "Trained batch 223 batch loss 5.87505817 epoch total loss 6.33700418\n",
      "Trained batch 224 batch loss 6.21973133 epoch total loss 6.33648062\n",
      "Trained batch 225 batch loss 6.73149 epoch total loss 6.33823633\n",
      "Trained batch 226 batch loss 6.45266867 epoch total loss 6.33874226\n",
      "Trained batch 227 batch loss 6.35193586 epoch total loss 6.33880043\n",
      "Trained batch 228 batch loss 6.52748299 epoch total loss 6.33962822\n",
      "Trained batch 229 batch loss 6.30326748 epoch total loss 6.33946896\n",
      "Trained batch 230 batch loss 6.63200474 epoch total loss 6.34074068\n",
      "Trained batch 231 batch loss 6.22877121 epoch total loss 6.34025574\n",
      "Trained batch 232 batch loss 6.41132 epoch total loss 6.34056234\n",
      "Trained batch 233 batch loss 5.87008 epoch total loss 6.33854342\n",
      "Trained batch 234 batch loss 6.39849234 epoch total loss 6.33879948\n",
      "Trained batch 235 batch loss 5.89506865 epoch total loss 6.33691072\n",
      "Trained batch 236 batch loss 6.20445 epoch total loss 6.33635\n",
      "Trained batch 237 batch loss 6.08263445 epoch total loss 6.33527946\n",
      "Trained batch 238 batch loss 6.1259 epoch total loss 6.33439922\n",
      "Trained batch 239 batch loss 5.94008 epoch total loss 6.33274937\n",
      "Trained batch 240 batch loss 6.16543722 epoch total loss 6.33205223\n",
      "Trained batch 241 batch loss 6.40326405 epoch total loss 6.33234787\n",
      "Trained batch 242 batch loss 6.41337061 epoch total loss 6.33268261\n",
      "Trained batch 243 batch loss 5.95967054 epoch total loss 6.33114767\n",
      "Trained batch 244 batch loss 5.69552946 epoch total loss 6.32854271\n",
      "Trained batch 245 batch loss 5.92089367 epoch total loss 6.32687902\n",
      "Trained batch 246 batch loss 6.2051158 epoch total loss 6.32638359\n",
      "Trained batch 247 batch loss 5.98448086 epoch total loss 6.325\n",
      "Trained batch 248 batch loss 6.01102209 epoch total loss 6.32373333\n",
      "Trained batch 249 batch loss 5.64890051 epoch total loss 6.32102346\n",
      "Trained batch 250 batch loss 5.71172142 epoch total loss 6.31858587\n",
      "Trained batch 251 batch loss 5.74884 epoch total loss 6.31631565\n",
      "Trained batch 252 batch loss 6.05266953 epoch total loss 6.31526947\n",
      "Trained batch 253 batch loss 6.18940639 epoch total loss 6.31477213\n",
      "Trained batch 254 batch loss 6.23744 epoch total loss 6.31446743\n",
      "Trained batch 255 batch loss 6.21512842 epoch total loss 6.31407785\n",
      "Trained batch 256 batch loss 6.50261164 epoch total loss 6.31481409\n",
      "Trained batch 257 batch loss 6.40631151 epoch total loss 6.31517029\n",
      "Trained batch 258 batch loss 6.73261881 epoch total loss 6.31678867\n",
      "Trained batch 259 batch loss 6.61399746 epoch total loss 6.31793594\n",
      "Trained batch 260 batch loss 6.56568718 epoch total loss 6.31888914\n",
      "Trained batch 261 batch loss 6.08282852 epoch total loss 6.31798458\n",
      "Trained batch 262 batch loss 6.41003227 epoch total loss 6.31833601\n",
      "Trained batch 263 batch loss 6.96192741 epoch total loss 6.32078314\n",
      "Trained batch 264 batch loss 6.61525249 epoch total loss 6.32189846\n",
      "Trained batch 265 batch loss 6.73246527 epoch total loss 6.3234477\n",
      "Trained batch 266 batch loss 6.79618 epoch total loss 6.32522488\n",
      "Trained batch 267 batch loss 6.66154528 epoch total loss 6.3264842\n",
      "Trained batch 268 batch loss 6.57071304 epoch total loss 6.32739544\n",
      "Trained batch 269 batch loss 6.46852875 epoch total loss 6.32792\n",
      "Trained batch 270 batch loss 6.87449837 epoch total loss 6.32994413\n",
      "Trained batch 271 batch loss 6.80638123 epoch total loss 6.33170223\n",
      "Trained batch 272 batch loss 6.55802679 epoch total loss 6.33253431\n",
      "Trained batch 273 batch loss 6.46014833 epoch total loss 6.33300209\n",
      "Trained batch 274 batch loss 6.68926954 epoch total loss 6.33430195\n",
      "Trained batch 275 batch loss 6.59446287 epoch total loss 6.33524799\n",
      "Trained batch 276 batch loss 6.57939291 epoch total loss 6.33613253\n",
      "Trained batch 277 batch loss 6.26498795 epoch total loss 6.33587599\n",
      "Trained batch 278 batch loss 6.55903435 epoch total loss 6.3366785\n",
      "Trained batch 279 batch loss 6.41108608 epoch total loss 6.33694553\n",
      "Trained batch 280 batch loss 5.76104403 epoch total loss 6.33488846\n",
      "Trained batch 281 batch loss 5.71149874 epoch total loss 6.33267\n",
      "Trained batch 282 batch loss 5.74215889 epoch total loss 6.33057642\n",
      "Trained batch 283 batch loss 6.03066492 epoch total loss 6.32951641\n",
      "Trained batch 284 batch loss 5.76637411 epoch total loss 6.32753372\n",
      "Trained batch 285 batch loss 5.22710943 epoch total loss 6.32367229\n",
      "Trained batch 286 batch loss 5.06341791 epoch total loss 6.31926584\n",
      "Trained batch 287 batch loss 5.46226311 epoch total loss 6.31628\n",
      "Trained batch 288 batch loss 6.12256479 epoch total loss 6.31560707\n",
      "Trained batch 289 batch loss 6.13437748 epoch total loss 6.31498\n",
      "Trained batch 290 batch loss 6.95436049 epoch total loss 6.31718493\n",
      "Trained batch 291 batch loss 6.74997854 epoch total loss 6.31867218\n",
      "Trained batch 292 batch loss 6.81297445 epoch total loss 6.32036495\n",
      "Trained batch 293 batch loss 6.54056025 epoch total loss 6.32111645\n",
      "Trained batch 294 batch loss 6.79359531 epoch total loss 6.32272339\n",
      "Trained batch 295 batch loss 6.91006 epoch total loss 6.32471466\n",
      "Trained batch 296 batch loss 6.71242762 epoch total loss 6.32602406\n",
      "Trained batch 297 batch loss 6.5036459 epoch total loss 6.32662249\n",
      "Trained batch 298 batch loss 6.6877203 epoch total loss 6.32783413\n",
      "Trained batch 299 batch loss 6.74418497 epoch total loss 6.32922649\n",
      "Trained batch 300 batch loss 6.37306118 epoch total loss 6.32937241\n",
      "Trained batch 301 batch loss 6.4566083 epoch total loss 6.32979536\n",
      "Trained batch 302 batch loss 6.17276478 epoch total loss 6.32927561\n",
      "Trained batch 303 batch loss 5.89994335 epoch total loss 6.32785845\n",
      "Trained batch 304 batch loss 5.93249273 epoch total loss 6.32655764\n",
      "Trained batch 305 batch loss 6.25840473 epoch total loss 6.32633448\n",
      "Trained batch 306 batch loss 6.25049 epoch total loss 6.32608652\n",
      "Trained batch 307 batch loss 6.36549854 epoch total loss 6.32621479\n",
      "Trained batch 308 batch loss 6.56803 epoch total loss 6.32699966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 309 batch loss 6.61542034 epoch total loss 6.32793331\n",
      "Trained batch 310 batch loss 6.26536751 epoch total loss 6.32773161\n",
      "Trained batch 311 batch loss 6.38291597 epoch total loss 6.32790899\n",
      "Trained batch 312 batch loss 6.56101513 epoch total loss 6.3286562\n",
      "Trained batch 313 batch loss 6.33280945 epoch total loss 6.32866955\n",
      "Trained batch 314 batch loss 6.53329182 epoch total loss 6.32932138\n",
      "Trained batch 315 batch loss 6.10847139 epoch total loss 6.32862043\n",
      "Trained batch 316 batch loss 5.64330959 epoch total loss 6.32645178\n",
      "Trained batch 317 batch loss 5.35012817 epoch total loss 6.32337141\n",
      "Trained batch 318 batch loss 6.15507793 epoch total loss 6.32284212\n",
      "Trained batch 319 batch loss 6.28318167 epoch total loss 6.32271814\n",
      "Trained batch 320 batch loss 6.64600658 epoch total loss 6.32372808\n",
      "Trained batch 321 batch loss 6.41916 epoch total loss 6.32402563\n",
      "Trained batch 322 batch loss 6.27685452 epoch total loss 6.32387924\n",
      "Trained batch 323 batch loss 5.88905621 epoch total loss 6.32253265\n",
      "Trained batch 324 batch loss 6.14334965 epoch total loss 6.32198\n",
      "Trained batch 325 batch loss 6.25116491 epoch total loss 6.32176208\n",
      "Trained batch 326 batch loss 6.10136795 epoch total loss 6.32108593\n",
      "Trained batch 327 batch loss 6.51660681 epoch total loss 6.32168388\n",
      "Trained batch 328 batch loss 6.5139637 epoch total loss 6.32227\n",
      "Trained batch 329 batch loss 6.55162191 epoch total loss 6.32296705\n",
      "Trained batch 330 batch loss 5.71534538 epoch total loss 6.32112551\n",
      "Trained batch 331 batch loss 5.43614483 epoch total loss 6.3184514\n",
      "Trained batch 332 batch loss 5.970438 epoch total loss 6.31740332\n",
      "Trained batch 333 batch loss 6.42323732 epoch total loss 6.31772137\n",
      "Trained batch 334 batch loss 6.54423761 epoch total loss 6.31839943\n",
      "Trained batch 335 batch loss 6.66980553 epoch total loss 6.31944895\n",
      "Trained batch 336 batch loss 6.76903152 epoch total loss 6.32078695\n",
      "Trained batch 337 batch loss 6.80328274 epoch total loss 6.32221842\n",
      "Trained batch 338 batch loss 6.52728653 epoch total loss 6.32282543\n",
      "Trained batch 339 batch loss 6.62651825 epoch total loss 6.32372093\n",
      "Trained batch 340 batch loss 6.53052664 epoch total loss 6.32432938\n",
      "Trained batch 341 batch loss 6.60288 epoch total loss 6.32514572\n",
      "Trained batch 342 batch loss 6.65321302 epoch total loss 6.32610559\n",
      "Trained batch 343 batch loss 6.05298376 epoch total loss 6.32530928\n",
      "Trained batch 344 batch loss 6.60382318 epoch total loss 6.32611847\n",
      "Trained batch 345 batch loss 6.40574932 epoch total loss 6.32634926\n",
      "Trained batch 346 batch loss 6.24091482 epoch total loss 6.32610273\n",
      "Trained batch 347 batch loss 6.35941267 epoch total loss 6.32619858\n",
      "Trained batch 348 batch loss 6.46363592 epoch total loss 6.3265934\n",
      "Trained batch 349 batch loss 6.45456457 epoch total loss 6.32696\n",
      "Trained batch 350 batch loss 6.40712 epoch total loss 6.32718945\n",
      "Trained batch 351 batch loss 6.44369411 epoch total loss 6.32752132\n",
      "Trained batch 352 batch loss 6.53358698 epoch total loss 6.32810688\n",
      "Trained batch 353 batch loss 6.3664875 epoch total loss 6.3282156\n",
      "Trained batch 354 batch loss 6.35806179 epoch total loss 6.3283\n",
      "Trained batch 355 batch loss 6.48518 epoch total loss 6.32874203\n",
      "Trained batch 356 batch loss 6.33734035 epoch total loss 6.32876635\n",
      "Trained batch 357 batch loss 6.6387372 epoch total loss 6.32963419\n",
      "Trained batch 358 batch loss 6.77135324 epoch total loss 6.33086777\n",
      "Trained batch 359 batch loss 7.02277565 epoch total loss 6.33279514\n",
      "Trained batch 360 batch loss 6.79754877 epoch total loss 6.33408594\n",
      "Trained batch 361 batch loss 6.76603699 epoch total loss 6.3352828\n",
      "Trained batch 362 batch loss 6.82287693 epoch total loss 6.33663034\n",
      "Trained batch 363 batch loss 6.75633717 epoch total loss 6.3377862\n",
      "Trained batch 364 batch loss 6.5389123 epoch total loss 6.33833885\n",
      "Trained batch 365 batch loss 6.43238831 epoch total loss 6.33859634\n",
      "Trained batch 366 batch loss 6.57494783 epoch total loss 6.33924198\n",
      "Trained batch 367 batch loss 6.63338947 epoch total loss 6.34004354\n",
      "Trained batch 368 batch loss 6.51264143 epoch total loss 6.34051228\n",
      "Trained batch 369 batch loss 6.5846386 epoch total loss 6.34117413\n",
      "Trained batch 370 batch loss 6.74365711 epoch total loss 6.34226227\n",
      "Trained batch 371 batch loss 6.78354788 epoch total loss 6.34345102\n",
      "Trained batch 372 batch loss 6.80343199 epoch total loss 6.34468794\n",
      "Trained batch 373 batch loss 6.85070944 epoch total loss 6.34604502\n",
      "Trained batch 374 batch loss 6.29590797 epoch total loss 6.34591055\n",
      "Trained batch 375 batch loss 6.70685673 epoch total loss 6.34687328\n",
      "Trained batch 376 batch loss 6.32694054 epoch total loss 6.34682\n",
      "Trained batch 377 batch loss 6.35997772 epoch total loss 6.34685469\n",
      "Trained batch 378 batch loss 5.91075706 epoch total loss 6.34570074\n",
      "Trained batch 379 batch loss 6.5206995 epoch total loss 6.34616232\n",
      "Trained batch 380 batch loss 6.40161324 epoch total loss 6.34630823\n",
      "Trained batch 381 batch loss 6.25455904 epoch total loss 6.34606791\n",
      "Trained batch 382 batch loss 6.15008402 epoch total loss 6.34555483\n",
      "Trained batch 383 batch loss 6.18367577 epoch total loss 6.34513187\n",
      "Trained batch 384 batch loss 6.29040909 epoch total loss 6.34499\n",
      "Trained batch 385 batch loss 5.99961901 epoch total loss 6.34409237\n",
      "Trained batch 386 batch loss 6.05264473 epoch total loss 6.34333754\n",
      "Trained batch 387 batch loss 6.34705544 epoch total loss 6.34334755\n",
      "Trained batch 388 batch loss 6.05784798 epoch total loss 6.34261179\n",
      "Trained batch 389 batch loss 5.6508913 epoch total loss 6.34083366\n",
      "Trained batch 390 batch loss 6.02594709 epoch total loss 6.3400259\n",
      "Trained batch 391 batch loss 5.09204721 epoch total loss 6.33683395\n",
      "Trained batch 392 batch loss 4.75469542 epoch total loss 6.332798\n",
      "Trained batch 393 batch loss 6.20768452 epoch total loss 6.33248\n",
      "Trained batch 394 batch loss 6.11774158 epoch total loss 6.33193445\n",
      "Trained batch 395 batch loss 6.41115475 epoch total loss 6.3321352\n",
      "Trained batch 396 batch loss 6.23136377 epoch total loss 6.33188105\n",
      "Trained batch 397 batch loss 6.39109135 epoch total loss 6.33203\n",
      "Trained batch 398 batch loss 6.1930604 epoch total loss 6.33168077\n",
      "Trained batch 399 batch loss 6.34120274 epoch total loss 6.33170509\n",
      "Trained batch 400 batch loss 6.44035959 epoch total loss 6.33197689\n",
      "Trained batch 401 batch loss 6.28435469 epoch total loss 6.33185816\n",
      "Trained batch 402 batch loss 5.97502136 epoch total loss 6.33097076\n",
      "Trained batch 403 batch loss 6.21496725 epoch total loss 6.33068323\n",
      "Trained batch 404 batch loss 6.04914141 epoch total loss 6.3299861\n",
      "Trained batch 405 batch loss 6.35185432 epoch total loss 6.33004\n",
      "Trained batch 406 batch loss 6.22499371 epoch total loss 6.32978153\n",
      "Trained batch 407 batch loss 6.89350176 epoch total loss 6.33116674\n",
      "Trained batch 408 batch loss 6.85662127 epoch total loss 6.33245468\n",
      "Trained batch 409 batch loss 5.81345606 epoch total loss 6.33118582\n",
      "Trained batch 410 batch loss 6.45154667 epoch total loss 6.33148\n",
      "Trained batch 411 batch loss 6.02737141 epoch total loss 6.33074\n",
      "Trained batch 412 batch loss 5.82966185 epoch total loss 6.32952356\n",
      "Trained batch 413 batch loss 6.08600426 epoch total loss 6.32893372\n",
      "Trained batch 414 batch loss 5.68580437 epoch total loss 6.32738\n",
      "Trained batch 415 batch loss 6.39373636 epoch total loss 6.3275404\n",
      "Trained batch 416 batch loss 5.99333525 epoch total loss 6.32673693\n",
      "Trained batch 417 batch loss 5.5866394 epoch total loss 6.32496214\n",
      "Trained batch 418 batch loss 5.82264614 epoch total loss 6.32376099\n",
      "Trained batch 419 batch loss 5.8814826 epoch total loss 6.32270575\n",
      "Trained batch 420 batch loss 6.07395697 epoch total loss 6.32211351\n",
      "Trained batch 421 batch loss 6.16164684 epoch total loss 6.32173204\n",
      "Trained batch 422 batch loss 6.61432648 epoch total loss 6.32242537\n",
      "Trained batch 423 batch loss 6.3619957 epoch total loss 6.32251883\n",
      "Trained batch 424 batch loss 6.19074631 epoch total loss 6.32220793\n",
      "Trained batch 425 batch loss 6.22044611 epoch total loss 6.32196856\n",
      "Trained batch 426 batch loss 6.19878721 epoch total loss 6.32167912\n",
      "Trained batch 427 batch loss 5.7274189 epoch total loss 6.3202877\n",
      "Trained batch 428 batch loss 6.14890242 epoch total loss 6.31988764\n",
      "Trained batch 429 batch loss 6.36253357 epoch total loss 6.31998682\n",
      "Trained batch 430 batch loss 6.47776794 epoch total loss 6.32035398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 431 batch loss 6.43907833 epoch total loss 6.32062912\n",
      "Trained batch 432 batch loss 6.39539146 epoch total loss 6.32080269\n",
      "Trained batch 433 batch loss 6.33044243 epoch total loss 6.32082462\n",
      "Trained batch 434 batch loss 6.66263628 epoch total loss 6.32161188\n",
      "Trained batch 435 batch loss 6.85102844 epoch total loss 6.32282925\n",
      "Trained batch 436 batch loss 5.75715256 epoch total loss 6.3215313\n",
      "Trained batch 437 batch loss 5.64744377 epoch total loss 6.3199892\n",
      "Trained batch 438 batch loss 5.29288 epoch total loss 6.31764412\n",
      "Trained batch 439 batch loss 6.44311619 epoch total loss 6.31793\n",
      "Trained batch 440 batch loss 6.3895936 epoch total loss 6.31809282\n",
      "Trained batch 441 batch loss 5.92634392 epoch total loss 6.31720448\n",
      "Trained batch 442 batch loss 6.57200146 epoch total loss 6.31778097\n",
      "Trained batch 443 batch loss 6.46872234 epoch total loss 6.31812191\n",
      "Trained batch 444 batch loss 6.23799038 epoch total loss 6.31794167\n",
      "Trained batch 445 batch loss 6.37971926 epoch total loss 6.31808\n",
      "Trained batch 446 batch loss 6.36784267 epoch total loss 6.31819201\n",
      "Trained batch 447 batch loss 5.87042236 epoch total loss 6.31719\n",
      "Trained batch 448 batch loss 6.44579601 epoch total loss 6.31747723\n",
      "Trained batch 449 batch loss 6.0877285 epoch total loss 6.3169651\n",
      "Trained batch 450 batch loss 6.28526402 epoch total loss 6.31689453\n",
      "Trained batch 451 batch loss 6.27768183 epoch total loss 6.31680727\n",
      "Trained batch 452 batch loss 6.39303732 epoch total loss 6.31697607\n",
      "Trained batch 453 batch loss 6.19778538 epoch total loss 6.31671286\n",
      "Trained batch 454 batch loss 6.15606642 epoch total loss 6.31635904\n",
      "Trained batch 455 batch loss 6.0539484 epoch total loss 6.31578207\n",
      "Trained batch 456 batch loss 6.29898882 epoch total loss 6.31574535\n",
      "Trained batch 457 batch loss 6.37490749 epoch total loss 6.31587505\n",
      "Trained batch 458 batch loss 6.21696854 epoch total loss 6.31565952\n",
      "Trained batch 459 batch loss 6.46222925 epoch total loss 6.31597853\n",
      "Trained batch 460 batch loss 6.57058764 epoch total loss 6.31653214\n",
      "Trained batch 461 batch loss 6.64089251 epoch total loss 6.31723547\n",
      "Trained batch 462 batch loss 6.60912418 epoch total loss 6.31786728\n",
      "Trained batch 463 batch loss 6.36237907 epoch total loss 6.3179636\n",
      "Trained batch 464 batch loss 6.23248768 epoch total loss 6.31777906\n",
      "Trained batch 465 batch loss 6.57406044 epoch total loss 6.31833\n",
      "Trained batch 466 batch loss 6.58373976 epoch total loss 6.31889963\n",
      "Trained batch 467 batch loss 6.6528883 epoch total loss 6.31961441\n",
      "Trained batch 468 batch loss 6.48577881 epoch total loss 6.31996965\n",
      "Trained batch 469 batch loss 6.28997278 epoch total loss 6.31990576\n",
      "Trained batch 470 batch loss 5.8509078 epoch total loss 6.31890774\n",
      "Trained batch 471 batch loss 5.84494543 epoch total loss 6.31790161\n",
      "Trained batch 472 batch loss 6.1976738 epoch total loss 6.31764698\n",
      "Trained batch 473 batch loss 6.37792969 epoch total loss 6.31777477\n",
      "Trained batch 474 batch loss 6.53457689 epoch total loss 6.31823206\n",
      "Trained batch 475 batch loss 6.39021158 epoch total loss 6.31838369\n",
      "Trained batch 476 batch loss 6.14053202 epoch total loss 6.31801\n",
      "Trained batch 477 batch loss 6.15118456 epoch total loss 6.31766033\n",
      "Trained batch 478 batch loss 6.71594238 epoch total loss 6.31849337\n",
      "Trained batch 479 batch loss 6.19138098 epoch total loss 6.31822777\n",
      "Trained batch 480 batch loss 6.55205584 epoch total loss 6.3187151\n",
      "Trained batch 481 batch loss 6.42909527 epoch total loss 6.31894445\n",
      "Trained batch 482 batch loss 6.43505192 epoch total loss 6.31918573\n",
      "Trained batch 483 batch loss 6.56492424 epoch total loss 6.31969452\n",
      "Trained batch 484 batch loss 6.33204746 epoch total loss 6.31972\n",
      "Trained batch 485 batch loss 6.24006939 epoch total loss 6.31955528\n",
      "Trained batch 486 batch loss 5.94020176 epoch total loss 6.3187747\n",
      "Trained batch 487 batch loss 5.90915585 epoch total loss 6.31793356\n",
      "Trained batch 488 batch loss 5.27139711 epoch total loss 6.31578922\n",
      "Trained batch 489 batch loss 5.69781542 epoch total loss 6.3145256\n",
      "Trained batch 490 batch loss 5.28662252 epoch total loss 6.31242752\n",
      "Trained batch 491 batch loss 5.258389 epoch total loss 6.3102808\n",
      "Trained batch 492 batch loss 4.95989 epoch total loss 6.30753613\n",
      "Trained batch 493 batch loss 4.76242685 epoch total loss 6.30440235\n",
      "Trained batch 494 batch loss 6.51197386 epoch total loss 6.30482244\n",
      "Trained batch 495 batch loss 6.48671389 epoch total loss 6.30519\n",
      "Trained batch 496 batch loss 6.26707363 epoch total loss 6.30511332\n",
      "Trained batch 497 batch loss 6.26438141 epoch total loss 6.3050313\n",
      "Trained batch 498 batch loss 6.20524216 epoch total loss 6.30483103\n",
      "Trained batch 499 batch loss 6.394063 epoch total loss 6.30501\n",
      "Trained batch 500 batch loss 6.42920494 epoch total loss 6.30525827\n",
      "Trained batch 501 batch loss 6.43502474 epoch total loss 6.3055172\n",
      "Trained batch 502 batch loss 6.34501934 epoch total loss 6.30559587\n",
      "Trained batch 503 batch loss 6.37968397 epoch total loss 6.30574322\n",
      "Trained batch 504 batch loss 6.65435362 epoch total loss 6.30643463\n",
      "Trained batch 505 batch loss 6.36593199 epoch total loss 6.30655241\n",
      "Trained batch 506 batch loss 6.21105909 epoch total loss 6.30636358\n",
      "Trained batch 507 batch loss 6.56686735 epoch total loss 6.30687761\n",
      "Trained batch 508 batch loss 6.35738182 epoch total loss 6.30697727\n",
      "Trained batch 509 batch loss 6.3843 epoch total loss 6.30712891\n",
      "Trained batch 510 batch loss 6.44119787 epoch total loss 6.30739164\n",
      "Trained batch 511 batch loss 6.20251036 epoch total loss 6.30718613\n",
      "Trained batch 512 batch loss 6.2574544 epoch total loss 6.30708933\n",
      "Trained batch 513 batch loss 6.82521 epoch total loss 6.30809927\n",
      "Trained batch 514 batch loss 6.85703516 epoch total loss 6.30916691\n",
      "Trained batch 515 batch loss 6.93451 epoch total loss 6.31038141\n",
      "Trained batch 516 batch loss 7.54008245 epoch total loss 6.31276464\n",
      "Trained batch 517 batch loss 7.45127344 epoch total loss 6.3149662\n",
      "Trained batch 518 batch loss 7.2513361 epoch total loss 6.31677389\n",
      "Trained batch 519 batch loss 7.32957935 epoch total loss 6.31872559\n",
      "Trained batch 520 batch loss 6.3700304 epoch total loss 6.31882429\n",
      "Trained batch 521 batch loss 6.27483654 epoch total loss 6.31874\n",
      "Trained batch 522 batch loss 6.4819684 epoch total loss 6.3190527\n",
      "Trained batch 523 batch loss 6.43455887 epoch total loss 6.31927347\n",
      "Trained batch 524 batch loss 6.45179796 epoch total loss 6.31952667\n",
      "Trained batch 525 batch loss 6.43326426 epoch total loss 6.31974316\n",
      "Trained batch 526 batch loss 6.69104767 epoch total loss 6.32044935\n",
      "Trained batch 527 batch loss 6.268713 epoch total loss 6.3203516\n",
      "Trained batch 528 batch loss 6.16107225 epoch total loss 6.32005\n",
      "Trained batch 529 batch loss 5.79955 epoch total loss 6.31906605\n",
      "Trained batch 530 batch loss 5.80572033 epoch total loss 6.31809711\n",
      "Trained batch 531 batch loss 6.07129192 epoch total loss 6.31763268\n",
      "Trained batch 532 batch loss 6.36163521 epoch total loss 6.31771517\n",
      "Trained batch 533 batch loss 6.35709858 epoch total loss 6.31778908\n",
      "Trained batch 534 batch loss 6.53943634 epoch total loss 6.3182044\n",
      "Trained batch 535 batch loss 6.42095518 epoch total loss 6.31839657\n",
      "Trained batch 536 batch loss 6.59307051 epoch total loss 6.31890869\n",
      "Trained batch 537 batch loss 6.47470713 epoch total loss 6.31919861\n",
      "Trained batch 538 batch loss 6.09324741 epoch total loss 6.31877851\n",
      "Trained batch 539 batch loss 6.31772089 epoch total loss 6.31877661\n",
      "Trained batch 540 batch loss 6.15273285 epoch total loss 6.31846905\n",
      "Trained batch 541 batch loss 6.46696281 epoch total loss 6.31874371\n",
      "Trained batch 542 batch loss 6.40388918 epoch total loss 6.31890059\n",
      "Trained batch 543 batch loss 5.75597095 epoch total loss 6.31786394\n",
      "Trained batch 544 batch loss 5.8444705 epoch total loss 6.31699371\n",
      "Trained batch 545 batch loss 6.08608913 epoch total loss 6.31657028\n",
      "Trained batch 546 batch loss 6.36889648 epoch total loss 6.31666613\n",
      "Trained batch 547 batch loss 6.53033876 epoch total loss 6.31705666\n",
      "Trained batch 548 batch loss 6.43207788 epoch total loss 6.31726646\n",
      "Trained batch 549 batch loss 6.72835255 epoch total loss 6.3180151\n",
      "Trained batch 550 batch loss 6.30441189 epoch total loss 6.3179903\n",
      "Trained batch 551 batch loss 6.54689741 epoch total loss 6.31840611\n",
      "Trained batch 552 batch loss 6.67298174 epoch total loss 6.3190484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 553 batch loss 6.32176 epoch total loss 6.31905317\n",
      "Trained batch 554 batch loss 6.29839945 epoch total loss 6.31901598\n",
      "Trained batch 555 batch loss 6.34844732 epoch total loss 6.31906891\n",
      "Trained batch 556 batch loss 6.80995846 epoch total loss 6.31995201\n",
      "Trained batch 557 batch loss 6.37262201 epoch total loss 6.32004642\n",
      "Trained batch 558 batch loss 6.61070156 epoch total loss 6.32056713\n",
      "Trained batch 559 batch loss 6.45235 epoch total loss 6.32080317\n",
      "Trained batch 560 batch loss 6.2725687 epoch total loss 6.32071686\n",
      "Trained batch 561 batch loss 6.31634665 epoch total loss 6.32070875\n",
      "Trained batch 562 batch loss 6.64308929 epoch total loss 6.32128239\n",
      "Trained batch 563 batch loss 6.2984 epoch total loss 6.32124186\n",
      "Trained batch 564 batch loss 6.27268887 epoch total loss 6.32115555\n",
      "Trained batch 565 batch loss 6.48688126 epoch total loss 6.3214488\n",
      "Trained batch 566 batch loss 6.21484756 epoch total loss 6.32126045\n",
      "Trained batch 567 batch loss 6.19235611 epoch total loss 6.32103348\n",
      "Trained batch 568 batch loss 6.05710459 epoch total loss 6.32056856\n",
      "Trained batch 569 batch loss 6.3286829 epoch total loss 6.32058287\n",
      "Trained batch 570 batch loss 6.70730686 epoch total loss 6.32126141\n",
      "Trained batch 571 batch loss 6.82853222 epoch total loss 6.32214975\n",
      "Trained batch 572 batch loss 6.40239859 epoch total loss 6.32229\n",
      "Trained batch 573 batch loss 6.82817268 epoch total loss 6.32317257\n",
      "Trained batch 574 batch loss 6.63495636 epoch total loss 6.32371616\n",
      "Trained batch 575 batch loss 6.77268171 epoch total loss 6.32449675\n",
      "Trained batch 576 batch loss 6.24235344 epoch total loss 6.32435417\n",
      "Trained batch 577 batch loss 6.62297773 epoch total loss 6.32487202\n",
      "Trained batch 578 batch loss 6.24787045 epoch total loss 6.3247385\n",
      "Trained batch 579 batch loss 6.37238741 epoch total loss 6.324821\n",
      "Trained batch 580 batch loss 6.95512915 epoch total loss 6.32590771\n",
      "Trained batch 581 batch loss 6.50629663 epoch total loss 6.32621813\n",
      "Trained batch 582 batch loss 6.50897884 epoch total loss 6.32653236\n",
      "Trained batch 583 batch loss 6.24667645 epoch total loss 6.32639503\n",
      "Trained batch 584 batch loss 6.42152596 epoch total loss 6.32655811\n",
      "Trained batch 585 batch loss 6.98416471 epoch total loss 6.32768202\n",
      "Trained batch 586 batch loss 6.53098059 epoch total loss 6.32802916\n",
      "Trained batch 587 batch loss 5.88624048 epoch total loss 6.32727671\n",
      "Trained batch 588 batch loss 6.35560894 epoch total loss 6.32732487\n",
      "Trained batch 589 batch loss 6.48482752 epoch total loss 6.32759237\n",
      "Trained batch 590 batch loss 5.86998749 epoch total loss 6.32681656\n",
      "Trained batch 591 batch loss 5.74261475 epoch total loss 6.32582808\n",
      "Trained batch 592 batch loss 5.88101482 epoch total loss 6.32507706\n",
      "Trained batch 593 batch loss 5.71251869 epoch total loss 6.32404375\n",
      "Trained batch 594 batch loss 5.89711 epoch total loss 6.32332516\n",
      "Trained batch 595 batch loss 6.28681374 epoch total loss 6.32326412\n",
      "Trained batch 596 batch loss 6.51942396 epoch total loss 6.32359314\n",
      "Trained batch 597 batch loss 6.49337912 epoch total loss 6.32387781\n",
      "Trained batch 598 batch loss 6.2667942 epoch total loss 6.32378244\n",
      "Trained batch 599 batch loss 6.01369238 epoch total loss 6.3232646\n",
      "Trained batch 600 batch loss 6.16398573 epoch total loss 6.32299948\n",
      "Trained batch 601 batch loss 6.40688086 epoch total loss 6.32313919\n",
      "Trained batch 602 batch loss 6.35207224 epoch total loss 6.32318687\n",
      "Trained batch 603 batch loss 6.10356665 epoch total loss 6.32282257\n",
      "Trained batch 604 batch loss 6.26853848 epoch total loss 6.32273293\n",
      "Trained batch 605 batch loss 6.22387791 epoch total loss 6.32256937\n",
      "Trained batch 606 batch loss 6.15491343 epoch total loss 6.3222928\n",
      "Trained batch 607 batch loss 6.25571203 epoch total loss 6.32218313\n",
      "Trained batch 608 batch loss 6.2972641 epoch total loss 6.32214212\n",
      "Trained batch 609 batch loss 6.06324959 epoch total loss 6.32171726\n",
      "Trained batch 610 batch loss 6.16778183 epoch total loss 6.32146454\n",
      "Trained batch 611 batch loss 6.37218952 epoch total loss 6.32154751\n",
      "Trained batch 612 batch loss 6.0334506 epoch total loss 6.32107687\n",
      "Trained batch 613 batch loss 6.40977764 epoch total loss 6.32122135\n",
      "Trained batch 614 batch loss 6.18415785 epoch total loss 6.32099819\n",
      "Trained batch 615 batch loss 6.16110945 epoch total loss 6.32073784\n",
      "Trained batch 616 batch loss 6.38749456 epoch total loss 6.32084656\n",
      "Trained batch 617 batch loss 6.38021469 epoch total loss 6.3209424\n",
      "Trained batch 618 batch loss 6.58372688 epoch total loss 6.32136774\n",
      "Trained batch 619 batch loss 6.45541382 epoch total loss 6.32158422\n",
      "Trained batch 620 batch loss 5.95241499 epoch total loss 6.32098866\n",
      "Trained batch 621 batch loss 6.21664381 epoch total loss 6.32082033\n",
      "Trained batch 622 batch loss 6.56619215 epoch total loss 6.32121468\n",
      "Trained batch 623 batch loss 6.51101208 epoch total loss 6.32151937\n",
      "Trained batch 624 batch loss 6.55295 epoch total loss 6.32189035\n",
      "Trained batch 625 batch loss 6.53333902 epoch total loss 6.32222891\n",
      "Trained batch 626 batch loss 6.25133801 epoch total loss 6.32211542\n",
      "Trained batch 627 batch loss 6.01133919 epoch total loss 6.32161951\n",
      "Trained batch 628 batch loss 5.95012712 epoch total loss 6.32102823\n",
      "Trained batch 629 batch loss 6.81383944 epoch total loss 6.32181168\n",
      "Trained batch 630 batch loss 6.76513386 epoch total loss 6.32251501\n",
      "Trained batch 631 batch loss 7.03320169 epoch total loss 6.3236413\n",
      "Trained batch 632 batch loss 6.74848938 epoch total loss 6.32431364\n",
      "Trained batch 633 batch loss 5.86736488 epoch total loss 6.32359219\n",
      "Trained batch 634 batch loss 6.33815336 epoch total loss 6.32361507\n",
      "Trained batch 635 batch loss 6.49129343 epoch total loss 6.32387877\n",
      "Trained batch 636 batch loss 6.63739491 epoch total loss 6.32437181\n",
      "Trained batch 637 batch loss 6.53313541 epoch total loss 6.3247\n",
      "Trained batch 638 batch loss 6.38650417 epoch total loss 6.32479668\n",
      "Trained batch 639 batch loss 6.268713 epoch total loss 6.32470894\n",
      "Trained batch 640 batch loss 6.1572628 epoch total loss 6.32444715\n",
      "Trained batch 641 batch loss 6.42315722 epoch total loss 6.32460117\n",
      "Trained batch 642 batch loss 6.73006535 epoch total loss 6.32523251\n",
      "Trained batch 643 batch loss 6.45497417 epoch total loss 6.32543468\n",
      "Trained batch 644 batch loss 6.68211842 epoch total loss 6.32598829\n",
      "Trained batch 645 batch loss 6.42556 epoch total loss 6.32614279\n",
      "Trained batch 646 batch loss 6.52440834 epoch total loss 6.32645\n",
      "Trained batch 647 batch loss 6.28345251 epoch total loss 6.32638311\n",
      "Trained batch 648 batch loss 6.11215878 epoch total loss 6.32605267\n",
      "Trained batch 649 batch loss 6.31555939 epoch total loss 6.32603645\n",
      "Trained batch 650 batch loss 6.86261892 epoch total loss 6.32686234\n",
      "Trained batch 651 batch loss 6.3435626 epoch total loss 6.32688808\n",
      "Trained batch 652 batch loss 6.71872854 epoch total loss 6.32748938\n",
      "Trained batch 653 batch loss 5.80911875 epoch total loss 6.32669544\n",
      "Trained batch 654 batch loss 6.27628279 epoch total loss 6.32661819\n",
      "Trained batch 655 batch loss 6.23108292 epoch total loss 6.32647228\n",
      "Trained batch 656 batch loss 5.72227812 epoch total loss 6.32555103\n",
      "Trained batch 657 batch loss 6.21695709 epoch total loss 6.32538557\n",
      "Trained batch 658 batch loss 6.01963949 epoch total loss 6.32492065\n",
      "Trained batch 659 batch loss 6.41827488 epoch total loss 6.32506275\n",
      "Trained batch 660 batch loss 6.31400537 epoch total loss 6.32504606\n",
      "Trained batch 661 batch loss 6.39435863 epoch total loss 6.32515097\n",
      "Trained batch 662 batch loss 6.08471489 epoch total loss 6.32478762\n",
      "Trained batch 663 batch loss 6.23669052 epoch total loss 6.32465458\n",
      "Trained batch 664 batch loss 6.12672186 epoch total loss 6.32435703\n",
      "Trained batch 665 batch loss 6.16501188 epoch total loss 6.32411766\n",
      "Trained batch 666 batch loss 6.29789734 epoch total loss 6.32407808\n",
      "Trained batch 667 batch loss 5.74929237 epoch total loss 6.32321644\n",
      "Trained batch 668 batch loss 6.32600164 epoch total loss 6.32322121\n",
      "Trained batch 669 batch loss 6.48099089 epoch total loss 6.32345676\n",
      "Trained batch 670 batch loss 6.26951361 epoch total loss 6.32337618\n",
      "Trained batch 671 batch loss 6.36294174 epoch total loss 6.32343483\n",
      "Trained batch 672 batch loss 6.02832365 epoch total loss 6.32299566\n",
      "Trained batch 673 batch loss 6.25768137 epoch total loss 6.32289886\n",
      "Trained batch 674 batch loss 6.30505276 epoch total loss 6.32287264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 675 batch loss 6.38032722 epoch total loss 6.32295799\n",
      "Trained batch 676 batch loss 6.43399668 epoch total loss 6.3231225\n",
      "Trained batch 677 batch loss 6.39959192 epoch total loss 6.32323503\n",
      "Trained batch 678 batch loss 6.46918058 epoch total loss 6.32345\n",
      "Trained batch 679 batch loss 6.06919098 epoch total loss 6.32307625\n",
      "Trained batch 680 batch loss 6.28028822 epoch total loss 6.32301331\n",
      "Trained batch 681 batch loss 6.45412922 epoch total loss 6.32320547\n",
      "Trained batch 682 batch loss 6.44192076 epoch total loss 6.32337952\n",
      "Trained batch 683 batch loss 6.54716206 epoch total loss 6.32370758\n",
      "Trained batch 684 batch loss 6.37766552 epoch total loss 6.32378626\n",
      "Trained batch 685 batch loss 6.27898026 epoch total loss 6.32372046\n",
      "Trained batch 686 batch loss 6.39658499 epoch total loss 6.32382679\n",
      "Trained batch 687 batch loss 6.46429205 epoch total loss 6.32403088\n",
      "Trained batch 688 batch loss 6.90999 epoch total loss 6.32488298\n",
      "Trained batch 689 batch loss 6.70337534 epoch total loss 6.32543278\n",
      "Trained batch 690 batch loss 6.4737072 epoch total loss 6.32564735\n",
      "Trained batch 691 batch loss 6.25955 epoch total loss 6.32555199\n",
      "Trained batch 692 batch loss 6.52174807 epoch total loss 6.32583618\n",
      "Trained batch 693 batch loss 6.77722216 epoch total loss 6.32648754\n",
      "Trained batch 694 batch loss 6.64606428 epoch total loss 6.32694769\n",
      "Trained batch 695 batch loss 6.27784729 epoch total loss 6.32687712\n",
      "Trained batch 696 batch loss 6.04699421 epoch total loss 6.32647514\n",
      "Trained batch 697 batch loss 5.91837358 epoch total loss 6.32588959\n",
      "Trained batch 698 batch loss 5.82451725 epoch total loss 6.32517147\n",
      "Trained batch 699 batch loss 5.74782467 epoch total loss 6.32434607\n",
      "Trained batch 700 batch loss 6.97358274 epoch total loss 6.32527351\n",
      "Trained batch 701 batch loss 6.68138266 epoch total loss 6.32578135\n",
      "Trained batch 702 batch loss 6.97123098 epoch total loss 6.32670069\n",
      "Trained batch 703 batch loss 6.77264643 epoch total loss 6.3273344\n",
      "Trained batch 704 batch loss 6.75717211 epoch total loss 6.32794523\n",
      "Trained batch 705 batch loss 6.80316639 epoch total loss 6.32861948\n",
      "Trained batch 706 batch loss 6.74420547 epoch total loss 6.3292079\n",
      "Trained batch 707 batch loss 6.37696886 epoch total loss 6.32927561\n",
      "Trained batch 708 batch loss 6.75176048 epoch total loss 6.32987261\n",
      "Trained batch 709 batch loss 6.42232037 epoch total loss 6.33000326\n",
      "Trained batch 710 batch loss 6.38183784 epoch total loss 6.33007622\n",
      "Trained batch 711 batch loss 6.44786549 epoch total loss 6.33024168\n",
      "Trained batch 712 batch loss 6.74628878 epoch total loss 6.33082581\n",
      "Trained batch 713 batch loss 6.39244413 epoch total loss 6.33091211\n",
      "Trained batch 714 batch loss 6.76713943 epoch total loss 6.33152294\n",
      "Trained batch 715 batch loss 6.44168186 epoch total loss 6.33167744\n",
      "Trained batch 716 batch loss 6.26156521 epoch total loss 6.33157969\n",
      "Trained batch 717 batch loss 6.61067724 epoch total loss 6.33196926\n",
      "Trained batch 718 batch loss 6.28574896 epoch total loss 6.33190489\n",
      "Trained batch 719 batch loss 6.82209301 epoch total loss 6.33258677\n",
      "Trained batch 720 batch loss 7.42906857 epoch total loss 6.33411\n",
      "Trained batch 721 batch loss 7.17906094 epoch total loss 6.33528185\n",
      "Trained batch 722 batch loss 6.83277464 epoch total loss 6.33597136\n",
      "Trained batch 723 batch loss 6.80189371 epoch total loss 6.33661556\n",
      "Trained batch 724 batch loss 6.78508759 epoch total loss 6.33723497\n",
      "Trained batch 725 batch loss 5.38488579 epoch total loss 6.33592129\n",
      "Trained batch 726 batch loss 6.17428732 epoch total loss 6.3356986\n",
      "Trained batch 727 batch loss 6.65884495 epoch total loss 6.33614302\n",
      "Trained batch 728 batch loss 6.83461189 epoch total loss 6.33682775\n",
      "Trained batch 729 batch loss 6.33109951 epoch total loss 6.33681965\n",
      "Trained batch 730 batch loss 6.37140608 epoch total loss 6.33686733\n",
      "Trained batch 731 batch loss 6.15485477 epoch total loss 6.33661795\n",
      "Trained batch 732 batch loss 6.2013588 epoch total loss 6.33643293\n",
      "Trained batch 733 batch loss 6.15109968 epoch total loss 6.33618\n",
      "Trained batch 734 batch loss 6.46477652 epoch total loss 6.33635521\n",
      "Trained batch 735 batch loss 5.9591 epoch total loss 6.33584166\n",
      "Trained batch 736 batch loss 6.22533894 epoch total loss 6.33569145\n",
      "Trained batch 737 batch loss 6.62516928 epoch total loss 6.33608389\n",
      "Trained batch 738 batch loss 6.22261381 epoch total loss 6.33593035\n",
      "Trained batch 739 batch loss 6.08672237 epoch total loss 6.33559322\n",
      "Trained batch 740 batch loss 6.04542923 epoch total loss 6.33520126\n",
      "Trained batch 741 batch loss 6.54658508 epoch total loss 6.33548594\n",
      "Trained batch 742 batch loss 6.08564758 epoch total loss 6.33514929\n",
      "Trained batch 743 batch loss 6.05441141 epoch total loss 6.33477116\n",
      "Trained batch 744 batch loss 6.2033453 epoch total loss 6.33459425\n",
      "Trained batch 745 batch loss 6.24247837 epoch total loss 6.33447075\n",
      "Trained batch 746 batch loss 6.36763573 epoch total loss 6.33451509\n",
      "Trained batch 747 batch loss 6.50630951 epoch total loss 6.33474541\n",
      "Trained batch 748 batch loss 6.64531946 epoch total loss 6.33516073\n",
      "Trained batch 749 batch loss 6.3651371 epoch total loss 6.33520079\n",
      "Trained batch 750 batch loss 5.79296 epoch total loss 6.3344779\n",
      "Trained batch 751 batch loss 6.39069 epoch total loss 6.33455276\n",
      "Trained batch 752 batch loss 6.50773573 epoch total loss 6.33478308\n",
      "Trained batch 753 batch loss 6.48363638 epoch total loss 6.33498049\n",
      "Trained batch 754 batch loss 6.567348 epoch total loss 6.33528852\n",
      "Trained batch 755 batch loss 6.42483139 epoch total loss 6.33540726\n",
      "Trained batch 756 batch loss 6.07094955 epoch total loss 6.33505726\n",
      "Trained batch 757 batch loss 6.08248091 epoch total loss 6.33472347\n",
      "Trained batch 758 batch loss 6.33540154 epoch total loss 6.33472443\n",
      "Trained batch 759 batch loss 5.80556488 epoch total loss 6.33402729\n",
      "Trained batch 760 batch loss 5.99288464 epoch total loss 6.33357811\n",
      "Trained batch 761 batch loss 6.30253839 epoch total loss 6.33353758\n",
      "Trained batch 762 batch loss 6.34567595 epoch total loss 6.33355379\n",
      "Trained batch 763 batch loss 6.34709454 epoch total loss 6.33357143\n",
      "Trained batch 764 batch loss 6.39161491 epoch total loss 6.33364773\n",
      "Trained batch 765 batch loss 6.3064537 epoch total loss 6.33361244\n",
      "Trained batch 766 batch loss 6.56194496 epoch total loss 6.33391047\n",
      "Trained batch 767 batch loss 6.14520025 epoch total loss 6.33366394\n",
      "Trained batch 768 batch loss 6.5231123 epoch total loss 6.33391047\n",
      "Trained batch 769 batch loss 6.53394127 epoch total loss 6.33417082\n",
      "Trained batch 770 batch loss 6.38215876 epoch total loss 6.33423376\n",
      "Trained batch 771 batch loss 5.43550062 epoch total loss 6.33306789\n",
      "Trained batch 772 batch loss 5.84865713 epoch total loss 6.33244038\n",
      "Trained batch 773 batch loss 5.68337965 epoch total loss 6.33160114\n",
      "Trained batch 774 batch loss 5.90827608 epoch total loss 6.33105421\n",
      "Trained batch 775 batch loss 6.25214577 epoch total loss 6.33095217\n",
      "Trained batch 776 batch loss 5.90116453 epoch total loss 6.33039856\n",
      "Trained batch 777 batch loss 6.17790842 epoch total loss 6.3302021\n",
      "Trained batch 778 batch loss 5.83771849 epoch total loss 6.32956934\n",
      "Trained batch 779 batch loss 5.99903345 epoch total loss 6.32914495\n",
      "Trained batch 780 batch loss 5.71565 epoch total loss 6.32835865\n",
      "Trained batch 781 batch loss 5.94269753 epoch total loss 6.32786512\n",
      "Trained batch 782 batch loss 6.06034 epoch total loss 6.32752323\n",
      "Trained batch 783 batch loss 6.32643 epoch total loss 6.3275218\n",
      "Trained batch 784 batch loss 6.3101635 epoch total loss 6.3275\n",
      "Trained batch 785 batch loss 6.75379038 epoch total loss 6.32804298\n",
      "Trained batch 786 batch loss 6.7767725 epoch total loss 6.32861376\n",
      "Trained batch 787 batch loss 6.5441227 epoch total loss 6.32888746\n",
      "Trained batch 788 batch loss 6.17926931 epoch total loss 6.32869768\n",
      "Trained batch 789 batch loss 6.18926477 epoch total loss 6.32852125\n",
      "Trained batch 790 batch loss 6.20727 epoch total loss 6.32836723\n",
      "Trained batch 791 batch loss 6.20679474 epoch total loss 6.32821369\n",
      "Trained batch 792 batch loss 6.49593592 epoch total loss 6.32842588\n",
      "Trained batch 793 batch loss 6.65024042 epoch total loss 6.32883167\n",
      "Trained batch 794 batch loss 6.83747625 epoch total loss 6.32947254\n",
      "Trained batch 795 batch loss 6.90064526 epoch total loss 6.33019114\n",
      "Trained batch 796 batch loss 6.97987032 epoch total loss 6.33100748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 797 batch loss 7.01086092 epoch total loss 6.33186054\n",
      "Trained batch 798 batch loss 7.30342197 epoch total loss 6.33307743\n",
      "Trained batch 799 batch loss 7.00411701 epoch total loss 6.33391714\n",
      "Trained batch 800 batch loss 6.90361309 epoch total loss 6.33462954\n",
      "Trained batch 801 batch loss 6.64150667 epoch total loss 6.33501291\n",
      "Trained batch 802 batch loss 6.31739569 epoch total loss 6.33499098\n",
      "Trained batch 803 batch loss 5.6057744 epoch total loss 6.33408308\n",
      "Trained batch 804 batch loss 5.88136816 epoch total loss 6.33352\n",
      "Trained batch 805 batch loss 6.06812477 epoch total loss 6.33319044\n",
      "Trained batch 806 batch loss 6.54731417 epoch total loss 6.33345604\n",
      "Trained batch 807 batch loss 6.49667454 epoch total loss 6.33365822\n",
      "Trained batch 808 batch loss 6.24778557 epoch total loss 6.33355188\n",
      "Trained batch 809 batch loss 6.1976819 epoch total loss 6.33338404\n",
      "Trained batch 810 batch loss 5.89458275 epoch total loss 6.33284187\n",
      "Trained batch 811 batch loss 6.22609186 epoch total loss 6.33271027\n",
      "Trained batch 812 batch loss 6.44297075 epoch total loss 6.33284616\n",
      "Trained batch 813 batch loss 6.56459188 epoch total loss 6.33313084\n",
      "Trained batch 814 batch loss 6.29824305 epoch total loss 6.3330884\n",
      "Trained batch 815 batch loss 6.03341532 epoch total loss 6.33272028\n",
      "Trained batch 816 batch loss 6.37479162 epoch total loss 6.33277225\n",
      "Trained batch 817 batch loss 6.25950813 epoch total loss 6.33268213\n",
      "Trained batch 818 batch loss 6.7175312 epoch total loss 6.33315277\n",
      "Trained batch 819 batch loss 6.75068808 epoch total loss 6.33366251\n",
      "Trained batch 820 batch loss 6.39152908 epoch total loss 6.33373308\n",
      "Trained batch 821 batch loss 6.36939621 epoch total loss 6.33377695\n",
      "Trained batch 822 batch loss 6.42136717 epoch total loss 6.33388329\n",
      "Trained batch 823 batch loss 6.17772484 epoch total loss 6.3336935\n",
      "Trained batch 824 batch loss 6.06191158 epoch total loss 6.33336401\n",
      "Trained batch 825 batch loss 6.22253036 epoch total loss 6.33322954\n",
      "Trained batch 826 batch loss 6.34292364 epoch total loss 6.33324146\n",
      "Trained batch 827 batch loss 6.20714045 epoch total loss 6.33308887\n",
      "Trained batch 828 batch loss 6.45652485 epoch total loss 6.33323765\n",
      "Trained batch 829 batch loss 6.37714195 epoch total loss 6.33329058\n",
      "Trained batch 830 batch loss 6.25305414 epoch total loss 6.33319378\n",
      "Trained batch 831 batch loss 6.29840899 epoch total loss 6.33315182\n",
      "Trained batch 832 batch loss 5.64801931 epoch total loss 6.33232832\n",
      "Trained batch 833 batch loss 5.36348248 epoch total loss 6.33116484\n",
      "Trained batch 834 batch loss 5.06064749 epoch total loss 6.32964134\n",
      "Trained batch 835 batch loss 5.77409887 epoch total loss 6.32897568\n",
      "Trained batch 836 batch loss 6.4935236 epoch total loss 6.32917261\n",
      "Trained batch 837 batch loss 6.36978674 epoch total loss 6.32922125\n",
      "Trained batch 838 batch loss 6.48331976 epoch total loss 6.32940531\n",
      "Trained batch 839 batch loss 5.99972725 epoch total loss 6.32901192\n",
      "Trained batch 840 batch loss 6.58382177 epoch total loss 6.32931566\n",
      "Trained batch 841 batch loss 5.60808659 epoch total loss 6.32845783\n",
      "Trained batch 842 batch loss 6.08999538 epoch total loss 6.32817411\n",
      "Trained batch 843 batch loss 5.98694944 epoch total loss 6.32776928\n",
      "Trained batch 844 batch loss 6.02358723 epoch total loss 6.32740879\n",
      "Trained batch 845 batch loss 5.8319416 epoch total loss 6.32682276\n",
      "Trained batch 846 batch loss 5.96167612 epoch total loss 6.32639122\n",
      "Trained batch 847 batch loss 5.27945662 epoch total loss 6.32515478\n",
      "Trained batch 848 batch loss 6.13130283 epoch total loss 6.32492638\n",
      "Trained batch 849 batch loss 6.36295891 epoch total loss 6.3249712\n",
      "Trained batch 850 batch loss 6.27143478 epoch total loss 6.32490826\n",
      "Trained batch 851 batch loss 6.60937214 epoch total loss 6.32524252\n",
      "Trained batch 852 batch loss 6.52782774 epoch total loss 6.32548\n",
      "Trained batch 853 batch loss 6.92961836 epoch total loss 6.32618856\n",
      "Trained batch 854 batch loss 7.05162954 epoch total loss 6.32703829\n",
      "Trained batch 855 batch loss 6.83962631 epoch total loss 6.32763767\n",
      "Trained batch 856 batch loss 7.00492477 epoch total loss 6.32842922\n",
      "Trained batch 857 batch loss 6.63078 epoch total loss 6.32878208\n",
      "Trained batch 858 batch loss 6.37635088 epoch total loss 6.32883739\n",
      "Trained batch 859 batch loss 6.59393167 epoch total loss 6.32914591\n",
      "Trained batch 860 batch loss 6.04382944 epoch total loss 6.32881451\n",
      "Trained batch 861 batch loss 5.90629959 epoch total loss 6.32832336\n",
      "Trained batch 862 batch loss 5.76777363 epoch total loss 6.32767296\n",
      "Trained batch 863 batch loss 6.6597929 epoch total loss 6.32805777\n",
      "Trained batch 864 batch loss 6.38691759 epoch total loss 6.32812548\n",
      "Trained batch 865 batch loss 6.4185791 epoch total loss 6.32823\n",
      "Trained batch 866 batch loss 6.26726294 epoch total loss 6.32815933\n",
      "Trained batch 867 batch loss 6.37023115 epoch total loss 6.32820797\n",
      "Trained batch 868 batch loss 6.3006525 epoch total loss 6.32817602\n",
      "Trained batch 869 batch loss 6.49859524 epoch total loss 6.328372\n",
      "Trained batch 870 batch loss 6.6076088 epoch total loss 6.32869291\n",
      "Trained batch 871 batch loss 6.36396 epoch total loss 6.32873344\n",
      "Trained batch 872 batch loss 7.12538576 epoch total loss 6.32964706\n",
      "Trained batch 873 batch loss 6.82294226 epoch total loss 6.33021164\n",
      "Trained batch 874 batch loss 6.32843447 epoch total loss 6.33021\n",
      "Trained batch 875 batch loss 6.17825937 epoch total loss 6.33003616\n",
      "Trained batch 876 batch loss 6.05921316 epoch total loss 6.32972717\n",
      "Trained batch 877 batch loss 5.56027269 epoch total loss 6.32884932\n",
      "Trained batch 878 batch loss 5.85013199 epoch total loss 6.32830429\n",
      "Trained batch 879 batch loss 5.78393555 epoch total loss 6.32768488\n",
      "Trained batch 880 batch loss 5.67277098 epoch total loss 6.32694101\n",
      "Trained batch 881 batch loss 4.95138311 epoch total loss 6.32537937\n",
      "Trained batch 882 batch loss 4.92633057 epoch total loss 6.32379293\n",
      "Trained batch 883 batch loss 5.52526665 epoch total loss 6.32288885\n",
      "Trained batch 884 batch loss 5.82890797 epoch total loss 6.32233047\n",
      "Trained batch 885 batch loss 6.15120935 epoch total loss 6.32213688\n",
      "Trained batch 886 batch loss 6.67584085 epoch total loss 6.32253599\n",
      "Trained batch 887 batch loss 6.592453 epoch total loss 6.32284\n",
      "Trained batch 888 batch loss 6.97112226 epoch total loss 6.32357025\n",
      "Trained batch 889 batch loss 6.65959072 epoch total loss 6.32394838\n",
      "Trained batch 890 batch loss 6.65165 epoch total loss 6.32431698\n",
      "Trained batch 891 batch loss 6.19509363 epoch total loss 6.32417202\n",
      "Trained batch 892 batch loss 5.7480073 epoch total loss 6.32352638\n",
      "Trained batch 893 batch loss 5.52396584 epoch total loss 6.32263088\n",
      "Trained batch 894 batch loss 6.01079 epoch total loss 6.32228184\n",
      "Trained batch 895 batch loss 5.86254787 epoch total loss 6.32176828\n",
      "Trained batch 896 batch loss 6.15676 epoch total loss 6.32158375\n",
      "Trained batch 897 batch loss 5.88835049 epoch total loss 6.32110071\n",
      "Trained batch 898 batch loss 6.3289423 epoch total loss 6.32111\n",
      "Trained batch 899 batch loss 6.2629962 epoch total loss 6.3210454\n",
      "Trained batch 900 batch loss 6.3495717 epoch total loss 6.32107687\n",
      "Trained batch 901 batch loss 6.39197159 epoch total loss 6.32115555\n",
      "Trained batch 902 batch loss 6.36587858 epoch total loss 6.32120514\n",
      "Trained batch 903 batch loss 6.73945093 epoch total loss 6.32166815\n",
      "Trained batch 904 batch loss 5.37098598 epoch total loss 6.32061672\n",
      "Trained batch 905 batch loss 5.07546568 epoch total loss 6.31924105\n",
      "Trained batch 906 batch loss 5.11850166 epoch total loss 6.31791592\n",
      "Trained batch 907 batch loss 6.29181957 epoch total loss 6.31788731\n",
      "Trained batch 908 batch loss 7.05803871 epoch total loss 6.3187027\n",
      "Trained batch 909 batch loss 7.0708437 epoch total loss 6.31953\n",
      "Trained batch 910 batch loss 6.91089392 epoch total loss 6.32018\n",
      "Trained batch 911 batch loss 6.4085474 epoch total loss 6.32027721\n",
      "Trained batch 912 batch loss 6.09501362 epoch total loss 6.32003\n",
      "Trained batch 913 batch loss 6.38447523 epoch total loss 6.32010078\n",
      "Trained batch 914 batch loss 6.43330812 epoch total loss 6.32022429\n",
      "Trained batch 915 batch loss 5.91860485 epoch total loss 6.31978512\n",
      "Trained batch 916 batch loss 6.15765667 epoch total loss 6.31960821\n",
      "Trained batch 917 batch loss 6.15653133 epoch total loss 6.31943083\n",
      "Trained batch 918 batch loss 6.50134468 epoch total loss 6.31962919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 919 batch loss 6.47461748 epoch total loss 6.31979752\n",
      "Trained batch 920 batch loss 6.30984068 epoch total loss 6.31978703\n",
      "Trained batch 921 batch loss 5.3105669 epoch total loss 6.31869125\n",
      "Trained batch 922 batch loss 5.41665554 epoch total loss 6.31771278\n",
      "Trained batch 923 batch loss 5.72516155 epoch total loss 6.31707048\n",
      "Trained batch 924 batch loss 6.04489803 epoch total loss 6.31677628\n",
      "Trained batch 925 batch loss 5.84761143 epoch total loss 6.31626892\n",
      "Trained batch 926 batch loss 6.3627224 epoch total loss 6.31631947\n",
      "Trained batch 927 batch loss 6.27445126 epoch total loss 6.31627417\n",
      "Trained batch 928 batch loss 6.2897892 epoch total loss 6.31624508\n",
      "Trained batch 929 batch loss 6.39210749 epoch total loss 6.3163271\n",
      "Trained batch 930 batch loss 6.44633 epoch total loss 6.31646681\n",
      "Trained batch 931 batch loss 6.99479818 epoch total loss 6.31719494\n",
      "Trained batch 932 batch loss 7.1506896 epoch total loss 6.31808949\n",
      "Trained batch 933 batch loss 6.88717556 epoch total loss 6.31869936\n",
      "Trained batch 934 batch loss 6.23080921 epoch total loss 6.31860542\n",
      "Trained batch 935 batch loss 6.06741524 epoch total loss 6.31833696\n",
      "Trained batch 936 batch loss 6.390131 epoch total loss 6.31841373\n",
      "Trained batch 937 batch loss 6.0159421 epoch total loss 6.31809092\n",
      "Trained batch 938 batch loss 6.73314905 epoch total loss 6.31853342\n",
      "Trained batch 939 batch loss 6.45666265 epoch total loss 6.31868029\n",
      "Trained batch 940 batch loss 6.24995947 epoch total loss 6.31860733\n",
      "Trained batch 941 batch loss 6.42496777 epoch total loss 6.31872\n",
      "Trained batch 942 batch loss 6.4395318 epoch total loss 6.31884813\n",
      "Trained batch 943 batch loss 6.25387764 epoch total loss 6.31877947\n",
      "Trained batch 944 batch loss 6.15034056 epoch total loss 6.31860113\n",
      "Trained batch 945 batch loss 6.03863621 epoch total loss 6.31830454\n",
      "Trained batch 946 batch loss 6.16934204 epoch total loss 6.31814718\n",
      "Trained batch 947 batch loss 6.2144475 epoch total loss 6.31803751\n",
      "Trained batch 948 batch loss 6.07018328 epoch total loss 6.3177762\n",
      "Trained batch 949 batch loss 6.51747179 epoch total loss 6.31798697\n",
      "Trained batch 950 batch loss 6.26329803 epoch total loss 6.31792927\n",
      "Trained batch 951 batch loss 6.28086948 epoch total loss 6.31789\n",
      "Trained batch 952 batch loss 6.22892332 epoch total loss 6.31779671\n",
      "Trained batch 953 batch loss 6.34337378 epoch total loss 6.31782341\n",
      "Trained batch 954 batch loss 6.08558369 epoch total loss 6.31757975\n",
      "Trained batch 955 batch loss 5.6165514 epoch total loss 6.31684589\n",
      "Trained batch 956 batch loss 5.93624496 epoch total loss 6.31644773\n",
      "Trained batch 957 batch loss 6.39552164 epoch total loss 6.31653\n",
      "Trained batch 958 batch loss 6.25369406 epoch total loss 6.3164649\n",
      "Trained batch 959 batch loss 6.27504063 epoch total loss 6.31642151\n",
      "Trained batch 960 batch loss 6.31919098 epoch total loss 6.31642437\n",
      "Trained batch 961 batch loss 6.34054899 epoch total loss 6.31644964\n",
      "Trained batch 962 batch loss 6.32367659 epoch total loss 6.31645679\n",
      "Trained batch 963 batch loss 6.76991844 epoch total loss 6.31692791\n",
      "Trained batch 964 batch loss 6.57059956 epoch total loss 6.31719112\n",
      "Trained batch 965 batch loss 6.55703068 epoch total loss 6.31744\n",
      "Trained batch 966 batch loss 6.37615681 epoch total loss 6.31750059\n",
      "Trained batch 967 batch loss 6.52090359 epoch total loss 6.31771088\n",
      "Trained batch 968 batch loss 5.83783436 epoch total loss 6.31721544\n",
      "Trained batch 969 batch loss 6.42448473 epoch total loss 6.31732607\n",
      "Trained batch 970 batch loss 6.83589315 epoch total loss 6.3178606\n",
      "Trained batch 971 batch loss 6.66619682 epoch total loss 6.31821918\n",
      "Trained batch 972 batch loss 6.65652132 epoch total loss 6.31856728\n",
      "Trained batch 973 batch loss 5.66270685 epoch total loss 6.31789303\n",
      "Trained batch 974 batch loss 6.01088762 epoch total loss 6.31757784\n",
      "Trained batch 975 batch loss 6.42246246 epoch total loss 6.31768513\n",
      "Trained batch 976 batch loss 6.53602362 epoch total loss 6.31790924\n",
      "Trained batch 977 batch loss 6.39970875 epoch total loss 6.31799316\n",
      "Trained batch 978 batch loss 6.44384098 epoch total loss 6.31812191\n",
      "Trained batch 979 batch loss 6.56030846 epoch total loss 6.31836939\n",
      "Trained batch 980 batch loss 6.52556229 epoch total loss 6.31858063\n",
      "Trained batch 981 batch loss 6.40053511 epoch total loss 6.31866407\n",
      "Trained batch 982 batch loss 6.56327152 epoch total loss 6.31891346\n",
      "Trained batch 983 batch loss 6.58135128 epoch total loss 6.31918049\n",
      "Trained batch 984 batch loss 6.42683601 epoch total loss 6.31928968\n",
      "Trained batch 985 batch loss 6.46188641 epoch total loss 6.31943464\n",
      "Trained batch 986 batch loss 6.67123127 epoch total loss 6.31979132\n",
      "Trained batch 987 batch loss 6.53101063 epoch total loss 6.32000589\n",
      "Trained batch 988 batch loss 5.80294752 epoch total loss 6.31948233\n",
      "Trained batch 989 batch loss 6.11614704 epoch total loss 6.31927681\n",
      "Trained batch 990 batch loss 6.64386559 epoch total loss 6.31960487\n",
      "Trained batch 991 batch loss 6.53296328 epoch total loss 6.3198204\n",
      "Trained batch 992 batch loss 6.22640848 epoch total loss 6.31972647\n",
      "Trained batch 993 batch loss 6.50160503 epoch total loss 6.3199091\n",
      "Trained batch 994 batch loss 6.67462444 epoch total loss 6.32026625\n",
      "Trained batch 995 batch loss 6.66519594 epoch total loss 6.32061291\n",
      "Trained batch 996 batch loss 6.74925041 epoch total loss 6.32104301\n",
      "Trained batch 997 batch loss 6.40690947 epoch total loss 6.32112885\n",
      "Trained batch 998 batch loss 5.8790288 epoch total loss 6.32068586\n",
      "Trained batch 999 batch loss 6.46850824 epoch total loss 6.32083416\n",
      "Trained batch 1000 batch loss 6.80781889 epoch total loss 6.32132101\n",
      "Trained batch 1001 batch loss 7.03507662 epoch total loss 6.32203388\n",
      "Trained batch 1002 batch loss 7.02190113 epoch total loss 6.32273245\n",
      "Trained batch 1003 batch loss 7.01442862 epoch total loss 6.32342243\n",
      "Trained batch 1004 batch loss 7.07329464 epoch total loss 6.32416916\n",
      "Trained batch 1005 batch loss 6.48326683 epoch total loss 6.32432747\n",
      "Trained batch 1006 batch loss 6.24417877 epoch total loss 6.32424784\n",
      "Trained batch 1007 batch loss 6.47575378 epoch total loss 6.32439804\n",
      "Trained batch 1008 batch loss 6.63167143 epoch total loss 6.32470322\n",
      "Trained batch 1009 batch loss 6.14874601 epoch total loss 6.32452917\n",
      "Trained batch 1010 batch loss 6.04674816 epoch total loss 6.32425404\n",
      "Trained batch 1011 batch loss 6.7027812 epoch total loss 6.32462835\n",
      "Trained batch 1012 batch loss 6.17229795 epoch total loss 6.32447767\n",
      "Trained batch 1013 batch loss 6.22657633 epoch total loss 6.32438135\n",
      "Trained batch 1014 batch loss 6.22798586 epoch total loss 6.32428598\n",
      "Trained batch 1015 batch loss 6.93367052 epoch total loss 6.32488632\n",
      "Trained batch 1016 batch loss 6.51613522 epoch total loss 6.32507467\n",
      "Trained batch 1017 batch loss 6.86205339 epoch total loss 6.32560253\n",
      "Trained batch 1018 batch loss 6.69813204 epoch total loss 6.32596874\n",
      "Trained batch 1019 batch loss 6.79207659 epoch total loss 6.32642603\n",
      "Trained batch 1020 batch loss 6.57587624 epoch total loss 6.32667\n",
      "Trained batch 1021 batch loss 6.26361179 epoch total loss 6.32660866\n",
      "Trained batch 1022 batch loss 6.39019537 epoch total loss 6.32667065\n",
      "Trained batch 1023 batch loss 5.78328276 epoch total loss 6.32613945\n",
      "Trained batch 1024 batch loss 5.71130228 epoch total loss 6.32553911\n",
      "Trained batch 1025 batch loss 6.22495127 epoch total loss 6.32544088\n",
      "Trained batch 1026 batch loss 5.92950916 epoch total loss 6.3250556\n",
      "Trained batch 1027 batch loss 5.61453676 epoch total loss 6.32436371\n",
      "Trained batch 1028 batch loss 5.38450432 epoch total loss 6.32344913\n",
      "Trained batch 1029 batch loss 5.34688568 epoch total loss 6.3225\n",
      "Trained batch 1030 batch loss 5.74435663 epoch total loss 6.32193851\n",
      "Trained batch 1031 batch loss 6.02344084 epoch total loss 6.32164907\n",
      "Trained batch 1032 batch loss 5.63571739 epoch total loss 6.32098436\n",
      "Trained batch 1033 batch loss 6.1744895 epoch total loss 6.32084227\n",
      "Trained batch 1034 batch loss 6.10035086 epoch total loss 6.3206296\n",
      "Trained batch 1035 batch loss 5.95421314 epoch total loss 6.32027531\n",
      "Trained batch 1036 batch loss 5.85067415 epoch total loss 6.31982183\n",
      "Trained batch 1037 batch loss 5.69379711 epoch total loss 6.31921816\n",
      "Trained batch 1038 batch loss 6.25887442 epoch total loss 6.31916\n",
      "Trained batch 1039 batch loss 6.58343458 epoch total loss 6.31941462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1040 batch loss 6.75440788 epoch total loss 6.3198328\n",
      "Trained batch 1041 batch loss 6.59644604 epoch total loss 6.3200984\n",
      "Trained batch 1042 batch loss 6.57557678 epoch total loss 6.32034397\n",
      "Trained batch 1043 batch loss 6.56837702 epoch total loss 6.32058191\n",
      "Trained batch 1044 batch loss 6.50267506 epoch total loss 6.32075596\n",
      "Trained batch 1045 batch loss 6.45195913 epoch total loss 6.32088184\n",
      "Trained batch 1046 batch loss 6.50846529 epoch total loss 6.32106066\n",
      "Trained batch 1047 batch loss 6.53657913 epoch total loss 6.32126665\n",
      "Trained batch 1048 batch loss 6.4885 epoch total loss 6.32142591\n",
      "Trained batch 1049 batch loss 6.6268568 epoch total loss 6.32171726\n",
      "Trained batch 1050 batch loss 6.66651917 epoch total loss 6.3220458\n",
      "Trained batch 1051 batch loss 6.36676836 epoch total loss 6.32208824\n",
      "Trained batch 1052 batch loss 6.5506773 epoch total loss 6.32230568\n",
      "Trained batch 1053 batch loss 6.51249886 epoch total loss 6.3224864\n",
      "Trained batch 1054 batch loss 6.36417 epoch total loss 6.32252598\n",
      "Trained batch 1055 batch loss 6.30657721 epoch total loss 6.32251072\n",
      "Trained batch 1056 batch loss 6.45921755 epoch total loss 6.32264\n",
      "Trained batch 1057 batch loss 6.34537506 epoch total loss 6.3226614\n",
      "Trained batch 1058 batch loss 6.19485092 epoch total loss 6.32254076\n",
      "Trained batch 1059 batch loss 6.44571257 epoch total loss 6.32265711\n",
      "Trained batch 1060 batch loss 6.48119259 epoch total loss 6.32280636\n",
      "Trained batch 1061 batch loss 6.3396678 epoch total loss 6.32282257\n",
      "Trained batch 1062 batch loss 6.4094758 epoch total loss 6.32290411\n",
      "Trained batch 1063 batch loss 6.41021061 epoch total loss 6.32298613\n",
      "Trained batch 1064 batch loss 6.43093681 epoch total loss 6.32308817\n",
      "Trained batch 1065 batch loss 6.09136581 epoch total loss 6.32287025\n",
      "Trained batch 1066 batch loss 6.09683371 epoch total loss 6.32265806\n",
      "Trained batch 1067 batch loss 6.2768631 epoch total loss 6.32261515\n",
      "Trained batch 1068 batch loss 6.45215034 epoch total loss 6.32273674\n",
      "Trained batch 1069 batch loss 6.41374922 epoch total loss 6.32282162\n",
      "Trained batch 1070 batch loss 6.39777231 epoch total loss 6.32289171\n",
      "Trained batch 1071 batch loss 5.91298485 epoch total loss 6.32250929\n",
      "Trained batch 1072 batch loss 6.19206142 epoch total loss 6.32238722\n",
      "Trained batch 1073 batch loss 6.3356266 epoch total loss 6.32239962\n",
      "Trained batch 1074 batch loss 6.17487764 epoch total loss 6.32226181\n",
      "Trained batch 1075 batch loss 6.30247 epoch total loss 6.32224321\n",
      "Trained batch 1076 batch loss 5.85962582 epoch total loss 6.32181358\n",
      "Trained batch 1077 batch loss 5.85614 epoch total loss 6.32138109\n",
      "Trained batch 1078 batch loss 6.38048601 epoch total loss 6.32143593\n",
      "Trained batch 1079 batch loss 6.45985842 epoch total loss 6.3215642\n",
      "Trained batch 1080 batch loss 6.49430227 epoch total loss 6.32172394\n",
      "Trained batch 1081 batch loss 6.53608179 epoch total loss 6.3219223\n",
      "Trained batch 1082 batch loss 6.56627512 epoch total loss 6.32214832\n",
      "Trained batch 1083 batch loss 6.02386189 epoch total loss 6.32187271\n",
      "Trained batch 1084 batch loss 6.20402861 epoch total loss 6.32176447\n",
      "Trained batch 1085 batch loss 6.02328253 epoch total loss 6.32148933\n",
      "Trained batch 1086 batch loss 5.99920511 epoch total loss 6.32119226\n",
      "Trained batch 1087 batch loss 6.33062792 epoch total loss 6.32120085\n",
      "Trained batch 1088 batch loss 6.17316055 epoch total loss 6.32106495\n",
      "Trained batch 1089 batch loss 6.04832125 epoch total loss 6.32081461\n",
      "Trained batch 1090 batch loss 5.87901068 epoch total loss 6.3204093\n",
      "Trained batch 1091 batch loss 6.06356812 epoch total loss 6.32017374\n",
      "Trained batch 1092 batch loss 6.08014202 epoch total loss 6.31995392\n",
      "Trained batch 1093 batch loss 6.16620922 epoch total loss 6.31981325\n",
      "Trained batch 1094 batch loss 6.07811832 epoch total loss 6.319592\n",
      "Trained batch 1095 batch loss 5.82003975 epoch total loss 6.31913567\n",
      "Trained batch 1096 batch loss 6.4334383 epoch total loss 6.31924\n",
      "Trained batch 1097 batch loss 6.72617149 epoch total loss 6.31961107\n",
      "Trained batch 1098 batch loss 6.52815151 epoch total loss 6.31980133\n",
      "Trained batch 1099 batch loss 6.3597331 epoch total loss 6.31983757\n",
      "Trained batch 1100 batch loss 5.89450693 epoch total loss 6.31945086\n",
      "Trained batch 1101 batch loss 6.42851448 epoch total loss 6.31955\n",
      "Trained batch 1102 batch loss 6.30891895 epoch total loss 6.3195405\n",
      "Trained batch 1103 batch loss 6.15621805 epoch total loss 6.31939268\n",
      "Trained batch 1104 batch loss 6.41122437 epoch total loss 6.31947565\n",
      "Trained batch 1105 batch loss 5.89945126 epoch total loss 6.31909561\n",
      "Trained batch 1106 batch loss 6.31131601 epoch total loss 6.31908894\n",
      "Trained batch 1107 batch loss 6.18399906 epoch total loss 6.31896687\n",
      "Trained batch 1108 batch loss 6.03581905 epoch total loss 6.31871128\n",
      "Trained batch 1109 batch loss 6.1734705 epoch total loss 6.31858\n",
      "Trained batch 1110 batch loss 6.22894 epoch total loss 6.31849909\n",
      "Trained batch 1111 batch loss 6.56977797 epoch total loss 6.31872559\n",
      "Trained batch 1112 batch loss 6.87786913 epoch total loss 6.31922817\n",
      "Trained batch 1113 batch loss 6.52337933 epoch total loss 6.31941175\n",
      "Trained batch 1114 batch loss 7.06277847 epoch total loss 6.32007933\n",
      "Trained batch 1115 batch loss 6.86959648 epoch total loss 6.32057238\n",
      "Trained batch 1116 batch loss 6.40870047 epoch total loss 6.32065105\n",
      "Trained batch 1117 batch loss 6.49432611 epoch total loss 6.3208065\n",
      "Trained batch 1118 batch loss 6.29399776 epoch total loss 6.32078266\n",
      "Trained batch 1119 batch loss 6.6662612 epoch total loss 6.32109118\n",
      "Trained batch 1120 batch loss 6.39861202 epoch total loss 6.32116032\n",
      "Trained batch 1121 batch loss 6.55669403 epoch total loss 6.3213706\n",
      "Trained batch 1122 batch loss 6.6959672 epoch total loss 6.32170439\n",
      "Trained batch 1123 batch loss 6.63385296 epoch total loss 6.32198191\n",
      "Trained batch 1124 batch loss 6.57188606 epoch total loss 6.32220459\n",
      "Trained batch 1125 batch loss 6.09794569 epoch total loss 6.32200527\n",
      "Trained batch 1126 batch loss 6.37702131 epoch total loss 6.32205391\n",
      "Trained batch 1127 batch loss 6.18989801 epoch total loss 6.32193661\n",
      "Trained batch 1128 batch loss 6.11629295 epoch total loss 6.32175446\n",
      "Trained batch 1129 batch loss 6.36526728 epoch total loss 6.32179308\n",
      "Trained batch 1130 batch loss 6.31329298 epoch total loss 6.32178545\n",
      "Trained batch 1131 batch loss 6.15602 epoch total loss 6.32163906\n",
      "Trained batch 1132 batch loss 6.52505541 epoch total loss 6.32181883\n",
      "Trained batch 1133 batch loss 6.57941294 epoch total loss 6.32204628\n",
      "Trained batch 1134 batch loss 6.54715681 epoch total loss 6.32224512\n",
      "Trained batch 1135 batch loss 6.46447086 epoch total loss 6.32237\n",
      "Trained batch 1136 batch loss 6.21947622 epoch total loss 6.32227945\n",
      "Trained batch 1137 batch loss 6.06788397 epoch total loss 6.32205582\n",
      "Trained batch 1138 batch loss 6.38415241 epoch total loss 6.32211\n",
      "Trained batch 1139 batch loss 6.29045153 epoch total loss 6.32208252\n",
      "Trained batch 1140 batch loss 6.02761459 epoch total loss 6.32182455\n",
      "Trained batch 1141 batch loss 6.35363817 epoch total loss 6.32185221\n",
      "Trained batch 1142 batch loss 6.3110733 epoch total loss 6.32184267\n",
      "Trained batch 1143 batch loss 6.6111331 epoch total loss 6.32209587\n",
      "Trained batch 1144 batch loss 6.19232368 epoch total loss 6.32198286\n",
      "Trained batch 1145 batch loss 6.02847576 epoch total loss 6.32172632\n",
      "Trained batch 1146 batch loss 6.14562559 epoch total loss 6.3215723\n",
      "Trained batch 1147 batch loss 6.53421211 epoch total loss 6.32175779\n",
      "Trained batch 1148 batch loss 6.46436691 epoch total loss 6.32188177\n",
      "Trained batch 1149 batch loss 6.45986843 epoch total loss 6.32200193\n",
      "Trained batch 1150 batch loss 5.92608213 epoch total loss 6.32165813\n",
      "Trained batch 1151 batch loss 6.16897 epoch total loss 6.32152557\n",
      "Trained batch 1152 batch loss 6.42114115 epoch total loss 6.3216114\n",
      "Trained batch 1153 batch loss 6.54021168 epoch total loss 6.32180119\n",
      "Trained batch 1154 batch loss 6.48930693 epoch total loss 6.32194614\n",
      "Trained batch 1155 batch loss 6.434021 epoch total loss 6.32204342\n",
      "Trained batch 1156 batch loss 6.72773027 epoch total loss 6.32239389\n",
      "Trained batch 1157 batch loss 6.59861279 epoch total loss 6.32263279\n",
      "Trained batch 1158 batch loss 6.2821126 epoch total loss 6.32259798\n",
      "Trained batch 1159 batch loss 6.26840591 epoch total loss 6.32255125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1160 batch loss 5.86216164 epoch total loss 6.32215452\n",
      "Trained batch 1161 batch loss 5.90520334 epoch total loss 6.32179546\n",
      "Trained batch 1162 batch loss 6.53522158 epoch total loss 6.32197905\n",
      "Trained batch 1163 batch loss 6.29069233 epoch total loss 6.32195187\n",
      "Trained batch 1164 batch loss 6.64503336 epoch total loss 6.32222939\n",
      "Trained batch 1165 batch loss 6.49064064 epoch total loss 6.32237434\n",
      "Trained batch 1166 batch loss 6.11467171 epoch total loss 6.32219601\n",
      "Trained batch 1167 batch loss 6.53948355 epoch total loss 6.32238245\n",
      "Trained batch 1168 batch loss 6.60574722 epoch total loss 6.32262516\n",
      "Trained batch 1169 batch loss 6.63696194 epoch total loss 6.32289362\n",
      "Trained batch 1170 batch loss 6.48596287 epoch total loss 6.32303333\n",
      "Trained batch 1171 batch loss 6.55699539 epoch total loss 6.32323313\n",
      "Trained batch 1172 batch loss 6.03549671 epoch total loss 6.32298756\n",
      "Trained batch 1173 batch loss 6.17860556 epoch total loss 6.32286453\n",
      "Trained batch 1174 batch loss 6.3779664 epoch total loss 6.32291174\n",
      "Trained batch 1175 batch loss 6.45890474 epoch total loss 6.32302713\n",
      "Trained batch 1176 batch loss 6.86822271 epoch total loss 6.3234911\n",
      "Trained batch 1177 batch loss 7.01370716 epoch total loss 6.32407713\n",
      "Trained batch 1178 batch loss 6.73834181 epoch total loss 6.32442904\n",
      "Trained batch 1179 batch loss 6.16313648 epoch total loss 6.32429218\n",
      "Trained batch 1180 batch loss 6.08810902 epoch total loss 6.32409191\n",
      "Trained batch 1181 batch loss 6.37150955 epoch total loss 6.32413197\n",
      "Trained batch 1182 batch loss 6.41015339 epoch total loss 6.32420492\n",
      "Trained batch 1183 batch loss 6.24632549 epoch total loss 6.32413864\n",
      "Trained batch 1184 batch loss 6.34116936 epoch total loss 6.32415295\n",
      "Trained batch 1185 batch loss 6.36698246 epoch total loss 6.32418966\n",
      "Trained batch 1186 batch loss 6.23893833 epoch total loss 6.32411766\n",
      "Trained batch 1187 batch loss 6.053689 epoch total loss 6.32388973\n",
      "Trained batch 1188 batch loss 6.30612564 epoch total loss 6.32387495\n",
      "Trained batch 1189 batch loss 6.29875278 epoch total loss 6.32385349\n",
      "Trained batch 1190 batch loss 6.23622942 epoch total loss 6.32378\n",
      "Trained batch 1191 batch loss 6.281106 epoch total loss 6.3237443\n",
      "Trained batch 1192 batch loss 6.1064291 epoch total loss 6.32356215\n",
      "Trained batch 1193 batch loss 6.61995649 epoch total loss 6.32381058\n",
      "Trained batch 1194 batch loss 6.44198275 epoch total loss 6.32390976\n",
      "Trained batch 1195 batch loss 6.73098135 epoch total loss 6.32425\n",
      "Trained batch 1196 batch loss 6.46078873 epoch total loss 6.32436466\n",
      "Trained batch 1197 batch loss 6.42574501 epoch total loss 6.32444906\n",
      "Trained batch 1198 batch loss 6.07535553 epoch total loss 6.32424116\n",
      "Trained batch 1199 batch loss 6.24689341 epoch total loss 6.32417679\n",
      "Trained batch 1200 batch loss 6.11705542 epoch total loss 6.32400417\n",
      "Trained batch 1201 batch loss 6.1328578 epoch total loss 6.32384491\n",
      "Trained batch 1202 batch loss 6.26698875 epoch total loss 6.3237977\n",
      "Trained batch 1203 batch loss 6.05968189 epoch total loss 6.32357836\n",
      "Trained batch 1204 batch loss 6.62162542 epoch total loss 6.32382584\n",
      "Trained batch 1205 batch loss 6.27787209 epoch total loss 6.32378769\n",
      "Trained batch 1206 batch loss 6.54218626 epoch total loss 6.32396841\n",
      "Trained batch 1207 batch loss 6.52212334 epoch total loss 6.32413244\n",
      "Trained batch 1208 batch loss 6.68541431 epoch total loss 6.3244319\n",
      "Trained batch 1209 batch loss 6.62173605 epoch total loss 6.32467747\n",
      "Trained batch 1210 batch loss 6.60293722 epoch total loss 6.32490778\n",
      "Trained batch 1211 batch loss 6.27821493 epoch total loss 6.32486916\n",
      "Trained batch 1212 batch loss 6.40377951 epoch total loss 6.32493448\n",
      "Trained batch 1213 batch loss 5.81991434 epoch total loss 6.32451773\n",
      "Trained batch 1214 batch loss 6.30963516 epoch total loss 6.32450533\n",
      "Trained batch 1215 batch loss 7.02690792 epoch total loss 6.32508373\n",
      "Trained batch 1216 batch loss 6.60608053 epoch total loss 6.32531452\n",
      "Trained batch 1217 batch loss 6.47415304 epoch total loss 6.32543707\n",
      "Trained batch 1218 batch loss 6.54910183 epoch total loss 6.32562065\n",
      "Trained batch 1219 batch loss 6.38479 epoch total loss 6.32566929\n",
      "Trained batch 1220 batch loss 6.81876326 epoch total loss 6.32607365\n",
      "Trained batch 1221 batch loss 6.26586819 epoch total loss 6.32602406\n",
      "Trained batch 1222 batch loss 6.43897104 epoch total loss 6.32611656\n",
      "Trained batch 1223 batch loss 6.46350479 epoch total loss 6.32622862\n",
      "Trained batch 1224 batch loss 6.47151899 epoch total loss 6.32634735\n",
      "Trained batch 1225 batch loss 6.25059891 epoch total loss 6.32628536\n",
      "Trained batch 1226 batch loss 6.36109161 epoch total loss 6.32631397\n",
      "Trained batch 1227 batch loss 6.52243137 epoch total loss 6.32647371\n",
      "Trained batch 1228 batch loss 6.76592207 epoch total loss 6.32683182\n",
      "Trained batch 1229 batch loss 6.58071232 epoch total loss 6.32703829\n",
      "Trained batch 1230 batch loss 6.50743151 epoch total loss 6.32718515\n",
      "Trained batch 1231 batch loss 6.35172844 epoch total loss 6.3272047\n",
      "Trained batch 1232 batch loss 6.54694939 epoch total loss 6.32738304\n",
      "Trained batch 1233 batch loss 6.22111177 epoch total loss 6.32729673\n",
      "Trained batch 1234 batch loss 5.81440878 epoch total loss 6.32688141\n",
      "Trained batch 1235 batch loss 6.26204252 epoch total loss 6.32682896\n",
      "Trained batch 1236 batch loss 6.46719027 epoch total loss 6.32694244\n",
      "Trained batch 1237 batch loss 6.34644699 epoch total loss 6.32695866\n",
      "Trained batch 1238 batch loss 6.69607 epoch total loss 6.32725668\n",
      "Trained batch 1239 batch loss 7.08019352 epoch total loss 6.32786465\n",
      "Trained batch 1240 batch loss 7.01736164 epoch total loss 6.32842064\n",
      "Trained batch 1241 batch loss 6.91871643 epoch total loss 6.32889652\n",
      "Trained batch 1242 batch loss 6.86372662 epoch total loss 6.32932711\n",
      "Trained batch 1243 batch loss 6.1624465 epoch total loss 6.32919312\n",
      "Trained batch 1244 batch loss 6.18964434 epoch total loss 6.32908058\n",
      "Trained batch 1245 batch loss 6.7055521 epoch total loss 6.32938337\n",
      "Trained batch 1246 batch loss 7.08139944 epoch total loss 6.32998705\n",
      "Trained batch 1247 batch loss 6.63308477 epoch total loss 6.33023\n",
      "Trained batch 1248 batch loss 6.52018404 epoch total loss 6.33038235\n",
      "Trained batch 1249 batch loss 6.56429672 epoch total loss 6.33056974\n",
      "Trained batch 1250 batch loss 6.4799118 epoch total loss 6.33068895\n",
      "Trained batch 1251 batch loss 6.43665886 epoch total loss 6.33077383\n",
      "Trained batch 1252 batch loss 6.85331154 epoch total loss 6.33119106\n",
      "Trained batch 1253 batch loss 7.02399349 epoch total loss 6.33174419\n",
      "Trained batch 1254 batch loss 6.78143692 epoch total loss 6.3321023\n",
      "Trained batch 1255 batch loss 6.55567837 epoch total loss 6.33228064\n",
      "Trained batch 1256 batch loss 6.55021334 epoch total loss 6.3324542\n",
      "Trained batch 1257 batch loss 6.88164902 epoch total loss 6.33289146\n",
      "Trained batch 1258 batch loss 6.9178834 epoch total loss 6.33335638\n",
      "Trained batch 1259 batch loss 6.71993113 epoch total loss 6.33366346\n",
      "Trained batch 1260 batch loss 5.74172878 epoch total loss 6.3331933\n",
      "Trained batch 1261 batch loss 6.22348309 epoch total loss 6.33310652\n",
      "Trained batch 1262 batch loss 6.54669571 epoch total loss 6.33327579\n",
      "Trained batch 1263 batch loss 6.40986252 epoch total loss 6.33333635\n",
      "Trained batch 1264 batch loss 6.5288105 epoch total loss 6.33349085\n",
      "Trained batch 1265 batch loss 6.24022961 epoch total loss 6.33341742\n",
      "Trained batch 1266 batch loss 6.37348175 epoch total loss 6.33344889\n",
      "Trained batch 1267 batch loss 6.60885763 epoch total loss 6.33366632\n",
      "Trained batch 1268 batch loss 6.75367498 epoch total loss 6.33399773\n",
      "Trained batch 1269 batch loss 6.60483313 epoch total loss 6.33421135\n",
      "Trained batch 1270 batch loss 6.50192213 epoch total loss 6.33434343\n",
      "Trained batch 1271 batch loss 6.2807126 epoch total loss 6.33430147\n",
      "Trained batch 1272 batch loss 6.24424791 epoch total loss 6.33423042\n",
      "Trained batch 1273 batch loss 6.3281312 epoch total loss 6.33422565\n",
      "Trained batch 1274 batch loss 6.01137066 epoch total loss 6.33397198\n",
      "Trained batch 1275 batch loss 5.92238092 epoch total loss 6.33364916\n",
      "Trained batch 1276 batch loss 6.20480537 epoch total loss 6.33354807\n",
      "Trained batch 1277 batch loss 6.28913355 epoch total loss 6.33351326\n",
      "Trained batch 1278 batch loss 6.34981632 epoch total loss 6.33352566\n",
      "Trained batch 1279 batch loss 6.47109365 epoch total loss 6.33363342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1280 batch loss 6.79961109 epoch total loss 6.33399773\n",
      "Trained batch 1281 batch loss 7.16808033 epoch total loss 6.33464861\n",
      "Trained batch 1282 batch loss 6.88359976 epoch total loss 6.33507729\n",
      "Trained batch 1283 batch loss 6.24061537 epoch total loss 6.33500338\n",
      "Trained batch 1284 batch loss 6.46625233 epoch total loss 6.3351059\n",
      "Trained batch 1285 batch loss 6.17440605 epoch total loss 6.33498049\n",
      "Trained batch 1286 batch loss 5.95212603 epoch total loss 6.33468294\n",
      "Trained batch 1287 batch loss 6.1655941 epoch total loss 6.33455133\n",
      "Trained batch 1288 batch loss 6.31977177 epoch total loss 6.33454037\n",
      "Trained batch 1289 batch loss 6.5228281 epoch total loss 6.33468628\n",
      "Trained batch 1290 batch loss 6.51620817 epoch total loss 6.33482695\n",
      "Trained batch 1291 batch loss 6.48799372 epoch total loss 6.3349452\n",
      "Trained batch 1292 batch loss 6.65873814 epoch total loss 6.33519602\n",
      "Trained batch 1293 batch loss 6.19890547 epoch total loss 6.33509064\n",
      "Trained batch 1294 batch loss 6.30073261 epoch total loss 6.33506346\n",
      "Trained batch 1295 batch loss 6.21937466 epoch total loss 6.33497429\n",
      "Trained batch 1296 batch loss 6.48697233 epoch total loss 6.33509207\n",
      "Trained batch 1297 batch loss 6.44194508 epoch total loss 6.33517456\n",
      "Trained batch 1298 batch loss 6.26202631 epoch total loss 6.33511829\n",
      "Trained batch 1299 batch loss 6.39012 epoch total loss 6.33516026\n",
      "Trained batch 1300 batch loss 6.35601902 epoch total loss 6.33517647\n",
      "Trained batch 1301 batch loss 6.17142916 epoch total loss 6.33505106\n",
      "Trained batch 1302 batch loss 6.37231112 epoch total loss 6.33507967\n",
      "Trained batch 1303 batch loss 6.3447032 epoch total loss 6.33508682\n",
      "Trained batch 1304 batch loss 6.23449 epoch total loss 6.33500957\n",
      "Trained batch 1305 batch loss 5.9322381 epoch total loss 6.33470106\n",
      "Trained batch 1306 batch loss 6.3274188 epoch total loss 6.33469534\n",
      "Trained batch 1307 batch loss 6.21778 epoch total loss 6.33460617\n",
      "Trained batch 1308 batch loss 6.06998205 epoch total loss 6.33440399\n",
      "Trained batch 1309 batch loss 6.11440039 epoch total loss 6.33423567\n",
      "Trained batch 1310 batch loss 6.60142422 epoch total loss 6.33443975\n",
      "Trained batch 1311 batch loss 6.4039278 epoch total loss 6.33449316\n",
      "Trained batch 1312 batch loss 6.26641273 epoch total loss 6.33444118\n",
      "Trained batch 1313 batch loss 6.40555334 epoch total loss 6.33449554\n",
      "Trained batch 1314 batch loss 6.62384701 epoch total loss 6.33471584\n",
      "Trained batch 1315 batch loss 6.6827774 epoch total loss 6.33498\n",
      "Trained batch 1316 batch loss 6.73900557 epoch total loss 6.33528757\n",
      "Trained batch 1317 batch loss 6.92052126 epoch total loss 6.33573198\n",
      "Trained batch 1318 batch loss 5.84909248 epoch total loss 6.33536243\n",
      "Trained batch 1319 batch loss 5.37426805 epoch total loss 6.33463383\n",
      "Trained batch 1320 batch loss 6.26759 epoch total loss 6.33458281\n",
      "Trained batch 1321 batch loss 6.34811068 epoch total loss 6.33459282\n",
      "Trained batch 1322 batch loss 6.68904257 epoch total loss 6.33486128\n",
      "Trained batch 1323 batch loss 6.32698441 epoch total loss 6.33485556\n",
      "Trained batch 1324 batch loss 5.73189783 epoch total loss 6.3343997\n",
      "Trained batch 1325 batch loss 6.41879 epoch total loss 6.3344636\n",
      "Trained batch 1326 batch loss 6.44418573 epoch total loss 6.33454609\n",
      "Trained batch 1327 batch loss 6.19324112 epoch total loss 6.33443975\n",
      "Trained batch 1328 batch loss 6.57958937 epoch total loss 6.33462429\n",
      "Trained batch 1329 batch loss 6.34151554 epoch total loss 6.33462954\n",
      "Trained batch 1330 batch loss 6.52292395 epoch total loss 6.33477068\n",
      "Trained batch 1331 batch loss 6.48108673 epoch total loss 6.33488083\n",
      "Trained batch 1332 batch loss 6.55487442 epoch total loss 6.33504581\n",
      "Trained batch 1333 batch loss 6.25719166 epoch total loss 6.33498716\n",
      "Trained batch 1334 batch loss 6.45886 epoch total loss 6.33508\n",
      "Trained batch 1335 batch loss 6.53430414 epoch total loss 6.3352294\n",
      "Trained batch 1336 batch loss 6.24114084 epoch total loss 6.33515882\n",
      "Trained batch 1337 batch loss 6.01616526 epoch total loss 6.33492088\n",
      "Trained batch 1338 batch loss 5.50511932 epoch total loss 6.33430052\n",
      "Trained batch 1339 batch loss 5.53737307 epoch total loss 6.33370495\n",
      "Trained batch 1340 batch loss 5.74635696 epoch total loss 6.33326674\n",
      "Trained batch 1341 batch loss 6.43769217 epoch total loss 6.33334446\n",
      "Trained batch 1342 batch loss 6.45655251 epoch total loss 6.33343649\n",
      "Trained batch 1343 batch loss 6.73456621 epoch total loss 6.33373499\n",
      "Trained batch 1344 batch loss 6.90289402 epoch total loss 6.3341589\n",
      "Trained batch 1345 batch loss 6.94152403 epoch total loss 6.33461046\n",
      "Trained batch 1346 batch loss 6.33106089 epoch total loss 6.3346076\n",
      "Trained batch 1347 batch loss 6.35764027 epoch total loss 6.33462477\n",
      "Trained batch 1348 batch loss 6.36362028 epoch total loss 6.33464575\n",
      "Trained batch 1349 batch loss 6.46780348 epoch total loss 6.33474445\n",
      "Trained batch 1350 batch loss 6.39000845 epoch total loss 6.33478498\n",
      "Trained batch 1351 batch loss 6.36399746 epoch total loss 6.33480692\n",
      "Trained batch 1352 batch loss 5.93195724 epoch total loss 6.3345089\n",
      "Trained batch 1353 batch loss 6.16449118 epoch total loss 6.33438301\n",
      "Trained batch 1354 batch loss 6.42027664 epoch total loss 6.33444595\n",
      "Trained batch 1355 batch loss 6.77704525 epoch total loss 6.33477306\n",
      "Trained batch 1356 batch loss 6.82090282 epoch total loss 6.33513165\n",
      "Trained batch 1357 batch loss 6.66422462 epoch total loss 6.33537388\n",
      "Trained batch 1358 batch loss 6.56480932 epoch total loss 6.33554268\n",
      "Trained batch 1359 batch loss 6.77976847 epoch total loss 6.33586931\n",
      "Trained batch 1360 batch loss 6.80038357 epoch total loss 6.3362112\n",
      "Trained batch 1361 batch loss 6.50793219 epoch total loss 6.33633709\n",
      "Trained batch 1362 batch loss 6.58165836 epoch total loss 6.33651733\n",
      "Trained batch 1363 batch loss 6.60718727 epoch total loss 6.33671618\n",
      "Trained batch 1364 batch loss 6.8827796 epoch total loss 6.33711672\n",
      "Trained batch 1365 batch loss 6.55085182 epoch total loss 6.33727312\n",
      "Trained batch 1366 batch loss 6.46500731 epoch total loss 6.33736658\n",
      "Trained batch 1367 batch loss 6.40918827 epoch total loss 6.33741903\n",
      "Trained batch 1368 batch loss 6.46254635 epoch total loss 6.33751106\n",
      "Trained batch 1369 batch loss 6.54868793 epoch total loss 6.33766508\n",
      "Trained batch 1370 batch loss 6.48817396 epoch total loss 6.33777523\n",
      "Trained batch 1371 batch loss 6.33842421 epoch total loss 6.33777571\n",
      "Trained batch 1372 batch loss 6.47338104 epoch total loss 6.33787489\n",
      "Trained batch 1373 batch loss 6.43878 epoch total loss 6.33794832\n",
      "Trained batch 1374 batch loss 6.28427124 epoch total loss 6.33790922\n",
      "Trained batch 1375 batch loss 6.35342169 epoch total loss 6.33792067\n",
      "Trained batch 1376 batch loss 5.99044847 epoch total loss 6.33766794\n",
      "Trained batch 1377 batch loss 5.61028814 epoch total loss 6.33713961\n",
      "Trained batch 1378 batch loss 6.50840187 epoch total loss 6.33726406\n",
      "Trained batch 1379 batch loss 6.16467333 epoch total loss 6.33713913\n",
      "Trained batch 1380 batch loss 6.2368679 epoch total loss 6.33706713\n",
      "Trained batch 1381 batch loss 6.17024183 epoch total loss 6.33694601\n",
      "Trained batch 1382 batch loss 6.05355549 epoch total loss 6.33674097\n",
      "Trained batch 1383 batch loss 6.11504364 epoch total loss 6.33658075\n",
      "Trained batch 1384 batch loss 6.3381319 epoch total loss 6.33658171\n",
      "Trained batch 1385 batch loss 6.14774036 epoch total loss 6.33644533\n",
      "Trained batch 1386 batch loss 6.012012 epoch total loss 6.33621073\n",
      "Trained batch 1387 batch loss 6.39777946 epoch total loss 6.33625507\n",
      "Trained batch 1388 batch loss 6.34566784 epoch total loss 6.33626175\n",
      "Epoch 5 train loss 6.336261749267578\n",
      "Validated batch 1 batch loss 6.23841143\n",
      "Validated batch 2 batch loss 5.78787\n",
      "Validated batch 3 batch loss 6.34678078\n",
      "Validated batch 4 batch loss 6.30360889\n",
      "Validated batch 5 batch loss 6.22151089\n",
      "Validated batch 6 batch loss 6.53458405\n",
      "Validated batch 7 batch loss 6.34521914\n",
      "Validated batch 8 batch loss 6.54310942\n",
      "Validated batch 9 batch loss 6.46844435\n",
      "Validated batch 10 batch loss 6.38131332\n",
      "Validated batch 11 batch loss 6.14053345\n",
      "Validated batch 12 batch loss 6.28852797\n",
      "Validated batch 13 batch loss 6.26375675\n",
      "Validated batch 14 batch loss 6.70516872\n",
      "Validated batch 15 batch loss 6.42425871\n",
      "Validated batch 16 batch loss 6.26768112\n",
      "Validated batch 17 batch loss 6.44985247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 18 batch loss 5.83555412\n",
      "Validated batch 19 batch loss 6.33430338\n",
      "Validated batch 20 batch loss 6.44060659\n",
      "Validated batch 21 batch loss 6.33216715\n",
      "Validated batch 22 batch loss 6.52272081\n",
      "Validated batch 23 batch loss 6.08234787\n",
      "Validated batch 24 batch loss 6.56336927\n",
      "Validated batch 25 batch loss 6.40425158\n",
      "Validated batch 26 batch loss 6.04467773\n",
      "Validated batch 27 batch loss 6.02249098\n",
      "Validated batch 28 batch loss 6.12068558\n",
      "Validated batch 29 batch loss 6.36726141\n",
      "Validated batch 30 batch loss 6.18335485\n",
      "Validated batch 31 batch loss 6.37167454\n",
      "Validated batch 32 batch loss 6.48853302\n",
      "Validated batch 33 batch loss 6.26476479\n",
      "Validated batch 34 batch loss 6.16343307\n",
      "Validated batch 35 batch loss 5.74493837\n",
      "Validated batch 36 batch loss 6.73562813\n",
      "Validated batch 37 batch loss 6.15541172\n",
      "Validated batch 38 batch loss 6.33938456\n",
      "Validated batch 39 batch loss 6.29338932\n",
      "Validated batch 40 batch loss 6.34302521\n",
      "Validated batch 41 batch loss 5.64467335\n",
      "Validated batch 42 batch loss 5.90316296\n",
      "Validated batch 43 batch loss 6.41467428\n",
      "Validated batch 44 batch loss 6.01196\n",
      "Validated batch 45 batch loss 6.23852825\n",
      "Validated batch 46 batch loss 6.27664757\n",
      "Validated batch 47 batch loss 6.9082036\n",
      "Validated batch 48 batch loss 6.44246197\n",
      "Validated batch 49 batch loss 6.45721674\n",
      "Validated batch 50 batch loss 6.27518892\n",
      "Validated batch 51 batch loss 6.36587906\n",
      "Validated batch 52 batch loss 6.50628424\n",
      "Validated batch 53 batch loss 6.38077497\n",
      "Validated batch 54 batch loss 5.9399457\n",
      "Validated batch 55 batch loss 6.26199198\n",
      "Validated batch 56 batch loss 6.19738197\n",
      "Validated batch 57 batch loss 6.35458755\n",
      "Validated batch 58 batch loss 6.13903522\n",
      "Validated batch 59 batch loss 6.14432812\n",
      "Validated batch 60 batch loss 5.995471\n",
      "Validated batch 61 batch loss 6.28372\n",
      "Validated batch 62 batch loss 6.45167351\n",
      "Validated batch 63 batch loss 6.53223801\n",
      "Validated batch 64 batch loss 6.58956861\n",
      "Validated batch 65 batch loss 6.96076727\n",
      "Validated batch 66 batch loss 7.25228691\n",
      "Validated batch 67 batch loss 6.74134636\n",
      "Validated batch 68 batch loss 6.36382961\n",
      "Validated batch 69 batch loss 5.91302204\n",
      "Validated batch 70 batch loss 6.08064556\n",
      "Validated batch 71 batch loss 6.4370513\n",
      "Validated batch 72 batch loss 6.04961634\n",
      "Validated batch 73 batch loss 5.70155954\n",
      "Validated batch 74 batch loss 6.05448151\n",
      "Validated batch 75 batch loss 6.63239527\n",
      "Validated batch 76 batch loss 6.1304121\n",
      "Validated batch 77 batch loss 6.32098484\n",
      "Validated batch 78 batch loss 6.39257622\n",
      "Validated batch 79 batch loss 6.54153728\n",
      "Validated batch 80 batch loss 6.45556974\n",
      "Validated batch 81 batch loss 6.28091431\n",
      "Validated batch 82 batch loss 6.36394691\n",
      "Validated batch 83 batch loss 6.30879545\n",
      "Validated batch 84 batch loss 6.58147669\n",
      "Validated batch 85 batch loss 6.66527414\n",
      "Validated batch 86 batch loss 6.17594481\n",
      "Validated batch 87 batch loss 6.70691347\n",
      "Validated batch 88 batch loss 6.42267513\n",
      "Validated batch 89 batch loss 6.02233124\n",
      "Validated batch 90 batch loss 6.23664427\n",
      "Validated batch 91 batch loss 6.41383886\n",
      "Validated batch 92 batch loss 6.66808844\n",
      "Validated batch 93 batch loss 6.45939445\n",
      "Validated batch 94 batch loss 6.25331688\n",
      "Validated batch 95 batch loss 6.13927507\n",
      "Validated batch 96 batch loss 6.06226969\n",
      "Validated batch 97 batch loss 6.45112514\n",
      "Validated batch 98 batch loss 6.29052114\n",
      "Validated batch 99 batch loss 6.3954978\n",
      "Validated batch 100 batch loss 6.33252907\n",
      "Validated batch 101 batch loss 6.09268379\n",
      "Validated batch 102 batch loss 6.47736025\n",
      "Validated batch 103 batch loss 6.83710623\n",
      "Validated batch 104 batch loss 6.44362736\n",
      "Validated batch 105 batch loss 6.37220573\n",
      "Validated batch 106 batch loss 6.23554707\n",
      "Validated batch 107 batch loss 6.47249269\n",
      "Validated batch 108 batch loss 6.17246246\n",
      "Validated batch 109 batch loss 6.68403149\n",
      "Validated batch 110 batch loss 6.0847578\n",
      "Validated batch 111 batch loss 6.32642746\n",
      "Validated batch 112 batch loss 6.32324171\n",
      "Validated batch 113 batch loss 6.1252737\n",
      "Validated batch 114 batch loss 6.48596096\n",
      "Validated batch 115 batch loss 6.46068096\n",
      "Validated batch 116 batch loss 6.54407024\n",
      "Validated batch 117 batch loss 6.92480707\n",
      "Validated batch 118 batch loss 6.29805517\n",
      "Validated batch 119 batch loss 5.68668652\n",
      "Validated batch 120 batch loss 6.37941408\n",
      "Validated batch 121 batch loss 6.15408325\n",
      "Validated batch 122 batch loss 5.92611\n",
      "Validated batch 123 batch loss 6.03603745\n",
      "Validated batch 124 batch loss 5.84608555\n",
      "Validated batch 125 batch loss 6.60471344\n",
      "Validated batch 126 batch loss 6.06715536\n",
      "Validated batch 127 batch loss 6.09723568\n",
      "Validated batch 128 batch loss 5.87184334\n",
      "Validated batch 129 batch loss 6.52713823\n",
      "Validated batch 130 batch loss 6.01211\n",
      "Validated batch 131 batch loss 6.32150507\n",
      "Validated batch 132 batch loss 6.20218706\n",
      "Validated batch 133 batch loss 6.54789352\n",
      "Validated batch 134 batch loss 6.20330524\n",
      "Validated batch 135 batch loss 6.43781233\n",
      "Validated batch 136 batch loss 6.48561335\n",
      "Validated batch 137 batch loss 5.46560431\n",
      "Validated batch 138 batch loss 6.38668346\n",
      "Validated batch 139 batch loss 6.41870737\n",
      "Validated batch 140 batch loss 6.43831\n",
      "Validated batch 141 batch loss 6.33254957\n",
      "Validated batch 142 batch loss 6.38965702\n",
      "Validated batch 143 batch loss 6.75382757\n",
      "Validated batch 144 batch loss 6.45588064\n",
      "Validated batch 145 batch loss 6.52253723\n",
      "Validated batch 146 batch loss 6.42961168\n",
      "Validated batch 147 batch loss 6.56319761\n",
      "Validated batch 148 batch loss 6.41514397\n",
      "Validated batch 149 batch loss 6.77824688\n",
      "Validated batch 150 batch loss 6.86856747\n",
      "Validated batch 151 batch loss 5.99232578\n",
      "Validated batch 152 batch loss 6.55762529\n",
      "Validated batch 153 batch loss 6.32264853\n",
      "Validated batch 154 batch loss 6.38838\n",
      "Validated batch 155 batch loss 6.55200863\n",
      "Validated batch 156 batch loss 5.90624523\n",
      "Validated batch 157 batch loss 6.02752256\n",
      "Validated batch 158 batch loss 6.5060606\n",
      "Validated batch 159 batch loss 6.27726221\n",
      "Validated batch 160 batch loss 6.82023478\n",
      "Validated batch 161 batch loss 6.23920774\n",
      "Validated batch 162 batch loss 6.22731066\n",
      "Validated batch 163 batch loss 6.42625809\n",
      "Validated batch 164 batch loss 6.41088676\n",
      "Validated batch 165 batch loss 6.59517813\n",
      "Validated batch 166 batch loss 6.35361624\n",
      "Validated batch 167 batch loss 6.60529041\n",
      "Validated batch 168 batch loss 6.14677382\n",
      "Validated batch 169 batch loss 6.72142792\n",
      "Validated batch 170 batch loss 6.23362064\n",
      "Validated batch 171 batch loss 6.31323099\n",
      "Validated batch 172 batch loss 6.43358564\n",
      "Validated batch 173 batch loss 6.14910316\n",
      "Validated batch 174 batch loss 5.3210125\n",
      "Validated batch 175 batch loss 6.55127382\n",
      "Validated batch 176 batch loss 6.50499916\n",
      "Validated batch 177 batch loss 6.4126215\n",
      "Validated batch 178 batch loss 6.29367828\n",
      "Validated batch 179 batch loss 6.4704442\n",
      "Validated batch 180 batch loss 6.55418921\n",
      "Validated batch 181 batch loss 6.55168104\n",
      "Validated batch 182 batch loss 6.45648384\n",
      "Validated batch 183 batch loss 6.48408175\n",
      "Validated batch 184 batch loss 5.74999809\n",
      "Validated batch 185 batch loss 3.12282181\n",
      "Epoch 5 val loss 6.31083345413208\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "model = simplebaseline_model()\n",
    "\n",
    "best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae072130",
   "metadata": {},
   "source": [
    "# 훈련그래프 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b48c8df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "     |████████████████████████████████| 250 kB 6.2 MB/s            \n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3984d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 파일 경로\n",
    "file_path = \"SimpleBaseLine_training_data.xlsx\"\n",
    "\n",
    "# 각 시트를 데이터 프레임으로 불러오기\n",
    "df_train = pd.read_excel(file_path, sheet_name=\"train\")\n",
    "df_validation = pd.read_excel(file_path, sheet_name=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c58e8234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Batch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Total_Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.724459</td>\n",
       "      <td>10.724459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11.341566</td>\n",
       "      <td>11.033012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9.460858</td>\n",
       "      <td>10.508961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8.644049</td>\n",
       "      <td>10.042732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7.758644</td>\n",
       "      <td>9.585915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch  Batch       Loss  Total_Loss\n",
       "0      1      1  10.724459   10.724459\n",
       "1      1      2  11.341566   11.033012\n",
       "2      1      3   9.460858   10.508961\n",
       "3      1      4   8.644049   10.042732\n",
       "4      1      5   7.758644    9.585915"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "079988d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.41911697, 6.36974907, 6.35542774, 6.34718752, 6.33626175]\n"
     ]
    }
   ],
   "source": [
    "train_last_total_loss_per_epoch = df_train.groupby(\"Epoch\")[\"Total_Loss\"].last().tolist()\n",
    "print(train_last_total_loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecc6fb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.356116292378379, 6.301889253243243, 6.31969587718919, 6.350685220972973, 6.310834558594594]\n"
     ]
    }
   ],
   "source": [
    "val_last_total_loss_per_epoch = df_validation.groupby(\"Epoch\")[\"Loss\"].mean().tolist()\n",
    "print(val_last_total_loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc7bc4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 에포크 리스트 (x축)\n",
    "epochs = list(range(1, len(train_last_total_loss_per_epoch) + 1))\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_last_total_loss_per_epoch, marker='o', linestyle='-', label='Train Loss', color='blue')\n",
    "plt.plot(epochs, val_last_total_loss_per_epoch, marker='s', linestyle='--', label='Validation Loss', color='red')\n",
    "\n",
    "# 그래프 제목 및 라벨\n",
    "plt.title(\"Train vs Validation Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xticks(epochs)  # X축 눈금 에포크에 맞추기\n",
    "plt.legend()  # 범례 추가\n",
    "plt.grid(True)  # 격자 표시\n",
    "\n",
    "# 그래프 출력\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95373fcd",
   "metadata": {},
   "source": [
    "# 추론 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18383161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 불러오기\n",
    "model = simplebaseline_model()\n",
    "model.load_weights('simplebaseline_model-epoch-2-loss-6.3019.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8096571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용했던 keypoint 들을 사용해야 하기 때문에 필요한 변수를 지정\n",
    "R_ANKLE = 0\n",
    "R_KNEE = 1\n",
    "R_HIP = 2\n",
    "L_HIP = 3\n",
    "L_KNEE = 4\n",
    "L_ANKLE = 5\n",
    "PELVIS = 6\n",
    "THORAX = 7\n",
    "UPPER_NECK = 8\n",
    "HEAD_TOP = 9\n",
    "R_WRIST = 10\n",
    "R_ELBOW = 11\n",
    "R_SHOULDER = 12\n",
    "L_SHOULDER = 13\n",
    "L_ELBOW = 14\n",
    "L_WRIST = 15\n",
    "\n",
    "MPII_BONES = [\n",
    "    [R_ANKLE, R_KNEE],\n",
    "    [R_KNEE, R_HIP],\n",
    "    [R_HIP, PELVIS],\n",
    "    [L_HIP, PELVIS],\n",
    "    [L_HIP, L_KNEE],\n",
    "    [L_KNEE, L_ANKLE],\n",
    "    [PELVIS, THORAX],\n",
    "    [THORAX, UPPER_NECK],\n",
    "    [UPPER_NECK, HEAD_TOP],\n",
    "    [R_WRIST, R_ELBOW],\n",
    "    [R_ELBOW, R_SHOULDER],\n",
    "    [THORAX, R_SHOULDER],\n",
    "    [THORAX, L_SHOULDER],\n",
    "    [L_SHOULDER, L_ELBOW],\n",
    "    [L_ELBOW, L_WRIST]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46a44763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 학습할 때 라벨이 되는 좌표를 heatmap으로 바꿨기 때문에 모델이 추론해 내놓은 결과도 heatmap이다.\n",
    "# 그래서 이 heatmap으로부터 좌표를 추출해야 한다.\n",
    "# heatmap중에 최대값을 갖는 지점을 찾아내면 된다.\n",
    "# 아래는 heatmap에서 최대값을 찾는 함수\n",
    "def find_max_coordinates(heatmaps):\n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (-1, 16))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    y = tf.cast(indices / 64, dtype=tf.int64)\n",
    "    x = indices - 64 * y\n",
    "    return tf.stack([x, y], axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3cae6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 함수만으로는 256x256 이미지에 64x64 heatmap max 값을 표현할 때 quantization 오차가 발생한다.\n",
    "# 때문에 실제 계산에서는 3x3 필터를 이용해서 근사치를 구한다.\n",
    "def extract_keypoints_from_heatmap(heatmaps):\n",
    "    max_keypoints = find_max_coordinates(heatmaps)\n",
    "\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        \n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        patch[1][1] = 0\n",
    "        \n",
    "        index = np.argmax(patch)\n",
    "        \n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        \n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "        \n",
    "    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n",
    "    normalized_keypoints = adjusted_keypoints / 64\n",
    "    return normalized_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7db774c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델과 이미지 경로를 입력하면 이미지와 keypoint를 출력하는 함수\n",
    "def predict(model, image_path):\n",
    "    encoded = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(encoded)\n",
    "    inputs = tf.image.resize(image, (256, 256))\n",
    "    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    outputs = model(inputs, training=False)\n",
    "    if type(outputs) != list:\n",
    "        outputs = [outputs]\n",
    "    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n",
    "    kp = extract_keypoints_from_heatmap(heatmap)\n",
    "    return image, kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01582940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화, 그림그리기\n",
    "# keypoint들과 뼈대 그리기. keypoint들은 관절 역할을 하고 keypoint들을 연결시킨 것이 뼈대.\n",
    "def draw_keypoints_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n",
    "    plt.show()\n",
    "\n",
    "def draw_skeleton_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        joints.append((joint_x, joint_y))\n",
    "    \n",
    "    for bone in MPII_BONES:\n",
    "        joint_1 = joints[bone[0]]\n",
    "        joint_2 = joints[bone[1]]\n",
    "        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9fe11f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 스트 이미지를 이용해 모델의 성능 확인\n",
    "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')\n",
    "\n",
    "image, keypoints = predict(model, test_image)\n",
    "draw_keypoints_on_image(image, keypoints)\n",
    "draw_skeleton_on_image(image, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff2a2110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엥?....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b00540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련 최적화가 안된듯 하다. Learning Rate 및 에포크를 조절하여 모델 최적화를 추가적으로 진행해야할 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df39156b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
