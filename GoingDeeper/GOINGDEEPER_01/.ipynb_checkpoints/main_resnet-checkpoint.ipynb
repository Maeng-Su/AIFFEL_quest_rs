{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e086b3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "# Tensorflow가 활용할 GPU가 장착되어 있는지 확인해 봅니다.\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for building ResNet Block\n",
    "\n",
    "def build_resnet_block34(input_layer,\n",
    "                       num_cnn=3,\n",
    "                       channel_ls=[64, 64],\n",
    "                       kernel_size_ls=[3, 3],\n",
    "                       block_num=2,\n",
    "                      ):\n",
    "    # 입력 레이어\n",
    "    inputs = input_layer\n",
    "    x = inputs\n",
    "\n",
    "    # CNN 레이어\n",
    "    for cnn_num in range(num_cnn):\n",
    "        for idx, (channel, kernel_size) in enumerate(zip(channel_ls, kernel_size_ls)):\n",
    "            x = keras.layers.Conv2D(\n",
    "                filters=channel,\n",
    "                kernel_size=(kernel_size, kernel_size),\n",
    "                kernel_initializer='he_normal',\n",
    "                padding='same',\n",
    "                name=f'block{block_num}_conv{cnn_num+1}_{idx+1}'\n",
    "            )(x)\n",
    "            x = keras.layers.BatchNormalization()(x)\n",
    "            \n",
    "            if idx+1 >= len(channel_ls):\n",
    "                break\n",
    "            \n",
    "            x = keras.layers.Activation('relu')(x)\n",
    "                \n",
    "        x = keras.layers.Add()([x, inputs])\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e394bc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for building ResNet Block\n",
    "\n",
    "def build_resnet_block50(input_layer,\n",
    "                       num_cnn=3,\n",
    "                       channel_ls=[64, 64, 256],\n",
    "                       kernel_size_ls=[3, 3],\n",
    "                       block_num=2,\n",
    "                      ):\n",
    "    # 입력 레이어\n",
    "    inputs = input_layer\n",
    "    inputs_conv = keras.layers.Conv2D(\n",
    "                                filters=channel_ls[-1],  # 2배로 증가\n",
    "                                kernel_size=(1, 1),       # 1×1 컨볼루션\n",
    "                                strides=1,\n",
    "                                padding='same',           # 패딩 유지\n",
    "                                kernel_initializer='he_normal'  # He 초기화\n",
    "                            )(inputs)\n",
    "    x = inputs\n",
    "\n",
    "    # CNN 레이어\n",
    "    for cnn_num in range(num_cnn):\n",
    "        for idx, (channel, kernel_size) in enumerate(zip(channel_ls, kernel_size_ls)):\n",
    "            x = keras.layers.Conv2D(\n",
    "                filters=channel,\n",
    "                kernel_size=(kernel_size, kernel_size),\n",
    "                kernel_initializer='he_normal',\n",
    "                padding='same',\n",
    "                name=f'block{block_num}_conv{cnn_num+1}_{idx+1}'\n",
    "            )(x)\n",
    "            x = keras.layers.BatchNormalization()(x)\n",
    "            \n",
    "            if idx+1 >= len(channel_ls):\n",
    "                break\n",
    "            \n",
    "            x = keras.layers.Activation('relu')(x)\n",
    "        \n",
    "        x = keras.layers.Add()([x, inputs_conv])\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_input_layer = keras.layers.Input(shape=(56,56,64))\n",
    "resnet_block_output = build_resnet_block34(input_layer=resnet_input_layer,\n",
    "                       num_cnn=3,\n",
    "                       channel_ls=[64, 64],\n",
    "                       kernel_size_ls=[3, 3],\n",
    "                       block_num=2,\n",
    "                      )\n",
    "\n",
    "model = keras.Model(inputs=resnet_input_layer, outputs=resnet_block_output)  \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c76dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2b6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet모델 자체를 생성하는 함수입니다.\n",
    "def build_resnet34(input_shape=(224,224,3),\n",
    "              num_cnn_list=[3,4,6,3],\n",
    "              channel_list=[64,128,256,512],\n",
    "              num_classes=10):\n",
    "    \n",
    "    assert len(num_cnn_list) == len(channel_list) #모델을 만들기 전에 config list들이 같은 길이인지 확인합니다.\n",
    "    \n",
    "    input_layer = keras.layers.Input(shape=input_shape)  # input layer를 만들어둡니다.\n",
    "    \n",
    "    output = keras.layers.Conv2D(\n",
    "                filters=64,\n",
    "                kernel_size=(7, 7),\n",
    "                strides=2,\n",
    "                padding='same',\n",
    "                name=f'block1_conv'\n",
    "            )(input_layer)\n",
    "    \n",
    "    output = keras.layers.BatchNormalization()(output)\n",
    "    output = keras.layers.Activation('relu')(output)\n",
    "    \n",
    "    output = keras.layers.MaxPooling2D(\n",
    "                pool_size=(3, 3),\n",
    "                strides=2,\n",
    "                padding='same',\n",
    "                name=f'block1_maxpool'\n",
    "            )(output)\n",
    "    \n",
    "    # config list들의 길이만큼 반복해서 블록을 생성합니다.\n",
    "    block_num=2\n",
    "    for i, (num_cnn, channel) in enumerate(zip(num_cnn_list, channel_list)):\n",
    "        output = build_resnet_block34(input_layer=output,\n",
    "                       num_cnn=num_cnn,\n",
    "                       channel_ls=[channel, channel],\n",
    "                       kernel_size_ls=[3, 3],\n",
    "                       block_num=block_num,\n",
    "                      )\n",
    "        \n",
    "        if i+1 >= len(num_cnn_list):\n",
    "            break\n",
    "        \n",
    "        output = keras.layers.Conv2D(\n",
    "                                filters=channel*2,  # 2배로 증가\n",
    "                                kernel_size=(1, 1),       # 1×1 컨볼루션\n",
    "                                strides=1,\n",
    "                                padding='same',           # 패딩 유지\n",
    "                                kernel_initializer='he_normal'  # He 초기화\n",
    "                            )(output)\n",
    "        block_num+=1\n",
    "        \n",
    "    output = keras.layers.AveragePooling2D(name='AveragePooling2D')(output)\n",
    "    output = keras.layers.Flatten()(output)\n",
    "    output = keras.layers.Dense(num_classes, activation='softmax', name='predictions')(output)\n",
    "    \n",
    "    model = keras.Model(\n",
    "        inputs=input_layer, \n",
    "        outputs=output\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32547f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet모델 자체를 생성하는 함수입니다.\n",
    "def build_resnet50(input_shape=(224,224,3),\n",
    "              num_cnn_list=[3,4,6,3],\n",
    "              channel_list=[64,128,256,512],\n",
    "              num_classes=10):\n",
    "    \n",
    "    assert len(num_cnn_list) == len(channel_list) #모델을 만들기 전에 config list들이 같은 길이인지 확인합니다.\n",
    "    \n",
    "    input_layer = keras.layers.Input(shape=input_shape)  # input layer를 만들어둡니다.\n",
    "    \n",
    "    output = keras.layers.Conv2D(\n",
    "                filters=64,\n",
    "                kernel_size=(7, 7),\n",
    "                strides=2,\n",
    "                padding='same',\n",
    "                name=f'block1_conv'\n",
    "            )(input_layer)\n",
    "    \n",
    "    output = keras.layers.BatchNormalization()(output)\n",
    "    output = keras.layers.Activation('relu')(output)\n",
    "    \n",
    "    output = keras.layers.MaxPooling2D(\n",
    "                pool_size=(3, 3),\n",
    "                strides=2,\n",
    "                padding='same',\n",
    "                name=f'block1_maxpool'\n",
    "            )(output)\n",
    "    \n",
    "    # config list들의 길이만큼 반복해서 블록을 생성합니다.\n",
    "    block_num=2\n",
    "    for i, (num_cnn, channel) in enumerate(zip(num_cnn_list, channel_list)):\n",
    "        output = build_resnet_block50(input_layer=output,\n",
    "                       num_cnn=num_cnn,\n",
    "                       channel_ls=[channel, channel, channel*4],\n",
    "                       kernel_size_ls=[1, 3, 1],\n",
    "                       block_num=block_num,\n",
    "                      )\n",
    "        \n",
    "        if i+1 >= len(num_cnn_list):\n",
    "            break\n",
    "        \n",
    "        output = keras.layers.Conv2D(\n",
    "                                filters=channel*2,  # 2배로 증가\n",
    "                                kernel_size=(1, 1),       # 1×1 컨볼루션\n",
    "                                strides=1,\n",
    "                                padding='same',           # 패딩 유지\n",
    "                                kernel_initializer='he_normal'  # He 초기화\n",
    "                            )(output)\n",
    "        block_num+=1\n",
    "        \n",
    "    output = keras.layers.AveragePooling2D(name='AveragePooling2D')(output)\n",
    "    output = keras.layers.Flatten()(output)\n",
    "    output = keras.layers.Dense(num_classes, activation='softmax', name='predictions')(output)\n",
    "    \n",
    "    model = keras.Model(\n",
    "        inputs=input_layer, \n",
    "        outputs=output\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet(input_shape=(224,224,3),\n",
    "              num_cnn_list=[3,4,6,3],\n",
    "              channel_list=[64,128,256,512],\n",
    "              num_classes=10,\n",
    "                is_50=False):\n",
    "    if not is_50:\n",
    "        return build_resnet34(input_shape=input_shape,\n",
    "              num_classes=num_classes)\n",
    "    \n",
    "    else:\n",
    "        return build_resnet50(input_shape=input_shape,\n",
    "              num_classes=num_classes)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db2e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 블록의 설계에 따라 매개변수로 리스트를 전달해 줍니다.\n",
    "resnet_34 = build_resnet(input_shape=(32,32,3))\n",
    "\n",
    "resnet_34.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b90e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "del resnet_34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee09138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 블록의 설계에 따라 매개변수로 리스트를 전달해 줍니다.\n",
    "resnet_50 = build_resnet(input_shape=(32,32,3), is_50=True)\n",
    "\n",
    "resnet_50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72094519",
   "metadata": {},
   "outputs": [],
   "source": [
    "del resnet_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde3f000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for building ResNet Block\n",
    "\n",
    "def build_plainnet_block34(input_layer,\n",
    "                       num_cnn=3,\n",
    "                       channel_ls=[64, 64],\n",
    "                       kernel_size_ls=[3, 3],\n",
    "                       block_num=2,\n",
    "                      ):\n",
    "    # 입력 레이어\n",
    "    inputs = input_layer\n",
    "    x = inputs\n",
    "\n",
    "    # CNN 레이어\n",
    "    for cnn_num in range(num_cnn):\n",
    "        for idx, (channel, kernel_size) in enumerate(zip(channel_ls, kernel_size_ls)):\n",
    "            x = keras.layers.Conv2D(\n",
    "                filters=channel,\n",
    "                kernel_size=(kernel_size, kernel_size),\n",
    "                kernel_initializer='he_normal',\n",
    "                padding='same',\n",
    "                name=f'block{block_num}_conv{cnn_num+1}_{idx+1}'\n",
    "            )(x)\n",
    "            x = keras.layers.BatchNormalization()(x)\n",
    "            \n",
    "            if idx+1 >= len(channel_ls):\n",
    "                break\n",
    "            \n",
    "            x = keras.layers.Activation('relu')(x)\n",
    "                \n",
    "#         x = keras.layers.Add()([x, inputs])\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8766b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for building ResNet Block\n",
    "\n",
    "def build_plainnet_block50(input_layer,\n",
    "                       num_cnn=3,\n",
    "                       channel_ls=[64, 64, 256],\n",
    "                       kernel_size_ls=[3, 3],\n",
    "                       block_num=2,\n",
    "                      ):\n",
    "    # 입력 레이어\n",
    "    inputs = input_layer\n",
    "#     inputs_conv = keras.layers.Conv2D(\n",
    "#                                 filters=channel_ls[-1],  # 2배로 증가\n",
    "#                                 kernel_size=(1, 1),       # 1×1 컨볼루션\n",
    "#                                 strides=1,\n",
    "#                                 padding='same',           # 패딩 유지\n",
    "#                                 kernel_initializer='he_normal'  # He 초기화\n",
    "#                             )(inputs)\n",
    "    x = inputs\n",
    "\n",
    "    # CNN 레이어\n",
    "    for cnn_num in range(num_cnn):\n",
    "        for idx, (channel, kernel_size) in enumerate(zip(channel_ls, kernel_size_ls)):\n",
    "            x = keras.layers.Conv2D(\n",
    "                filters=channel,\n",
    "                kernel_size=(kernel_size, kernel_size),\n",
    "                kernel_initializer='he_normal',\n",
    "                padding='same',\n",
    "                name=f'block{block_num}_conv{cnn_num+1}_{idx+1}'\n",
    "            )(x)\n",
    "            x = keras.layers.BatchNormalization()(x)\n",
    "            \n",
    "            if idx+1 >= len(channel_ls):\n",
    "                break\n",
    "            \n",
    "            x = keras.layers.Activation('relu')(x)\n",
    "        \n",
    "#         x = keras.layers.Add()([x, inputs_conv])\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet모델 자체를 생성하는 함수입니다.\n",
    "def build_plainnet34(input_shape=(224,224,3),\n",
    "              num_cnn_list=[3,4,6,3],\n",
    "              channel_list=[64,128,256,512],\n",
    "              num_classes=10):\n",
    "    \n",
    "    assert len(num_cnn_list) == len(channel_list) #모델을 만들기 전에 config list들이 같은 길이인지 확인합니다.\n",
    "    \n",
    "    input_layer = keras.layers.Input(shape=input_shape)  # input layer를 만들어둡니다.\n",
    "    \n",
    "    output = keras.layers.Conv2D(\n",
    "                filters=64,\n",
    "                kernel_size=(7, 7),\n",
    "                strides=2,\n",
    "                padding='same',\n",
    "                name=f'block1_conv'\n",
    "            )(input_layer)\n",
    "    \n",
    "    output = keras.layers.BatchNormalization()(output)\n",
    "    output = keras.layers.Activation('relu')(output)\n",
    "    \n",
    "    output = keras.layers.MaxPooling2D(\n",
    "                pool_size=(3, 3),\n",
    "                strides=2,\n",
    "                padding='same',\n",
    "                name=f'block1_maxpool'\n",
    "            )(output)\n",
    "    \n",
    "    # config list들의 길이만큼 반복해서 블록을 생성합니다.\n",
    "    block_num=2\n",
    "    for i, (num_cnn, channel) in enumerate(zip(num_cnn_list, channel_list)):\n",
    "        output = build_plainnet_block34(input_layer=output,\n",
    "                       num_cnn=num_cnn,\n",
    "                       channel_ls=[channel, channel],\n",
    "                       kernel_size_ls=[3, 3],\n",
    "                       block_num=block_num,\n",
    "                      )\n",
    "        \n",
    "        if i+1 >= len(num_cnn_list):\n",
    "            break\n",
    "        \n",
    "        output = keras.layers.Conv2D(\n",
    "                                filters=channel*2,  # 2배로 증가\n",
    "                                kernel_size=(1, 1),       # 1×1 컨볼루션\n",
    "                                strides=1,\n",
    "                                padding='same',           # 패딩 유지\n",
    "                                kernel_initializer='he_normal'  # He 초기화\n",
    "                            )(output)\n",
    "        block_num+=1\n",
    "        \n",
    "    output = keras.layers.AveragePooling2D(name='AveragePooling2D')(output)\n",
    "    output = keras.layers.Flatten()(output)\n",
    "    output = keras.layers.Dense(num_classes, activation='softmax', name='predictions')(output)\n",
    "    \n",
    "    model = keras.Model(\n",
    "        inputs=input_layer, \n",
    "        outputs=output\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a7e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet모델 자체를 생성하는 함수입니다.\n",
    "def build_plainnet50(input_shape=(224,224,3),\n",
    "              num_cnn_list=[3,4,6,3],\n",
    "              channel_list=[64,128,256,512],\n",
    "              num_classes=10):\n",
    "    \n",
    "    assert len(num_cnn_list) == len(channel_list) #모델을 만들기 전에 config list들이 같은 길이인지 확인합니다.\n",
    "    \n",
    "    input_layer = keras.layers.Input(shape=input_shape)  # input layer를 만들어둡니다.\n",
    "    \n",
    "    output = keras.layers.Conv2D(\n",
    "                filters=64,\n",
    "                kernel_size=(7, 7),\n",
    "                strides=2,\n",
    "                padding='same',\n",
    "                name=f'block1_conv'\n",
    "            )(input_layer)\n",
    "    \n",
    "    output = keras.layers.BatchNormalization()(output)\n",
    "    output = keras.layers.Activation('relu')(output)\n",
    "    \n",
    "    output = keras.layers.MaxPooling2D(\n",
    "                pool_size=(3, 3),\n",
    "                strides=2,\n",
    "                padding='same',\n",
    "                name=f'block1_maxpool'\n",
    "            )(output)\n",
    "    \n",
    "    # config list들의 길이만큼 반복해서 블록을 생성합니다.\n",
    "    block_num=2\n",
    "    for i, (num_cnn, channel) in enumerate(zip(num_cnn_list, channel_list)):\n",
    "        output = build_plainnet_block50(input_layer=output,\n",
    "                       num_cnn=num_cnn,\n",
    "                       channel_ls=[channel, channel, channel*4],\n",
    "                       kernel_size_ls=[1, 3, 1],\n",
    "                       block_num=block_num,\n",
    "                      )\n",
    "        \n",
    "        if i+1 >= len(num_cnn_list):\n",
    "            break\n",
    "        \n",
    "        output = keras.layers.Conv2D(\n",
    "                                filters=channel*2,  # 2배로 증가\n",
    "                                kernel_size=(1, 1),       # 1×1 컨볼루션\n",
    "                                strides=1,\n",
    "                                padding='same',           # 패딩 유지\n",
    "                                kernel_initializer='he_normal'  # He 초기화\n",
    "                            )(output)\n",
    "        block_num+=1\n",
    "        \n",
    "    output = keras.layers.AveragePooling2D(name='AveragePooling2D')(output)\n",
    "    output = keras.layers.Flatten()(output)\n",
    "    output = keras.layers.Dense(num_classes, activation='softmax', name='predictions')(output)\n",
    "    \n",
    "    model = keras.Model(\n",
    "        inputs=input_layer, \n",
    "        outputs=output\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa891a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_plainnet(input_shape=(224,224,3),\n",
    "              num_cnn_list=[3,4,6,3],\n",
    "              channel_list=[64,128,256,512],\n",
    "              num_classes=10,\n",
    "                is_50=False):\n",
    "\n",
    "    if not is_50:\n",
    "        return build_plainnet34(input_shape=input_shape,\n",
    "              num_classes=num_classes)\n",
    "    \n",
    "    else:\n",
    "        return build_plainnet50(input_shape=input_shape,\n",
    "              num_classes=num_classes)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05383d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 블록의 설계에 따라 매개변수로 리스트를 전달해 줍니다.\n",
    "plaainnet_34 = build_plainnet(input_shape=(32,32,3))\n",
    "\n",
    "plaainnet_34.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del plaainnet_34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9805d7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 블록의 설계에 따라 매개변수로 리스트를 전달해 줍니다.\n",
    "plaainnet_50 = build_plainnet(input_shape=(32,32,3), is_50=True)\n",
    "\n",
    "plaainnet_50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad05a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del plaainnet_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84518f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCH = 40\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "def preprocess_image(image, label):\n",
    "    \"\"\"이미지 크기를 일정하게 변환하는 전처리 함수\"\"\"\n",
    "    image = tf.image.resize(image, IMG_SIZE)  # ✅ 이미지 크기 조정\n",
    "    image = image / 255.0  # ✅ 0~1 사이로 정규화\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(tfds.image_classification.cats_vs_dogs, '_URL',\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\")\n",
    "\n",
    "ds_train, ds_info = tfds.load(\n",
    "    'cats_vs_dogs',\n",
    "    split=['train'],\n",
    "    as_supervised=True,\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")\n",
    "ds_train = ds_train[0]\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ `map()`을 사용하여 데이터 변환 적용\n",
    "ds_train = ds_train.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1014c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = ds_info.splits['train'].num_examples\n",
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe07730",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(num_samples * 0.8)\n",
    "test_size = num_samples - train_size  # 나머지 20%\n",
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d334a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_split = ds_train.take(train_size)  # 처음 80% 가져오기\n",
    "ds_test_split = ds_train.skip(train_size)   # 이후 20% 가져오기\n",
    "\n",
    "print(\"Train 데이터 개수:\", len(list(ds_train_split.as_numpy_iterator())))\n",
    "print(\"Test 데이터 개수:\", len(list(ds_test_split.as_numpy_iterator())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f678f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 데이터셋\n",
    "ds_train = ds_train_split.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Test 데이터셋\n",
    "ds_test = ds_test_split.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 데이터셋 확인\n",
    "for image, label in ds_train.take(1):\n",
    "    print(\"Train 이미지 크기:\", image.shape)\n",
    "    print(\"Train 레이블:\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9565c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b608972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch in [30, 60, 90]:  # 특정 epoch마다 0.1배 감소\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e525bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트가 존재하면 모델 가중치 불러오기\n",
    "def load_checkpoint(model):\n",
    "    try:\n",
    "        model.load_weights(checkpoint_path)\n",
    "        print(\"Checkpoint Loaded: Resuming Training\")\n",
    "    except Exception as e:\n",
    "        print(\"No checkpoint found, starting from scratch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66302c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def save_history(history):\n",
    "    with open(history_path, \"wb\") as f:\n",
    "        pickle.dump(history.history, f)\n",
    "\n",
    "def load_history():\n",
    "    if os.path.exists(history_path):\n",
    "        with open(history_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return {\"loss\": [], \"val_loss\": []}  # 초기값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33fa01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "222/582 [==========>...................] - ETA: 10:12 - loss: 0.9170 - accuracy: 0.5162"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 99 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258/582 [============>.................] - ETA: 9:10 - loss: 0.9193 - accuracy: 0.5170"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: unknown JFIF revision number 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/582 [=============>................] - ETA: 8:45 - loss: 0.9176 - accuracy: 0.5183"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 396 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/582 [===============>..............] - ETA: 7:35 - loss: 0.9132 - accuracy: 0.5206"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 162 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362/582 [=================>............] - ETA: 6:14 - loss: 0.9125 - accuracy: 0.5186"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 252 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/582 [=================>............] - ETA: 6:05 - loss: 0.9120 - accuracy: 0.5181"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 65 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380/582 [==================>...........] - ETA: 5:43 - loss: 0.9120 - accuracy: 0.5179"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 1403 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538/582 [==========================>...] - ETA: 1:14 - loss: 0.9006 - accuracy: 0.5207"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 214 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582/582 [==============================] - ETA: 0s - loss: 0.8964 - accuracy: 0.5210"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 99 extraneous bytes before marker 0xd9\n",
      "Warning: unknown JFIF revision number 0.00\n",
      "Corrupt JPEG data: 396 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 162 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 252 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 65 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 1403 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 214 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 2226 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 128 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 239 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 228 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582/582 [==============================] - 1100s 2s/step - loss: 0.8964 - accuracy: 0.5210 - val_loss: 0.8500 - val_accuracy: 0.5147\n",
      "Epoch 2/40\n",
      "224/582 [==========>...................] - ETA: 10:07 - loss: 0.8424 - accuracy: 0.5257"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 99 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260/582 [============>.................] - ETA: 9:06 - loss: 0.8410 - accuracy: 0.5251"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: unknown JFIF revision number 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/582 [=============>................] - ETA: 8:44 - loss: 0.8402 - accuracy: 0.5228"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 396 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/582 [===============>..............] - ETA: 7:31 - loss: 0.8413 - accuracy: 0.5216"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 162 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/582 [=================>............] - ETA: 6:10 - loss: 0.8385 - accuracy: 0.5202"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 252 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366/582 [=================>............] - ETA: 6:06 - loss: 0.8388 - accuracy: 0.5197"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 65 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/582 [==================>...........] - ETA: 5:44 - loss: 0.8386 - accuracy: 0.5186"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 1403 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540/582 [==========================>...] - ETA: 1:11 - loss: 0.8246 - accuracy: 0.5210"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 214 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582/582 [==============================] - ETA: 0s - loss: 0.8224 - accuracy: 0.5187"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 99 extraneous bytes before marker 0xd9\n",
      "Warning: unknown JFIF revision number 0.00\n",
      "Corrupt JPEG data: 396 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 162 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 252 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 65 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 1403 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 214 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 2226 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 128 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 239 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 228 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582/582 [==============================] - 1089s 2s/step - loss: 0.8224 - accuracy: 0.5187 - val_loss: 0.9900 - val_accuracy: 0.5317\n",
      "Epoch 3/40\n",
      "224/582 [==========>...................] - ETA: 10:06 - loss: 0.7724 - accuracy: 0.5135"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 99 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261/582 [============>.................] - ETA: 9:04 - loss: 0.7693 - accuracy: 0.5117"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: unknown JFIF revision number 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/582 [=============>................] - ETA: 8:43 - loss: 0.7680 - accuracy: 0.5118"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 396 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/582 [===============>..............] - ETA: 7:29 - loss: 0.7653 - accuracy: 0.5149"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 162 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/582 [=================>............] - ETA: 6:08 - loss: 0.7614 - accuracy: 0.5169"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 252 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/582 [=================>............] - ETA: 6:04 - loss: 0.7614 - accuracy: 0.5169"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 65 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380/582 [==================>...........] - ETA: 5:42 - loss: 0.7610 - accuracy: 0.5176"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 1403 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454/582 [======================>.......] - ETA: 3:36 - loss: 0.7551 - accuracy: 0.5242"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"plaainnet_34_checkpoint.h5\"\n",
    "history_path = \"plaainnet_34_history.pkl\"\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, monitor=\"val_loss\", save_best_only=True, save_weights_only=True, verbose=1\n",
    ")\n",
    "\n",
    "plaainnet_34 = build_plainnet(input_shape=(224,224,3),\n",
    "              num_cnn_list=[3,4,6,3],\n",
    "              channel_list=[64,128,256,512],\n",
    "              num_classes=2,\n",
    "                is_50=False)\n",
    "\n",
    "plaainnet_34.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history_plaainnet_34 = plaainnet_34.fit(\n",
    "    ds_train,\n",
    "#     steps_per_epoch=int(ds_info.splits['train'].num_examples/BATCH_SIZE),\n",
    "#     validation_steps=int(ds_info.splits['test'].num_examples/BATCH_SIZE),\n",
    "    epochs=EPOCH,\n",
    "    validation_data=ds_test,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    "    callbacks=[lr_callback, checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e97f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_34 = build_resnet(input_shape=(224,224,3),\n",
    "              num_cnn_list=[3,4,6,3],\n",
    "              channel_list=[64,128,256,512],\n",
    "              num_classes=2,\n",
    "                is_50=False)\n",
    "\n",
    "resnet_34.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.SGD(lr=0.01, clipnorm=1.),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history_resnet_34 = resnet_34.fit(\n",
    "    ds_train,\n",
    "#     steps_per_epoch=int(ds_info.splits['train'].num_examples/BATCH_SIZE),\n",
    "#     validation_steps=int(ds_info.splits['test'].num_examples/BATCH_SIZE),\n",
    "    epochs=EPOCH,\n",
    "    validation_data=ds_test,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4236208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history_plaainnet_34.history['loss'], 'r')\n",
    "plt.plot(history_resnet_34.history['loss'], 'b')\n",
    "plt.title('Model training loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['plaainnet_34', 'resnet_34'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007adf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_plaainnet_34.history['val_accuracy'], 'r')\n",
    "plt.plot(history_resnet_34.history['val_accuracy'], 'b')\n",
    "plt.title('Model validation accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['plaainnet_34', 'resnet_34'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38004f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "plaainnet_50 = build_plainnet(input_shape=(224,224,3),\n",
    "              num_cnn_list=[3,4,6,3],\n",
    "              channel_list=[64,128,256,512],\n",
    "              num_classes=2,\n",
    "                is_50=True)\n",
    "\n",
    "plaainnet_50.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.SGD(lr=0.01, clipnorm=1.),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history_plaainnet_50 = plaainnet_50.fit(\n",
    "    ds_train,\n",
    "#     steps_per_epoch=int(ds_info.splits['train'].num_examples/BATCH_SIZE),\n",
    "#     validation_steps=int(ds_info.splits['test'].num_examples/BATCH_SIZE),\n",
    "    epochs=EPOCH,\n",
    "    validation_data=ds_test,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdba608",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_50 = build_resnet(input_shape=(224,224,3),\n",
    "              num_cnn_list=[3,4,6,3],\n",
    "              channel_list=[64,128,256,512],\n",
    "              num_classes=2,\n",
    "                is_50=True)\n",
    "\n",
    "resnet_50.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.SGD(lr=0.01, clipnorm=1.),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history_resnet_50 = resnet_50.fit(\n",
    "    ds_train,\n",
    "#     steps_per_epoch=int(ds_info.splits['train'].num_examples/BATCH_SIZE),\n",
    "#     validation_steps=int(ds_info.splits['test'].num_examples/BATCH_SIZE),\n",
    "    epochs=EPOCH,\n",
    "    validation_data=ds_test,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb747a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_plaainnet_50.history['loss'], 'r')\n",
    "plt.plot(history_resnet_50.history['loss'], 'b')\n",
    "plt.title('Model training loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['plaainnet_50', 'resnet_50'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f630c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_plaainnet_50.history['val_accuracy'], 'r')\n",
    "plt.plot(history_resnet_50.history['val_accuracy'], 'b')\n",
    "plt.title('Model validation accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['plaainnet_50', 'resnet_50'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3caf8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
